<!doctype html><html lang=en dir=auto><head><meta name=description content="Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."><meta name=keywords content="AI,Machine Learning,Product Engineering,Investment Analysis,Multi-Agent Systems,Content Understanding,Emerging Technologies,Startup Analysis,AI Multipliers"><meta name=author content="Terry Chen"><meta name=robots content="index, follow"><meta name=language content="en-us"><meta name=revisit-after content="7 days"><meta name=distribution content="global"><meta name=rating content="general"><link rel=canonical href=https://chenterry.com/product/><meta property="og:title" content="Product | Terry Chen"><meta property="og:description" content="Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."><meta property="og:type" content="website"><meta property="og:url" content="https://chenterry.com/product/"><meta property="og:site_name" content="Terry Chen"><meta property="og:locale" content="en-us"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="Terry Chen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@terrychen_ai"><meta name=twitter:creator content="@terrychen_ai"><meta name=twitter:title content="Product | Terry Chen"><meta name=twitter:description content="Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta name=twitter:image:alt content="Terry Chen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Product","description":"Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy.","url":"https:\/\/chenterry.com\/product\/","author":{"@type":"Person","name":"Terry Chen"}}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//www.google-analytics.com><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Product | Terry Chen</title>
<meta name=keywords content><meta name=author content="Terry Chen"><link rel=canonical href=https://chenterry.com/product/><link crossorigin=anonymous href=/assets/css/stylesheet.f495fe1dedb119b2969e64d021ab84ebb9f24a5086308bd0222ece1b182e151e.css integrity="sha256-9JX+He2xGbKWnmTQIauE67nySlCGMIvQIi7OGxguFR4=" rel="preload stylesheet" as=style><link rel=icon href=https://chenterry.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://chenterry.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://chenterry.com/favicon-32x32.png><link rel=apple-touch-icon href=https://chenterry.com/apple-touch-icon.png><link rel=mask-icon href=https://chenterry.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=manifest href=https://chenterry.com/manifest.json><link rel=alternate type=application/rss+xml href=https://chenterry.com/product/index.xml><link rel=sitemap type=application/xml href=https://chenterry.com/product/sitemap.xml><link rel=alternate hreflang=en href=https://chenterry.com/product/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>" type=image/svg+xml><link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>"><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M6GS8Q702L")}</script><meta property="og:url" content="https://chenterry.com/product/"><meta property="og:site_name" content="Terry Chen"><meta property="og:title" content="Product"><meta property="og:description" content="Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta name=twitter:title content="Product"><meta name=twitter:description content="Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Product","item":"https://chenterry.com/product/"}]}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","MEASUREMENT_ID")</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Product","description":"Product ideas, implementations, and case studies from startups, professional work, research projects, and prototypes. Covering AI applications, user experience, and product strategy."}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/chenterry.com\/"},{"@type":"ListItem","position":2,"name":"Product","item":"https:\/\/chenterry.com\/product/"},{"@type":"ListItem","position":3,"name":"Product","item":"https:\/\/chenterry.com\/product\/"}]}</script><style>link[rel=icon]{content:url("data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>")}</style></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chenterry.com/ accesskey=h title="Terry Chen (Alt + H)">Terry Chen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/posts/ title=Posts><span>Posts</span></a></li><li><a href=/product/ title=Product><span class=active>Product</span></a></li><li><a href=/investing/ title=Investing><span>Investing</span></a></li><li><a href=/search/ title=Explore accesskey=/><span>Explore</span></a></li><li><a href=/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><style>.projects-container{max-width:var(--content-width);margin:0 auto;padding:2rem 0}.theme-carousel{margin-bottom:4rem;position:relative}.theme-slides{display:flex;overflow:hidden;scroll-behavior:smooth}.theme-slide{min-width:100%;padding:2rem;background:linear-gradient(135deg,#e8f0ff 0%,#ffffff 100%);border-radius:12px;box-shadow:0 4px 6px rgba(0,0,0,5%)}.theme-content{max-width:800px;margin:0 auto}.theme-nav{display:flex;justify-content:center;gap:1rem;margin-top:1rem}.theme-nav button{width:12px;height:12px;border-radius:50%;border:none;background:#999;cursor:pointer;transition:background .3s ease}.theme-nav button.active{background:#222}.section{margin-bottom:4rem}.section-header{margin-bottom:2rem;text-align:center}.section-title{font-size:2rem;font-weight:700;color:#333;margin-bottom:.5rem}.section-subtitle{color:#666;font-size:1.1rem}.projects-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:2rem}.project-card{background:#fff;border-radius:12px;padding:1.5rem;box-shadow:0 2px 8px rgba(0,0,0,6%);transition:transform .3s ease,box-shadow .3s ease;cursor:pointer}.project-card:hover{transform:translateY(-4px);box-shadow:0 4px 12px rgba(0,0,0,.1)}.project-card.featured,.work-card{grid-column:span 2;background:linear-gradient(to right bottom,#ffffff,#f8f9fa)}.modal-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,.7);z-index:1000;justify-content:center;align-items:center}.modal{background:#fff;padding:2rem;border-radius:12px;max-width:800px;width:90%;max-height:90vh;overflow-y:auto;position:relative}.modal-close{position:absolute;top:1rem;right:1rem;font-size:1.5rem;border:none;background:0 0;cursor:pointer}.modal-overlay.active{display:flex}.theme-slide h2{color:#111!important;font-weight:700!important}.theme-slide p{color:#333!important;font-weight:500!important}.theme-carousel .theme-slide,.theme-content{color:#111!important}body.dark .section-title,body.dark .projects-container .section-header h2.section-title,.dark .section-title{color:#fff!important;font-weight:700!important}.theme-slide a,.theme-content a{color:#111!important;font-weight:600!important}a.read-more,.read-more-link,a.read-more-link,.entry-link::after,.post-entry .entry-footer::after,a[aria-label="Read more"],.read-more-icon,.post-entry:hover .entry-footer::after,.post-entry:focus .entry-footer::after,.post-entry:active .entry-footer::after,footer a,.post-meta a,.entry-content a,[class*=read-more]{color:#111!important;font-weight:600!important;opacity:1!important}.post-entry::after,.entry-link::after,[class*=read]::after{color:#111!important;opacity:1!important}.archived-link{margin-top:1rem!important;text-align:center!important}.archived-link span{color:#888!important;font-size:.85em!important;display:block!important}.archived-link a{color:#666!important;text-decoration:underline!important;font-weight:400!important}.archived-link a:hover{color:#333!important}body.dark .archived-link span{color:#aaa!important}body.dark .archived-link a{color:#888!important}body.dark .archived-link a:hover{color:#bbb!important}.theme-nav button{background:#999}.theme-nav button.active{background:#222}:root[data-theme=dark] .section-title,.dark .section-title{color:#fff!important}:root[data-theme=dark] .section-subtitle,.dark .section-subtitle{color:#ddd!important}:root[data-theme=dark] .post-entry .entry-footer,.dark .post-entry .entry-footer{color:#fff!important}:root[data-theme=dark] .post-entry .entry-link,.dark .post-entry .entry-link{color:#fff!important}:root[data-theme=dark] .post-meta,.dark .post-meta{color:#ddd!important}:root[data-theme=dark] .project-card,.dark .project-card,:root[data-theme=dark] .post-entry,.dark .post-entry{background:var(--entry);border:1px solid var(--border)}:root[data-theme=dark] .modal,.dark .modal{background:var(--entry);color:var(--primary);border:1px solid var(--border)}body.dark .section-title,body.dark .projects-container .section-header h2.section-title,.dark .section-title{color:#fff!important;font-weight:700!important}body.dark .post-entry .entry-header h2,.dark .post-entry .entry-header h2{color:#fff!important}body.dark .entry-footer a,body.dark a[href*="/projects/"],.dark .post-entry .entry-footer a{color:#fff!important;font-weight:600!important}.subtle-quote{margin-top:2rem;padding-top:2rem;border-top:1px solid var(--border);text-align:center;font-style:italic;color:var(--secondary);opacity:.8}.subtle-quote p{font-size:.95rem;max-width:600px;margin:0 auto}.section:last-child{margin-bottom:2rem}.find-more-posts{text-align:center;margin:2rem 0;padding:1rem}.find-more-posts p{color:#666}.find-more-posts a{color:#06c;text-decoration:underline;font-weight:500}.find-more-posts a:hover{color:#0052a3}body.dark .find-more-posts p,.dark .find-more-posts p{color:#ddd!important}body.dark .find-more-posts a,.dark .find-more-posts a{color:#93cdff!important}</style><div class=projects-container><section class=theme-carousel><div class=theme-slides id=themeSlides><div class=theme-slide><div class=theme-content><h2>Discovery Surfaces</h2><p>Enhancing personalization in content ecosystems and insights from large-scale social conversations.</p></div></div><div class=theme-slide><div class=theme-content><h2>Personalized Learning</h2><p>Enabling personalized learning and knowledge discovery through intelligent AI interfaces.</p></div></div><div class=theme-slide><div class=theme-content><h2>Advertising Technology</h2><p>Building tools for actionable insight and creative multi-modal advertisement content generation.</p></div></div><div class=theme-slide><div class=theme-content><h2>Future of Work</h2><p>Enabling agentic workforces and automated workflows for everyday productivity and efficiency.</p></div></div></div><div class=theme-nav id=themeNav></div></section><section class=section><div class=section-header><h2 class=section-title>Discovery Surfaces</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Search & Discovery Features</h2></header><div class=entry-content>Development of an intelligent search system using AI and machine learning for enhanced content discovery. Technical implementation, user experience design, and performance optimization for content recommendation platforms.</div><footer class=entry-footer><div class=post-meta><span title='2025-05-15 00:00:00 +0000 UTC'>May 15, 2025</span> • 📖 16 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/search/></a></article><article class=post-entry><header class=entry-header><h2>Content Recommendation Systems</h2></header><div class=entry-content>Development of a multi-modal recommendation system for podcast content using AI and machine learning. Technical architecture, user experience design, and performance analysis for intelligent content discovery.</div><footer class=entry-footer><div class=post-meta><span title='2024-07-10 00:00:00 +0000 UTC'>July 10, 2024</span> • 📖 9 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/recommendation/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Personalized Learning</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Exploring Unknown Unknowns</h2></header><div class=entry-content>Development of intelligent knowledge interfaces that help users discover unknown unknowns. Technical implementation of AI-powered learning systems and knowledge discovery platforms.</div><footer class=entry-footer><div class=post-meta><span title='2024-10-05 00:00:00 +0000 UTC'>October 5, 2024</span> • 📖 6 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/learning_interface/></a></article><article class=post-entry><header class=entry-header><h2>AI Content Creation - From Generation to Synthesis</h2></header><div class=entry-content>Moving beyond basic AI summarization to knowledge liquefaction - creating truly personalized learning experiences that adapt content to individual needs, contexts, and cognitive patterns.</div><footer class=entry-footer><div class=post-meta><span title='2024-09-20 00:00:00 +0000 UTC'>September 20, 2024</span> • 📖 5 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/personalized_content/></a></article><article class=post-entry><header class=entry-header><h2>Improving and Scaling Coaching through LLMs</h2></header><div class=entry-content>LLM-enhanced coaching system for project-based learning that scales personalized mentorship through AI. Research on semantic matching, regulation gap identification, and automated coaching suggestions.</div><footer class=entry-footer><div class=post-meta><span title='2024-08-15 00:00:00 +0000 UTC'>August 15, 2024</span> • 📖 34 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/llmcoaching/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Advertising Technology</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Multi-modal Creative Ad Generation</h2></header><div class=entry-content>Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.</div><footer class=entry-footer><div class=post-meta><span title='2024-05-20 00:00:00 +0000 UTC'>May 20, 2024</span> • 📖 10 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/copilot/></a></article><article class=post-entry><header class=entry-header><h2>Billion Parameter Trend Insight Analysis for Ads</h2></header><div class=entry-content>Development of AI-powered content analysis tools for TikTok trend identification and agency insights. Multi-modal content understanding and actionable trend analysis for advertising agencies.</div><footer class=entry-footer><div class=post-meta><span title='2024-04-10 00:00:00 +0000 UTC'>April 10, 2024</span> • 📖 11 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/insight/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Future of Work</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Human-Mediated Agentic Workflows</h2></header><div class=entry-content>Explore the emerging landscape of agentic workforces - autonomous AI agents that reason, plan, act, and learn to handle complex tasks and workflows. Analysis of key players, technology strategies, and market opportunities.</div><footer class=entry-footer><div class=post-meta><span title='2025-07-13 00:00:00 +0000 UTC'>July 13, 2025</span> • 📖 3 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/agentic_workforce/></a></article><article class=post-entry><header class=entry-header><h2>An Agentic Social Operating System</h2></header><div class=entry-content>Transform large-scale social conversations into actionable insights. Understand crowd sentiment, track emerging opinions, and identify key narratives.</div><footer class=entry-footer><div class=post-meta><span title='2023-04-15 00:00:00 +0000 UTC'>April 15, 2023</span> • 📖 8 min read • Terry Chen</div></footer><a class=entry-link href=https://chenterry.com/product/crowdlistening/></a></article></div></section></div><div class=find-more-posts><p>To view archived content, <a href=/archived/>click here</a>.</p></div><div class=modal-container><div id=modal-human-mediated-agentic-workflows class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("human-mediated-agentic-workflows")'>&#215;</button><h2>Human-Mediated Agentic Workflows</h2><p class=project-meta>July 13, 2025</p><div class=modal-content><h2 id=agentic-workforce>Agentic Workforce</h2><p>Our current rate of adoption for agentic workforces has significant room for improvement. AI coding is mainly for developers, but the true value unlock is when everyday people can integrate entire workflows (think assembly lines for repetitive work). All the work that one can conceive of how to do but needs to sit through should be delegated.</p><h3 id=defining-the-business>Defining the Business</h3><p>An agentic workforce involves autonomous AI agents—systems that reason, plan, act, learn, and adapt—to handle complex tasks and workflows, augmenting or replacing human labor in repetitive or decision-heavy roles. This business solves inefficiencies in traditional work structures, such as high labor costs, error-prone manual processes, and scalability limits, by deploying AI agents that operate as &ldquo;digital teammates&rdquo; for tasks like data analysis, customer service, and automation. Efficiency is achieved through hyperautomation (e.g., 30% productivity gains), personalized experiences, and reduced MTTR in operations, with adoption projected to jump 327% by 2027. The market, part of broader AI, sees agentic AI driving $4.4T in value, but faces challenges like 40% project cancellations by 2027 due to costs and risks.</p><h3 id=key-players--competitive-landscape>Key Players & Competitive Landscape</h3><p>The landscape features AI leaders building agentic tools, with $33.9B in GenAI investments (2024-2025) and acquisitions like Capgemini-WNS ($3.4B) for agentic ops. Startups like Gradient Labs ($13M) target regulated sectors. Competition focuses on enterprise vs. consumer, with stocks like UiPath, NVIDIA rising 20-50% on agentic bets.</p><table><thead><tr><th>Player</th><th>Key Offerings</th><th>Differentiation</th><th>Investments/Acquisitions</th></tr></thead><tbody><tr><td>OpenAI</td><td>GPT agents; o1 model for reasoning.</td><td>Advanced reasoning; agentic frameworks for workflows.</td><td>$157B valuation; io Products acquisition for hardware.</td></tr><tr><td>Microsoft</td><td>Copilot agents in Dynamics/365; Azure AI Studio.</td><td>Enterprise integration; hybrid human-AI decisions.</td><td>$1.3B AI; OpenAI partnership.</td></tr><tr><td>Google</td><td>Gemini agents; Project Astra.</td><td>Decision intelligence; Android ecosystem.</td><td>$75B data centers; AI acquisitions.</td></tr><tr><td>Anthropic</td><td>Claude for agentic tasks; constitutional AI.</td><td>Ethical alignment; safe automation.</td><td>$61.5B valuation; Amazon investments.</td></tr><tr><td>UiPath</td><td>RPA with agentic AI for processes.</td><td>Hyperautomation; workflow orchestration.</td><td>Stock focus; partnerships.</td></tr><tr><td>Gradient Labs</td><td>Agentic AI for customer support in regulated industries.</td><td>Compliance-focused; reskilling integration.</td><td>$13M raised (Monzo alums).</td></tr></tbody></table><h3 id=the-technology--strategy>The Technology & Strategy</h3><p>Tech: Agentic AI uses LLMs with tools/memory for autonomous actions (e.g., reasoning/planning in o1 models); multi-agent systems coordinate tasks. Strategies: Hybrid workforces (AI-human collaboration), governance frameworks; 2025 trends: Reasoning models, MoE, synthetic data. AI adoption: 70% orgs operationalize by 2025; productivity +30%, but 40% cancellations.</p><h3 id=finding-the-edge>Finding the Edge</h3><p>Edges: Ethical AI (Anthropic), enterprise scale (Microsoft), reasoning (OpenAI). Field heads to cognitive enterprises, hybrid workforces; investments like RSM&rsquo;s $1B signal maturity. Differentiation via data governance, multi-agent orchestration.</p><h3 id=prototyping--explorations>Prototyping & Explorations</h3><p>Prototypes: Multi-agent systems (Chain-of-Agents); explorations: AI data engineers, agentic L&amp;D for upskilling. VC memos: Focus on agentic for ROI, but caution costs.</p><h3 id=remaining-questions>Remaining Questions</h3><ul><li>How will agentic AI reshape traditional job roles and responsibilities?</li><li>Can organizations effectively manage the transition to hybrid human-AI workforces?</li><li>What regulatory frameworks are needed for autonomous AI agents in the workplace?</li></ul><h3 id=references>References:</h3><ul><li><strong>A2A Catalog</strong>: <a href=https://a2acatalog.com>Agentic AI Tools Directory</a> - Comprehensive directory of agentic AI tools and platforms</li><li><strong>McKinsey</strong>: <a href=https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work>AI in the Workplace</a> - AI&rsquo;s impact on workplace productivity and agentic systems</li><li><strong>Gartner</strong>: <a href=https://www.gartner.com/en/documents/4025678>Agentic AI Trends</a> - Agentic AI market trends and adoption forecasts</li><li><strong>Forrester</strong>: <a href=https://www.forrester.com/report/The-Forrester-Wave-Automation-Platforms-Q2-2025/>Workforce Automation</a> - Automation platforms and agentic workforce solutions</li><li><strong>IDC</strong>: <a href="https://www.idc.com/getdoc.jsp?containerId=prUS51245625">AI Workforce Market</a> - AI workforce market analysis and projections</li></ul><h3 id=appendix>Appendix</h3><p>This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to <a href=mailto:terrychen2026@u.northwestern.edu>terrychen2026@u.northwestern.edu</a>.</p></div></div></div><div id=modal-search-discovery-features class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("search-discovery-features")'>&#215;</button><h2>Search & Discovery Features</h2><p class=project-meta>May 15, 2025</p><div class=modal-content><p>An analysis of traditional search paradigms and a framework for integrating AI capabilities into content discovery platforms.</p><h2 id=the-content-discovery-challenge>The Content Discovery Challenge</h2><p>Traditional search paradigms face fundamental limitations in today&rsquo;s information-rich environment. Users often struggle to articulate their information needs precisely, leading to iterative query refinement and incomplete discovery. Current search systems prioritize keyword matching over intent understanding, resulting in high precision for specific queries but poor recall for exploratory or contextual searches.</p><p>The challenge extends beyond technical limitations to cognitive ones. Users don&rsquo;t always know what they&rsquo;re looking for until they find it, making traditional query-based search inadequate for discovery scenarios. This creates a significant opportunity for AI-enhanced systems that can understand and anticipate user needs rather than simply matching keywords.</p><h2 id=evolution-framework-for-search-intelligence>Evolution Framework for Search Intelligence</h2><p>The transformation of search and discovery systems follows a three-stage evolutionary framework that addresses increasingly sophisticated user needs and capabilities. This progression moves from basic content retrieval through enhanced scenario coverage to fundamentally reimagined discovery experiences.</p><h3 id=stage-1-foundation---meeting-basic-requirements>Stage 1: Foundation - Meeting Basic Requirements</h3><p>The foundational stage focuses on perfecting core search functionality while establishing the infrastructure for more advanced capabilities. <strong>Precise Content Location</strong> ensures that users who know exactly what they&rsquo;re looking for can find it efficiently through refined keyword search that delivers accurate results for specific queries. <strong>Experience Completeness</strong> enhances user interaction through comprehensive search history that enables users to revisit and build upon previous discoveries, intelligent input prediction that accelerates query formulation, and seamless cross-session continuity that maintains context across multiple search interactions.</p><p><strong>Recommendation Integration</strong> introduces proactive discovery elements that complement traditional search, including contextual suggestions that surface relevant content based on current search behavior, curated entry points that guide users toward high-quality content collections, and algorithmic recommendations that leverage user patterns to suggest potentially valuable discoveries.</p><h3 id=stage-2-expansion---covering-diverse-search-scenarios>Stage 2: Expansion - Covering Diverse Search Scenarios</h3><p>The expansion stage addresses the limitations of traditional search by supporting more nuanced and complex user needs. <strong>Fuzzy Requirements Support</strong> enables the system to interpret and respond to ambiguous or incomplete user expressions through natural language processing that understands intent beyond keywords, semantic matching that connects user needs with relevant content even when terminology differs, and contextual interpretation that considers the user&rsquo;s situation and background when generating responses.</p><p><strong>Personalization with Transparency</strong> creates tailored experiences while maintaining user understanding and control. The system provides clear reasoning for recommendations that explains why specific content appears relevant, develops adaptive algorithms that learn from user interactions while respecting privacy preferences, and implements granular search capabilities that support both high-level topic exploration and detailed, specific content discovery.</p><p><strong>Multi-Granularity Discovery</strong> supports different levels of content exploration, from individual article or viewpoint-level recommendations that address specific questions or perspectives, to comprehensive collection-level suggestions that provide structured learning paths or thematic exploration opportunities.</p><h3 id=stage-3-transformation---ai-powered-creation-and-curation>Stage 3: Transformation - AI-Powered Creation and Curation</h3><p>The transformation stage represents a fundamental shift from search as retrieval to search as an intelligent creation and curation platform. <strong>Knowledge Association and Generation</strong> enables the system to synthesize personalized content that matches individual consumption preferences, connecting disparate information sources into coherent, customized presentations that serve specific user needs and contexts.</p><p><strong>Adaptive Content Matching</strong> ensures that discovered or generated content aligns with current user capabilities and preferences, including difficulty calibration that presents information at appropriate complexity levels, modality optimization that delivers content in formats best suited to user preferences and contexts, and contextual relevance that considers the user&rsquo;s immediate goals and longer-term interests.</p><p><strong>Dynamic Content Creation</strong> addresses gaps in existing content through real-time synthesis that generates answers or explanations when suitable existing content cannot be found, asynchronous content development that creates comprehensive resources for recurring user needs, and collaborative generation that combines AI capabilities with human expertise to produce high-quality, tailored content.</p><p>This evolutionary framework recognizes that modern search must progress beyond simple information retrieval to become an intelligent partner in knowledge discovery, learning, and content creation. Each stage builds upon the previous one while introducing fundamentally new capabilities that expand what users can accomplish through search interactions.</p><h2 id=competitive-landscape-analysis>Competitive Landscape Analysis</h2><table><thead><tr><th>Platform Type</th><th>Strengths</th><th>Weaknesses</th></tr></thead><tbody><tr><td><strong>Traditional Search Engines</strong></td><td>Comprehensive indexing, fast retrieval, established user behavior patterns</td><td>Limited contextual understanding, poor handling of ambiguous queries, difficulty with exploratory search</td></tr><tr><td><strong>Recommendation Systems</strong></td><td>Personalization capabilities, learning from user behavior, serendipitous discovery</td><td>Filter bubble effects, cold start problems, limited transparency</td></tr><tr><td><strong>AI-Enhanced Platforms</strong></td><td>Natural language understanding, conversational interfaces, contextual awareness</td><td>Computational costs, accuracy concerns, user trust issues</td></tr></tbody></table><h2 id=proposed-ai-integration-framework>Proposed AI Integration Framework</h2><p>The AI integration framework follows a three-phase approach designed to incrementally enhance search capabilities while maintaining user trust and system reliability.</p><h3 id=phase-1-query-enhancement>Phase 1: Query Enhancement</h3><p>The foundation phase implements LLM-powered query understanding that can interpret ambiguous or incomplete queries, transforming user intent into actionable search parameters. This includes suggesting query refinements and alternatives based on contextual understanding, extracting clear intent from natural language descriptions, and providing contextual query expansion that broadens search scope while maintaining relevance. This phase establishes the groundwork for more sophisticated AI interactions while delivering immediate value to users struggling with traditional keyword-based search limitations.</p><h3 id=phase-2-content-synthesis>Phase 2: Content Synthesis</h3><p>Building on enhanced query understanding, the content synthesis phase develops advanced generation capabilities that summarize multiple sources into coherent overviews, enabling users to quickly grasp complex topics from diverse perspectives. The system generates comparative analyses across different viewpoints, creates personalized explanations based on user background and expertise level, and synthesizes comprehensive answers from distributed information sources. This phase transforms search from simple information retrieval into intelligent content curation that adapts to individual user needs and contexts.</p><h3 id=phase-3-conversational-discovery>Phase 3: Conversational Discovery</h3><p>The final phase creates interactive discovery experiences that fundamentally change how users explore information. This includes sophisticated follow-up question generation that guides users toward deeper understanding, exploratory conversation flows that encourage serendipitous discovery, contextual recommendations that surface related concepts and ideas, and learning path suggestions that create structured journeys through complex domains. This phase represents the full realization of AI-enhanced search, where the system becomes a collaborative partner in knowledge discovery rather than simply a retrieval tool.</p><h2 id=user-interface-design-recommendations>User Interface Design Recommendations</h2><h3 id=progressive-disclosure-architecture>Progressive Disclosure Architecture</h3><p>Effective AI-enhanced search interfaces must design for complexity while maintaining simplicity. The progressive disclosure approach starts with simple, high-level answers that provide immediate value while indicating deeper information availability. Clear pathways to more detailed information allow users to drill down based on their specific needs and interests. Topic exploration through related concepts creates natural discovery paths that expand user understanding beyond their original query. Supporting both focused and exploratory search modes ensures the interface adapts to different user intents and discovery styles.</p><h3 id=conversational-elements>Conversational Elements</h3><p>Modern search interfaces benefit from integrating chat-like interactions that feel natural and intuitive. Contextual follow-up questions guide users toward more specific or broader explorations based on their demonstrated interests. Maintaining conversation history and context enables the system to build understanding over multiple interactions, creating more personalized and relevant responses. Clarification and refinement capabilities allow users to iteratively improve search results through natural dialogue. Multi-turn information gathering transforms search from isolated queries into collaborative knowledge-building sessions.</p><h3 id=personalization-without-intrusion>Personalization Without Intrusion</h3><p>Successful AI search systems must balance personalization benefits with user privacy and control concerns. Learning from interaction patterns rather than explicit profiling reduces user burden while building sophisticated understanding of preferences and needs. Transparency into personalization decisions builds user trust by explaining why certain results or recommendations appear. User control over recommendation intensity allows individuals to adjust the system&rsquo;s proactive behavior based on their current goals and contexts. Balancing novelty with relevance ensures that personalization enhances rather than constrains discovery opportunities.</p><h2 id=technical-implementation-strategy>Technical Implementation Strategy</h2><h3 id=hybrid-architecture-approach>Hybrid Architecture Approach</h3><p>The most effective AI-enhanced search systems combine traditional infrastructure with advanced AI capabilities rather than replacing existing systems entirely. Maintaining fast keyword-based retrieval for precise queries ensures excellent performance for straightforward information needs. Layering AI enhancement for ambiguous or exploratory searches adds intelligence where it provides the most value. Implementing fallback mechanisms for AI system failures ensures reliability and user trust. Optimizing for both speed and understanding creates systems that excel across diverse use cases and user needs.</p><h3 id=vector-search-integration>Vector Search Integration</h3><p>Semantic search capabilities through vector embedding technology enable understanding of meaning beyond keyword matching. Document embedding for content similarity allows the system to find conceptually related information even when specific terminology differs. Query embedding for intent matching translates user needs into semantic space for more accurate retrieval. Contextual embedding updates based on user interaction enable the system to learn and adapt over time. Multi-modal embedding for diverse content types creates unified search experiences across text, images, audio, and other media formats.</p><h3 id=real-time-learning-systems>Real-time Learning Systems</h3><p>Adaptive systems that improve through use represent a key advantage of AI-enhanced search platforms. Capturing implicit feedback from user interactions provides rich signals about content quality and relevance without requiring explicit user effort. Updating recommendations based on session context enables dynamic adaptation to evolving user needs within single search sessions. Learning from successful discovery patterns helps the system recognize and replicate effective search strategies. Adapting to changing user needs over time ensures long-term relevance and value as user interests and expertise evolve.</p><h2 id=quality-assurance-framework>Quality Assurance Framework</h2><h3 id=content-accuracy-measures>Content Accuracy Measures</h3><p>Maintaining content quality in AI-enhanced systems requires comprehensive validation approaches. Source attribution and verification ensure that generated content maintains clear provenance and accuracy standards. Fact-checking against authoritative sources provides additional validation layers for critical information. Confidence scoring for generated responses helps users understand the reliability of AI-generated content. Human review workflows for critical information ensure that important decisions receive appropriate oversight and validation.</p><h3 id=user-experience-validation>User Experience Validation</h3><p>Measuring the effectiveness of AI-enhanced discovery requires sophisticated assessment approaches beyond traditional engagement metrics. Task completion rate analysis reveals whether users successfully achieve their information goals. User satisfaction with discovery outcomes provides direct feedback about system value and effectiveness. Time-to-insight metrics measure how quickly users can find valuable information and understanding. Exploration depth and breadth measurement assesses whether the system successfully encourages beneficial discovery behaviors.</p><h3 id=bias-detection-and-mitigation>Bias Detection and Mitigation</h3><p>AI systems require ongoing monitoring to ensure fair and equitable information access. Search result ranking and selection algorithms must be regularly audited for potential biases that could disadvantage certain topics, perspectives, or user groups. Content recommendation algorithms need continuous evaluation to prevent filter bubble effects that limit user exposure to diverse viewpoints. Personalization filter effects require monitoring to ensure that customization enhances rather than constrains user discovery opportunities. Representation across different user groups must be regularly assessed to ensure equitable access and value delivery.</p><h2 id=strategic-implementation-roadmap>Strategic Implementation Roadmap</h2><table><thead><tr><th>Quarter</th><th>Focus Area</th><th>Key Deliverables</th></tr></thead><tbody><tr><td><strong>Q1: Foundation</strong></td><td>Basic AI Integration</td><td>LLM query enhancement, semantic search deployment, content quality frameworks, pilot user testing</td></tr><tr><td><strong>Q2: Enhancement</strong></td><td>Advanced Features</td><td>Conversational discovery, personalization algorithms, content synthesis capabilities, infrastructure scaling</td></tr><tr><td><strong>Q3: Optimization</strong></td><td>Data-Driven Improvement</td><td>Algorithm refinement, UI enhancement, advanced quality assurance, expanded content integration</td></tr><tr><td><strong>Q4: Scale</strong></td><td>Full Deployment</td><td>Complete AI-enhanced search, comprehensive testing, advanced analytics, next-generation planning</td></tr></tbody></table><hr><h1 id=ai-book-search-prototype>AI Book Search Prototype</h1><p>An intelligent book search system that understands your needs and provides personalized recommendations using GPT-4o.</p><p><img src=/images/projects/search/search_input.png alt="AI Book Search Interface" loading=lazy></p><h2 id=product-rationale>Product Rationale</h2><h3 id=the-problem>The Problem</h3><p>Traditional book discovery suffers from several fundamental limitations that prevent readers from finding content that truly resonates with their needs and interests. Conventional search relies on exact keyword matches, failing to understand the nuanced intent behind user queries and missing opportunities for meaningful discovery. Most systems provide broad, one-size-fits-all suggestions without considering the user&rsquo;s specific emotional state, learning goals, or contextual needs, resulting in generic recommendations that often miss the mark.</p><p>People frequently remember books by fragments rather than complete details, recalling a powerful quote, a character trait, or an emotional impact but struggling to translate these memories into findable search terms. Current systems cannot distinguish between fundamentally different user intents, such as needing practical advice versus wanting to explore ideas versus seeking emotional catharsis, leading to misaligned recommendations that frustrate rather than inspire.</p><h3 id=the-vision>The Vision</h3><p>AI Book Search represents a paradigm shift from information retrieval to intelligent content curation. Instead of searching for books, users can now search through their intentions, memories, and needs. The system acts as a knowledgeable librarian who understands not just what you&rsquo;re asking, but why you&rsquo;re asking and how to help you discover content that truly resonates with your current situation and interests.</p><p><img src=/images/projects/search/categories.png alt="Query Categories" loading=lazy></p><h3 id=core-innovation>Core Innovation</h3><p>The breakthrough lies in intent-aware content discovery that combines semantic understanding with contextual curation. The AI interprets the deeper meaning behind queries, not just surface keywords, enabling understanding of complex, nuanced requests. Recommendations adapt to user intent categories, providing contextually relevant content that matches both explicit needs and implicit desires.</p><p>Memory-based discovery allows users to search using fragments of memory, including quotes, character descriptions, and plot elements, transforming partial recollections into successful book discoveries. Progressive learning journeys ensure that content cards form logical progressions from foundational to advanced concepts, supporting sustained learning and exploration. Multi-modal content integration seamlessly combines books, podcasts, articles, and quotes into cohesive recommendations that provide comprehensive coverage of user interests.</p><h3 id=target-impact>Target Impact</h3><p>The system aims to dramatically reduce discovery friction, transforming the typical 15+ minute browsing session into instant, relevant recommendations that immediately connect with user needs. Enhanced learning outcomes result from curated progressions that build knowledge systematically rather than randomly. Emotional resonance ensures that content matches not just intellectual interests but emotional and contextual needs, creating more meaningful reading experiences. Democratizing expert curation provides every user with personalized librarian-level guidance, regardless of their access to professional recommendation services.</p><h2 id=implementation-strategy>Implementation Strategy</h2><h3 id=design-philosophy>Design Philosophy</h3><p>Rather than retrofitting AI onto traditional search, the system builds entirely around LLM capabilities to create fundamentally new discovery experiences. Query understanding through GPT-4o analyzes user intent before any content retrieval, ensuring that recommendations address actual needs rather than surface keyword matches. Contextual generation creates content cards based on analyzed intent rather than pre-stored data, enabling dynamic responses that adapt to specific user contexts.</p><p>Progressive disclosure reveals information in logical layers, moving from intent analysis through content generation to card rendering. Simple queries receive focused responses while complex topics get comprehensive exploration, ensuring appropriate depth without overwhelming users. Debug mode availability for developers supports system understanding and continuous improvement of the AI decision-making process.</p><p>Modular component design ensures that each system element has a single responsibility, creating maintainable and extensible architecture. Type-safe data structures using Pydantic provide reliable data handling, while isolated AI interactions with fallback handling ensure system resilience. Reusable, stateless rendering functions support consistent user experience, and pure orchestration between components maintains clean system architecture.</p><p><img src=/images/projects/search/loading.png alt="Loading State" loading=lazy></p><h3 id=technical-implementation>Technical Implementation</h3><p>The core technology stack centers on Streamlit for rapid prototyping and clean UI development, enabling quick iteration and user testing. OpenAI GPT-4o provides sophisticated query analysis and content generation capabilities that understand nuanced user intent. Pydantic ensures type-safe JSON processing that maintains data integrity throughout the system. Design for Streamlit Cloud deployment with environment variable configuration supports scalable hosting and easy maintenance.</p><p>Key technical decisions prioritize AI-generated content over database search to enable understanding of nuanced, contextual queries that wouldn&rsquo;t match traditional database keywords. This approach allows creative, synthesized recommendations that combine multiple sources while eliminating the need for massive content databases in the prototype phase. The system provides immediate value without complex data ingestion pipelines, though this creates trade-offs with less precision for specific book details while providing higher relevance for intent-based discovery.</p><p>Intent-category classification into seven predefined categories before content generation provides structured understanding of user needs and enables category-specific prompt engineering for better results. This creates predictable, testable system behavior while allowing targeted improvements per intent type. Implementation through a two-stage LLM pipeline with structured JSON responses ensures reliable and interpretable results.</p><p>Dynamic card generation produces one to five content cards based on query complexity, ensuring that simple queries receive focused, direct answers while complex topics benefit from multi-perspective exploration. This approach avoids overwhelming users with irrelevant content while creating natural learning progressions that support sustained engagement.</p><p>Component-based UI design builds interfaces from reusable, type-specific content cards that support diverse content types including books, podcasts, quotes, and themes. This enables consistent styling across different recommendation types while facilitating easy extension for new content types and providing better responsive design capabilities.</p><h3 id=system-architecture>System Architecture</h3><p>The data flow follows a clear progression from user query through intent analysis to content generation and finally UI rendering. Raw user queries captured via Streamlit undergo GPT-4o analysis that classifies query type and intent category. Category-specific prompts then generate relevant content cards, with Pydantic models ensuring type safety and structure. Dynamic card components render based on content type, while graceful fallbacks at each stage maintain system reliability even when individual components encounter issues.</p><p><img src=/images/projects/search/final_result.png alt="Final Results" loading=lazy></p><h2 id=features-and-capabilities>Features and Capabilities</h2><h3 id=smart-query-analysis>Smart Query Analysis</h3><p>The system includes sophisticated specific book detection that identifies when users ask about particular titles, enabling targeted responses with detailed information about known works. Intent classification categorizes general queries into seven distinct types that cover the spectrum of user needs and discovery patterns.</p><p>Problem-solving queries like &ldquo;How to deal with difficult colleagues&rdquo; receive practical, actionable recommendations focused on applicable knowledge and skills. Exploration and discovery requests such as &ldquo;Mind-bending science books (not too technical)&rdquo; generate curated selections that balance intellectual challenge with accessibility. Quote and concept memory searches like &ldquo;Books about &lsquo;flow state&rsquo;&rdquo; help users rediscover influential ideas and their sources.</p><p>Plot fragment memory searches such as &ldquo;Book with a girl counting prime numbers&rdquo; transform partial story recollections into successful identifications. Character and scene description queries like &ldquo;London autistic detective&rdquo; leverage memorable details to find specific works. Emotional and thematic searches such as &ldquo;Books that will make me cry (in a good way)&rdquo; connect users with content that matches their desired emotional experience. Comparative searches like &ldquo;Like Harry Potter but for adults&rdquo; help users find similar works that match their established preferences while introducing new elements.</p><h3 id=dynamic-ui-components>Dynamic UI Components</h3><p>Responsive card design adapts to different content types with distinct visual approaches for quotes, summaries, recommendations, and themes. Book recommendation cards provide rich information including relevance scores and detailed reasoning that helps users understand why specific books match their needs. Quote cards feature special formatting with book attribution and page numbers that enable easy reference and verification. Work-in-progress components with preview data demonstrate planned features and gather user feedback for future development.</p><h3 id=ai-powered-responses>AI-Powered Responses</h3><p>Real GPT-4o integration ensures sophisticated query analysis and content generation that understands complex user intent and context. Structured JSON responses rendered into beautiful UI components provide consistent, reliable user experiences. Fallback handling for API errors ensures system reliability even when external services encounter issues. Debug mode enables understanding of query analysis processes, supporting system improvement and user education about AI decision-making.</p><p><img src=/images/projects/search/process_output.png alt="Process Output" loading=lazy></p><h2 id=technical-details-and-customization>Technical Details and Customization</h2><p>The system employs flexible data models including QueryType classifications for specific books versus general queries, UserIntentCategory definitions covering seven predefined categories, and ContentCard systems with type-specific rendering capabilities. BookRecommendation models provide rich information with reasoning, while PlaceholderFeature components support future development.</p><p>API integration utilizes OpenAI GPT-4o for all AI processing with structured JSON responses and Pydantic validation ensuring data integrity. Error handling and fallback responses maintain system reliability, while temperature controls optimize output for different use cases and user needs.</p><p>User interface features include custom CSS styling with gradients and animations that create engaging visual experiences. Responsive card layouts adapt to various screen sizes and content types. Debug mode supports development and troubleshooting, while comprehensive loading states and error handling ensure smooth user experiences even when systems encounter difficulties.</p><h2 id=future-development-and-roadmap>Future Development and Roadmap</h2><p>Planned enhancements include semantic vector search capabilities that leverage advanced embedding-based approaches for even more sophisticated content discovery. User preference learning from search history will enable increasingly personalized recommendations that improve over time. Book database integration will provide access to real book data and availability information, enhancing the practical utility of recommendations.</p><p>Social features will enable sharing and collaborative recommendations, creating community-driven discovery experiences. Mobile optimization through progressive web app features will extend access across devices and usage contexts. The modular architecture supports continuous enhancement and experimentation with new recommendation techniques while maintaining stable, high-quality user experiences.</p><p>The system represents a foundation for increasingly sophisticated content discovery applications that understand and serve individual user needs through intelligent, adaptive technology. As user data accumulates and understanding of content consumption patterns improves, the system will evolve to provide even more valuable and relevant discovery experiences that genuinely enhance rather than simply automate the content discovery process.</p></div></div></div><div id=modal-exploring-unknown-unknowns class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("exploring-unknown-unknowns")'>&#215;</button><h2>Exploring Unknown Unknowns</h2><p class=project-meta>October 5, 2024</p><div class=modal-content><h1 id=exploring-unknown-unknowns-the-future-of-knowledge-interfaces>Exploring Unknown Unknowns: The Future of Knowledge Interfaces</h1><p>We live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don&rsquo;t know what to read, and we don&rsquo;t understand what we&rsquo;ve read. These pain points—&ldquo;not knowing how to choose&rdquo; and &ldquo;not knowing how to comprehend&rdquo;—represent a massive opportunity for reimagining how we interact with knowledge.</p><p>The core insight driving next-generation learning interfaces is simple but profound: most people don&rsquo;t know what they don&rsquo;t know. We can&rsquo;t formulate good questions about topics we&rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.</p><h2 id=beyond-search-learning-from-googles-experiments>Beyond Search: Learning from Google&rsquo;s Experiments</h2><p>Google&rsquo;s Learn About product offers a compelling glimpse of this future. Unlike traditional search, which requires users to know what to look for, Learn About allows users to &ldquo;zoom out and look at the space of questions around your question.&rdquo; It combines the information accuracy of search with the flexible, dynamic interaction of AI chat, creating an exploratory learning experience that goes far beyond simple Q&amp;A.</p><p>This approach represents a fundamental shift from information retrieval to knowledge discovery. Instead of returning static results, the system actively helps users explore adjacent concepts and ask better questions. Users can pursue their immediate curiosity while simultaneously discovering related topics they never thought to investigate.</p><p>The most innovative learning interfaces take this concept further by specializing in specific domains. Rather than trying to handle all possible queries, they focus on particular knowledge areas—like literature, technical documentation, or professional development—where they can provide genuinely superior experiences compared to general-purpose tools.</p><h2 id=the-llm-architecture-behind-intelligent-learning>The LLM Architecture Behind Intelligent Learning</h2><p>The technical foundation of these systems relies on sophisticated prompt engineering and modular content generation. Large language models serve as the cognitive engine, but their raw output must be carefully structured to create coherent learning experiences. The key innovation lies in using LLMs to generate JSON-formatted responses that populate predefined UI templates, creating consistent yet dynamic interfaces.</p><p>This architecture allows the system to maintain conversational flow while presenting information in learner-friendly formats. For example, instead of generating wall-of-text responses, the LLM outputs structured data that renders as interactive cards, related questions, and exploration pathways. Each response includes not just content, but also suggested next steps and connection points to related topics.</p><p>The prompt engineering becomes crucial here. Effective systems use detailed behavioral instructions that guide the LLM to act as a knowledgeable teacher rather than a simple question-answering service. These prompts specify tone, content depth, interaction style, and response structure, ensuring consistency across thousands of potential learning conversations.</p><h2 id=reducing-cognitive-friction-through-design>Reducing Cognitive Friction Through Design</h2><p>Traditional learning interfaces suffer from what could be called &ldquo;prompt friction&rdquo;—the cognitive overhead of formulating good questions and organizing complex thoughts into text. The most successful knowledge interfaces minimize this friction through several design strategies.</p><p>First, they embed potential questions directly into content responses. Instead of requiring users to think of follow-up questions, the system generates three or four relevant next steps that users can explore with a simple click. This transforms learning from an active questioning process into a guided exploration where curiosity can flow naturally.</p><p>Second, they use modular response formats that pack high knowledge density into digestible chunks. Rather than lengthy explanations, responses combine concise answers with interactive elements: reflection prompts, knowledge checks, relevance connections, and vocabulary builders. Users receive exactly the information they need while being invited to go deeper on specific aspects that interest them.</p><p>Third, they implement &ldquo;prompt prefills&rdquo;—pre-written questions and conversation starters that help users begin productive dialogues without staring at blank input fields. These aren&rsquo;t generic suggestions but contextually relevant questions based on the current topic and common learning patterns.</p><h2 id=personalization-through-conversational-intelligence>Personalization Through Conversational Intelligence</h2><p>Unlike traditional recommendation systems that rely on explicit preferences or behavioral tracking, conversational learning interfaces build user understanding organically through dialogue. Each interaction reveals information about the user&rsquo;s background knowledge, interests, learning goals, and preferred depth of explanation.</p><p>This conversational profiling enables increasingly sophisticated personalization. The system learns whether a user prefers concrete examples or abstract concepts, detailed explanations or high-level overviews, historical context or contemporary applications. Over time, responses become naturally calibrated to individual learning styles and knowledge levels.</p><p>The personalization extends beyond content delivery to include book recommendations, topic suggestions, and learning path optimization. By understanding what concepts a user struggles with and what types of explanations resonate, the system can proactively surface relevant material and adapt its teaching approach in real-time.</p><h2 id=technical-implementation-rag-and-knowledge-curation>Technical Implementation: RAG and Knowledge Curation</h2><p>Behind the conversational interface lies a sophisticated knowledge management system. Rather than relying solely on LLM training data, effective learning platforms implement Retrieval-Augmented Generation (RAG) architectures that combine real-time information retrieval with language generation.</p><p>This approach proves particularly valuable for specialized domains like literature analysis, where high-quality, curated knowledge sources significantly improve response accuracy and depth. Systems can draw from structured databases of book analyses, expert commentary, reader discussions, and academic sources to provide richer, more authoritative answers than general-purpose models alone.</p><p>The challenge lies in balancing different information sources. Community discussions from platforms like Reddit offer authentic reader perspectives and common questions, while academic sources provide authoritative analysis. Professional reviews and curated summaries add editorial quality. Effective systems learn to synthesize these different knowledge types based on the specific question and user context.</p><h2 id=measuring-success-beyond-engagement>Measuring Success Beyond Engagement</h2><p>Traditional educational metrics often miss the point of exploratory learning. While engagement metrics like session length and click-through rates provide some insight, the real value lies in knowledge acquisition and curiosity development. The most meaningful measures focus on learning outcomes: Do users ask better questions over time? Do they make novel connections between concepts? Do they pursue deeper investigation of topics that initially seemed uninteresting?</p><p>Advanced systems track conversation quality through several indicators: the progression from basic to sophisticated questions, the frequency of cross-topic connections, the depth of follow-up exploration, and user-generated insights that suggest genuine understanding. These metrics help optimize not just for engagement, but for actual learning effectiveness.</p><h2 id=the-future-of-knowledge-work>The Future of Knowledge Work</h2><p>As these interfaces mature, they point toward a fundamental transformation in how we approach knowledge work. Instead of consuming information passively, we&rsquo;ll increasingly collaborate with AI systems to explore ideas, test understanding, and discover unexpected connections. The goal isn&rsquo;t to replace human thinking but to augment it with better tools for curiosity and exploration.</p><p>The most promising applications extend beyond individual learning to collaborative knowledge building. Imagine research environments where teams can explore complex topics together, with AI facilitators helping surface relevant connections, identify knowledge gaps, and guide productive discussions. Or educational settings where students learn not just facts but how to ask increasingly sophisticated questions about any domain.</p><p>The technical foundation already exists. The remaining challenge is design: creating interfaces that feel natural, educational experiences that genuinely improve understanding, and systems that scale personalized learning without losing the human touch that makes great teaching transformative.</p><hr><p><em>The next time you encounter a complex topic, imagine having a knowledgeable guide who not only answers your questions but helps you discover the questions you didn&rsquo;t know to ask. That&rsquo;s the promise of intelligent knowledge interfaces—and it&rsquo;s closer than you might think.</em></p></div></div></div><div id=modal-ai-content-creation-from-generation-to-synthesis class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("ai-content-creation-from-generation-to-synthesis")'>&#215;</button><h2>AI Content Creation - From Generation to Synthesis</h2><p class=project-meta>September 20, 2024</p><div class=modal-content><p>In the rapidly evolving landscape of artificial intelligence, we&rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.</p><h2 id=the-limitations-of-current-approaches>The Limitations of Current Approaches</h2><p>Today&rsquo;s AI content tools largely excel at taking existing information and reformatting it into different modalities. We can convert text to audio, create video summaries, or generate podcast-style conversations from written material. However, as Large Language Model context windows continue to expand, simple content summarization becomes increasingly commoditized. The real value lies not in these mechanical transformations, but in the creative synthesis and novel perspectives that emerge when AI systems understand both the content and the consumer.</p><p>The essence of creativity lies in finding fresh angles of approach. Quality content distinguishes itself through novel perspectives, clear structure, and genuine utility to the reader. As we move beyond basic summarization, the challenge becomes how to help AI systems discover these unique entry points that make content both engaging and personally relevant.</p><h2 id=knowledge-liquefaction-the-new-content-paradigm>Knowledge Liquefaction: The New Content Paradigm</h2><p>We&rsquo;re entering an era of &ldquo;knowledge liquefaction&rdquo; where any piece of information can be rapidly transformed into formats that match specific consumption scenarios. Whether someone needs structured learning materials for deep study or fragmentary content for casual listening during commutes, AI systems can now adapt the same core knowledge to fit these different contexts seamlessly.</p><p>This capability extends far beyond simple format conversion. The most compelling applications combine high-quality human-created content with AI&rsquo;s ability to find unexpected connections and generate personalized frameworks. Rather than replacing human creativity, these systems amplify it by identifying patterns and relationships that might not be immediately obvious, then presenting them through personalized lenses that resonate with individual users.</p><h2 id=the-personalization-challenge>The Personalization Challenge</h2><p>Creating truly personalized content presents a fundamental tension between scale and customization. If every piece of content requires individual adaptation for each user, the costs become prohibitive. However, knowledge fusion offers a solution through its inherent modularity. Many elements of content remain constant across audiences—core concepts, fundamental principles, and essential facts—while the variable elements involve how these concepts connect to individual interests, goals, and existing knowledge.</p><p>The key insight is that personalization doesn&rsquo;t require generating entirely new content for each user. Instead, it involves intelligent selection and combination of existing content elements, supplemented by targeted customization that creates meaningful connections to the user&rsquo;s specific context and needs.</p><h2 id=dynamic-user-understanding-through-interaction>Dynamic User Understanding Through Interaction</h2><p>Modern AI systems have unprecedented access to rich user interaction data through natural language conversations, reading highlights, and behavioral patterns. Unlike traditional recommendation systems that rely primarily on click-through data, AI-native platforms can analyze the semantic content of user queries, the topics they explore, and the questions they ask to build sophisticated models of their interests and learning preferences.</p><p>This approach moves beyond simple topic matching to understand cognitive patterns and learning styles. For example, the system might recognize that one user prefers concrete examples and case studies, while another gravitates toward theoretical frameworks and abstract principles. These insights enable the generation of content that not only covers relevant topics but presents them in ways that align with how each individual processes and retains information.</p><h2 id=building-memory-systems-that-learn-and-adapt>Building Memory Systems That Learn and Adapt</h2><p>The most sophisticated AI-native learning platforms implement memory architectures inspired by human cognition, incorporating episodic memory for recent interactions, semantic memory for abstracted patterns, and procedural memory for learned preferences and behaviors. This multi-layered approach enables systems to maintain context over time while continuously refining their understanding of user needs.</p><p>Rather than treating each interaction as isolated, these systems build cumulative knowledge about user interests, expertise levels, and learning goals. They can recognize when someone is exploring a new domain versus deepening existing knowledge, and adjust their content generation accordingly. This longitudinal understanding becomes increasingly valuable as it enables the system to suggest unexpected but relevant connections between seemingly disparate areas of interest.</p><h2 id=the-promise-of-adaptive-content-creation>The Promise of Adaptive Content Creation</h2><p>The ultimate vision extends beyond personalized recommendation to adaptive content creation. Imagine a system that can take a classic work like Sun Tzu&rsquo;s &ldquo;The Art of War&rdquo; and generate multiple interpretations tailored to different audiences and applications. For a business professional, it might emphasize strategic planning and competitive analysis. For a parent, it could explore family dynamics and conflict resolution. For a student, it might focus on historical context and philosophical implications.</p><p>Each version would maintain the core insights of the original work while presenting them through frameworks and examples that resonate with the specific audience. This approach recognizes that great ideas have universal applicability, but their accessibility depends heavily on how they&rsquo;re presented and contextualized.</p><h2 id=technical-implementation-and-practical-considerations>Technical Implementation and Practical Considerations</h2><p>Building these capabilities requires sophisticated orchestration of multiple AI systems working in concert. Content generation engines must work alongside user modeling systems, recommendation algorithms, and quality control mechanisms. The challenge lies not just in generating personalized content, but in ensuring it maintains accuracy, coherence, and genuine value while adapting to individual preferences.</p><p>Recent advances in multimodal AI and agent-based architectures provide the technical foundation for these applications. Tools like MCP (Model Context Protocol) servers enable modular, composable AI capabilities that can be combined and recombined to address specific user needs. This architectural approach allows for the kind of flexible, adaptive content generation that personalized learning requires.</p><h2 id=the-road-ahead>The Road Ahead</h2><p>As we look toward the future of AI-native learning platforms, the focus shifts from simple automation of existing processes to the creation of entirely new forms of educational experience. The most successful applications will be those that understand the deep relationship between content, context, and individual cognition, using this understanding to create learning experiences that are not just personalized, but genuinely transformative.</p><p>The transition from traditional content consumption to AI-enhanced learning represents more than a technological upgrade. It&rsquo;s a fundamental reimagining of how knowledge can be packaged, presented, and absorbed in ways that honor both the richness of human understanding and the unique cognitive patterns of individual learners. In this future, every question becomes an opportunity for personalized exploration, and every piece of content becomes a starting point for deeper, more meaningful engagement with ideas.</p></div></div></div><div id=modal-improving-and-scaling-coaching-through-llms class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("improving-and-scaling-coaching-through-llms")'>&#215;</button><h2>Improving and Scaling Coaching through LLMs</h2><p class=project-meta>August 15, 2024</p><div class=modal-content><p><em>Terry Chen, Allyson Lee</em></p><h2 id=abstract>Abstract</h2><p>Effective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. Coaches struggle to track students&rsquo; progress across multiple teams and projects, making it difficult to provide targeted feedback and adapt recommendations based on evolving student needs. Existing AI-based project management tools facilitate task tracking but fail to capture the nuanced ways students approach their work. Large Language Models (LLMs) have shown promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.</p><p>This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with a regulation gap to a peer who has addressed it in the past, and extracting personalized step-by-step frameworks for them to follow. Our system allows the user to record their conversation with a coach, then integrates our novel codebook, which includes regulation gap and key term definitions gathered across learning science literature, with the LLM to generate the user’s regulation gap. Utilizing the codebook and semantic matching, the system pairs one student who is currently facing that regulation gap and the other having addressed it in the past. This quarter, we explored how to facilitate conversations that will allow the novice to gain a new understanding of how to address their regulation gap, and created a tailored plan that keeps them accountable towards achieving it.</p><p>By utilizing LLMs throughout this system, this work advances AI-driven coaching methodologies, providing a scalable, adaptable solution for improving self-regulated learning in project-based environments. Our findings contribute to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.</p><p><img src=/images/projects/llmcoaching/novice2expert.png alt="Novice to Expert Learning System" loading=lazy>
<em>Novice to Expert Learning System</em></p><hr><h2 id=-try-the-live-prototype>🚀 Try the Live Prototype</h2><p><strong><a href=https://llmcoaching.streamlit.app/>➤ Access the LLM Coaching System Prototype</a></strong></p><p>Experience our LLM-enhanced coaching system firsthand through this interactive prototype.</p><hr><h2 id=introduction>Introduction</h2><p>Training college students to tackle complex, open-ended innovation work in a variety of fields such as design, entrepreneurship, and research is integral to preparing them for implementing innovative solutions upon graduation <a href=#ref1>[1]</a>. Project-based learning environments provide a stepping stone for preparing students to tackle complex, open-ended problems in design and research <a href=#ref2>[2]</a>, yet are insufficient in helping them develop the needed regulation skills to self-direct innovation work <a href=#ref3>[3]</a>. Previous research has found that college students who effectively regulate their learning have stronger problem solving skills, allowing them to succeed in innovation work <a href=#ref4>[4]</a>, <a href=#ref5>[5]</a>. Coaches help guide the development of these regulation skills, helping them develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach their desired outcomes <a href=#ref3>[3]</a>. This includes assessing risks, help-seeking, understanding one’s fears and anxieties, dealing with failure, and more.</p><p>Effective coaching requires understanding students&rsquo; work practices, identifying gaps in their problem-solving approaches, and suggesting targeted improvements. However, coaches face significant challenges in providing personalized guidance to multiple student teams. It is difficult for coaches to assess the effectiveness of their coaching, which can result in neglected alternative practice strategies or regulation gaps. Additionally, this process of recognizing and recording students’ regulation gaps and subsequent practice strategies is time consuming, resulting in coaches becoming increasingly overwhelmed as the number of student teams grows <a href=#ref6>[6]</a>.</p><p>Existing AI-based project management tools like Asana and Trello help coaches track tasks and feedback through functions such as goal setting, scheduling, task monitoring, reflections, and visualizing workflows <a href=#ref8>[8]</a>, <a href=#ref9>[9]</a>. However, these tools do not capture the nuanced ways students approach their work or help coaches provide targeted, context-based guidance. Recent advances in Large Language Models (LLMs) have shown promise in analyzing text-based interactions and generating outputs based on multi-step reasoning, but their application to educational coaching through contextual suggestions remains unexplored. LLMs can assist in understanding unstructured input to provide feedback to coaches by analyzing student practices, identifying recurring issues, and formulating practice suggestions based on similar cases. They can automate the generation of tailored practice strategies, thus reducing the cognitive load on coaches while ensuring consistent quality of feedback <a href=#ref7>[7]</a>. Furthermore, LLMs can bridge the gap between coaching sessions by delivering scaffolds and feedback to students, fostering continuous improvement in regulation skills.</p><p>To address these issues, we propose utilizing LLMs to develop and integrate the web application: Peer Connections, which pairs one student who is currently facing that regulation gap and the other having addressed it in the past, helps them to facilitate effective conversation which builds upon their understanding of the regulation gap and how to address it, then assist them creating a tailored plan that keeps them accountable towards achieving it.</p><p>To achieve this, we used a knowledge base of historical coaching cases documented through the <em>Context Assessment Plan</em> (CAP) notes from Northwestern&rsquo;s <em>Design, Technology, and Research</em> (DTR) Program (See <a href=#appendix-c>Appendix C</a>). The core idea behind these solutions is creating a system model that identifies similar notes by regulation gaps and also contextual similarity. By doing so, we can match people with similar problems for Peer Connections, as well as identify similar contextually similar cases to the one inputted and return commonly found regulation gaps and practice suggestions to achieve the Coaching Reflections and Practice Suggestions applications. We tested three different systems. First, we tested basic semantic matching using word embeddings, evaluating one version that took in the whole note, and another that weighted the semantic similarity of the regulation gap as 70%, and the rest of the note as the remaining 30%. Next, we tested the matching of notes using LLMs. We created a novel codebook consisting of tier 1 and tier 2 tags to categorize each note (See <a href=#appendix-e>Appendix E</a>). The codebook was created using a taxonomy of regulation skills and corresponding practice suggestions gathered across learning science literature <a href=#ref10>[10]</a>, <a href=#ref11>[11]</a>. It also features definitions for key terms often used in the CAP notes, such as “canvas”, “slice”, “design argument”, “sprint”, and more. Utilizing prompting techniques such as few-shot learning, we utilized the LLM to identify the tier 1 and tier 2 categories of all the notes, and matched notes on the basis of having the most similar tags. Lastly, we tested a hybrid of these two systems, utilizing the LLM to incorporate the tier 1 and tier 2 tags of each as metadata to our notes, then using semantic matching to identify the most similar cases.</p><p>To evaluate the effectiveness of these systems, we ran each model with the same three notes, and evaluated the k=5 returned similar notes. We utilized our codebook as a rubric to manually identify the tier 1 and tier 2 regulation gaps of each returned note. If the returned note had the same regulation gap as the original note, then we marked it as correct. We found that the semantic matching had significant errors when addressing emotional regulation gaps, since the terms used are often less repetitive than that used when identifying cognitive and metacognitive gaps. While the codebook prompted the LLM model showed promise in accurately identifying the regulation gaps of models, it required that we input all the notes, causing longer processing times and at times exceeding model context window. Our last model, which is a hybrid of LLMs using the codebook and semantic matching, was able to consistently and efficiently identify notes that had the same regulation gap, as well as were contextually similar. This indicates that while some changes still need to be made, we have established a working baseline model to achieve our goal of matching similar notes.</p><p>This quarter, we explored how to facilitate conversations that will allow the novice to gain a new understanding of how to address their regulation gap, and how to create actionable plans to hold them accountable towards doing it.</p><p>This paper makes the following contributions:</p><ol><li><p>We created a codebook consisting of regulation gap definitions and examples, drawing upon work gathered across learning science literature. Unlike existing educational taxonomies, this codebook provides structured categorization of regulation gaps through tiers 1 and 2. It synthesizes research self-regulation, scaffolding, and instructional coaching that can be utilized across a variety of use cases, such as AI-driven classification.</p></li><li><p>We utilized LLMs and semantic matching to create a metadata-based note-matching system to match students with similar regulation gaps, and provide coaches with regulation gaps and practice suggestions given the issue and context of their students.</p></li><li><p>We provided an empirical comparison of word embedding similarity, LLM-generated metadata, and a combined system for note retrieval, underscoring the need for a codebook.</p></li><li><p>We define what facilitating effective conversations between a novice peer and an experienced peer would look like, and begin to explore ways towards facilitating such conversations.</p></li></ol><p>This paper proceeds as follows. First, we introduce related work about AI-based coaching, Intelligent Tutoring Systems, LLMs in education, and structured coaching documentation. Then, we describe and justify our system designs, followed by our evaluation and the results of each model we tested, our preliminary user testing and results, culminating in final words about our limitations and future directions for the project.</p><h2 id=related-work>Related Work</h2><p>Effective coaching in project-based learning environments requires identifying students&rsquo; self-regulation gaps, providing targeted feedback, and facilitating continuous improvement. Research has explored AI-driven coaching, self-regulated learning, and human-AI collaboration in education. However, existing approaches struggle to provide scalable, context-specific coaching support. Our work builds upon these research areas by integrating Large Language Model (LLM)-based analysis with structured cognitive models to enhance coaching efficacy.</p><p>Intelligent Tutoring Systems (ITS) have been extensively studied for delivering automated feedback in structured learning domains such as mathematics and programming. These systems employ artificial intelligence techniques to provide personalized and adaptive instruction, modeling students&rsquo; psychological states, prior knowledge, skills, and preferences <a href=#ref12>[12]</a>. ITS have been widely applied in computer science education, medical training, and mathematics learning, demonstrating their ability to provide structured, task-level feedback <a href=#ref13>[13]</a>. However, while ITS excel in well-defined domains with clear problem-solving steps, they often struggle in contexts where strategies are not easily codified, such as research coaching and innovation work. AI-driven coaching systems that extend beyond traditional ITS have been developed to support open-ended learning tasks, such as entrepreneurship coaching, where metacognitive processes like problem articulation, risk assessment, and strategic planning are essential <a href=#ref14>[14]</a>. Our approach builds on these advancements by integrating LLM-driven semantic matching with structured coaching documentation, ensuring that feedback is personalized and context-aware.</p><p>Large language models have shown promise in augmenting human expertise across various domains, including education and coaching. LLMs can generate reflective prompts, assist with problem diagnosis, and provide personalized guidance based on historical data <a href=#ref15>[15]</a>. However, a major challenge in applying LLMs to coaching is ensuring reliability, as these models are prone to hallucinations, lack domain specificity, and struggle with contextual reasoning <a href=#ref16>[16]</a>. Recent research has sought to mitigate these challenges by grounding LLM outputs in structured cognitive models that encode domain knowledge, allowing AI-generated feedback to align more closely with expert coaching strategies <a href=#ref17>[17]</a>. Our system builds on this approach by combining LLM-driven semantic matching with structured metadata for coaching notes, ensuring that generated suggestions are not only relevant but also informed by prior cases.</p><p>Structured reflection tools, such as Context Assessment Plan (CAP) notes, have been widely used to document coaching insights and track student progress over time <a href=#ref18>[18]</a>. Prior research has shown that these tools enhance coaching effectiveness by making students&rsquo; learning processes more visible to mentors, facilitating more targeted and data-driven feedback. However, manually analyzing CAP notes is time-intensive and cognitively demanding, limiting the extent to which insights can be effectively leveraged across multiple coaching sessions. Existing efforts to automate aspects of coaching documentation have primarily focused on extracting key themes or summarizing student progress but have not fully addressed the challenge of identifying recurring regulation gaps and providing tailored recommendations at scale <a href=#ref19>[19]</a>. Our work builds upon CAP notes by automating similarity analysis using LLMs to identify common regulation challenges and retrieve relevant past cases that align with a student&rsquo;s current needs. This approach reduces the cognitive burden on coaches while ensuring that recommendations are grounded in expert-validated coaching strategies, making feedback more actionable and contextually relevant.</p><p>When having a conversation with a peer, there has also been some discourse about how to scaffold peer-questioning strategies to facilitate metacognition. The research suggests scaffolding questions that include: clarification/elaboration questions, counter-arguments, and context/perspective-oriented questions <a href=#ref20>[20]</a>. However, the most significant gap in this research was that there was no statistically significant improvement in quality of the peer-generated questions, which they attribute to two possible reasons &ldquo;(a) the scaffolds themselves may have been ineffective in supporting students to have better questioning skills; and (b) students may have needed additional training or modeling on how to use the provided scaffolds&rdquo; <a href=#ref20>[20]</a>. Thus, leaving space to explore different methods towards facilitating effective questioning, prompting, and conversation.</p><p>By synthesizing research on AI-based coaching, ITS, LLMs in education, and structured coaching documentation, our work introduces a hybrid LLM-coaching system that enhances scalability, adaptability, and feedback quality in project-based learning environments. Unlike prior work that focuses on either structured tutoring or broad AI-driven feedback generation, our system integrates LLM-driven semantic matching with structured cognitive models to provide adaptive, context-aware coaching support. In doing so, we extend the capabilities of AI-driven coaching by dynamically retrieving and adapting practice suggestions from historical coaching cases. We also aim to facilitate better conversations between peers, in order to allow them to address their regulation gap faster. By addressing these limitations in existing research, our work contributes to the broader field of AI-enhanced education and human-AI collaboration, demonstrating a scalable approach to improving coaching efficacy in complex, project-based learning environments.</p><h2 id=system-description>System Description</h2><p>Our system consists of three key components. First, users will be able to upload an audio recording of their conversation with the coach. Then, the user will be matched with a student who had experienced and addressed that regulation gap in the past through our student regulation gap analysis system. Lastly, our system will assist them in facilitating a conversation that guides how they will plan their next sprint.</p><h3 id=1-speech-to-text-transcription>1. Speech-to-text Transcription</h3><p>The first component of our system utilizes speech-to-text transcription with NLP-based feedback, to intake a student&rsquo;s audio recording of their conversation with the coach, allowing the system to analyze the student&rsquo;s regulation gap, while reducing the time restraints required by CAP notes.</p><p><img src=/images/projects/llmcoaching/figure1.png alt="Speech-to-text Transcription Interface" loading=lazy>
<em>Figure 1: Speech-to-text Transcription System Interface</em></p><h3 id=2-student-regulation-gap-analysis-system>2. Student regulation gap analysis system</h3><p>Thereafter, we employ a student regulation gap analysis system that leverages vector embeddings and large language models to identify patterns in student regulation behaviors across learning contexts. The system provides targeted coaching suggestions based on similar historical cases, addressing the challenge of effective coaching for developing regulation skills in design, research, and STEM innovation.</p><p>Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach. Initially, student regulation notes are pre-processed without a reasoning model to include metadata on tier 1 and tier 2 regulation gaps, then encoded into text embeddings. The vector database retrieves the most similar historical cases, which are then sent along with the original query to an LLM (Deepseek). The LLM generates a structured response including a diagnosis of potential regulation gaps, practice suggestions targeted to these gaps, and references to similar historical cases. This approach grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.</p><p><img src=/images/projects/llmcoaching/figure2.png alt="Student Regulation Gap Analysis" loading=lazy>
<em>Figure 2: Student Regulation Gap Analysis Page</em></p><h4 id=i-data-preprocessing>I. Data Preprocessing</h4><p>We exported all existing notes from the CAP (Context-Assessment-Plan) note system and encountered several challenges during preprocessing. Many notes lacked clear assessment sections identifying regulation gaps, provided insufficient context about the student&rsquo;s situation, and used inconsistent terminology to describe similar regulation gaps. To address these issues, we removed duplicated notes and filtered out entries with incomplete fields. To better structure the data for our system, we standardized the information into several key fields.</p><p>These fields include a unique identifier for each case, the key learning gap identified in the assessment, additional information such as project details, title, context, and plan, the complete original coaching note, the project name, high-level regulation categories (e.g., &ldquo;Cognitive,&rdquo; &ldquo;Metacognitive&rdquo;), and more specific skill categories (e.g., &ldquo;Critical thinking,&rdquo; &ldquo;Forming feasible plans&rdquo;). The last two fields—tier1_categories and tier2_categories—were generated using the Deepseek reasoning model with a specialized regulation skills codebook that provides the conceptual backbone for our semantic matching approach.</p><p><img src=/images/projects/llmcoaching/figure3.png alt="CAP Note Structure" loading=lazy>
<em>Figure 3: Organized CAP Note Structure</em></p><h4 id=ii-the-regulation-skills-codebook-structured-knowledge-for-semantic-matching>II. The Regulation Skills Codebook: Structured Knowledge for Semantic Matching</h4><p>The core innovation of our system is the integration of a structured codebook that categorizes student regulation gaps according to a research-grounded framework. The codebook provides a hierarchical organization of regulation skills across three domains, as detailed in <a href=#appendix-e>Appendix E</a>. This framework serves as the foundation for our semantic matching system.</p><p>The codebook defines three main categories of regulation skills: <em>Cognitive Skills</em>, which encompass abilities for approaching problems with unknown answers; <em>Metacognitive Skills</em>, which include capacities for planning, help-seeking, collaboration, and reflection; and <em>Emotional Regulation</em>, which covers dispositions toward self and learning that affect motivation.</p><h4 id=iii-bridging-language-and-concepts-semantic-enrichment>III. Bridging Language and Concepts: Semantic Enrichment</h4><p>The codebook functions as a semantic bridge between varied descriptions of similar problems. Consider examples such as &ldquo;Student struggles to create clear visual representations of system architecture&rdquo; (categorized as Cognitive > Representing problem and solution spaces), &ldquo;Prototype missing key feature&rdquo; (also Cognitive > Representing problem and solution spaces), &ldquo;Not slicing work to risk&rdquo; (Metacognitive > Planning effective iterations), and &ldquo;Student gets stuck and then stops thinking about strategy&rdquo; (Emotional > Embracing challenges/learning/independence). Even without textual similarity, the codebook categorization reveals conceptual relationships. For instance, &ldquo;stopping too soon with examples&rdquo; and &ldquo;not thinking deeply about risks&rdquo; would both be categorized under &ldquo;Critical thinking and argumentation,&rdquo; enabling the system to recognize their conceptual similarity despite different phrasing.</p><p>This structured approach provides several advantages. It standardizes vocabulary across different coaching contexts, reveals underlying patterns in student regulation behavior, enables transfer of coaching knowledge across project domains, and prioritizes skill-based similarity over surface-level textual matching.</p><table><thead><tr><th>Gap Description</th><th>Tier 1</th><th>Tier 2</th></tr></thead><tbody><tr><td>&ldquo;Student struggles to create clear visual representations of system architecture&rdquo;</td><td>Cognitive</td><td>Representing problem and solution spaces</td></tr><tr><td>&ldquo;Prototype missing key feature&rdquo;</td><td>Cognitive</td><td>Representing problem and solution spaces</td></tr><tr><td>&ldquo;Not slicing work to risk&rdquo;</td><td>Metacognitive</td><td>Planning effective iterations</td></tr><tr><td>&ldquo;Jiayi gets stuck and then stops thinking about strategy&rdquo;</td><td>Emotional</td><td>Embracing challenges/learning/independence</td></tr></tbody></table><h4 id=iv-similarity-methods-for-regulation-gap-analysis>IV. Similarity Methods for Regulation Gap Analysis</h4><p>To find cases with the same regulation gaps, we started with the semantic-based approach and gradually enhanced our methods. Our initial baseline approach uses vector embeddings to find similar cases based on textual similarity. We encode the full text of student issues using OpenAI&rsquo;s ada-002 embedding model and compare them using cosine similarity. This pure semantic similarity approach treats all text equally and serves as a useful baseline. However, while straightforward, this approach often prioritizes surface-level similarities like project domain rather than underlying regulation patterns.</p><p>To improve matching relevance, we enhanced our approach by separating the regulation gap description from contextual information, applying higher weight to the gap text (default: 0.7 for regulation gap, 0.3 for other content). The gap text often contains the most critical information about the learning challenge, so it deserves higher weight. This weighted semantic similarity approach better identifies regulation similarities even when project contexts differ, as it emphasizes the specific regulation gap rather than surrounding information.</p><p>However, we also realized limitations with the semantic-based matching methods, namely that semantic similarity only worked when there were apparent keywords to match. To address this limitation, we created a comprehensive codebook containing regulation gap definitions and examples, using a reasoning model (Deepseek-reasoning) to generate metadata with tier 1 and tier 2 regulation gap tags for enhanced matching. This LLM reasoning with codebook approach assigns the highest weight (0.5) to tier 2 categories and a lower weight (0.1) to tier 1 categories, with gap text and other content each receiving 0.2 weight. This metadata represents specific regulation skills, allowing the system to match cases addressing similar skills regardless of how they&rsquo;re described or in what context they appear.</p><h4 id=v-applying-the-codebook-in-evaluation-scripts>V. Applying the Codebook in Evaluation Scripts</h4><p>The codebook plays a crucial role in the tiered similarity evaluation methods. The categorization process begins with the categorize_regulation_gaps_deepseek_v0.2.py script, which uses the codebook definitions to classify each gap text into tier 1 and tier 2 categories. Each gap is analyzed against the codebook&rsquo;s framework, identifying which cognitive, metacognitive, or emotional regulation skills are being addressed. These categorizations are then stored in the tiered_weighted_cases.json file as structured metadata for later retrieval and analysis.</p><p>The codebook improves evaluation outcomes in several important ways. It provides a standardized vocabulary with a consistent framework of terms and concepts, helping to match cases that use different wording but address the same underlying skills. It enables contextual understanding by categorizing gaps according to the codebook, allowing the system to understand the deeper educational context beyond surface-level language similarities. The codebook&rsquo;s three-category structure (cognitive, metacognitive, emotional) aligns with research on key regulation skills central to the situated practice system, providing coaching concept integration. Even when projects differ completely, the codebook categories help identify transferable skills and learning patterns across domains, enabling cross-project relevance. The higher weight given to tier 2 categories (0.5) in the tiered similarity approach reflects their importance in precisely identifying the specific skill gaps being addressed.</p><p>For instance, when processing a gap like &ldquo;stopping too soon with examples,&rdquo; without the codebook approach, the system might only match cases with similar phrasing about &ldquo;stopping&rdquo; or &ldquo;examples.&rdquo; With the codebook integration, this gap is properly categorized as &ldquo;Cognitive > Critical thinking and argumentation,&rdquo; allowing matches to conceptually similar gaps like &ldquo;not thinking deeply enough about risks&rdquo; even when the phrasing differs completely.</p><h3 id=3-facilitating-conversation-and-formulating-tailored-plans>3. Facilitating Conversation and Formulating Tailored Plans</h3><p>To generate helpful and contextualized conversation facilitation questions, we created a codebook based on Choi et al.&rsquo;s question methodology, then prompted an LLM to generate responses based on conversation context (streamed) and codebook <a href=#ref20>[20]</a> (See <a href=#appendix-f>Appendix F</a>). The codebook and question generation is still within its early stages, until we conduct more user testing to discover the most pertinent questions to ask. Additionally, while not implemented yet, formulating tailored plans would take place in the form of filling out their sprint log/plan for their week, which they would be either linked to or the system will parse it into the sheet for them. This will allow them to enact their plans to address their regulation gap while still incorporating the tasks they need to complete for the week.</p><p><img src=/images/projects/llmcoaching/figure4.png alt="Discussion Questions Interface" loading=lazy>
<em>Figure 4: Suggested Discussion Questions</em></p><h2 id=study--experiment--deployment>Study / Experiment / Deployment</h2><p>We conducted manual qualitative coding on a set of 28 CAP notes utilizing our codebook definitions to use as ground truth to evaluate our model against. When comparing our tier 2 codes on these notes with the codes given by our system, we observed a precision of 0.875, and a recall of 0.893. Given the possible subjectivity and overlap of regulation gaps, these results suggest we can rely on our system to accurately categorize student cases’ regulation gaps.</p><p>Next, we investigated how to facilitate effective conversation between the peers once they are matched. Most notably, how do we define what an “effective” conversation looks like?</p><p>To begin, we conducted some casual conversations with peers in the DTR program, asking them to explain about an issue they struggled with in the past, and how they were able to overcome it. Our conversations had no structure, as it was just to give us an idea of how students talked about regulation with a peer. From this preliminary research, we observed the main finding that experienced students struggle to articulate their experience in a way that is helpful to the peer. This is due to several factors:</p><p>A lack of project context/terminology
Not articulating their experience addressing the regulation gap in a step-by-step, clear way, making it hard for the peer to abstract their thinking process into a high-level framework they could use
Both peers have trouble remembering what they experienced and what actionable next steps to take</p><p>Based upon these findings, we defined an &ldquo;effective&rdquo; conversation to be one that helps them create a more concrete, actionable plan than they had before, and that they actually implement this and solve their regulation gap faster. We then identified this causal structure for which to guide our prototype and user testing:</p><p><img src=/images/projects/llmcoaching/diagram.png alt="Causal Structure Diagram" loading=lazy>
<em>Figure: Causal structure diagram showing the relationship between peer conversations and regulation gap resolution</em></p><p>Given this causal diagram, we set up a second, more formal user testing scenario. Given the work of Choi et al., which highlights how scaffolding peer-questioning strategies can help facilitate metacognition, we formulated a list of questions that fell under the clarification/elaboration and ⁠⁠context/perspective-oriented categories suggested in the paper <a href=#ref20>[20]</a>. We created two lists of questions, one for the novice and one for the experienced peer, to guide the conversation, in the goal of assisting the experienced peer in articulating their experience in a step-by-step manner, and helping the novice understand how they can apply this to their own situation (See <a href=#appendix-g>Appendix G</a>).</p><p>We then conducted some preliminary user testing, by having one of us act as the experienced peer, and having another student in DTR be the novice. We simulated the conversation by having the novice ask us the questions provided in the scaffold, while we acted as the experienced peer answering the questions and providing advice. To qualitatively assess how the conversation influenced the peer, we asked them the same questions before and after (See <a href=#appendix-h>Appendix H</a>). During one of our user tests, one of our users, who struggled with the fear of imperfection, demonstrated this transformation:</p><table><thead><tr><th>Before peer conversation</th><th>After peer conversation</th></tr></thead><tbody><tr><td>&ldquo;I have no time to get things done, since my internship is starting tomorrow. There&rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it&rsquo;s even lower.&rdquo;</td><td>&ldquo;You know what, I&rsquo;m gonna send this to my partner first, and then I&rsquo;m gonna send this to Haoqi.&rdquo;</td></tr></tbody></table><p>This illustrates multiple findings, first that the student shifted their mindset towards already deciding that she wasn&rsquo;t going to get what she wanted to do, despite not having started yet, to taking actionable steps towards working on her project and addressing her regulation gap. Furthermore, the student changed the steps they were going to dedicate time halfway between her tasks for the week to send deliverables over to the coach. The next step would be to follow up and see if this was accomplished, and how it affected her project outcome and regulation.</p><h2 id=discussion>Discussion</h2><p>Our system effectively facilitates AI-enhanced coaching by leveraging Large Language Models (LLMs) and structured metadata to support project-based learning environments. By integrating semantic matching with structured codebook metadata, our approach identifies relevant coaching cases, reducing cognitive load on mentors while maintaining high-quality, context-aware feedback. We also began to explore how to facilitate effective and productive conversation amongst peers about regulation gaps. This discussion outlines the generalizable design elements and techniques that contributed to the prototype’s effectiveness, highlighting key takeaways for future socio-technical system development.
<strong>Hybrid AI-Driven Case Retrieval:</strong> Our system employs a hybrid approach that combines LLM-driven metadata tagging with traditional semantic matching. While pure semantic similarity methods struggled to capture nuanced regulation gaps due to limited repetitive terminology, and LLM-based approaches returned too many broadly relevant cases, integrating both techniques enabled <em>precision</em> in retrieving the most relevant coaching cases. This demonstrates the efficacy of hybrid AI-driven retrieval in domains where contextual similarity and structured knowledge are both crucial. Future systems designed for education or expert-driven fields can benefit from this combined methodology to ensure both relevance and specificity in recommendations.</p><p><strong>Structured Codebooks for Domain-Specific AI Applications:</strong> A key enabler of our system&rsquo;s success was the development of a structured codebook that categorizes regulation gaps into tiered classifications, allowing advanced reasoning models to more accurately categorize regulation gaps. By leveraging <em>cognitive</em>, <em>metacognitive</em>, and <em>emotional</em> regulation categories, the system grounded LLM-based reasoning in expert-validated pedagogical frameworks and effectively addresses the issue of LLM hallucinations. The implication for broader AI applications is that structured codebooks can serve as a mechanism to guide AI reasoning, especially for test time scaled models, and in fields requiring human-like judgment and contextual understanding.</p><p><strong>Scaffolding productive conversations for addressing regulation gaps:</strong> Providing enough context and terminology of the other&rsquo;s project and issue is crucial to set them up for success. Additionally, adopting peer-questioning scaffolds assists in driving a conversation where the more experienced peer can better articulate and reflect upon their step-by-step strategy to addressing their regulation gap, which can be adapted and applied by the novice.</p><h2 id=limitations-and-future-work>Limitations and Future Work</h2><p>There are inherent limitations in our current approach that necessitate attention. The CAP note system is succinctly written and does not always provide sufficient context for robust reasoning. Additionally, our codebook currently focuses primarily on high-level descriptions of regulation gaps along with their examples, which makes it difficult for the language model to develop domain-specific knowledge and reasoning for effective categorization.
To address these limitations and plan for iterative improvements, we will work with CAP note developers to identify ways toward either:</p><ul><li>Improving clarity of writing in notes</li><li>Collecting more data through alternative data sources (SIG meeting transcripts, etc.)</li></ul><p>For future work, we propose developing a sub-categorized codebook that further segments existing regulation gaps and contains specific examples along with reasoning chains for arriving at regulation gap categorizations. With such a codebook, we can first perform a tier 1 categorization of the gap to route to a corresponding model identifying each of the subcategories for further reasoning (using two sets of language models and corresponding reasoning prompts) for few-shot learning and prompt-based LLM-enabled categorization.</p><p>As LLMs continue to progress, we believe there is merit in more sophisticated reasoning methods such as the use of external knowledge bases or memory systems for persistent storage of student regulation gap progression. These approaches could enable more tailored and precise gap understanding, ultimately leading to more effective coaching support for developing regulation skills in design, research, and STEM innovation contexts.</p><p>In terms of facilitating effective conversation between peers, we plan to continually design testing strategies of feature slices, focusing on evaluating what intervention strategies are most effective in facilitating peers to more clearly articulate and discuss their task plans in context of their regulation gaps, thereby facilitating meaningful peer to peer interaction enabling effective regulation skill learning. This involves testing whether real time generation of follow up questions for explanation elaboration or regulation skill connection allows better articulation of questions during peer to peer conversations, as well as whether more context regarding the peer’s project and regulation skills should be provided in advance to foster more meaningful conversations grounded in discussion of actionable next steps for regulation skill learning.</p><p>Most notably, there is still more work to be done with testing these strategies, specifically figuring out how to ensure that it impacts students not just in the moment, but in the long-term. To do so, we plan to run formal user tests which both qualitatively and quantitatively examine how they will and if they are addressing their regulation gap, by surveying them before using the prototype and having the conversation, after, and the subsequent weeks.</p><h2 id=references-in-ieee-format>References in IEEE Format</h2><p><strong>[1]</strong> E. M. Gerber, J. M. Olson, and R. L. D. Komarek, &ldquo;Extracurricular design-based learning: Preparing students for careers in innovation,&rdquo; International Journal of Engineering Education, vol. 28, no. 2, p. 317, 2012. {#ref1}</p><p><strong>[2]</strong> J. C. Dunlap, &ldquo;Problem-based learning and self-efficacy: How a capstone course prepares students for a profession,&rdquo; Educational Technology Research and Development, vol. 53, no. 1, pp. 65–83, Mar. 2005. {#ref2}</p><p><strong>[3]</strong> B. J. Zimmerman, &ldquo;Becoming a self-regulated learner: An overview,&rdquo; Theory Into Practice, vol. 41, no. 2, pp. 64–70, May 2002. {#ref3}</p><p><strong>[4]</strong> D. R. Lewis, M. Easterday, and C. Riesbeck, &ldquo;Research slices: Core processes for effective iteration in eder,&rdquo; EDeR. Educational Design Research, vol. 8, no. 1, 2024. {#ref4}</p><p><strong>[5]</strong> D. G. R. Lewis, S. E. Carlson, C. K. Riesbeck, E. M. Gerber, and M. W. Easterday, &ldquo;Encouraging engineering design teams to engage in expert iterative practices with tools to support coaching in problem-based learning,&rdquo; Journal of Engineering Education, vol. 112, no. 4, pp. 1012–1031, 2023. {#ref5}</p><p><strong>[6]</strong> E. J. Huang, D. R. Lewis, S. Gaudani, M. Easterday, and E. Gerber, &ldquo;Intelligent coaching systems: Understanding one-to-many coaching for ill-defined problem solving,&rdquo; Proceedings of the ACM on Human-Computer Interaction, vol. 7, no. CSCW1, pp. 138:1–138:24, Apr. 2023. {#ref6}</p><p><strong>[7]</strong> H. Zhang, M. Easterday, and S. Shah, &ldquo;Collaborative Research: Situated Practice Systems: Supporting Coaches and Students to Develop Regulation Skills for Design, Research, and STEM Innovation,&rdquo; National Science Foundation Grant Proposal, 2024. {#ref7}</p><p><strong>[8]</strong> Asana. <a href=https://asana.com/product>https://asana.com/product</a>, 2025. {#ref8}</p><p><strong>[9]</strong> Trello. <a href=https://trello.com/>https://trello.com/</a>, 2025. {#ref9}</p><p><strong>[10]</strong> Z. Xiao, X. Yuan, Q. V. Liao, R. Abdelghani, and P.-Y. Oudeyer, &ldquo;Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding,&rdquo; in Companion Proceedings of the 28th International Conference on Intelligent User Interfaces, pp. 75–78, 2023. {#ref10}</p><p><strong>[11]</strong> C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang, &ldquo;Can large language models transform computational social science?&rdquo; Computational Linguistics, vol. 50, no. 1, pp. 237–291, 2024. {#ref11}</p><p><strong>[12]</strong> K. F. Shaalan, &ldquo;An Intelligent Computer Assisted Language Learning System for Arabic Learners,&rdquo; Computer Assisted Language Learning, vol. 18, no. 1-2, pp. 81-108, Feb. 2005. {#ref12}</p><p><strong>[13]</strong> E. Mousavinasab, M. Zarifsanaiey, S. R. Niakan Kalhori, M. R. Rakhshan, M. Keikha, and A. Ghazi Saeedi, &ldquo;Intelligent Tutoring Systems: A Systematic Review of Characteristics, Applications, and Evaluation Methods,&rdquo; Interactive Learning Environments, vol. 29, no. 1, pp. 142-163, Jan. 2021. {#ref13}</p><p><strong>[14]</strong> M. Zawacki-Richter, V. Marín, M. Bond, and F. Gouverneur, &ldquo;Systematic Review of Research on Artificial Intelligence Applications in Higher Education – Where Are the Educators?&rdquo; International Journal of Educational Technology in Higher Education, vol. 16, no. 1, pp. 1-27, Dec. 2019. {#ref14}</p><p><strong>[15]</strong> P. Arnau-González, M. Arevalillo-Herráez, R. Albornoz-De Luise, and D. Arnau, &ldquo;A Methodological Approach to Enable Natural Language Interaction in an Intelligent Tutoring System,&rdquo; Computer Speech & Language, vol. 77, p. 101386, Jun. 2023. {#ref15}</p><p><strong>[16]</strong> B. Woolf, &ldquo;Building Intelligent Interactive Tutors: Student-Centered Strategies for Revolutionizing E-Learning,&rdquo; Morgan Kaufmann, 2009. {#ref16}</p><p><strong>[17]</strong> K. R. Koedinger and A. Corbett, &ldquo;Cognitive Tutors: Technology Bringing Learning Science to the Classroom,&rdquo; in The Cambridge Handbook of the Learning Sciences, R. K. Sawyer, Ed. Cambridge: Cambridge University Press, 2006, pp. 61-78. {#ref17}</p><p><strong>[18]</strong> J. Evens and J. Michael, &ldquo;One-on-One Tutoring by Humans and Computers,&rdquo; Routledge, 2006. {#ref18}</p><p><strong>[19]</strong> E. Wenger, &ldquo;Artificial Intelligence and Learning in Context: A Review,&rdquo; AI in Education Journal, vol. 21, no. 4, pp. 457-473, 2022. {#ref19}</p><p><strong>[20]</strong> I. Choi, S. M. Land, and A. J. Turgeon, &ldquo;Scaffolding peer-questioning strategies to facilitate metacognition during online small group discussion,&rdquo; Instructional Science, vol. 33, no. 5–6, pp. 483–511, 2005, doi: 10.1007/s11251-005-1277-4. {#ref20}</p><h2 id=appendix-b>Appendix B</h2><p>Improving and Scaling Coaching is part three of <em>Situated Practice Systems</em> (SPS), which offers tools to help coaches and learners understand work practices and develop self-directed innovation skills <a href=#ref7>[7]</a>.</p><p><img src=/images/projects/llmcoaching/appendixb.png alt="Appendix B Diagram" loading=lazy>
<em>Figure: Situated Practice Systems overview showing the three-part framework</em></p><h2 id=appendix-c>Appendix C</h2><p>CAP Notes helps coaches elicit information about a student&rsquo;s work issue and regulation gaps by tracking their activities, outputs, and reflections <a href=#ref7>[7]</a>.</p><p><img src=/images/projects/llmcoaching/appendixc.png alt="CAP Notes Structure" loading=lazy>
<em>Figure: CAP Notes structure showing Context, Assessment, and Plan components</em></p><h2 id=appendix-d>Appendix D</h2><p>Practice Objects contain information about a student&rsquo;s work issues, current and tracked regulation gaps, suggested practices, and practice traces <a href=#ref7>[7]</a>.</p><p><img src=/images/projects/llmcoaching/appendixd.png alt="Practice Objects Framework" loading=lazy>
<em>Figure: Practice Objects framework showing work issues, regulation gaps, and practice traces</em></p><h2 id=appendix-e>Appendix E</h2><p>Prompting of the LLM with the codebook incorporated.</p><p>“You are an expert in analyzing student regulation gaps. You need to categorize each CAP (Context, Assessment, Plan) note into these three tier 1 categories and their corresponding tier 2 categories. Each case may be categorized into multiple tier 1 and tier 2 categories:</p><ol><li><p>Cognitive: The student lacks skills for approaching problems with an unknown answer, or even, knowing what the problem is exactly. This includes:
Representing problem and solution spaces: The way the student structures or presents the information is not effectively supporting reasoning, analysis, or communication.
Example: Jacob struggles to create a representation that would help him show a working example and an example where the system breaks.
Assessing risks: The student struggles with identifying the riskiest risks and/or prioritizing them. They may skip ahead, do unnecessary and unimportant tasks, or have impractical plans due to not properly addressing and prioritizing the risk.
Example: John wasted time jumping ahead (he created multiple very detailed mockups) when he should’ve been focusing on addressing the riskiest risk at hand, which was identifying his target audience.
Critical thinking and argumentation: The student struggles to construct well-reasoned arguments supported by evidence or lacks a conceptual understanding of the task at hand. They might find it difficult to identify conceptual differences (they are treating concepts as too similar when they actually have meaningful differences).
Example: Eloise isn’t fully understanding what a regulation gap is and how to distinguish between the different types, and keeps taking the wrong changes to her prototypes because of that.</p></li><li><p>Metacognitive: The student struggles in areas of planning, help-seeking and collaboration, and reflection. This includes:
Forming feasible plans: The student struggles to develop structured, realistic, and actionable plans. This could include what their outcome should be and how to measure their outcome.
Example: Penelope overloaded herself with tasks this sprint and while she got a lot of it done, it wasn’t good work.
Planning effective iterations: The student struggles to create a deliverable that addresses the sprint’s riskiest risk. The student may struggle due to problems with slicing (breaking larger problems down), prioritization, or understanding the problem.
Example: David didn’t incorporate the feedback the coach gave him last week, so his work this week was not effective.
Leveraging resources and seeking help: The skill of identifying and utilizing available materials, expertise from others, and information to enhance their learning and problem-solving.</p></li><li><p>Emotional: The student has regulation and dispositions toward self and learning that affects their motivation, cognition, and metacognition. This includes:
Fears and anxieties: The student may have a fear of imperfection which causes them to shy away from the work, and/or doesn’t want to try things themselves.
Example: Jennifer had a well-planned sprint to carry out, but got too caught up trying to perfectly design the solution rather than creating a first prototype.
Embracing challenges and learning: The student tries to brute force their way through a solution or runs away from it, rather than thinking about the strategy and approach.
Example: Riley&rsquo;s system was not producing her optimal output, so she tried to overfit on one example rather than take a step back and observe what patterns are causing the system to fail.</p></li></ol><p>Based on the regulation gap (assessment), issue title, and context provided, categorize this case any categories (can be multiple) it applies to (Cognitive, Metacognitive, or Emotional).</p><p>To help you understand the notes better, here are some definitions of key terms:
Slice: A well-defined, manageable portion of a larger task or goal that can be completed within a short timeframe (typically a week), contributing to incremental progress.
Sprint: A time-boxed, iterative work cycle in agile project management, typically lasting 2 weeks, during which a team completes a set amount of work toward a project goal. Sprints emphasize rapid progress, continuous feedback, and adaptability.
Mysore: A structured learning and practice time where students work on their projects while a mentor provides feedback.
SIG: Special Interest Group meetings (SIG meetings) bring together undergraduate students, graduate students, and faculty working on different projects in the same research area. Each SIG is its own mini-studio initially led by a faculty member whose leadership fades over time as a graduate student SIG lead gains competencies in mentoring and becomes the leader of their own SIG. At the start of a sprint, teams share the outcome of their last sprint and present their current sprint plans for review. Halfway through a sprint, teams present their progress and SIG members help devise strategies for overcoming blockers.</p><p>Categorization should be based 80% on the “Assessment” part of the note, as this is the coach’s perceived regulation gap. Be careful not to get confused-you should be categorizing their regulation gap, not the implications of the regulation gap. For instance, let’s take a look at this CAP note:</p><p>&ldquo;Student: Improving and Scaling LLMs for Coaching
Improving and Scaling LLMs for Coaching | 2025-02-01
Items of Concern: Really hard to see how the conceptual examples would work / workout
Context: - Not sure why a lot of the categories are there.. not sure how they are actually relevant and what you learned from the analysis
Assessment: - Analysis not quite showing <em>why</em> the system suggested/showed what it showed?
Practice Suggestions: - [self-work] I couldn&rsquo;t tell from the output examples <em>why</em> the system was generating what it was generating. Can you think about a representation that would help you show, for an example category that you think are good and one that isn&rsquo;t, <em>why</em> the system got it right or wrong? Perhaps you can do this by showing component TF-IDF scores for each term, and also by showing some of the matching regulation gaps?&rdquo;
Looking at the note as a whole, one might identify it under the tier 2 Representing problem and solution spaces, Critical thinking and argumentation, Forming feasible plans, and Planning effective iterations. However, forming feasible plans and planning effective iterations were identified from the “practice suggestions” section, meaning that this is an implication of the regulation gap rather than the regulation gap itself.</p><p>In this case, the correct tier 2 categorization of the regulation gap would be Representing problem and solution spaces and Critical thinking and argumentation based on the “assessment” section.</p><p>Provide your reasoning and then your final category choice in this format:
Reasoning: [your step-by-step reasoning]
Categories: [tier 1 category name] [tier 2 category name]”</p><h2 id=appendix-f>Appendix F</h2><p>Codebook used to generate contextualized conversation facilitation questions <a href=#ref20>[20]</a>.
Generate conversation facilitation questions for the user. This scaffolding should include:</p><ul><li>Clarification/elaboration questions (seeking missing information)</li><li>Counter-arguments (expressing disagreement to prompt cognitive conflict)</li><li>Context/perspective-oriented questions (hypothetical scenarios, different viewpoints)</li></ul><h2 id=appendix-g>Appendix G</h2><p>Question scaffolds used for the second round of user testing.</p><p><strong>Questions novice might ask:</strong></p><ul><li>What was the issue you were encountering, was it recurring? How does it manifest in the tasks you did?</li><li>What were the steps you took to address it?</li><li>What made the process hard? Was there a moment where you did something that helped?</li><li>Were you able to utilize this framework/way of thinking in other instances where you experienced this regulation gap?</li><li>How were you able to break the cycle of constantly having this regulation gap?</li></ul><p><strong>Questions experienced peer might ask:</strong></p><ul><li>What is your issue and how is it manifesting in your work?</li><li>How can I use that framework to address my regulation gap (What analogies can you draw from my experience)?</li></ul><h2 id=appendix-h>Appendix H</h2><p>Question used in the before and after survey for the second round of user testing.</p><ul><li>What are the steps you will take to address your regulation gap this week?</li><li>What is the high-level framework for addressing this regulation gap?</li><li>What is your level of confidence in this plan/framework? (Scale of 1-10)</li><li>How comfortable are you with taking these steps? (Scale of 1-10)</li><li>What are your goals and project outcomes for this week?</li></ul></div></div></div><div id=modal-content-recommendation-systems class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("content-recommendation-systems")'>&#215;</button><h2>Content Recommendation Systems</h2><p class=project-meta>July 10, 2024</p><div class=modal-content><h1 id=building-intelligent-content-discovery-a-multi-modal-recommendation-system-for-podcast-consumption>Building Intelligent Content Discovery: A Multi-Modal Recommendation System for Podcast Consumption</h1><p>Modern content platforms face a fundamental challenge: how to help users discover relevant, high-quality content that matches their interests while avoiding the trap of information overload. This challenge becomes particularly acute in podcast consumption, where users need to find content that not only aligns with their interests but also fits different consumption contexts and scenarios. An ideal recommendation system addresses these core problems through a sophisticated approach that combines content-based filtering, collaborative filtering, and contextual understanding to create a truly personalized discovery experience.</p><h2 id=the-product-vision-beyond-simple-matching>The Product Vision: Beyond Simple Matching</h2><p>The core rationale behind the described recommendation system centers on the understanding that effective content discovery requires more than just matching keywords or categories. Users consume content differently based on their current context, mood, and goals. Sometimes they want to explore familiar territory and go deeper into topics they already know and love. Other times, they&rsquo;re seeking fresh perspectives or entirely new domains to expand their horizons. A truly intelligent recommendation system must understand these nuanced user states and adapt accordingly.</p><p>This system addresses three fundamental problems in content discovery. First, the overwhelming volume of available content makes it difficult for users to find what they&rsquo;re actually looking for. Second, traditional recommendation approaches often create filter bubbles that limit user discovery of potentially valuable content outside their established preferences. Third, most systems fail to account for the contextual nature of content consumption, treating all recommendation scenarios as identical when they&rsquo;re fundamentally different.</p><h2 id=a-phased-technical-evolution>A Phased Technical Evolution</h2><p>Rather than attempting to build the perfect recommendation system from day one, thi is a phased approach that evolves with our understanding of user behavior and available data. The initial phase focuses on content-based filtering using vector similarity matching, which provides an excellent solution to the cold start problem that plagues many recommendation systems. By representing both content and users as vectors and calculating cosine similarity, we can provide relevant recommendations even for new users with limited behavioral data.</p><p>The system&rsquo;s content modeling process involves sophisticated natural language processing to extract meaningful features from podcast titles, descriptions, and transcripts. We employ a two-level tagging structure that captures both broad categories and specific entities, allowing for precise content categorization while maintaining semantic relationships. This approach enables our system to understand not just what content is about, but how different pieces of content relate to each other conceptually.</p><p>As we accumulate user interaction data, the system transitions to collaborative filtering techniques that can discover hidden patterns and relationships between users and content. This phase leverages user-content interaction matrices and applies matrix factorization techniques to identify latent factors that influence user preferences. The collaborative approach excels at uncovering content relationships that might not be obvious from content analysis alone, leading to more sophisticated and accurate recommendations.</p><p>The long-term vision encompasses multi-modal personalized recommendations that incorporate contextual factors, user mood, consumption patterns, and environmental considerations. This future state will enable truly intelligent routing between different recommendation strategies based on real-time assessment of user needs and preferences.</p><h2 id=understanding-users-through-multiple-lenses>Understanding Users Through Multiple Lenses</h2><p>Effective personalization requires deep understanding of user preferences, and our system employs both explicit and implicit data collection strategies to build comprehensive user profiles. The explicit data collection process begins with carefully designed onboarding questions that efficiently capture essential user characteristics including age demographics, gender identity, interest areas, and inspirational figures. These questions are designed based on extensive analysis of successful onboarding flows from leading content platforms, optimized to gather maximum signal while minimizing user friction.</p><p>The system maps user responses to our content taxonomy through sophisticated attribute mapping. For example, when a user indicates admiration for Steve Jobs, the system automatically associates attributes like innovation, leadership, technology, design, and entrepreneurship with their profile. This mapping approach allows us to infer detailed interest profiles from relatively simple user inputs.</p><p>Implicit data collection focuses on user behavior patterns including play duration, completion rates, engagement actions like likes and shares, and negative signals such as skipping or hiding content. These behavioral signals often provide more accurate insight into true user preferences than explicit feedback, as they reflect actual consumption patterns rather than stated preferences. The system applies different weights to various interaction types, recognizing that sharing content represents a much stronger positive signal than simply playing it.</p><h2 id=intelligent-multi-channel-recall-strategy>Intelligent Multi-Channel Recall Strategy</h2><p>The described recommendation engine employs a sophisticated multi-channel recall strategy that combines different approaches to maximize both relevance and diversity. Vector similarity recall serves as the primary channel, leveraging user profile vectors to find semantically similar content through cosine similarity calculations in our embedding space. This approach excels at discovering content that matches the conceptual themes and topics that resonate with individual users.</p><p>Tag-based matching provides a complementary recall channel that ensures precise alignment with explicitly stated user interests. By directly matching user preference tags with content categorizations, this channel guarantees that recommendations include content from areas users have specifically indicated interest in. The tag-based approach offers high precision and interpretability, making it particularly valuable for users who have clear, well-defined interests.</p><p>Collaborative filtering recall identifies content enjoyed by users with similar preference patterns, enabling discovery of potentially relevant content that might not be obvious from content analysis alone. This channel is particularly effective at uncovering serendipitous recommendations and helping users discover new areas of interest based on the wisdom of crowds.</p><p>The system includes trending content recall as a quality backstop that ensures recommendations always include high-engagement, recent content. This channel serves multiple purposes: it provides a fallback when other channels produce insufficient results, introduces temporal relevance signals, and helps surface content that has demonstrated broad appeal across user segments.</p><h2 id=contextual-adaptation-through-recommendation-modes>Contextual Adaptation Through Recommendation Modes</h2><p>Understanding that users consume content differently based on their current context and goals, our system implements multiple recommendation modes that optimize for different user states and intentions. The default mode provides a balanced approach that combines relevance with exploration, offering users a mix of familiar and novel content that maintains engagement while encouraging discovery.</p><p>Fresh mode activates when the system detects user fatigue with current recommendations or when users explicitly seek the latest content. This mode heavily weights recent publications and trending topics, ensuring users stay current with evolving conversations in their areas of interest. The system automatically triggers fresh mode based on behavioral signals like consecutive content skipping or explicit user requests for newer content.</p><p>Familiar mode serves users who want to go deeper into established interest areas or seek stable, reliable content experiences. This mode emphasizes content similarity to previously consumed and highly-rated items, helping users build expertise in specific domains. The system may automatically suggest familiar mode for users demonstrating deep engagement with particular topics or content creators.</p><p>Explore mode encourages users to venture into new territory and expand their interest horizons. This mode deliberately introduces content from adjacent or entirely new categories, balanced with enough familiar elements to maintain relevance. The system&rsquo;s explore mode incorporates sophisticated algorithms to identify promising expansion areas based on user&rsquo;s existing interests and successful exploration patterns from similar users.</p><h2 id=quality-assurance-through-filtering-and-diversification>Quality Assurance Through Filtering and Diversification</h2><p>Raw similarity-based recommendations often suffer from homogeneity and filter bubble effects that limit user discovery and engagement. Our system addresses these challenges through sophisticated filtering and diversification algorithms that maintain relevance while ensuring recommendation variety. The filtering process removes content users have already consumed unless they&rsquo;re specifically in familiar mode, and applies quality thresholds based on content ratings and engagement metrics.</p><p>Diversification algorithms ensure that recommendation sets include variety across multiple dimensions including content categories, author backgrounds, content length, and topic complexity. The system employs greedy diversification selection that iteratively chooses content to maximize overall set diversity while maintaining individual item relevance. This approach prevents recommendation lists from becoming monotonous while still providing users with content that matches their interests.</p><p>The system also implements content quality filters that prioritize well-reviewed, highly-engaged content while giving newer content opportunities to surface based on early engagement signals. These quality mechanisms help maintain user trust in recommendations while supporting content creator discovery.</p><h2 id=real-time-learning-and-adaptation>Real-Time Learning and Adaptation</h2><p>Modern recommendation systems must continuously evolve based on user feedback and changing preferences. Our system implements real-time feedback processing that immediately incorporates user interactions into the recommendation engine. Different interaction types receive different weight values, with explicit positive actions like sharing carrying more influence than passive consumption metrics.</p><p>The system captures detailed interaction data including play duration, completion rates, engagement timing, and skip patterns to understand not just what users like, but how they engage with different types of content. This granular behavioral data enables sophisticated preference modeling that adapts recommendations based on consumption context and user engagement patterns.</p><p>When users provide strong positive or negative feedback, the system triggers immediate user vector updates to ensure subsequent recommendations reflect their latest preferences. This real-time adaptation capability ensures that the recommendation experience continuously improves as users interact with the platform.</p><h2 id=measuring-success-through-comprehensive-metrics>Measuring Success Through Comprehensive Metrics</h2><p>The effectiveness of our recommendation system is evaluated through multiple complementary metrics that capture different aspects of user satisfaction and engagement. Click-through rates measure the immediate appeal of recommendations, while completion rates and time-per-recommendation indicate whether users find recommended content genuinely valuable. Long-term metrics including user retention rates and subscription conversion provide insight into the system&rsquo;s impact on overall platform success.</p><p>Beyond traditional engagement metrics, we employ LLM-based evaluation frameworks that assess recommendation quality across multiple dimensions including personal context alignment, discovery value, diversity contribution, and fundamental quality indicators. This comprehensive evaluation approach ensures that our optimization efforts improve genuine user satisfaction rather than simply maximizing narrow engagement metrics.</p><p>The measurement framework also includes user feedback mechanisms that allow direct quality assessment through rating systems and explicit feedback collection. This human feedback serves both as a training signal for our algorithms and a validation mechanism for our automated quality assessments.</p><h2 id=looking-forward-the-future-of-intelligent-content-discovery>Looking Forward: The Future of Intelligent Content Discovery</h2><p>The described recommendation system represents a significant step forward in personalized content discovery, but it&rsquo;s designed as a foundation for even more sophisticated future capabilities. Planned enhancements include integration with large language models for natural language recommendation interfaces, advanced content segmentation for granular recommendations within longer content pieces, and cross-platform recommendation capabilities that understand user preferences across different content consumption contexts.</p><p>The system&rsquo;s modular architecture enables continuous enhancement and experimentation with new recommendation techniques while maintaining stable, high-quality user experiences. As we gather more user data and refine our understanding of content consumption patterns, the system will evolve to provide increasingly sophisticated and valuable recommendations that truly understand and serve individual user needs.</p><p>This approach to recommendation system design demonstrates that effective personalization requires deep understanding of both content and users, sophisticated technical implementation, and continuous adaptation based on real-world usage patterns. By focusing on the fundamental problems users face in content discovery and building flexible, evolving solutions, we can create recommendation experiences that genuinely enhance rather than simply automate the content discovery process.</p></div></div></div><div id=modal-multi-modal-creative-ad-generation class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("multi-modal-creative-ad-generation")'>&#215;</button><h2>Multi-modal Creative Ad Generation</h2><p class=project-meta>May 20, 2024</p><div class=modal-content><p>Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)
<a href="https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc">https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc</a></p><p>Credits: TikTok Creative Team</p><h1 id=building-agentic-workflows>Building Agentic Workflows</h1><h2 id=from-llms-to-agents>From LLMs to Agents</h2><p>The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren&rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.</p><h2 id=from-google-next-to-baidu-create>From Google Next to Baidu Create</h2><p>Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.</p><p>From a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.</p><p>From a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.</p><p>LLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.</p><p>We should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?</p><h2 id=11-consensus-task-specific-moe-agents-routing>1.1 Consensus: Task Specific, MoE, Agents, Routing</h2><p>Content that has reached consensus:</p><p>Most companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).</p><p>The MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.</p><p>The core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.</p><p>Implementing an MoE architecture typically requires the following steps:</p><ol><li><p>Define expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.</p></li><li><p>Train expert networks: Use labeled training data to train each expert network to obtain weights and parameters.</p></li><li><p>Allocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.</p></li><li><p>Summarize results: Weight and sum the output results of each expert network to get the final output.</p></li><li><p>Train the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.</p></li></ol><h3 id=longer-context-window---llm-routing>Longer Context Window -> LLM Routing</h3><p>At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.</p><h3 id=ai-routing-uses>AI Routing Uses</h3><p>Writesonic (<a href=https://writesonic.com>https://writesonic.com</a>) uses GPT Router for LLM Routing during AI Model Selection.</p><p>GPT Router (<a href=https://github.com/Writesonic/GPTRouter>https://github.com/Writesonic/GPTRouter</a>) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.client <span style=color:#f92672>import</span> GPTRouterClient
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.models <span style=color:#f92672>import</span> ModelGenerationRequest, GenerationParams
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.enums <span style=color:#f92672>import</span> ModelsEnum, ProvidersEnum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> GPTRouterClient(base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_base_url&#39;</span>, api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_api_key&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Write me a short poem&#34;</span>},
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>prompt_params <span style=color:#f92672>=</span> GenerationParams(messages<span style=color:#f92672>=</span>messages)
</span></span><span style=display:flex><span>claude2_request <span style=color:#f92672>=</span> ModelGenerationRequest(
</span></span><span style=display:flex><span>    model_name<span style=color:#f92672>=</span>ModelsEnum<span style=color:#f92672>.</span>CLAUDE_INSTANT_12,
</span></span><span style=display:flex><span>    provider_name<span style=color:#f92672>=</span>ProvidersEnum<span style=color:#f92672>.</span>ANTHROPIC<span style=color:#f92672>.</span>value,
</span></span><span style=display:flex><span>    order<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    prompt_params<span style=color:#f92672>=</span>prompt_params,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>generate(ordered_generation_requests<span style=color:#f92672>=</span>[claude2_request])
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text)
</span></span></code></pre></div><h2 id=12-non-consensus-scenarios-market-differentiation>1.2 Non-Consensus: Scenarios, Market, Differentiation</h2><p>Content that is still not determined:</p><p>What constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge</p><p>Many companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures</p><h2 id=how-agents-can-help-creators-achieve-10x-efficiency>How Agents Can Help Creators Achieve 10x Efficiency</h2><h3 id=21-agent-application-cases>2.1 Agent Application Cases</h3><h4 id=autogpt>AutoGPT</h4><p>AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters.
<a href=https://github.com/Significant-Gravitas/AutoGPT>https://github.com/Significant-Gravitas/AutoGPT</a></p><h4 id=gpt-researcher>GPT Researcher</h4><p>A GPT-based autonomous agent that conducts comprehensive online research on any given topic.
<a href=https://github.com/assafelovic/gpt-researcher>https://github.com/assafelovic/gpt-researcher</a></p><p>The advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.</p><h3 id=workflow-building---video-creation-example>Workflow Building - Video Creation Example</h3><p>Concept Design: Midjourney
Script + Storyboard: ChatGPT
AI Image Generation: Midjourney, Stable Diffusion, D3
AI Video: Runway, Pika, Pixverse, Morph Studio
Dialogue + Narration: Eleven Labs, Ruisheng
Sound Effects + Music: SUNO, UDIO, AUDIOGEN
Video Enhancement: Topaz Video
Subtitles + Editing: CapCut, JianYing</p><h3 id=22-improving-agent-user-experience>2.2 Improving Agent User Experience</h3><h4 id=personalized-memory--style-customization>Personalized Memory & Style Customization</h4><p>User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments.
Example: Typeface Brand Kit</p><h4 id=rewind--edit>Rewind & Edit</h4><p>User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality.
Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)</p><h4 id=choose-from-variations>Choose from Variations</h4><p>User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience.
Example: Typeface Image Generator (also supports favoriting)</p><h4 id=workflows-not-skills>Workflows, Not Skills</h4><p>User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation.
Example: Typeface Workflow (all capabilities presented at the appropriate stages)</p><h3 id=typeface---product-reference-from-former-adobe-cto>Typeface - Product Reference from Former Adobe CTO</h3><p><a href=https://www.typeface.ai>https://www.typeface.ai</a></p><p>Typeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft&rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)</p><h3 id=31-performance-data---40000-monthly-active-users-459-average-usage-time>3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time</h3><h3 id=32-feature-breakdown---customized-content-generation-for-brands>3.2 Feature Breakdown - Customized Content Generation for Brands</h3><p>Multiple Agent calls centered around the core document editing experience.</p><p>When users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.</p><h4 id=features>Features:</h4><h5 id=brands>Brands</h5><p>When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:</p><ul><li>Image Styles: Users can upload existing images for subsequent generation style adjustment.</li><li>Color Palettes: Users can upload brand color palettes to standardize generated image colors.</li><li>Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image.</li></ul><h5 id=projects>Projects</h5><p>When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:</p><ul><li>Create a new image</li><li>Create a product shot</li><li>Generate text</li><li>Create from template</li></ul><p>Additionally, users can select Refine to adjust generation language and tone (fixed options).</p><h5 id=create-an-image>Create an Image</h5><p>After clicking Create an image, users enter the image editing page with six integrated functions on the left: &ldquo;Add, select, extend, lighting, color, effects, adobe express.&rdquo; Users can generate and adjust images directly and favorite preferred generations.</p><h5 id=create-a-product-shot>Create a Product Shot</h5><p>The difference from Create an image is that Product shot includes specific products, while image isn&rsquo;t necessarily product-related.</p><h5 id=generate-text>Generate Text</h5><p>After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.</p><h5 id=templates>Templates</h5><p>Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.</p><h5 id=audiences>Audiences</h5><p>When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).</p><h5 id=integrations>Integrations</h5><p>These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.</p><p><a href=https://www.typeface.ai/product/integrations>https://www.typeface.ai/product/integrations</a></p><h6 id=microsoft-dynamics-365>Microsoft Dynamics 365</h6><p>Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.</p><h6 id=salesforce-marketing>Salesforce Marketing</h6><p>Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.</p><h6 id=google-bigquery>Google BigQuery</h6><p>Users can define audience segments with customer intelligence from BigQuery&rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.</p><h6 id=google-workspace>Google Workspace</h6><p>Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.</p><h6 id=microsoft-teams>Microsoft Teams</h6><p>Create content in Teams using Typeface&rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.</p><h2 id=summary>Summary</h2><p>Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.</p></div></div></div><div id=modal-billion-parameter-trend-insight-analysis-for-ads class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("billion-parameter-trend-insight-analysis-for-ads")'>&#215;</button><h2>Billion Parameter Trend Insight Analysis for Ads</h2><p class=project-meta>April 10, 2024</p><div class=modal-content><p>Analyze thousands of tiktoks to provide actionable trends & insights for key agencies. (Worked on multi-modal content understanding)
To be released on TikTok Creative Center (<a href=https://ads.tiktok.com/business/creativecenter/pc/en>https://ads.tiktok.com/business/creativecenter/pc/en</a>)</p><p>Credits: TikTok Creative Team</p><h2 id=tiktok-insight-spotlight-launches-at-advertiser-summit>TikTok Insight Spotlight Launches at Advertiser Summit</h2><p>Jun 3 - Excited to share that TikTok Insight Spotlight, a product I worked on during my time at TikTok, was officially unveiled at the company&rsquo;s annual advertiser summit on June 3rd, 2025. <a href=https://www.theverge.com/news/678255/tiktok-advertiser-summit-ai-targeting-data-seo>The Verge covered the launch extensively</a>, highlighting the AI-driven capabilities I helped develop.</p><p><img src=/docs/images/projects/insight/insight_spotlight.jpeg alt="TikTok Insight Spotlight Interface" loading=lazy>
<em>TikTok&rsquo;s Insight Spotlight interface showing AI-generated analysis - a product I contributed to while at TikTok</em></p><p>As The Verge reported: &ldquo;TikTok introduced a slew of new advertiser tools at the company&rsquo;s annual advertiser summit on June 3rd. The new products range from AI-powered ad tools to new features connecting creators and brands, but the overall picture is clear: advertiser content on TikTok is about to become much more tailored and specific.&rdquo;</p><p>The article specifically highlighted the core functionality I worked on: &ldquo;The company will give brands precise details about how their target audience is using the platform — including AI-generated suggestions on ads to run. Using a tool called Insight Spotlight, advertisers will be able to sort by user demographics and industry to see what videos users in the target group are watching and what keywords are associated with popular content.&rdquo;</p><h2 id=behind-the-product>Behind the Product:</h2><p>During my time at TikTok, I worked specifically on the multi-modal content understanding capabilities that power Insight Spotlight&rsquo;s core functionality. Here&rsquo;s how the features I helped develop work in practice:</p><h3 id=ai-generated-content-recommendations>AI-Generated Content Recommendations</h3><p>The Verge highlighted a key example of the system in action: &ldquo;In an example provided by TikTok, an AI-generated suggestion recommends that a brand &lsquo;produce video content focused on &lsquo;hormonal health&rsquo; for female, English-speaking users&rsquo; and to include a specific keyword.&rdquo; This type of precise, data-driven recommendation is made possible by the multi-modal analysis systems I contributed to.</p><h3 id=trend-prediction-through-content-analysis>Trend Prediction Through Content Analysis</h3><p>As The Verge noted: &ldquo;Another feature in Insight Spotlight analyzes users&rsquo; viewing history to identify types of content that are bubbling up.&rdquo; This capability stems from the advanced content understanding work I did, which enables the system to analyze both visual and audio elements of videos to predict emerging trends before they become mainstream.</p><p>The article captures the significance of this shift: &ldquo;TikTok rose in prominence partly because of its spin-the-wheel, seemingly random quality: anything or anyone could go viral overnight. Brands did their best to keep up by jumping on trends, using popular formats and songs, and partnered with influencers who seemed to be at the center. The new tools will give advertisers even more ways to specifically tailor their content toward what is already happening on the platform and what people are searching for and watching.&rdquo;</p><p>My work on multi-modal content understanding was focused on making this transition from random virality to predictable, data-driven insights possible. By analyzing thousands of TikToks across visual, audio, and textual dimensions, we created a system that can identify patterns and provide actionable recommendations to advertisers.</p><h3 id=platform-as-search-engine-vision>Platform-as-Search-Engine Vision</h3><p>The Verge perfectly captured the broader vision: &ldquo;In this way, advertiser tools are TikTok&rsquo;s equivalent of search engine optimization (SEO) — flooding the zone with content that attempts to capture organic user behaviors. The idea that TikTok can be used as a search engine has been around for several years, and the company says that one in four users search for something within 30 seconds of opening the app.&rdquo;</p><p>This transformation from entertainment platform to search-driven insights tool represents our team&rsquo;s work - using AI to understand not just what content performs, but why it performs and how brands can leverage those insights.</p><h1 id=beyond-data-the-evolution-of-ai-driven-insight-products-for-content-creation>Beyond Data: The Evolution of AI-Driven Insight Products for Content Creation</h1><h2 id=introduction-the-shifting-landscape-of-creative-ai-tools>Introduction: The Shifting Landscape of Creative AI Tools</h2><p>In the rapidly evolving space of AI-driven creative tools, we&rsquo;re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.</p><p>While many current GenAI applications focus heavily on content generation capabilities, the true creative bottleneck often isn&rsquo;t in the generation step itself. Rather, it lies in the quality of insights that inform and guide the creative process. Without meaningful data and analysis, even the most sophisticated generation tools produce generic, uninspired content.</p><p>This blog explores how data insight products are evolving alongside generative AI technologies, and how their integration could fundamentally transform content creation workflows.</p><h2 id=the-evolution-of-ai-capabilities>The Evolution of AI Capabilities</h2><p>The limitations of general-purpose large language models have become increasingly apparent when handling complex creative tasks. To address issues like hallucinations and improve task completion capabilities, the industry has largely reached a consensus around agent-based approaches.</p><p>Different types of agent workflows have emerged to address specific needs. Non-agentic workflows generate content linearly without backtracking, suitable for straightforward tasks. Reflection-based systems introduce iterative improvement cycles where the AI criticizes and refines its own outputs. Tool use capabilities enable function calls and web browsing for enhanced research capabilities.</p><p>More advanced systems implement planning algorithms that decompose complex tasks into manageable steps, similar to how human creators break down projects. At the frontier, multi-agent collaboration enables specialized AI agents to work together, each handling different aspects of a complex creative process.</p><p>This evolution toward more sophisticated agent architectures reflects a growing understanding that creative work isn&rsquo;t linear—it requires iteration, refinement, and the ability to leverage different capabilities at different stages of the process.</p><h2 id=the-workflow-challenge>The Workflow Challenge</h2><p>One of the key limitations in current AI creative products is their focus on isolated capabilities rather than integrated workflows. In the advertising and marketing industry, there&rsquo;s a high concentration of AI tools, but most provide only single functions or partial capabilities.</p><p>A content creator typically needs to move through multiple stages: gathering insights, analyzing competitors, developing concepts, generating scripts, creating visual assets, and optimizing the final product. Currently, this requires juggling multiple disconnected tools, manually transferring context between them, and piecing together a cohesive workflow.</p><p>Users don&rsquo;t simply need better individual tools—they need comprehensive workflows that connect these steps seamlessly. The value proposition shifts from &ldquo;what can this AI do?&rdquo; to &ldquo;how does this AI fit into my creative process?&rdquo; This represents a fundamental shift in how we should design and evaluate AI creative systems.</p><h2 id=market-analysis-of-insight-products>Market Analysis of Insight Products</h2><p>The current market for insight products shows several distinct categories, each addressing different aspects of the creative process. Here&rsquo;s a structured analysis of the landscape:</p><h3 id=ad-compilation-tools>Ad Compilation Tools</h3><p>Products in this category focus on collecting, organizing, and analyzing existing advertisements across platforms. Pipi Ads maintains a library of over 20 million TikTok ads with extensive filtering capabilities, allowing users to study successful campaigns and identify trending approaches. Foreplay offers a more workflow-oriented solution, enabling users to save ads from multiple platforms, organize them with custom tags, and build creative briefs based on existing successful content.</p><p>The value proposition of these tools centers on learning from what already works. By studying high-performing ads, creators can identify patterns and strategies that resonate with specific audiences. However, most of these tools stop at the analysis stage without directly connecting insights to content generation.</p><h3 id=competitor-analysis-tools>Competitor Analysis Tools</h3><p>Tools like Social Peta, Big Spy, and Story Clash provide deeper analysis of competitive activities. Social Peta offers insights into content distribution across 69 countries and 70 networks, analyzing multimedia types and dimensions. Big Spy enables cross-network ad searching with multiple filters, while Story Clash specializes in TikTok influencer tracking and performance analysis.</p><p>The competitive analysis market has grown substantially with the rise of social media advertising, with new players continuously entering the space to address specialized niches and platforms. These tools typically provide dashboard interfaces with various filters for monitoring competitor strategies, but most lack direct integration with content creation workflows.</p><h3 id=brand-insight-tools>Brand Insight Tools</h3><p>Social listening platforms like Springklr, Exolyt, and Keyhole monitor brand mentions and sentiment across social channels. These tools analyze both posts and comments, providing valuable data on how audiences perceive brands and their content. Springklr offers comprehensive post and comment analysis with sentiment tracking, while Exolyt specializes in TikTok-specific insights, comparing brand content with user-generated content.</p><p>Keyhole delivers profile analytics, social trend monitoring, and campaign tracking. These tools excel at capturing the audience&rsquo;s voice and identifying shifts in perception, but typically require significant manual analysis to translate these insights into actionable creative strategies.</p><h3 id=performance-analysis-tools>Performance Analysis Tools</h3><p>Platforms such as Social Insider, Motion App, and RivalQ focus on analyzing ad performance metrics. These tools help marketers understand what content performs best, with detailed analytics on engagement, conversion, and return on investment. By identifying high-performing content patterns, these tools can inform future creative decisions.</p><p>However, there remains a significant gap between identifying what works and automatically generating new content based on those insights. Most performance analysis tools remain separated from content creation workflows, requiring manual interpretation and application of insights.</p><h2 id=deep-dive-notable-products>Deep Dive: Notable Products</h2><p>Several standout products illustrate different approaches to the insight-generation challenge:</p><h3 id=tikbuddy-platform-specific-analysis>TikBuddy: Platform-Specific Analysis</h3><p>TikBuddy focuses exclusively on TikTok analytics, offering creator rankings by category, follower count, and growth rate. The tool provides comprehensive account performance monitoring and video data analysis through a convenient Chrome extension.</p><p>Its specialized focus allows for deeper platform-specific insights, but its utility is limited to a single platform and doesn&rsquo;t extend to content creation. Users must manually apply any insights gained to their creative process.</p><h3 id=foreplay-workflow-integration>Foreplay: Workflow Integration</h3><p>Foreplay stands out for its more integrated workflow approach. The platform enables users to collect ad content across platforms, preserve it even after platform deletion, and organize it with tags and categories. Its brief creation tools facilitate the transition from insight to execution, with support for brand information and specific generation requirements.</p><p>The platform&rsquo;s AI storyboard generator creates hooks and develops scripts based on collected insights. Foreplay also integrates discovery features organized by community, brand, and experts, alongside competitor monitoring capabilities.</p><p>This approach begins to bridge the gap between insights and creation, though the integration remains partial rather than fully automated.</p><h3 id=keyhole-deep-analytics>Keyhole: Deep Analytics</h3><p>Keyhole exemplifies the analytics-focused approach, tracking keywords and brand mentions with temporal context. The platform offers detailed post analysis, influencer identification, trending topics visualization, and profile analytics with optimization recommendations.</p><p>Its strength lies in comprehensive data collection and visualization, but like many analytics platforms, it requires significant human interpretation to translate insights into creative decisions.</p><h2 id=emerging-innovations-in-data-insight-products>Emerging Innovations in Data Insight Products</h2><p>The data insight landscape continues to evolve rapidly, with several notable innovations emerging:</p><p>Open source projects like Vanna are revolutionizing text-to-SQL capabilities, making database querying more accessible to non-technical users. These tools enable creators to extract specific insights from complex datasets without specialized database knowledge.</p><p>Recent startups are developing interactive data dashboards that visualize complex datasets in more intuitive ways, allowing for easier pattern identification and insight extraction. These tools employ advanced visualization techniques to make data more accessible and actionable.</p><p>User feedback aggregation tools are also gaining traction, automatically summarizing and categorizing customer sentiment from reviews and comments. These systems can identify common themes and concerns, providing valuable input for content creators looking to address audience needs.</p><p>The most promising innovations focus on reducing the cognitive load required to extract meaningful insights from data, making the path from analysis to action more direct and intuitive.</p><h2 id=the-future-insight-driven-content-generation>The Future: Insight-Driven Content Generation</h2><p>The next evolution in creative AI tools will likely center on high-quality content generation based on data insights. Current GenAI applications often produce unnecessary content redundancy—different from hallucinations, but equally problematic for effective communication.</p><p>The real creative barrier isn&rsquo;t typically in the generation process itself, but in the prompts—the insights that inform decision-making. When using agent-based systems, the quality of instructions and background information directly impacts the output quality.</p><p>For example, advanced AI systems can now decompose complex goals like &ldquo;How can a lifestyle channel creator get 1,000 subscribers on YouTube?&rdquo; into specific tasks: analyzing successful channels, generating targeted content ideas, and implementing optimization strategies. However, the quality of these recommendations depends entirely on the AI&rsquo;s access to relevant, accurate data about what actually works.</p><p>By leveraging sophisticated content analysis, we can identify truly effective patterns in high-performing content. Multimodal understanding can reveal why certain creative approaches resonate with specific audiences, providing creators with concrete, unique insights rather than generic advice.</p><p>The future lies in connecting these insights directly to the generation process—using what we know works as the foundation for creating new content that maintains brand uniqueness while leveraging proven patterns.</p><h2 id=conclusion-the-integration-opportunity>Conclusion: The Integration Opportunity</h2><p>Compared to other GenAI creative tools, insight products place greater emphasis on data quality and quantity. The next leap in AI-generated content quality will likely come from precise generation guided by robust insight data.</p><p>The most promising opportunity lies in creating systems that can automatically analyze successful content across platforms, extract meaningful patterns from this analysis, and directly translate these insights into generation guidance. This approach would produce highly targeted content that leverages proven patterns while maintaining brand distinctiveness.</p><p>As we move forward, the focus will shift from mere information generation toward sophisticated information synthesis—providing not just content, but content informed by actionable insights derived from real-world performance data. Organizations that successfully integrate insight gathering with content generation will gain a significant competitive advantage in an increasingly crowded digital landscape.</p><p>The future belongs not to those with the most powerful generative models, but to those who can effectively transform data into creative insight, and insight into compelling content.</p></div></div></div><div id=modal-llm-enabled-homework-helper-for-all-subjects class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("llm-enabled-homework-helper-for-all-subjects")'>&#215;</button><h2>LLM Enabled Homework Helper for All Subjects</h2><p class=project-meta>August 20, 2023</p><div class=modal-content><p>AI Homework helper with advanced reasoning and visualization for all school subjects. (Worked on LLM Reasoning)
<a href=https://www.gauthmath.com/>https://www.gauthmath.com/</a></p><p>Credits: Lexi Ling, Gauth Team</p></div></div></div><div id=modal-an-agentic-social-operating-system class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("an-agentic-social-operating-system")'>&#215;</button><h2>An Agentic Social Operating System</h2><p class=project-meta>April 15, 2023</p><div class=modal-content><h2 id=inspiring-insights-amplifying-voices-crowdlisteningcom>Inspiring insights, amplifying voices. (crowdlistening.com)</h2><p><img src=/images/posts/crowdlistening/webpage.png alt=Webpage loading=lazy></p><h2 id=from-content-aggregation-to-original-research>From Content Aggregation to Original Research</h2><p>Crowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. While being able to quantatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven&rsquo;t yet appeared in established reporting.</p><p>Deep research features provide professional-looking research reports, yet the contents are far from original, as they&rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet&rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they&rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.</p><p>I&rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlistening, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn&rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows.</p><p>I paused Crowdlistening in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I&rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).</p><p>What I&rsquo;m particularly interested in exploring and building with Crowdlistening is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.</p><h2 id=the-insight-paradox>The Insight Paradox</h2><p><img src=/images/posts/crowdlistening/insight_paradox.png alt="Insight Paradox" loading=lazy></p><p>Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can&rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlistening bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in &ldquo;Essence of Creativity&rdquo; - helping users understand massive amounts of information and generate meaningful insights when they &ldquo;don&rsquo;t know what output they want.&rdquo;</p><h2 id=technical-architecture-multi-modal-by-design>Technical Architecture: Multi-Modal by Design</h2><p>The rationale behind Crowdlistening&rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video&rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.</p><p><img src=/images/posts/crowdlistening/analysis_page.png alt="Analysis Page" loading=lazy></p><p>Crowdlistening&rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM&rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities.</p><p>This dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlistening can generate insights that reflect not just what is being said, but how it&rsquo;s being said, the surrounding context, and the audience&rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the &ldquo;insight paradox&rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlistening: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.</p><h2 id=detailed-analysis-capabilities>Detailed Analysis Capabilities</h2><p>The platform provides granular breakdowns of content performance and audience reactions. As shown in our analysis results page, users can explore specific themes, track sentiment over time, and identify the most engaging content types. This helps brands understand not just what is being said, but why certain content resonates with their audience.</p><p><img src=/images/posts/crowdlistening/analysis_results.png alt="Analysis Results" loading=lazy></p><p>The opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.</p><p><img src=/images/posts/crowdlistening/opinion_analysis.png alt="Opinion Analysis" loading=lazy></p><h2 id=the-mcp-advantage-accessible-functional-calls>The MCP Advantage: Accessible Functional Calls</h2><p><img src=/images/posts/crowdlistening/claude_client.png alt="Claude MCP Entrance" loading=lazy></p><p>We have integrated Model Context Protocols (MCPs) - an emerging standard that simplifies how LLMs interact with specialized tools and data sources. Rather than simple API calls, MCPs provide structured interfaces for LLMs to access specialized capabilities while maintaining context awareness throughout the analysis process.</p><p><img src=/images/posts/crowdlistening/claude_mcp_process.png alt="Claude MCP Process" loading=lazy></p><p>As shown here, when a user submits a research question, the system dynamically determines which analytical capabilities to deploy. The Claude interface serves as the orchestration layer, identifying relevant MCP tools to activate and calling them sequentially:</p><ol><li>First gathering baseline information through web search</li><li>Then performing targeted data collection via specialized TikTok MCP tools</li><li>Following with multi-layered analysis of videos and comments</li><li>Finally synthesizing everything into coherent, actionable insights</li></ol><p>This MCP-driven approach creates a dramatic efficiency improvement, reducing complex social media analysis from weeks to minutes while maintaining remarkable analytical depth.</p><h2 id=case-study---trump-tariffs>Case Study - Trump Tariffs</h2><p>To demonstrate Crowdlistening&rsquo;s capabilities, we conducted a comprehensive analysis of public sentiment regarding Trump&rsquo;s tariff policies. This serves as an excellent test case due to its complexity, polarizing nature, and economic impact.</p><p>When a user inputs the query about Trump&rsquo;s tariff policies, our system activates the appropriate MCP tools in sequence. First, it gathers factual background information on the policies themselves, as shown below:</p><p><img src=/images/posts/crowdlistening/trump_background.png alt="Research Background" loading=lazy></p><p>This background research provides context on what the current tariff policies are, including the 10% baseline tariff on all imports that took effect in April 2025, plus the higher &ldquo;reciprocal&rdquo; tariffs on countries with which the US has trade deficits (34% for China, 20% for the EU, and 24% for Japan).</p><p>Next, the system analyzes public opinion on these policies by examining social media content. The analysis reveals highly polarized reactions, categorized into three main perspectives:</p><p><img src=/images/posts/crowdlistening/public_opinion.png alt="Public Opinion" loading=lazy></p><p>The sentiment analysis dashboard shows that opinions on Trump&rsquo;s tariff policies are distributed as 38% supportive, 42% critical, and 20% neutral or mixed. This visualization helps brands and researchers quickly understand the overall public response landscape.</p><p><img src=/images/posts/crowdlistening/trump_sentiment.png alt="Trump Sentiment" loading=lazy></p><p>One of the most valuable outputs is our projected economic impact analysis. This data visualization clearly presents the concrete financial implications of these policies across multiple domains:</p><p><img src=/images/posts/crowdlistening/economic_impact.png alt="Economic Impact" loading=lazy></p><p>The analysis shows an estimated $1,300 annual cost increase per US household, a projected 0.8% reduction in long-run US GDP, significant auto price increases ($3,000 for US vehicles, $6,000 for imports), and warnings about market volatility.</p><p>Beyond simple pro/con sentiment, our opinion analysis feature categorizes specific viewpoints with remarkable granularity. For instance, when examining comments on related content, we can identify nuanced perspectives and their prevalence:</p><p><img src=/images/posts/crowdlistening/comment_analysis.png alt="Trump Comments" loading=lazy></p><p>This example shows how our system can identify several different comment themes, including positive views of content creators (37.5%), appreciation for intelligent discussion (25%), and concerns about media echo chambers (12.5%). This level of nuanced understanding would be impossible through traditional keyword or basic sentiment analysis.</p><h2 id=validation-and-impact>Validation and Impact</h2><p>Our solution has been validated through interviews with major brands like L&rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlistening enables:</p><ul><li>Rapid response to emerging trends</li><li>Deep understanding of consumer sentiment across demographics</li><li>Identification of microtrends before they become mainstream</li><li>Competitive intelligence at unprecedented scale</li></ul><h2 id=the-future-of-mcp-driven-research>The Future of MCP-Driven Research</h2><p>We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.</p><p>This approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude&rsquo;s interface) emerge as this paradigm gains traction.</p><p>For social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they&rsquo;re covered in traditional reporting.</p><h2 id=on-social-intelligence>On Social Intelligence</h2><p>Crowdlistening represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we&rsquo;re helping brands make more informed decisions faster than ever before.</p><p>As noted in &ldquo;Essence of Creativity,&rdquo; the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we&rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.</p><hr><p>Credits: This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Roy, Ruiwan, Qiping, Tongming and other members on the Creative team at TikTok, who I&rsquo;ve discussed early conceptions of this idea with.</p></div></div></div></div><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll(".theme-slide"),n=document.getElementById("themeNav");let e=0;t.forEach((e,t)=>{const s=document.createElement("button");s.onclick=()=>o(t),n.appendChild(s)});function s(){const t=n.querySelectorAll("button");t.forEach((t,n)=>{t.classList.toggle("active",n===e)})}function o(n){const o=t[0].offsetWidth;document.getElementById("themeSlides").scrollLeft=o*n,e=n,s()}s(),setInterval(()=>{e=(e+1)%t.length,o(e)},5e3)});function openModal(e){const t=document.getElementById(`modal-${e}`);t&&(t.classList.add("active"),document.body.style.overflow="hidden")}function closeModal(e){const t=document.getElementById(`modal-${e}`);t&&(t.classList.remove("active"),document.body.style.overflow="auto")}document.addEventListener("click",function(e){e.target.classList.contains("modal-overlay")&&(e.target.classList.remove("active"),document.body.style.overflow="auto")}),document.addEventListener("keydown",function(e){if(e.key==="Escape"){const e=document.querySelector(".modal-overlay.active");e&&(e.classList.remove("active"),document.body.style.overflow="auto")}})</script></main><footer class=footer><span>&copy; 2025 <a href=https://chenterry.com/>Terry Chen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=archived-link><span style=font-size:.85em;color:#888;margin-top:8px;display:block>To view archived content, <a href=/archived/ style=color:#666;text-decoration:underline>click here</a></span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>window.addEventListener("error",function(e){console.error("Page error:",e.error),console.error("Error details:",{message:e.message,filename:e.filename,lineno:e.lineno,colno:e.colno})}),history.scrollRestoration&&(history.scrollRestoration="manual"),window.addEventListener("load",function(){window.scrollTo(0,0)}),window.addEventListener("pageshow",function(e){e.persisted&&window.scrollTo(0,0)});let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>