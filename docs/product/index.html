<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<meta name="description" content="Hi, this is Terry. I&#39;m documenting my product ideas and learning notes in this blog. I&#39;m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.">
<meta name="keywords" content="AI, Machine Learning, Product Engineering, Investment Analysis, Multi-Agent Systems, Content Understanding, Emerging Technologies, Startup Analysis, AI Multipliers">
<meta name="author" content="Terry Chen">
<meta name="robots" content="index, follow">
<meta name="language" content="en-us">
<meta name="revisit-after" content="7 days">
<meta name="distribution" content="global">
<meta name="rating" content="general">
<link rel="canonical" href="https://chenterry.com/product/">


<meta property="og:title" content="Product | Terry Chen">
<meta property="og:description" content="Hi, this is Terry. I&#39;m documenting my product ideas and learning notes in this blog. I&#39;m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.">
<meta property="og:type" content="website">
<meta property="og:url" content="https://chenterry.com/product/">
<meta property="og:site_name" content="Terry Chen">
<meta property="og:locale" content="en-us">



<meta property="og:image" content="https://chenterry.com/images/profile.jpg">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:image:alt" content="Terry Chen">




<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@terrychen_ai">
<meta name="twitter:creator" content="@terrychen_ai">
<meta name="twitter:title" content="Product | Terry Chen">
<meta name="twitter:description" content="Hi, this is Terry. I&#39;m documenting my product ideas and learning notes in this blog. I&#39;m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.">


<meta name="twitter:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:image:alt" content="Terry Chen">





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebPage",
  "name": "Product",
  "description": "Hi, this is Terry. I\u0027m documenting my product ideas and learning notes in this blog. I\u0027m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.",
  "url": "https:\/\/chenterry.com\/product\/",
  "author": {
    "@type": "Person",
    "name": "Terry Chen"
  }
}
</script>






<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<meta name="bingbot" content="index, follow">
<meta name="slurp" content="index, follow">
<meta name="duckduckbot" content="index, follow">



<meta name="section" content="product">


<meta name="content-type" content="product">



<meta name="geo.region" content="US">
<meta name="geo.placename" content="United States">
<meta name="language" content="en-us">
<meta http-equiv="content-language" content="en-us">



<link rel="alternate" type="application/rss+xml" title="Terry Chen RSS Feed" href="https://chenterry.com/product/index.xml">



<meta property="og:determiner" content="the">
<meta property="og:rich_attachment" content="true">



<meta name="format-detection" content="telephone=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="apple-mobile-web-app-title" content="Terry Chen">


<meta http-equiv="Cache-Control" content="public, max-age=31536000">
<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">


<meta name="referrer" content="strict-origin-when-cross-origin">



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https:\/\/chenterry.com\/"
    }
    ,
    {
      "@type": "ListItem", 
      "position": 2,
      "name": "Product",
      "item": "https:\/\/chenterry.com\/product/"
    }
    
    
  ]
}
</script>



<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">


<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="application-name" content="Terry Chen">


 <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow"><title>Product | Terry Chen</title>
<meta name="keywords" content="">
<meta name="author" content="Terry Chen">
<link rel="canonical" href="https://chenterry.com/product/">
    <link crossorigin="anonymous" href="/assets/css/stylesheet.f495fe1dedb119b2969e64d021ab84ebb9f24a5086308bd0222ece1b182e151e.css" integrity="sha256-9JX&#43;He2xGbKWnmTQIauE67nySlCGMIvQIi7OGxguFR4=" rel="preload stylesheet" as="style"><link rel="icon" href="https://chenterry.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenterry.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenterry.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenterry.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenterry.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="manifest" href="https://chenterry.com/manifest.json"><link rel="alternate" type="application/rss+xml" href="https://chenterry.com/product/index.xml">
<link rel="alternate" hreflang="en" href="https://chenterry.com/product/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script src="https://analytics.ahrefs.com/analytics.js" data-key="RfhOdZ+DOpWNlhV9QqLgTQ" async></script>


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜œ</text></svg>" type="image/svg+xml">

<link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜œ</text></svg>"> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-M6GS8Q702L');
        }
      </script><meta property="og:url" content="https://chenterry.com/product/">
  <meta property="og:site_name" content="Terry Chen">
  <meta property="og:title" content="Product">
  <meta property="og:description" content="Product portfolio and project work.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
      <meta property="og:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:title" content="Product">
<meta name="twitter:description" content="Hi, this is Terry. I&#39;m documenting my product ideas and learning notes in this blog. I&#39;m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Product",
      "item": "https://chenterry.com/product/"
    }
  ]
}
</script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'MEASUREMENT_ID');
</script><script type="application/ld+json">
{
  "@context": "https://schema.org",
  
  "@type": "WebPage",
  "name": "Product",
  "description": "Hi, this is Terry. I\u0027m documenting my product ideas and learning notes in this blog. I\u0027m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction."
  
}
</script>


 

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https:\/\/chenterry.com\/"
    }
    
    ,{
      "@type": "ListItem",
      "position": 2,
      "name": "Product",
      "item": "https:\/\/chenterry.com\/product/"
    }
    
    
    ,{
      "@type": "ListItem",
      "position": 3,
      "name": "Product",
      "item": "https:\/\/chenterry.com\/product\/"
    }
  ]
}
</script>
 



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Terry Chen",
  "url": "https:\/\/chenterry.com\/",
  "sameAs": [
    "https://www.linkedin.com/in/terry-chen-3b44911a4/",
    "https://github.com/terrylinhaochen"
  ],
  "jobTitle": "AI Product Engineer",
  "description": "AI Product Engineer and Investor exploring multi-agent systems, content understanding, and emerging technology investments",
  "knowsAbout": [
    "Artificial Intelligence",
    "Product Engineering", 
    "Investment Analysis",
    "Multi-Agent Systems",
    "Machine Learning",
    "Technology Strategy"
  ],
  "alumniOf": "Northwestern University"
}
</script>






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "CollectionPage",
  "name": "Product",
  "url": "https:\/\/chenterry.com\/product\/",
  "description": "Hi, this is Terry. I\u0027m documenting my product ideas and learning notes in this blog. I\u0027m interested in creating new user experiences through generative ai, focusing on synthesized content generation and actionable insight extraction.",
  "mainEntity": {
    "@type": "ItemList",
    "numberOfItems":  0 ,
    "itemListElement": [
      
    ]
  }
}
</script>











<style>
   
  link[rel="icon"] {
    content: url("data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜œ</text></svg>");
  }
</style> 
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenterry.com/" accesskey="h" title="Terry Chen (Alt + H)">Terry Chen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            
            <li>
                <a href="/posts/" title="Posts">
                    <span>
                        Posts
                    </span>
                </a>
            </li>
            <li>
                <a href="/product/" title="Product">
                    <span class="active">
                        Product
                    </span>
                </a>
            </li>
            <li>
                <a href="/investing/" title="Investing">
                    <span>
                        Investing
                    </span>
                </a>
            </li>
            <li>
                <a href="/search/" title="Explore" accesskey="/">
                    <span>
                        Explore
                    </span>
                </a>
            </li>
            <li>
                <a href="/about/" title="About">
                    <span>
                        About
                    </span>
                </a>
            </li>
        </ul>
    </nav>
</header> <main class="main">
<style>
  .projects-container {
    max-width: var(--content-width);
    margin: 0 auto;
    padding: 2rem 0;
  }

   
  .theme-carousel {
    margin-bottom: 4rem;
    position: relative;
  }

  .theme-slides {
    display: flex;
    overflow: hidden;
    scroll-behavior: smooth;
  }

  .theme-slide {
    min-width: 100%;
    padding: 2rem;
    background: linear-gradient(135deg, #e8f0ff 0%, #ffffff 100%);
    border-radius: 12px;
    box-shadow: 0 4px 6px rgba(0,0,0,0.05);
  }

  .theme-content {
    max-width: 800px;
    margin: 0 auto;
  }

  .theme-nav {
    display: flex;
    justify-content: center;
    gap: 1rem;
    margin-top: 1rem;
  }

  .theme-nav button {
    width: 12px;
    height: 12px;
    border-radius: 50%;
    border: none;
    background: #999;
    cursor: pointer;
    transition: background 0.3s ease;
  }

  .theme-nav button.active {
    background: #222;
  }

   
  .section {
    margin-bottom: 4rem;
  }

  .section-header {
    margin-bottom: 2rem;
    text-align: center;
  }

  .section-title {
    font-size: 2rem;
    font-weight: 700;
    color: #333;
    margin-bottom: 0.5rem;
  }

  .section-subtitle {
    color: #666;
    font-size: 1.1rem;
  }

  .projects-grid {
    display: flex;
    flex-direction: column;
    gap: 2rem;
    margin-top: 2rem;
  }

  .project-card {
    background: white;
    border-radius: 12px;
    padding: 1.5rem;
    box-shadow: 0 2px 8px rgba(0,0,0,0.06);
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    cursor: pointer;
  }

  .project-card:hover {
    transform: translateY(-4px);
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }

   
  .thesis-card {
    cursor: default;
  }

  .thesis-header {
    border-bottom: 1px solid rgba(0,0,0,0.1);
    padding-bottom: 1rem;
    margin-bottom: 1.5rem;
  }

  .thesis-header h2 {
    color: var(--primary);
    font-size: 1.5rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
  }

  .thesis-description {
    color: var(--secondary);
    font-size: 0.95rem;
    line-height: 1.5;
    margin: 0;
  }

  .thesis-content {
    display: flex;
    flex-direction: column;
    gap: 1rem;
  }

  .thesis-content .post-entry {
    background: rgba(0,0,0,0.02);
    border-radius: 8px;
    padding: 1rem;
    margin: 0;
    cursor: pointer;
    transition: background 0.2s ease;
  }

  .thesis-content .post-entry:hover {
    background: rgba(0,0,0,0.05);
    transform: none;
    box-shadow: none;
  }

  .thesis-content .entry-header h3 {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--primary);
    margin-bottom: 0.5rem;
  }

  .thesis-content .entry-content {
    font-size: 0.9rem;
    color: var(--content);
    line-height: 1.4;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
    text-overflow: ellipsis;
    height: 2.8rem;  
  }

  .project-card.featured,
  .work-card {
    grid-column: span 2;
    background: linear-gradient(to right bottom, #ffffff, #f8f9fa);
  }

   
  .modal-overlay {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.7);
    z-index: 1000;
    justify-content: center;
    align-items: center;
  }

  .modal {
    background: white;
    padding: 2rem;
    border-radius: 12px;
    max-width: 800px;
    width: 90%;
    max-height: 90vh;
    overflow-y: auto;
    position: relative;
  }

  .modal-close {
    position: absolute;
    top: 1rem;
    right: 1rem;
    font-size: 1.5rem;
    border: none;
    background: none;
    cursor: pointer;
  }

  .modal-overlay.active {
    display: flex;
  }

   
  .theme-slide h2 {
    color: #111 !important;  
    font-weight: 700 !important;  
  }

  .theme-slide p {
    color: #333 !important;  
    font-weight: 500 !important;  
  }

   
  .theme-carousel .theme-slide,
  .theme-content {
    color: #111 !important; 
  }

   
  body.dark .section-title,
  body.dark .projects-container .section-header h2.section-title,
  .dark .section-title {
    color: white !important;
    font-weight: 700 !important;
  }

   
  .theme-slide a, 
  .theme-content a {
    color: #111 !important;
    font-weight: 600 !important;
  }

   
  a.read-more,
  .read-more-link,
  a.read-more-link,
  .entry-link::after,
  .post-entry .entry-footer::after,
  a[aria-label="Read more"],
  .read-more-icon,
  .post-entry:hover .entry-footer::after,
  .post-entry:focus .entry-footer::after,
  .post-entry:active .entry-footer::after,
  footer a,
  .post-meta a,
  .entry-content a,
  [class*="read-more"] {
    color: #111 !important;  
    font-weight: 600 !important;  
    opacity: 1 !important;  
  }

   
  .post-entry::after,
  .entry-link::after,
  [class*="read"]::after {
    color: #111 !important;
    opacity: 1 !important;
  }

   
  .archived-link {
    margin-top: 1rem !important;
    text-align: center !important;
  }

  .archived-link span {
    color: #888 !important;
    font-size: 0.85em !important;
    display: block !important;
  }

  .archived-link a {
    color: #666 !important;
    text-decoration: underline !important;
    font-weight: normal !important;
  }

  .archived-link a:hover {
    color: #333 !important;
  }

   
  body.dark .archived-link span {
    color: #aaa !important;
  }

  body.dark .archived-link a {
    color: #888 !important;
  }

  body.dark .archived-link a:hover {
    color: #bbb !important;
  }

   
  .theme-nav button {
    background: #999;  
  }

  .theme-nav button.active {
    background: #222;  
  }

   
  :root[data-theme="dark"] .section-title,
  .dark .section-title {
    color: #ffffff !important;  
  }

  :root[data-theme="dark"] .section-subtitle,
  .dark .section-subtitle {
    color: #dddddd !important;  
  }

   
  :root[data-theme="dark"] .post-entry .entry-footer,
  .dark .post-entry .entry-footer {
    color: #ffffff !important;
  }

  :root[data-theme="dark"] .post-entry .entry-link,
  .dark .post-entry .entry-link {
    color: #ffffff !important;
  }

   
  :root[data-theme="dark"] .post-meta,
  .dark .post-meta {
    color: #dddddd !important;
  }

   
  :root[data-theme="dark"] .project-card,
  .dark .project-card,
  :root[data-theme="dark"] .post-entry,
  .dark .post-entry {
    background: var(--entry);
    border: 1px solid var(--border);
  }

   
  :root[data-theme="dark"] .modal,
  .dark .modal {
    background: var(--entry);
    color: var(--primary);
    border: 1px solid var(--border);
  }

   
  body.dark .section-title,
  body.dark .projects-container .section-header h2.section-title,
  .dark .section-title {
    color: white !important;
    font-weight: 700 !important;
  }

  body.dark .post-entry .entry-header h2,
  .dark .post-entry .entry-header h2 {
    color: white !important;
  }

  body.dark .entry-footer a,
  body.dark a[href*="/projects/"],
  .dark .post-entry .entry-footer a {
    color: white !important;
    font-weight: 600 !important;
  }

   
  .subtle-quote {
    margin-top: 2rem;
    padding-top: 2rem;
    border-top: 1px solid var(--border);
    text-align: center;
    font-style: italic;
    color: var(--secondary);
    opacity: 0.8;
  }
  
  .subtle-quote p {
    font-size: 0.95rem;
    max-width: 600px;
    margin: 0 auto;
  }

   
  .section:last-child {
    margin-bottom: 2rem;
  }

   
  .find-more-posts {
    text-align: center;
    margin: 2rem 0;
    padding: 1rem;
  }

  .find-more-posts p {
    color: #666;
  }

  .find-more-posts a {
    color: #0066cc;
    text-decoration: underline;
    font-weight: 500;
  }

  .find-more-posts a:hover {
    color: #0052a3;
  }

   
  body.dark .find-more-posts p,
  .dark .find-more-posts p {
    color: #dddddd !important;
  }

  body.dark .find-more-posts a,
  .dark .find-more-posts a {
    color: #93cdff !important;
  }

   
  .thesis-work {
    margin-top: 1rem;
    padding: 0.75rem;
    background: rgba(0,0,0,0.02);
    border-radius: 8px;
    border: 1px solid rgba(0,0,0,0.05);
  }

  .thesis-work p {
    margin: 0;
    font-size: 0.85rem;
    color: var(--secondary);
    font-style: italic;
  }

  .work-items {
    font-weight: 600;
    color: var(--primary);
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  }

   
  :root[data-theme="dark"] .thesis-work,
  .dark .thesis-work {
    background: rgba(255,255,255,0.05);
    border: 1px solid rgba(255,255,255,0.1);
  }

   
  :root[data-theme="dark"] .thesis-header,
  .dark .thesis-header {
    border-bottom: 1px solid rgba(255,255,255,0.1);
  }

  :root[data-theme="dark"] .thesis-content .post-entry,
  .dark .thesis-content .post-entry {
    background: rgba(255,255,255,0.05);
  }

  :root[data-theme="dark"] .thesis-content .post-entry:hover,
  .dark .thesis-content .post-entry:hover {
    background: rgba(255,255,255,0.1);
  }
</style>

<div class="projects-container">
  
  <section class="theme-carousel">
    <div class="theme-slides" id="themeSlides">
      <div class="theme-slide">
        <div class="theme-content">
          <h2>Crowdlisten</h2>
          <p>Accelerating primary market research by analyzing crowd discussions.</p>
        </div>
      </div>
      <div class="theme-slide">
        <div class="theme-content">
          <h2>Microsoft</h2>
          <p>Understanding what types of problems demand agentic solutions, how to build them, and pitfalls to avoid.</p>
        </div>
      </div>
      <div class="theme-slide">
        <div class="theme-content">
          <h2>Northwestern (DTR)</h2>
          <p>Improving and scaling self-regulation skill coaching through AI agent systems.</p>
        </div>
      </div>
      <div class="theme-slide">
        <div class="theme-content">
          <h2>Aibrary</h2>
          <p>Exploring knowledge representation and agentic AI for lifelong learning.</p>
        </div>
      </div>
      <div class="theme-slide">
        <div class="theme-content">
          <h2>TikTok</h2>
          <p>Exploring what it means to be creative amidst the wave of AI generated content.</p>
        </div>
      </div>
    </div>
    <div class="theme-nav" id="themeNav"></div>
  </section>

  
  <div class="projects-grid">
    
    <div class="project-card thesis-card">
      <div class="thesis-header">
        <h2>Crowdlisten</h2>
        <p class="thesis-description">Accelerating primary market research by analyzing crowd discussions.</p>
        <div class="thesis-work">
          <p>Work focus: <span class="work-items">Topic modeling with weighted clustering algorithm on multimodal data</span></p>
        </div>
      </div>
      <div class="thesis-content">
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Google NotebookLM - Long Context Windows and Multimodality</h3>
          </header>
          <div class="entry-content">
            Exploring Google NotebookLM&#39;s approach to long context windows and multimodal AI interactions for research and knowledge management.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-11-11 00:00:00 +0000 UTC'>November 11, 2025</span> â€¢ ðŸ“– 1 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/google_notebooklm/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Weighing Social Interactions: Reconstructing Meaning from Large Multi-modal Datasets</h3>
          </header>
          <div class="entry-content">
            Technical analysis of AI-powered search systems and topic modeling approaches, exploring semantic retrieval, embedding spaces, and the challenges of modern web crawling.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-11-11 00:00:00 +0000 UTC'>November 11, 2025</span> â€¢ ðŸ“– 1 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/search_topic/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>From Raw Social Data to Real Research</h3>
          </header>
          <div class="entry-content">
            Transform large-scale social conversations into actionable insights. Understand crowd sentiment, track emerging opinions, and identify key narratives.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-09-15 00:00:00 +0000 UTC'>September 15, 2025</span> â€¢ ðŸ“– 11 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/crowdlisten/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Why People Care What Others Think</h3>
          </header>
          <div class="entry-content">
            Understanding collective intelligence and social dynamics. Why crowd psychology matters for product builders and how human need for belonging drives engagement patterns.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-05-12 00:00:00 +0000 UTC'>May 12, 2025</span> â€¢ ðŸ“– 7 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/crowd_thesis/"></a>
        </article>
        
      </div>
    </div>

    
    <div class="project-card thesis-card">
      <div class="thesis-header">
        <h2>Microsoft</h2>
        <p class="thesis-description">Understanding what types of problems demand agentic solutions, how to build them, and pitfalls to avoid.</p>
        <div class="thesis-work">
          <p>Work focus: <span class="work-items">Agent system for estimating cost with consumption based pricing</span></p>
        </div>
      </div>
      <div class="thesis-content">
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Iterating at the Pace of AI</h3>
          </header>
          <div class="entry-content">
            Two approaches to building agentic experiences: platform-first vs scenario-first. How AI coding tools enable faster prototyping and cross-functional collaboration for agent product development.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-08-29 00:00:00 +0000 UTC'>August 29, 2025</span> â€¢ ðŸ“– 8 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/agent_prototyping/"></a>
        </article>
        
      </div>
    </div>

    
    <div class="project-card thesis-card">
      <div class="thesis-header">
        <h2>Northwestern (DTR)</h2>
        <p class="thesis-description">Improving and scaling self-regulation skill coaching through AI agent systems.</p>
        <div class="thesis-work">
          <p>Work focus: <span class="work-items">Regulation informed learning, case library enabled diagnosis and intervention</span></p>
        </div>
      </div>
      <div class="thesis-content">
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>The Best Practices Lie: On Dealing with Ambiguity</h3>
          </header>
          <div class="entry-content">
            The methodologies that get you into elite institutionsâ€”optimized for speed and pattern recognitionâ€”become liabilities when facing the genuine ambiguity of startups and real-world problems.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-10-31 00:00:00 +0000 UTC'>October 31, 2025</span> â€¢ ðŸ“– 8 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/ambiguity/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Building AI That Actually Understands How Students Learn</h3>
          </header>
          <div class="entry-content">
            How we built an LLM system that identifies student learning gaps and connects them with peers who&#39;ve solved similar problemsâ€”scaling personalized mentorship through AI that understands the nuances of how people actually learn.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2024-08-15 00:00:00 +0000 UTC'>August 15, 2024</span> â€¢ ðŸ“– 27 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/llmcoaching/"></a>
        </article>
        
      </div>
    </div>

    
    <div class="project-card thesis-card">
      <div class="thesis-header">
        <h2>Aibrary</h2>
        <p class="thesis-description">Exploring knowledge representation and agentic AI for lifelong learning.</p>
        <div class="thesis-work">
          <p>Work focus: <span class="work-items">Search, Recommendation, Agent interfaces for knowledge distillation and communication</span></p>
        </div>
      </div>
      <div class="thesis-content">
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>From Indexing to Understanding Intent in Discovery Systems</h3>
          </header>
          <div class="entry-content">
            Discovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoning.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-10-26 00:00:00 +0000 UTC'>October 26, 2025</span> â€¢ ðŸ“– 5 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/intent_driven_discovery/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>When Knowledge Becomes Fluid: How AI Transforms Learning</h3>
          </header>
          <div class="entry-content">
            Exploring how generative AI transforms static knowledge into dynamic, adaptive learning experiences. From fixed formats to fluid, contextual understanding that reshapes education and content consumption.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-10-26 00:00:00 +0000 UTC'>October 26, 2025</span> â€¢ ðŸ“– 6 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/fluid-knowledge-ai-transforms-learning/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Value Add of AI: Generation as Distribution</h3>
          </header>
          <div class="entry-content">
            Exploring how AI&#39;s greatest value shifts from content creation to personalized distribution and synthesis. From RSS feeds to knowledge liquefaction and the future of adaptive content experiences.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2025-05-08 00:00:00 +0000 UTC'>May 8, 2025</span> â€¢ ðŸ“– 4 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/generation_distribution/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Exploring Unknown Unknowns</h3>
          </header>
          <div class="entry-content">
            Development of intelligent knowledge interfaces that help users discover unknown unknowns. Technical implementation of AI-powered learning systems and knowledge discovery platforms.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2024-10-05 00:00:00 +0000 UTC'>October 5, 2024</span> â€¢ ðŸ“– 6 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/learning_interface/"></a>
        </article>
        
      </div>
    </div>

    
    <div class="project-card thesis-card">
      <div class="thesis-header">
        <h2>TikTok</h2>
        <p class="thesis-description">Exploring what it means to be creative amidst the wave of AI generated content.</p>
        <div class="thesis-work">
          <p>Work focus: <span class="work-items">Symphony Assistant, Insight Spotlight, GenAI For Ads, AI Content Understanding</span></p>
        </div>
      </div>
      <div class="thesis-content">
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Why Context is Everything in AI Content Generation</h3>
          </header>
          <div class="entry-content">
            Why context is the critical factor in AI content quality - moving beyond single-prompt expectations to rich, contextual content generation that adapts to individual needs and cognitive patterns.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2024-09-20 00:00:00 +0000 UTC'>September 20, 2024</span> â€¢ ðŸ“– 6 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/personalized_content/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Essence of Creativity: Future of Creative Work</h3>
          </header>
          <div class="entry-content">
            Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2024-08-25 00:00:00 +0000 UTC'>August 25, 2024</span> â€¢ ðŸ“– 14 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/essense_of_creativity/"></a>
        </article>
        
        <article class="post-entry"> 
          <header class="entry-header">
            <h3>Multi-modal Creative Ad Generation</h3>
          </header>
          <div class="entry-content">
            Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.
          </div>
          <footer class="entry-footer">
            <div class="post-meta"><span title='2024-05-20 00:00:00 +0000 UTC'>May 20, 2024</span> â€¢ ðŸ“– 12 min read â€¢ Terry Chen</div>
          </footer>
          <a class="entry-link" href="https://chenterry.com/posts/copilot/"></a>
        </article>
        
      </div>
    </div>
  </div>
</div>


<div class="find-more-posts">
  <p>To view archived content, <a href="/archived/">click here</a>.</p>
</div>


<div class="modal-container">
  
  <div id="modal-running-a-1-person-billion-dollar-company-the-ai-stack-that-makes-it-possible" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('running-a-1-person-billion-dollar-company-the-ai-stack-that-makes-it-possible')">&times;</button>
      <h2>Running a 1-Person Billion Dollar Company: The AI Stack That Makes It Possible</h2>
      <p class="project-meta">February 22, 2026</p>
      <div class="modal-content">
        <p>There&rsquo;s a famous quote from Sam Altman: &ldquo;In the near future, we will see one-person billion-dollar companies.&rdquo; We&rsquo;re not in the near future anymore. We&rsquo;re there.</p>
<p>The question isn&rsquo;t whether it&rsquo;s possibleâ€”it&rsquo;s about which tools make it possible. After building and running <a href="https://crowdlisten.com">CrowdListen</a>, I&rsquo;ve developed a stack that handles everything from market research to code deployment to content creation. What follows isn&rsquo;t a listicle of productivity apps. It&rsquo;s a philosophy of work, implemented through four AI systems that each handle a distinct function of running a company.</p>
<p>Running a company solo with AI requires four capabilities: knowing what to build, building it, running it, and growing it. Each pillar has a dedicated AI system. None of them require you to context-switch or manage a team. They just work.</p>
<hr>
<h2 id="crowdlisten-knowing-what-to-build">CrowdListen: Knowing What to Build</h2>
<p>The hardest part of building products isn&rsquo;t codingâ€”it&rsquo;s knowing what to code. Most founders waste months building features nobody wants because they&rsquo;re operating on intuition instead of evidence. I&rsquo;ve been guilty of this myself. You convince yourself that because you personally experience a pain point, others must too. Sometimes you&rsquo;re right. Often you&rsquo;re not.</p>
<p><a href="https://crowdlisten.com">CrowdListen</a> is an AI-powered audience insight platform that transforms social conversations into actionable product specs. It aggregates discussions from Reddit, TikTok, Twitter/X, and the broader web, then uses multi-modal AI pipelines to extract themes, cluster feature requests, and rank them by demand signals. The key insight is evidence-backed product decisions. Every insight links to real conversations. You&rsquo;re not guessingâ€”you&rsquo;re synthesizing what thousands of people have already told you.</p>
<p>Before building anything significant, I run a deep analysis on the problem space. What are people actually complaining about? What solutions have they tried? What&rsquo;s missing? For prioritization, every feature request in CrowdListen&rsquo;s kanban board links back to source conversations with quotes and confidence scores. No more &ldquo;I think users want X.&rdquo; Understanding how different segments talk about problems also helps craft messaging that resonatesâ€”you learn not just what to build, but how to talk about it.</p>
<p>This might seem like an obvious approach, but most founders skip it. There&rsquo;s something seductive about the blank page, about building from pure imagination. But imagination untethered from reality produces products that solve problems nobody has. The discipline of starting with evidence, even when you think you already know the answer, consistently surfaces insights that would have taken months to discover through iteration alone.</p>
<hr>
<h2 id="claude-code-building-everything">Claude Code: Building Everything</h2>
<p>Once you know what to build, you need to build it. <a href="https://claude.ai/claude-code">Claude Code</a> is Anthropic&rsquo;s CLI tool for software engineering, and it&rsquo;s become my entire engineering team.</p>
<p>This isn&rsquo;t GitHub Copilot-style autocomplete. This is an agent that plans and executes multi-step tasks autonomously, understands entire codebases through semantic analysis, writes tests and debugs production code, manages git workflows and creates PRs, and uses MCP servers for extended capabilities like database operations and browser testing. The magic is in the iteration. Claude Code doesn&rsquo;t just write codeâ€”it thinks through implications, checks for edge cases, and maintains consistency with existing patterns.</p>
<p>My typical workflow looks like this: I describe the feature in plain English. Claude Code explores the codebase, understands the architecture. It writes the implementation across multiple files. Runs tests, fixes issues, creates a commit. I review, approve, merge. For CrowdListen, Claude Code handles everythingâ€”React frontend, Python backend, Supabase integrations, MCP server development, deployment configurations. It&rsquo;s not a coding assistant. It&rsquo;s a coding agent.</p>
<p>What strikes me most about this shift is how it changes the nature of engineering work. The bottleneck is no longer implementation speedâ€”it&rsquo;s clarity of thought. If I can articulate precisely what I want, it gets built. If my thinking is muddled, the output reflects that. In some ways, Claude Code has made me a better product thinker because it forces me to be explicit about decisions I might have previously hand-waved through.</p>
<hr>
<h2 id="openclaw-operating-the-business">OpenClaw: Operating the Business</h2>
<p>Coding is maybe 30% of running a company. The rest is operations: responding to users, writing documentation, managing tasks, coordinating between systems, handling the endless administrative work that scales with growth.</p>
<p><a href="https://github.com/terrylinhaochen/crowdlisten_video">OpenClaw</a> is an AI agent that runs locally on my machine and treats my entire workspace as its operating environment. It communicates via natural language through Signal, Telegram, or web, and executes tasks across all my systems. The mental model is simple: OpenClaw is the AI that talks to me in natural language but acts on systems. It reads SOPs, follows workflows, and executes multi-step operations without me touching a keyboard.</p>
<p>What does this look like in practice? I can ask &ldquo;What clips do we have about AI agents?&rdquo; and it searches the library, returning top matches with scores. I can say &ldquo;Make a week of content&rdquo; and it picks clips, queues renders, and monitors progress. &ldquo;What&rsquo;s ready to post?&rdquo; triggers a check of published and review queues with a summary. &ldquo;Sync the library&rdquo; triggers API calls and manages state. These aren&rsquo;t pre-programmed commandsâ€”they&rsquo;re natural language requests that OpenClaw interprets and executes.</p>
<p>The key difference from Claude Code is that OpenClaw handles non-coding tasks. It&rsquo;s for operations, not implementation. When I need to write code, I use Claude Code. When I need to run the business, I use OpenClaw. This division matters because the mental models are different. Coding requires precision and technical context. Operations require workflow awareness and system coordination. Having specialized agents for each prevents the cognitive overhead of context-switching.</p>
<hr>
<h2 id="crowdlisten-video-marketing-at-scale">CrowdListen Video: Marketing at Scale</h2>
<p>You can build the best product in the world, but if nobody knows about it, you have nothing. Marketing for a solo founder used to mean either spending hours on content creation or spending money on agencies. Neither option scales well when you&rsquo;re trying to move fast.</p>
<p><a href="https://github.com/terrylinhaochen/crowdlisten_video">CrowdListen Video</a>, internally called CrowdListen Studio, is a two-interface system for creating short-form video content for TikTok and Instagram. It combines a visual Studio web app with OpenClaw&rsquo;s conversational interface. The pipeline works like this: you drop a video into the system, Gemini AI watches it and returns timestamped &ldquo;meme moments&rdquo; ranked by energy score. This creates a clip library with 38+ pre-analyzed moments, each with scores, captions, and visual descriptions that you can browse by source or filter by score.</p>
<p>Content creation becomes conversational. I can tell OpenClaw &ldquo;Make a clip about scope creep&rdquo; and it searches the library for the best match, picks a clip with the right energy, queues the render, and notifies me when it&rsquo;s done. No video editing software. No context-switching. Just describe what you want and it appears in your review queue.</p>
<p>What makes this approach different from hiring a content team or using a traditional video editor is the feedback loop speed. I can go from idea to published content in minutes rather than days. This matters because marketing effectiveness depends on iterationâ€”testing different angles, seeing what resonates, doubling down on what works. When each iteration takes days, you might get a few dozen attempts per quarter. When each iteration takes minutes, you can run hundreds of experiments.</p>
<hr>
<h2 id="the-integration-layer">The Integration Layer</h2>
<p>These four tools don&rsquo;t operate in isolationâ€”they form an integrated system. Market intelligence from CrowdListen tells me what to build. Implementation via Claude Code turns insights into products. Operations via OpenClaw keep everything running. Growth via CrowdListen Video brings in more users. More users generate more feedback, which feeds back into CrowdListen&rsquo;s analysis. This is a flywheel, and each cycle compounds.</p>
<p>The philosophy underlying this stack is simple: automate everything except the parts that require your unique judgment. Visionâ€”what should this company become? Tasteâ€”what feels right versus what the data says? Relationshipsâ€”the human connections that can&rsquo;t be delegated. Riskâ€”the bets only you can make. Everything else is infrastructure. And infrastructure should run itself.</p>
<p>I catch myself sometimes marveling at the absurdity of it. A single person, running what amounts to a fully-staffed company, through conversations with AI systems. It sounds like science fiction. But it&rsquo;s Tuesday. This is just how work happens now.</p>
<hr>
<h2 id="whats-next">What&rsquo;s Next</h2>
<p>The current stack is version 1.0. What&rsquo;s coming excites me more than what already exists. CrowdListen Video will soon auto-post directly to TikTok and Instagram via their APIs. OpenClaw will manage a content calendar, scheduling and queuing automatically based on optimal timing. An analytics loop will track performance and feed back into clip selection algorithms. Most importantly, cross-system memory will allow all agents to share context about what&rsquo;s been built, what&rsquo;s working, and what&rsquo;s not.</p>
<p>The one-person billion-dollar company isn&rsquo;t a distant future. It&rsquo;s a toolkit problem. And the toolkit is here.</p>
<hr>
<h2 id="try-the-stack">Try the Stack</h2>
<p>If you&rsquo;re curious to experiment with any of these tools, here&rsquo;s where to start. <a href="https://crowdlisten.com">CrowdListen</a> is the audience insight platformâ€”use it to understand what to build. Claude Code is available through <a href="https://claude.ai/claude-code">Anthropic</a>. <a href="https://github.com/terrylinhaochen/crowdlisten_video">CrowdListen Video</a> is open source if you want to build your own content pipeline.</p>
<p>The future of work isn&rsquo;t about hiring more people. It&rsquo;s about deploying the right agents.</p>
<hr>
<h2 id="related-reading">Related Reading</h2>
<ul>
<li><a href="/posts/product_engineers/">Product Engineers and AI Multipliers</a> - How AI is transforming product engineering teams into smaller, more efficient units</li>
<li><a href="/posts/ambiguity/">The Best Practices Lie: On Dealing with Ambiguity</a> - Why institutional training becomes a liability in genuinely uncertain situations</li>
<li><a href="/posts/crowdlisten/">From Raw Social Data to Real Research</a> - The technical architecture behind CrowdListen&rsquo;s insight engine</li>
</ul>

      </div>
    </div>
  </div>
  
  <div id="modal-google-learn-about-interest-directed-ai-aided-learning" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('google-learn-about-interest-directed-ai-aided-learning')">&times;</button>
      <h2>Google Learn About - Interest Directed, AI Aided Learning</h2>
      <p class="project-meta">November 11, 2025</p>
      <div class="modal-content">
        <p>Google&rsquo;s Learn About reframes both search and chat as an explore-first learning loop. It lowers the cost of asking good questions, manages cognitive load with structured, clickable cards, and continuously surfaces &ldquo;questions you didn&rsquo;t know to ask.&rdquo; Users resolve a specific doubt while simultaneously zooming out to the surrounding question space, all without breaking their thinking flow.</p>
<p>

  <img src="/images/posts/google-learn-about/learn-about-main-interface.png" alt="Google Learn About Main Interface" loading="lazy">
 
<em>Learn About&rsquo;s clean interface design centers around the core question &ldquo;What would you like to learn about?&rdquo; The PDF Reading Companion offers guided document analysis, while The Reading Nook provides curated book exploration with visual book covers that invite browsing and discovery.</em></p>
<h2 id="the-problem-friction-in-learning-discovery">The Problem: Friction in Learning Discovery</h2>
<p>Most learners struggle with two recurring frictions: picking what to study and understanding what they picked. Learn About addresses both by turning fragmented minutes into genuine learning and durable curiosity. It does this by minimizing the friction of asking, embedding follow-up modules directly inside answers, and maintaining interactivity so that exploration continues without re-prompting.</p>
<p>The experience privileges clarity, moderate information density, and explicit emphasis on what matters most, which together reduce overload while preserving momentum. Reflection is encouraged through timely prompts and lightweight feedback, and the system adapts to the learner by adjusting difficulty, sequencing, and recommendations as it observes preferences and progress.</p>
<p>

  <img src="/images/posts/google-learn-about/main-interface-1984-topic.png" alt="Learn About Main Interface with 1984 Topic" loading="lazy">
 
<em>Learn About&rsquo;s interface for exploring &ldquo;Dystopian Literature and Nineteen Eighty-Four&rdquo; demonstrates the card-based approach with suggested topics, contextual information, and interactive elements that maintain learning flow.</em></p>
<h2 id="what-makes-learn-about-different">What Makes &ldquo;Learn About&rdquo; Different</h2>
<p>The single biggest unlock is that Learn About lowers the cost of asking. Instead of waiting for users to craft perfect prompts, it embeds ready-to-tap follow-ups into each response so the next question is one click away. Equally important, it lets users zoom out without losing the thread by showing neighboring questions alongside the current answer. This arrangement promotes deeper dives and lateral pivots within the same cognitive flow.</p>
<p>Because it blends search-grade grounding and multi-modal sourcing with the flexibility of conversational chat, users get precise, cite-able synthesis that remains responsive and compact. Most distinctively, Learn About treats &ldquo;unknown unknowns&rdquo; as a first-class job to be done by regularly proposing high-leverage sub-questions and adjacent topics that expand the learner&rsquo;s map.</p>
<p>

  <img src="/images/posts/google-learn-about/system-architecture-flow.png" alt="System Architecture Flow" loading="lazy">
 
<em>The conversational AI system architecture shows how Learn About structures the learning experience through suggested topics, learning aids, exploration cards, and smart prefills that reduce prompting friction.</em></p>
<h2 id="interactive-cards-the-core-design-pattern">Interactive Cards: The Core Design Pattern</h2>
<p>Cards are the core unit of meaning and motion in Learn About. The system provides multiple card types that give learners different ways to proceed without retyping:</p>
<p><strong>Interactive Lists</strong> present related content in browsable, clickable formats. For example, when exploring &ldquo;The Scream,&rdquo; users can browse different versions of the artwork with visual thumbnails and contextual information.</p>
<p>

  <img src="/images/posts/google-learn-about/interactive-list-card-example.png" alt="Interactive List Card Example" loading="lazy">
 
<em>Interactive List cards allow users to explore related content visually, as shown with different versions of &ldquo;The Scream&rdquo; painting, each with thumbnail images and descriptive details.</em></p>
<p><strong>&ldquo;Why It Matters&rdquo; Cards</strong> explain contemporary relevance and applications. These cards help learners understand why historical or abstract concepts remain significant today.</p>
<p>

  <img src="/images/posts/google-learn-about/why-it-matters-card-example.png" alt="Why It Matters Card Example" loading="lazy">
 
<em>&ldquo;Why It Matters&rdquo; cards provide contemporary context, explaining how &ldquo;The Scream&rdquo; became a universal cultural icon representing anxiety and inner turmoil across different media.</em></p>
<p><strong>Knowledge Checks</strong> convert passive reading into active recall. These lightweight quizzes help reinforce learning without breaking the exploration flow.</p>
<p>

  <img src="/images/posts/google-learn-about/test-knowledge-quiz-example.png" alt="Test Knowledge Quiz Example" loading="lazy">
 
<em>Knowledge check cards engage users with interactive quizzes that test understanding while maintaining the conversational flow.</em></p>
<p><strong>Vocabulary Builders</strong> introduce and define key terms with pronunciation guides and clear explanations.</p>
<p>

  <img src="/images/posts/google-learn-about/build-vocab-card-example.png" alt="Build Vocab Card Example" loading="lazy">
 
<em>Vocabulary cards provide pronunciation guides and comprehensive definitions, helping learners build domain-specific knowledge progressively.</em></p>
<p>Simple controls like &ldquo;Simplify,&rdquo; &ldquo;Go Deeper,&rdquo; and &ldquo;Get Quotes&rdquo; let learners tune granularity on demand. Smart prefills provide a few contextually relevant starters so that continuing the thread always feels one tap away.</p>
<h2 id="learn-about-adapted-to-books">&ldquo;Learn About,&rdquo; Adapted to Books</h2>
<p>A reading-first Learn About helps people quickly grasp and then deepen a book&rsquo;s ideas through short, lively exchanges that unfold as modular replies. The system demonstrates sophisticated contextual awareness by dynamically suggesting related topics and themes as users explore literary works.</p>
<p>

  <img src="/images/posts/google-learn-about/contextual-suggestions-sidebar.png" alt="Contextual Learning Suggestions" loading="lazy">
 
<em>The contextual sidebar shows how Learn About surfaces related historical and cultural topics while reading The Great Gatsby, including &ldquo;The Roaring Twenties,&rdquo; &ldquo;The Jazz Age,&rdquo; and &ldquo;Prohibition in the United States,&rdquo; creating natural pathways for deeper exploration.</em></p>
<p>Key components include <strong>Contextual Background</strong> that clarifies the time, author, and intellectual setting, <strong>Key Concepts</strong> that introduce terms and definitions with crisp explanations, <strong>Reflection Prompts</strong> that invite a pause to think before proceeding, <strong>Relevance Notes</strong> that explain why an idea matters now and where it applies, <strong>Intertextual Pointers</strong> that connect the current book to contrasting viewpoints elsewhere, and <strong>Quotations</strong> that anchor the discussion in the text with light annotation that preserves the author&rsquo;s voice.</p>
<p>

  <img src="/images/posts/google-learn-about/gatsby-analysis-sidebar.png" alt="Advanced Analysis Interface" loading="lazy">
 
<em>The analysis interface demonstrates Learn About&rsquo;s depth by providing thematic breakdowns like &ldquo;The Geography of Wealth and Class,&rdquo; &ldquo;The Illusion of the American Dream,&rdquo; and &ldquo;The Role of Women in the 1920s&rdquo; alongside the primary text, enabling rich contextual learning.</em></p>
<p>Every element is designed to be tappable so that the next micro-question feels like continuing a conversation rather than composing a fresh prompt. This creates a seamless learning experience where curiosity drives discovery rather than predetermined lesson plans.</p>
<h2 id="competitive-positioning">Competitive Positioning</h2>
<p>General-purpose LLMs offer flexible dialogue but depend heavily on the user&rsquo;s prompting skill and do not consistently deliver structured learning surfaces. Traditional search excels at source grounding but leaves the curation, triage, and synthesis to the user. Community forums provide depth and texture at the cost of time and noise, since value is scattered across threads.</p>
<p>A reading-focused Learn About occupies the middle ground: it retains search-level grounding and community-like richness while orchestrating everything into concise, interactive cards that reduce question friction and guide depth. In practice, this means less skimming, less context-switching, and more time spent understanding.</p>
<h2 id="core-product-logic">Core Product Logic</h2>
<p>Two design commitments drive the experience:</p>
<ol>
<li>
<p><strong>Reduce Prompting Cost</strong>: The interface embeds good questions into the prior answer so the user advances with clicks rather than blank-box composition.</p>
</li>
<li>
<p><strong>Maximize Effective Information</strong>: Each turn delivers the smallest amount needed to unlock a next action, whether that means asking a sharper question, recognizing a pivotal idea, or applying something concrete.</p>
</li>
</ol>
<p>Short, modular replies keep attention intact and make it obvious where to go next.</p>
<h2 id="implementation-strategy">Implementation Strategy</h2>
<h3 id="scope-and-roadmap-for-books-domain">Scope and Roadmap for Books Domain</h3>
<p>Coverage should begin with a well-curated canon of classics that already have rich secondary literature, then expand toward the long tail through retrieval-augmented generation. Structured sources such as high-quality guides and glossaries can provide reliable scaffolds, while unstructured sources such as reviews and community discussions add nuance and counterpoints.</p>
<p>Multiple entry points make the system feel native in different contexts:</p>
<ul>
<li><strong>Main conversation</strong> for direct book Q&amp;A</li>
<li><strong>Podcast page</strong> for audio-led overview that feeds context into chat</li>
<li><strong>Search page</strong> for natural-language book finding grounded in catalog corpus</li>
<li><strong>Book detail page</strong> for deep dives seeded by highlights or chapters</li>
</ul>
<h3 id="personalization-and-memory">Personalization and Memory</h3>
<p>A lightweight user model grows with every interaction by observing which cards the learner opens, where they came from, and what they bookmark. Over time, the system tunes difficulty, chooses modalities that fit a person&rsquo;s habits, and proposes recommendations that align with their goals. The aim is not heavy-handed adaptation but gentle alignment that keeps the learning curve inside the user&rsquo;s zone of proximal development.</p>
<h3 id="technical-implementation">Technical Implementation</h3>
<p>

  <img src="/images/posts/google-learn-about/pdf-reading-companion.png" alt="PDF Reading Companion Interface" loading="lazy">
 
<em>The PDF Reading Companion showcases Learn About&rsquo;s document analysis capabilities, providing contextual information about &ldquo;The Great Gatsby&rdquo; with options to &ldquo;Simplify,&rdquo; &ldquo;Go deeper,&rdquo; or &ldquo;Get images,&rdquo; demonstrating the system&rsquo;s adaptive response to user learning preferences.</em></p>
<p>A simple Node.js and Firebase stack is sufficient for a working prototype. An ad-hoc but consistent JSON schema can power card templates, and a clear behavioral system prompt can keep outputs compact, structured, and render-ready. Resource types may include articles, diagrams, videos, photos, timelines, and tables, each with known fields that the renderer expects.</p>
<p>The PDF Reading Companion interface reveals sophisticated document processing capabilities that maintain context across different interaction modes. Users can seamlessly transition between reading the source material and exploring related concepts, with the system maintaining awareness of their current position and learning objectives throughout the session.</p>
<p>Start with a few shot examples and conservative temperature, then graduate to lightweight finetunes for the related-question generator as traffic grows and patterns stabilize.</p>
<p>Great questions live in the long tail, so the system benefits from synthetic query generation to probe coverage gaps and from semantic clustering to compare what people ask with what they ought to ask to progress. Early on, a straightforward retrieval patternâ€”embeddings, top-k selection, and generation that honors a card schemaâ€”establishes reliability.</p>
<h2 id="measuring-success">Measuring Success</h2>
<p><strong>Interaction Quality</strong> shows up directly in average turns per session and card click-through rates, as well as explicit helpfulness signals such as upvotes and downvotes. <strong>Learning Outcomes</strong> can be proxied through completed knowledge checks, intertextual clicks that lead to saved or started books, and the frequency with which users choose to go deeper rather than simplify. <strong>Personalization</strong> should be evaluated by comparing engagement and conversion on individualized suggestions against generic baselines.</p>
<h2 id="extensions-listen-in-experiences">Extensions: Listen-In Experiences</h2>
<p>Two sidecar experiences can broaden appeal without complicating the main loop:</p>
<p><strong>Reader Panel</strong> synthesizes community viewpoints into a short, host-led exchange that captures live debate in a couple of minutes of text or audio. <strong>Expert Panel</strong> synthesizes book content and academic sources into a focused discussion on a theme. Both can be pre-cached and surfaced as contextual cards that invite a quick listen before jumping back into interactive exploration.</p>
<h2 id="why-this-pattern-wins">Why This Pattern Wins</h2>
<p>Learn About is learner-centric rather than content-centric. It starts from a live confusion and expands the neighborhood around it without demanding more effort than a tap. It preserves attention by keeping answers short, modular, and emphatic about what matters now. It compounds value as every interaction sharpens the system&rsquo;s sense of what to show next.</p>
<p>Most importantly, it is a portable blueprint: the same mechanics that make it excellent for books also apply to lectures, papers, podcasts, and any domain where exploration precedes mastery. By treating curiosity as a navigable space rather than a series of isolated questions, Learn About transforms how we think about AI-assisted learningâ€”from reactive query-response to proactive knowledge exploration.</p>
<p>The future of learning isn&rsquo;t about better answers to the questions we already know to ask. It&rsquo;s about systems that help us discover the questions we never thought to ask, making the unknown knowns visible and actionable. Google&rsquo;s Learn About shows us what that future looks like: not just smarter search, but curiosity as an interface.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-google-notebooklm-long-context-windows-and-multimodality" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('google-notebooklm-long-context-windows-and-multimodality')">&times;</button>
      <h2>Google NotebookLM - Long Context Windows and Multimodality</h2>
      <p class="project-meta">November 11, 2025</p>
      <div class="modal-content">
        <p>

  <img src="/images/posts/google-notebooklm/notebooklm-main-interface.png" alt="Google NotebookLM Main Interface" loading="lazy">
 
<em>Google NotebookLM&rsquo;s main interface showing featured notebooks and recent projects, demonstrating the platform&rsquo;s approach to organizing and managing research across multiple sources and domains.</em></p>

      </div>
    </div>
  </div>
  
  <div id="modal-the-monetization-paradox-of-ai" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('the-monetization-paradox-of-ai')">&times;</button>
      <h2>The Monetization Paradox of AI</h2>
      <p class="project-meta">November 11, 2025</p>
      <div class="modal-content">
        <p>AI has attracted extraordinary investment, but the question of what consumers will actually pay for remains unsettled. Todayâ€™s monetization strategiesâ€”subscriptions like ChatGPT Plus or Kimi, API usage fees, and ad-driven AI applicationsâ€”resemble the early internet era, where infrastructure outpaced clear consumer value. On the B2B side, AI often enhances existing workflows, such as creative generation in advertising, but rarely justifies direct pricing. Advertisers care less about model sophistication than about measurable outcomesâ€”new creative material, better engagement, or lower cost per click. As a result, product value is abstracted into incremental revenue lift, not standalone payment. For consumers, subscription-based AI products work only when the perceived utility is personal, persistent, and irreplaceableâ€”writing, coding, or tutoring assistants that feel indispensable.</p>
<p>Ultimately, AI monetization may hinge not on the intelligence itself but on its embodimentâ€”how seamlessly it integrates into habits, tools, and ecosystems people already pay for. The next wave of value capture will likely come from domain-specific AI layers that own distribution (e.g., productivity, media, or commerce), rather than from general intelligence sold as a service.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-weighing-social-interactions-reconstructing-meaning-from-large-multi-modal-datasets" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('weighing-social-interactions-reconstructing-meaning-from-large-multi-modal-datasets')">&times;</button>
      <h2>Weighing Social Interactions: Reconstructing Meaning from Large Multi-modal Datasets</h2>
      <p class="project-meta">November 11, 2025</p>
      <div class="modal-content">
        <h2 id="semantic-retrieval-vs-traditional-search">Semantic Retrieval vs. Traditional Search</h2>
<p>

  <img src="/images/posts/ai-search-topic-modeling/semantic-retrieval-explanation.png" alt="Semantic Retrieval Explanation" loading="lazy">
 
<em>Understanding the fundamental difference between semantic retrieval systems and traditional web crawlers: Exa operates as a semantic retrieval index over a pre-crawled corpus, focusing on high-quality, text-rich pages suitable for LLM context windows rather than real-time web crawling.</em></p>
<h2 id="modern-web-crawling-challenges">Modern Web Crawling Challenges</h2>
<p>

  <img src="/images/posts/ai-search-topic-modeling/crawler-restrictions-challenges.png" alt="Crawler Restrictions and Challenges" loading="lazy">
 
<em>The evolving landscape of web access restrictions: Modern platforms like Reddit, Xueqiu, and others implement sophisticated barriers to automated crawling, fundamentally changing how AI systems can access and index web content.</em></p>

      </div>
    </div>
  </div>
  
  <div id="modal-the-best-practices-lie-on-dealing-with-ambiguity" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('the-best-practices-lie-on-dealing-with-ambiguity')">&times;</button>
      <h2>The Best Practices Lie: On Dealing with Ambiguity</h2>
      <p class="project-meta">October 31, 2025</p>
      <div class="modal-content">
        <p>Every methodology has its breaking point.</p>
<p>

  <img src="/images/posts/dealing-with-ambiguity/ambiguity.png" alt="Mountain landscape" loading="lazy">
 </p>
<p>The problem-solving frameworks that get you into a top school, the pattern recognition that earns you a job at a big tech company, the &ldquo;best practices&rdquo; that make you a star studentâ€”all of these become systematic liabilities the moment you encounter genuinely ill-defined problems. Which, unfortunately for ambitious people, is exactly where the most important work happens.</p>
<p>As I explore what I want to do with my life, and reflect past decision, I realize: we&rsquo;ve been trained to excel at the wrong things.</p>
<p>As much as I take pride in my ability to &ldquo;ask the right questions,&rdquo; identify effective methods, and &ldquo;navigate ambiguity&rdquo; by finding answers others overlook, I&rsquo;m starting to realize how much I&rsquo;ve been playing in a safe zone. Having a job lined up while wanting to pursue startups part-time in college isn&rsquo;t exactly what I&rsquo;d call &ldquo;navigating ambiguity.&rdquo; When it comes to genuinely not knowing what I&rsquo;ll be doing or where I&rsquo;ll end up six months from now, I panic at the idea.</p>
<p>This reveals something uncomfortable about how institutional training actually works. In school and corporate environments, you can often be rewarded for <em>appearing</em> to work hardâ€”for having the right frameworks, asking sophisticated questions, and demonstrating systematic thinking. The performance of competence gets rewarded alongside actual competence.</p>
<p>But in startups, that entire system breaks down. People won&rsquo;t listen to you or think differently of you just because you have &ldquo;Founder&rdquo; or &ldquo;Head of Product&rdquo; stamped on your LinkedIn profile or email signature. The quality of your product and depth of your customer empathy is all that matters. The institutional markers of successâ€”the titles, the frameworks, the sophisticated analysisâ€”become irrelevant when you&rsquo;re face-to-face with users who simply don&rsquo;t care about your credentials.</p>
<p>Paul Graham&rsquo;s insight about startups being counterintuitive reveals something deeper about this disconnect. As he notes in <a href="https://paulgraham.com/before.html">&ldquo;Before the Startup&rdquo;</a>, there are many ski instructors but few running instructorsâ€”not because running coaching is difficult, but because running feels intuitive while skiing requires overriding your instincts. Similarly, startups are counterintuitive, which is why startup advice exists at all. But there&rsquo;s a related phenomenon: institutional training teaches us to excel at activities that <em>seem</em> like entrepreneurship but aren&rsquo;t.</p>
<p>Research on complex decision-making reveals something uncomfortable: without strong self-regulation skills, people default to limiting patterns when facing uncertainty. They avoid difficulty, fall into perfectionist paralysis, or break problems down mechanically without going deep enough.</p>
<p>The ability to step back, see problems clearly, and consciously adapt behavior is rare. It doesn&rsquo;t naturally develop with age or experienceâ€”many people carry the same behavioral patterns into middle age.</p>
<p>This shows up everywhere, but it&rsquo;s particularly visible in how we approach complex problems.</p>
<hr>
<h2 id="the-problem-decomposition-trap">The Problem Decomposition Trap</h2>
<p>Institutions, as a product, have done a great job of creating the impression that skill is associated with completing coursework. While this might have been true years backâ€”you&rsquo;d have to do a CS degree to learn how to codeâ€”it&rsquo;s becoming less relevant as an indicator. You become good at something by doing that thing, not necessarily learning how to become good at that thing. And to actually do something well, you have to develop an understanding, rather than just follow patterns.</p>
<p>At leading institutions, students are trained to find the quickest path to a solutionâ€”to identify &ldquo;hacks&rdquo; and implement &ldquo;best practices.&rdquo; This system works brilliantly for well-defined problems with known solution spaces. Get the grade, pass the test, optimize the metric. But it creates a dangerous mental model when applied to genuinely ambiguous situations.</p>
<p>The institutional approach teaches us to pattern-match rapidly: <em>This looks like that problem I solved before, so I&rsquo;ll apply that framework.</em> In startups, this intuition becomes a liability. The most valuable insights often come from staying with uncertainty longer than feels comfortable, from digging deeper into assumptions everyone else accepts as given.</p>
<p>This pattern-matching problem gets worse when you consider how different environments reward different approaches.</p>
<hr>
<h2 id="safety-nets-and-startup-reality">Safety Nets and Startup Reality</h2>
<p>While I have limited exposure to working in big tech, I&rsquo;ve developed somewhat of hints at why at large companies, the standard for success is different. If you can clearly and persuasively articulate an idea and convince your superiors, the system backs you up. In existing markets, you&rsquo;ll capture some share. In growth markets, if your approach doesn&rsquo;t work, you can pivot quickly. The infrastructure absorbs your experimental failures.</p>
<p>Startups operate under entirely different constraints. Competition edges in from multiple directions, resources are brutally limited, and the cold start problem is real. The only real differentiation isn&rsquo;t execution efficiencyâ€”it&rsquo;s unique understanding of the problem itself. Whether that&rsquo;s insight into user pain points everyone else misses, or redefining how to reach customers in ways that weren&rsquo;t obvious.</p>
<p>This difference reveals why methodologies optimized for big company environments often fail in startup contexts. Big companies can afford to be &ldquo;approximately right&rdquo; because they have scale, distribution, and capital to make imperfect solutions work. Startups need to be &ldquo;precisely right&rdquo; about something others are wrong about.</p>
<hr>
<h2 id="when-uncertainty-becomes-unbearable">When Uncertainty Becomes Unbearable</h2>
<p>International students experience this pattern more intensely because the stakes genuinely feel higher when navigating unfamiliar systems.</p>
<p>The human brain isn&rsquo;t well-equipped to handle uncertainty. We anchor to whatever evidence seems solid. When you&rsquo;re navigating an unfamiliar system (US college admissions) with limited information, hiring a college consultant provides reassurance. It&rsquo;s not about whether it&rsquo;s objectively helpfulâ€”it&rsquo;s about having something concrete to hold onto when everything else feels uncertain.</p>
<p>This intensifies in college. Visa status, a competitive job market, policy uncertaintiesâ€”these aren&rsquo;t abstract concerns but existential realities. So when companies promise &ldquo;insider guides&rdquo; to breaking into finance (organizations that offer an impression to having these guides are able to charge high premiums), even students with genuine ambitions to start companies or do research end up signing contracts for tracks of &ldquo;breaking into&rdquo; Wall Street or Silicon Valley. The path feels safer because it&rsquo;s more defined.</p>
<p>This reveals something deeper about how uncertainty gets commodified. When stakes feel existential, the promise of &ldquo;insider knowledge&rdquo; becomes irresistibleâ€”regardless of whether that knowledge actually helps. These services often reinforce the exact pattern-matching mentality that becomes counterproductive in genuinely ambiguous situations.</p>
<p>The almost counterintuitive observation is that many of these students are precisely the people who should be exploring unknown territories. They have unique perspectives, cross-cultural insights, and often genuine intellectual curiosity. But the very conditions that make ambiguity feel threateningâ€”uncertainty about belonging, about future prospectsâ€”are what make engaging with it most valuable. It&rsquo;s a cruel catch-22: those who would benefit most from embracing uncertainty are often those who can least afford to.</p>
<hr>
<h2 id="breaking-free-from-institutional-patterns">Breaking Free From Institutional Patterns</h2>
<p>The solution isn&rsquo;t to abandon systematic thinking entirely, but to develop meta-awareness about when these approaches help and when they hinder. This requires recognizing the fundamental difference between well-defined problems (where institutional training excels) and ill-defined problems (where it often misleads).</p>
<p>Effective navigation of ambiguous situations involves capabilities that institutional training rarely develops: comfort with not knowing while still making progress, iterative planning rather than linear execution, and the ability to hold assumptions lightly while testing them thoroughly.</p>
<p>Most importantly, it requires learning to recognize when you&rsquo;re applying familiar frameworks to genuinely novel situationsâ€”when you&rsquo;re &ldquo;playing house&rdquo; rather than engaging with real complexity. The very thoroughness of elite institutional training can create blind spots, making it harder to step back and question whether your cognitive tools are adequate for the problem at hand.</p>
<p>Which brings me back to my own struggle with this.</p>
<hr>
<h2 id="maybe-its-time-to-loosen-up-a-little">Maybe It&rsquo;s Time to Loosen Up a Little</h2>
<p>Paul Graham notes that knowledge grows fractallyâ€”from a distance its edges look smooth, but when you get close enough, you notice gaps that seem obvious. The feeling of &ldquo;surely someone has figured this out already&rdquo; might be wrong.</p>
<p>This hits at something I&rsquo;ve been wrestling with personally. We spend so much energy learning to navigate the known world efficiently that we forget how to be genuinely curious about the unknown parts. But those gapsâ€”the things that seem obviously missing once you get close enough to see themâ€”that&rsquo;s where the interesting work lives.</p>
<p>I catch myself falling into these same patterns constantly. Defaulting to frameworks when I should be sitting with confusion. Seeking pattern matches when I should be noticing what doesn&rsquo;t fit any pattern I know. It&rsquo;s uncomfortable to admit how often I&rsquo;ve been &ldquo;playing house&rdquo; with ambiguity rather than actually engaging with it.</p>
<p>But here&rsquo;s what I&rsquo;m starting to understand: the discomfort of not knowing isn&rsquo;t a bug to be fixedâ€”it&rsquo;s a signal. When your institutional training doesn&rsquo;t immediately apply, when familiar frameworks feel inadequate, when you can&rsquo;t quickly decompose the problem into manageable pieces, you might be looking at something genuinely important.</p>
<p>The real skill isn&rsquo;t learning to eliminate that discomfort, but learning to sit with it productively. To stay curious about gaps that seem obvious but somehow remain unexplored. To trust that the feeling of &ldquo;surely someone has figured this out already&rdquo; might be wrongâ€”and that if you get close enough to the fractal edge of knowledge, you&rsquo;ll find whole territories waiting to be explored.</p>
<p>Maybe the goal isn&rsquo;t to get better at navigating ambiguity, but to get more comfortable being genuinely confused by things that matter.</p>
<hr>
<p>If you&rsquo;ve managed to read till here, it&rsquo;s probably worth checking out <a href="https://dtr.northwestern.edu/">Design Technology Research (DTR)</a>. DTR is a research and learning community where students design and study technologies that support how people learn, collaborate, and create. If you go to Northwestern, I&rsquo;d  recommend you apply.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-from-indexing-to-understanding-intent-in-discovery-systems" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('from-indexing-to-understanding-intent-in-discovery-systems')">&times;</button>
      <h2>From Indexing to Understanding Intent in Discovery Systems</h2>
      <p class="project-meta">October 26, 2025</p>
      <div class="modal-content">
        <h1 id="from-indexing-to-understanding-intent-in-discovery-systems">From Indexing to Understanding Intent in Discovery Systems</h1>
<p>Discovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoningâ€”delivering fast, contextually relevant answers while offering deeper AI-powered synthesis when users explore unfamiliar or complex topics.</p>
<hr>
<h2 id="why-traditional-search-falls-short">Why Traditional Search Falls Short</h2>
<p>Traditional keyword search works best when the user knows exactly what to ask. However, most discovery today begins with uncertainty: a half-remembered quote, a general theme, or a desire for inspiration. Users often don&rsquo;t know what they&rsquo;re looking for until they see it. The problem is both cognitive and technicalâ€”users&rsquo; mental models evolve as they explore. Modern discovery tools must anticipate that evolution, interpret vague intent, and surface meaningfully related ideas rather than exact word matches.</p>
<hr>
<h2 id="from-indexing-to-intent-based-query-completion">From Indexing to Intent-Based Query Completion</h2>
<p>The transition from indexing to intent-driven discovery mirrors a broader movement from databases that store information to systems that reason about context. In practice, this takes two complementary forms: <em>fast responses</em> and <em>deep responses</em>.</p>
<p>Fast responses are built for immediate clarity. They combine multiple recall routesâ€”lexical, semantic, behavioral, and socialâ€”to provide accurate answers quickly. This approach excels in known-item retrieval and factual questions. For instance, <strong>Fable&rsquo;s new search experience</strong> goes beyond simple keyword matching to understand emotional and stylistic dimensions. When a reader searches &ldquo;books that feel like autumn evenings,&rdquo; the system infers ambience and tone rather than relying solely on metadata tags. This design transforms search from literal matching to contextual association.</p>
<p>

  <img src="/images/product/intent-discovery/fable-predictive-search.png" alt="Fable&rsquo;s predictive search improvements" loading="lazy">
 
<em>Fable&rsquo;s enhanced search showing predictive completion for incomplete queries like &ldquo;heaven and earth g&rdquo;</em></p>
<p>The system demonstrates superior intent understanding by providing relevant suggestions before users complete their queries. The &ldquo;After&rdquo; version shows how modern search anticipates user needs, surfacing &ldquo;The Heaven &amp; Earth Grocery Store&rdquo; and related titles when users type partial queries.</p>
<p>

  <img src="/images/product/intent-discovery/fable-relevant-matches.png" alt="Fable&rsquo;s contextual matching" loading="lazy">
 
<em>Fable&rsquo;s improved relevance matching for author searches like &ldquo;james&rdquo;</em></p>
<p>Similarly, when searching for &ldquo;james,&rdquo; the enhanced system prioritizes contextually relevant results like &ldquo;James (Pulitzer Prize Winner)&rdquo; rather than generic matches, demonstrating how intent-aware systems understand query context.</p>
<p>Deep responses, in contrast, serve users exploring open-ended questions. They rely on large language models to synthesize information, drawing connections across content sources and explaining the reasoning behind conclusions. <strong>Red (Xiaohongshu)</strong> offers a compelling example through its <em>AskNow</em> feature, where AI-generated insights are grounded on verified community posts, maintaining both authenticity and transparency. Similarly, <strong>Reddit Answers</strong> (currently in beta) uses real user discussions as evidence for its responses, ensuring that generated insights remain rooted in human context rather than abstract data patterns.</p>
<p>

  <img src="/images/product/intent-discovery/red-asknow-results.png" alt="Red AskNow analysis" loading="lazy">
 
<em>Red&rsquo;s AskNow feature providing AI-generated company analysis with structured insights on market competition, privacy policies, and legal disputes</em></p>
<p>The AskNow interface demonstrates sophisticated content synthesis, taking user queries about companies like AppLovin and generating comprehensive analyses that include market positioning (competition with Meta, Google), regulatory challenges (privacy policies like IDFA), and risk factors (legal disputes affecting stock prices)â€”all grounded in community discussions rather than abstract data.</p>
<p>This dual architectureâ€”fast for confidence, deep for curiosityâ€”captures how systems can dynamically adapt to user intent during a single session.</p>
<hr>
<h2 id="the-two-loop-discovery-engine">The Two-Loop Discovery Engine</h2>
<p>Intent-aware systems operate through a continuous feedback structure combining retrieval and reasoning. The retrieval loop aggregates relevant results using hybrid signals such as keyword relevance, vector similarity, and recency. The reasoning loop interprets ongoing behaviorâ€”clicks, refinements, and skipsâ€”to update an <em>Intent State</em> that guides subsequent outputs. Each iteration yields one of three outcomes: a direct answer, a curated content set, or an AI-generated synthesis.</p>
<hr>
<h2 id="mapping-intent-and-context">Mapping Intent and Context</h2>
<p>Different user goals demand different discovery experiences. The table below illustrates how systems can tailor responses according to intent and specificity.</p>
<table>
  <thead>
      <tr>
          <th>User Goal</th>
          <th>Specificity</th>
          <th>Ideal Response</th>
          <th>Interface Type</th>
          <th>Example</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Retrieve</td>
          <td>Exact</td>
          <td>Concise factual snippet with citation</td>
          <td>Inline summary</td>
          <td>&ldquo;Release date of Dune Part Two.&rdquo;</td>
      </tr>
      <tr>
          <td>Learn</td>
          <td>Fuzzy</td>
          <td>Conceptual overview with examples and follow-ups</td>
          <td>Accordion-style cards</td>
          <td>&ldquo;How do neural embeddings improve search?&rdquo;</td>
      </tr>
      <tr>
          <td>Decide</td>
          <td>Mid</td>
          <td>Structured comparison with trade-offs</td>
          <td>Comparison grid with rationale notes</td>
          <td>&ldquo;Which AI search tools balance transparency and cost?&rdquo;</td>
      </tr>
      <tr>
          <td>Explore</td>
          <td>Open</td>
          <td>Serendipitous content spanning adjacent ideas</td>
          <td>Visual knowledge map or gallery</td>
          <td>&ldquo;Books that inspire design thinking.&rdquo;</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="design-principles-for-intent-aware-search">Design Principles for Intent-Aware Search</h2>
<p>An effective discovery system prioritizes clarity, adaptability, and transparency. Each result should show why it was retrievedâ€”through cues such as &ldquo;popular in similar sessions&rdquo; or &ldquo;matches your theme and tone.&rdquo; Systems should learn within each session, refining their understanding as the user interacts. They must balance novelty with relevance, maintaining an exploratory rhythm without overwhelming the user. Transparency matters most: showing data sources, confidence levels, and offering manual control over personalization builds long-term trust. Lastly, discovery should feel seamless across formatsâ€”books, posts, videos, and conversationsâ€”as part of a single cognitive journey.</p>
<hr>
<h2 id="when-the-corpus-falls-short">When the Corpus Falls Short</h2>
<p>An intent-aware system recognizes when no direct answer exists. Instead of ending in failure, it synthesizes a grounded response that discloses its sources and confidence level. These generated insights transform information gaps into moments of learningâ€”highlighting missing viewpoints, summarizing scattered data, or surfacing counterexamples that broaden understanding.</p>
<hr>
<h2 id="measuring-success-through-comprehension">Measuring Success Through Comprehension</h2>
<p>Success in intent-based discovery extends beyond engagement metrics. Systems should measure <strong>Time to Insight (TTI)</strong> to understand how quickly users reach clarity, <strong>Exploration Depth</strong> to gauge how broadly users traverse concepts, and <strong>Trust Indicators</strong> reflecting how often users expand citations or rely on AI explanations. Another key metric, <strong>Intent Alignment</strong>, measures how closely the system&rsquo;s inferred goal matches the user&rsquo;s evolving intent. These metrics move focus from clicks to comprehension, rewarding designs that help people think more efficiently.</p>
<hr>
<h2 id="why-intent-awareness-matters">Why Intent Awareness Matters</h2>
<p>Search and recommendation are converging into a unified discipline focused on <em>intent understanding</em>. Platforms like Fable, Red, and Reddit show how discovery is shifting from indexing content to interpreting context. The future of search will belong to systems that bridge speed and synthesisâ€”those that recognize a user&rsquo;s goal, respond in real time, and expand understanding with each interaction.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-when-knowledge-becomes-fluid-how-ai-transforms-learning" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('when-knowledge-becomes-fluid-how-ai-transforms-learning')">&times;</button>
      <h2>When Knowledge Becomes Fluid: How AI Transforms Learning</h2>
      <p class="project-meta">October 26, 2025</p>
      <div class="modal-content">
        <h1 id="when-knowledge-becomes-fluid">When Knowledge Becomes Fluid</h1>
<p>For most of history, knowledge has been bound â€” fixed in books, trapped in formats, and constrained by how it could be consumed. You could read a page, listen to a lecture, or watch a documentary, but each existed in isolation. With generative models, that boundary begins to dissolve. Knowledge itself becomes fluid â€” able to reshape, reframe, and re-express itself across contexts and mediums.</p>
<p>As <a href="https://every.to/@danshipper">Dan Shipper</a> illuminates in his exploration of language models, we now have what amounts to &ldquo;free energy for text&rdquo; â€” the ability to transform any piece of knowledge through compression, expansion, and translation operations. What once required manual summarization and editing can now be synthesized in real time. A single idea can compress into focused insights or expand into comprehensive explorations, adapting not just in length but across dimensions of style, tone, and perspective. Instead of treating knowledge as static, we can begin to design it as something alive â€” capable of adapting to how, when, and where we learn.</p>
<hr>
<h2 id="scaling-knowledge">Scaling Knowledge</h2>
<p>Most modern learning apps do a good job of compressing information, but they still rely on manual curation. Platforms like Blinkist summarize a few thousand books, turning complex ideas into short, digestible snippets. Useful, but limited. Language models transcend this constraint by operating as cultural technologies â€” giving us access to the best of what humanity knows about any topic, compressed into the right form for any given context.</p>
<p>But compression itself has evolved beyond simple summarization. Knowledge can now be compressed across multiple dimensions simultaneously: comprehensive for breadth, engaging for attention, stylistic for voice, or contextual for specific audiences. The same complex text can become a technical analysis, a conversational explanation, or an irreverent commentary â€” each compression preserving different aspects of the original while serving different cognitive needs.</p>
<p>

  <img src="/images/posts/fluid-knowledge/compression-process-diagram.png" alt="Language Model Compression Process" loading="lazy">
 
<em>Source: Dan Shipper, Every</em></p>
<p>

  <img src="/images/posts/fluid-knowledge/attention-compression-types.png" alt="Attention vs Compression Types" loading="lazy">
 
<em>Source: Dan Shipper, Every</em></p>
<p>This multi-dimensional transformation means that accessibility is no longer just about reading level or length. Knowledge can adapt its entire presentation â€” its tone, complexity, cultural references, and emotional register â€” to match not just what you need to know, but how you need to encounter it. The scale of knowledge expands not by adding editors, but by giving knowledge itself the ability to self-express across infinite dimensions of human understanding.</p>
<hr>
<h2 id="fluid-learning">Fluid Learning</h2>
<p>Learning is deeply contextual. Reading a dense essay might work at a desk, but not while commuting or cooking. The same knowledge can take different forms depending on where we are and what we&rsquo;re doing. Language models enable this flexibility through three types of expansion: comprehensive expansions that provide broad overviews, contextual expansions that fit information to your specific background and circumstances, and creative expansions that explore possibility spaces and generate new connections.</p>
<p>This transformation respects the fundamental constraint of human attention while creating new pathways for engagement. When you ask a question, it can expand into an answer tailored not just to what you want to know, but to who you are, what you already understand, and how much cognitive energy you have available. A complex concept might become a Wikipedia-style explanation for comprehensive understanding, a personalized tutorial fitted to your experience level, or a creative exploration that connects the idea to your existing interests.</p>
<p>The result is a more continuous, natural learning flow â€” where every question contains its answer, waiting to be expanded in the precise form you need. Instead of forcing you to adapt to the medium, the medium adapts to you, creating conditions for understanding and stepping back to let learning emerge.</p>
<hr>
<h2 id="connecting-ideas">Connecting Ideas</h2>
<p>The next step is not just summarizing knowledge, but connecting it. A truly generative library doesn&rsquo;t only compress information â€” it discovers relationships. It can find the echoes that cut across books, fields, and centuries. It might reveal that The Art of War and Measure What Matters both explore alignment under uncertainty â€” one in ancient warfare, the other in modern management.</p>
<p>This kind of synthesis transforms learning from retrieval to insight. Instead of static archives, we begin to build creative constellations of ideas. Knowledge becomes something that grows through its connections, not just its content. We move from consuming isolated summaries to experiencing patterns of thought that evolve as we explore them.</p>
<hr>
<h2 id="interactive-understanding">Interactive Understanding</h2>
<p>Summaries can tell you what to think, but they rarely teach you how to think. A new generation of learning systems makes this process interactive by leveraging the fundamental difference between compression and expansion operations. Compression is predictable â€” like squeezing a lemon, you get concentrated essence of what was already there. Expansion is creative and unpredictable â€” like an acorn growing into a tree, the final form depends on the conditions and interactions along the way.</p>
<p>This distinction transforms how we design learning experiences. Rather than passively reading compressed knowledge, you engage with systems that can expand your questions into explorations. You ask about Stoicism, and the system doesn&rsquo;t just compress existing texts â€” it expands the inquiry into new territories, generating connections you hadn&rsquo;t considered, posing questions that emerge from the intersection of your curiosity and the knowledge space.</p>
<p>Over time, these systems learn from your curiosity patterns. The experience becomes conversational â€” a collaboration between human intuition and machine reasoning. Learning feels less like consumption and more like cultivation, where understanding grows through the unpredictable but guided expansion of ideas, shaped by your attention and questions.</p>
<hr>
<h2 id="a-living-medium">A Living Medium</h2>
<p>Generative AI changes what a book even is. It turns knowledge into a living medium â€” fluid, adaptive, and co-creative. Instead of locking ideas into fixed containers, we can let them flow. A book becomes a conversation, a lecture becomes an experience, and learning becomes something that moves with us.</p>
<p>This transformation builds on something uniquely human: our capacity to ask questions. Question-asking creates room for answers, and answers create room for more questions â€” the fundamental engine of learning and creativity. Language models extend this capacity by creating a world where every question already contains its answer, waiting to be expanded in whatever form serves your understanding best.</p>
<p>When knowledge becomes fluid, understanding no longer depends on how much we can read or memorize. It depends on how well we can collaborate with systems that transform our curiosity into insight â€” compressing vast knowledge into focused understanding, expanding simple questions into rich explorations, and translating ideas across the boundaries that once separated disciplines, formats, and minds. Knowledge moves, and we move with it.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-from-raw-social-data-to-real-research" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('from-raw-social-data-to-real-research')">&times;</button>
      <h2>From Raw Social Data to Real Research</h2>
      <p class="project-meta">September 15, 2025</p>
      <div class="modal-content">
        <p>

  <img src="/images/projects/crowdlistening/homepage-new.png" alt="CrowdListen Homepage" loading="lazy">
 </p>
<h2 id="from-content-aggregation-to-original-research">From Content Aggregation to Original Research</h2>
<p><a href="https://crowdlisten.com">Crowdlisten</a> transforms large-scale social conversations into actionable insight by integrating LLM reasoning with extensive model context protocol (MCP) capabilities. While being able to quantitatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven&rsquo;t yet appeared in established reporting.</p>
<p>Deep research features provide professional-looking research reports, yet the contents are far from original, as they&rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet&rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they&rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.</p>
<p>I&rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlisten, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn&rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows.</p>
<p>I paused Crowdlisten in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I&rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).</p>
<p>What I&rsquo;m particularly interested in exploring and building with Crowdlisten is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.</p>
<h2 id="product-suite-overview">Product Suite Overview</h2>
<p>

  <img src="/images/projects/crowdlistening/product-suite-new.png" alt="Product Suite" loading="lazy">
 </p>
<p>CrowdListen has evolved into a comprehensive suite of AI-powered products designed to address different aspects of social intelligence and content strategy. The Analyze product serves as our core offering, enabling users to discover what people really think about any topic through sophisticated AI-powered sentiment analysis and opinion mining capabilities. This goes beyond simple positive/negative categorization to understand nuanced perspectives, emotional context, and the underlying reasons behind audience reactions.</p>
<p>Our Research product delivers an agentic research experience that systematically analyzes large volumes of social media content at scale. This comprehensive approach takes longer to run but provides significantly more thorough coverage across platforms, enabling researchers to uncover deeper insights and emerging patterns that automated dashboards typically miss.</p>
<p>The Predict product represents our foray into predictive analytics, allowing users to test content variations and predict audience engagement before publishing. Using AI simulation technology, teams can experiment with different messaging approaches and understand likely audience reactions without the risk and cost of live testing.</p>
<p>Finally, our Insights+ product caters to enterprise users and power analysts who need advanced analytics capabilities and custom reporting features. This tier provides the depth and customization necessary for organizations making strategic decisions based on social intelligence data.</p>
<h2 id="the-insight-paradox">The Insight Paradox</h2>
<p>Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can&rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlisten bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in <a href="/posts/essense_of_creativity/">&ldquo;Essence of Creativity&rdquo;</a> - helping users understand massive amounts of information and generate meaningful insights when they &ldquo;don&rsquo;t know what output they want.&rdquo;</p>
<h2 id="technical-architecture-multi-modal-by-design">Technical Architecture: Multi-Modal by Design</h2>
<p>The rationale behind Crowdlisten&rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video&rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.</p>
<p>

  <img src="/images/projects/crowdlistening/analyze-interface.png" alt="Analysis Interface" loading="lazy">
 </p>
<p>Crowdlisten&rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM&rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities.</p>
<p>This dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlisten can generate insights that reflect not just what is being said, but how it&rsquo;s being said, the surrounding context, and the audience&rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the &ldquo;insight paradox&rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlisten: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.</p>
<h2 id="detailed-analysis-capabilities">Detailed Analysis Capabilities</h2>
<p>The platform provides granular breakdowns of content performance and audience reactions. Users can explore specific themes, track sentiment over time, and identify the most engaging content types across different categories and industries. This helps brands understand not just what is being said, but why certain content resonates with their audience.</p>
<p>

  <img src="/images/projects/crowdlistening/category-analysis.png" alt="Category Analysis" loading="lazy">
 </p>
<p>The opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.</p>
<h2 id="advanced-research-infrastructure">Advanced Research Infrastructure</h2>
<p>

  <img src="/images/projects/crowdlistening/research-command-center-new.png" alt="Research Command Center" loading="lazy">
 </p>
<p>CrowdListen&rsquo;s research infrastructure is built around a sophisticated orchestration system that coordinates multiple specialized AI engines. The Research Command Center provides users with a unified interface to launch complex analysis workflows while monitoring the progress of different analytical engines in real-time.</p>
<p>Our system utilizes the Research Engine, which orchestrates various AI engines including the Insight Engine for sentiment analysis, Media Engine for multimodal content processing, Query Engine for information retrieval, and Report Engine for generating executive-ready reports. This modular architecture allows for scalable analysis that can adapt to different research requirements.</p>
<p>

  <img src="/images/projects/crowdlistening/research-interface-new.png" alt="Research Interface" loading="lazy">
 </p>
<p>The research interface enables users to input complex queries and optionally upload analysis templates to guide the investigation. The system then automatically determines which analytical capabilities to deploy, processing everything from web search and specialized platform data collection to multi-layered content analysis and synthesis.</p>
<p>This integrated approach represents a significant advancement over traditional social media monitoring tools, enabling researchers to conduct comprehensive investigations that would typically require weeks of manual work in a matter of minutes while maintaining the depth and rigor of human-led research.</p>
<h2 id="case-study-google-notebooklm-analysis">Case Study: Google NotebookLM Analysis</h2>
<p>To demonstrate Crowdlisten&rsquo;s capabilities in product intelligence, we conducted a comprehensive analysis of user sentiment regarding Google&rsquo;s NotebookLM tool. This case study showcases our platform&rsquo;s ability to extract nuanced insights about emerging AI tools and understand user adoption patterns.</p>
<p>

  <img src="/images/projects/crowdlistening/notebooklm-analysis-new.png" alt="NotebookLM Analysis" loading="lazy">
 </p>
<p>When analyzing user sentiment around NotebookLM, our system provided a comprehensive overview showing that customer feedback indicates NotebookLM is effective for information synthesis and content generation, particularly in educational settings. However, users express concerns about the lack of persistent chat history, word count limits, and potential biases in the auto-generated podcast feature. Approximately 56% of users have a positive sentiment, praising its summarization capabilities and educational applications, while 34% express negative sentiment due to usability issues and accuracy concerns.</p>
<p>

  <img src="/images/projects/crowdlistening/notebooklm-themes-new.png" alt="Theme Analysis" loading="lazy">
 </p>
<p>Our thematic analysis reveals that Information Synthesis and Summarization is the most discussed topic, with 100 mentions representing 33.39% of all conversations. The sentiment breakdown shows overwhelmingly positive feedback for this core functionality, with users particularly appreciating the tool&rsquo;s ability to synthesize information from uploaded documents and aid in quick comprehension and analysis.</p>
<p>The detailed sentiment analysis shows specific user opinions, including praise for NotebookLM&rsquo;s effectiveness in summarizing and synthesizing information from uploaded documents, its utility for creating study guides and educational materials, and its ability to provide citations for generated information to help users verify accuracy and build trust in the tool&rsquo;s output.</p>
<p>

  <img src="/images/projects/crowdlistening/notebooklm-sources-new.png" alt="Source Analysis" loading="lazy">
 </p>
<p>Our analysis draws from 31 sources across 25 unique domains, indicating a moderate level of source diversity at 81%. The sources encompass various types including blogs, news outlets, and other platforms, offering a mix of perspectives. This comprehensive source analysis helps validate the reliability and breadth of our insights.</p>
<p>

  <img src="/images/projects/crowdlistening/notebooklm-related-new.png" alt="Related Topics" loading="lazy">
 </p>
<p>The platform also identifies related research opportunities, suggesting additional analysis areas such as specific research or writing challenges that NotebookLM helps users overcome, how effectively it addresses information overload, the biggest frustrations users encounter, and whether it has improved research workflows. This demonstrates our system&rsquo;s ability to not only analyze current sentiment but also identify strategic research directions.</p>
<h2 id="content-predictor-ai-powered-engagement-forecasting">Content Predictor: AI-Powered Engagement Forecasting</h2>
<p>

  <img src="/images/projects/crowdlistening/content-predictor-new.png" alt="Content Predictor" loading="lazy">
 </p>
<p>One of our most innovative features is the Content Predictor, which allows users to test content variations and predict audience engagement before publishing. This tool represents a significant advancement in social media strategy, enabling teams to experiment with different messaging approaches without the traditional risks and costs associated with live testing.</p>
<p>The Content Predictor uses a sophisticated three-step workflow. Users begin by generating multiple versions of their content, allowing our AI to create variations optimized for specific platforms like Twitter, Instagram, or LinkedIn. Next, the system runs engagement simulations using AI-powered user reactions that model realistic audience behavior patterns. Finally, users can view detailed simulation results and select the most promising content variations based on predicted performance metrics.</p>
<p>This capability is particularly valuable for brands and content creators who need to maximize the impact of their social media presence. Rather than relying on intuition or conducting expensive A/B tests with real audiences, teams can now validate their content strategies in a controlled environment before committing to publication. The system considers factors such as platform-specific audience behaviors, trending topics, and historical engagement patterns to provide accurate predictions.</p>
<p>The Content Predictor exemplifies our broader mission of transforming social media from a reactive medium to a strategic tool where decisions are informed by data and predictive intelligence rather than guesswork.</p>
<h2 id="validation-and-impact">Validation and Impact</h2>
<p>Our solution has been validated through interviews with major brands like L&rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlisten enables:</p>
<ul>
<li>Rapid response to emerging trends</li>
<li>Deep understanding of consumer sentiment across demographics</li>
<li>Identification of microtrends before they become mainstream</li>
<li>Competitive intelligence at unprecedented scale</li>
</ul>
<h2 id="the-future-of-mcp-driven-research">The Future of MCP-Driven Research</h2>
<p>We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.</p>
<p>This approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude&rsquo;s interface) emerge as this paradigm gains traction.</p>
<p>For social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they&rsquo;re covered in traditional reporting.</p>
<h2 id="credits">Credits</h2>
<p>This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Roy, Ruiwan, Qiping, Tongming and other members on the Creative team at TikTok, who I&rsquo;ve discussed early conceptions of this idea with.</p>
<h2 id="on-social-intelligence">On Social Intelligence</h2>
<p>Crowdlisten represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we&rsquo;re helping brands make more informed decisions faster than ever before.</p>
<p>As noted in <a href="/posts/essense_of_creativity/">&ldquo;Essence of Creativity&rdquo;</a>, the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we&rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.</p>
<hr>
<h2 id="-version-history">ðŸ“‹ Version History</h2>
<p><strong>v1.1</strong> â€¢ Oct 25, 2025 â€¢ <a href="https://github.com/terrylinhaochen/terrylinhaochen.github.io/commits/main/content/product/crowdlistening.md">View changes</a> â€¢ Updated Title</p>
<p>ðŸ’¡ <em>Click &ldquo;View changes&rdquo; to see exactly what changed between versions</em></p>

      </div>
    </div>
  </div>
  
  <div id="modal-iterating-at-the-pace-of-ai" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('iterating-at-the-pace-of-ai')">&times;</button>
      <h2>Iterating at the Pace of AI</h2>
      <p class="project-meta">August 29, 2025</p>
      <div class="modal-content">
        <p>There are two credible paths to building agentic experiences. The first is platform-first: stand up a unified agent framework with the core capabilitiesâ€”multi-turn conversation, a knowledge base, and memoryâ€”and then layer in signifiers and affordances that fit your environment. The second is scenario-first: begin with the thinnest viable surface and add only the features that demonstrably create value beyond what ChatGPT or Copilot already provide, bringing in memory and other &ldquo;platform&rdquo; features only once they have earned their keep. The platform-first approach yields a consistent engineering experience and lets teams reuse prior agent work, but it risks poor agentâ€“scenario fit. The scenario-first approach can feel messier and demands more from product managers, yet it validates real-world use cases faster. I don&rsquo;t claim one approach is universally betterâ€”startups and large companies face different constraintsâ€”but I do believe there is only one way to prototype: ship quickly, test explicit hypotheses, and iterate without delay.</p>
<p>A clarifying question keeps this cadence honest: what is the minimum version of the product that lets us learn whether the solution can find productâ€“market fit? Counterintuitively, you often do not need a working prototype to answer that. Walking through end-to-end customer scenarios frequently reveals whether a proposed feature fits existing workflows and where it will break. That said, some questions hinge on new engineeringâ€”experiences that are hard to reason about in the abstract. In those cases, the objective is not to &ldquo;build the demo,&rdquo; but to surface and test the assumptions that matter. Each design choice should map to the outcome it seeks and to the user challenge it addresses. The simpler the stack, the more learning cycles you can run with less effort, which is the real engine of progress.</p>
<h2 id="the-ai-powered-development-advantage">The AI-Powered Development Advantage</h2>
<p>

  <img src="/images/projects/agent_prototyping/ai-advantages-survey.png" alt="AI vs Low-Code Survey Results" loading="lazy">
 </p>
<p>Survey data from a16z Enterprise reveals why AI-powered tools are gaining traction over traditional low-code solutions, with natural language interaction and rapid prototyping leading the advantages.</p>
<p>Modern AI coding tools make this possible. Cursor, GitHub Copilot, and Claude Code compress build time by generating boilerplate, suggesting common patterns, and helping troubleshoot. A single engineer can now produce a functional MVP in a fraction of the time that used to require a small team. Much like Figma tightened the collaboration loop in design, these tools narrow the gap between product intent and implementation. The result is not merely faster engineering; it is broader participation. Product managers, designers, even sales and customer success teams can test ideas more directly, while engineers concentrate on production-grade systems and reliability concerns that truly benefit from their specialization.</p>
<h2 id="involving-cross-functional-stakeholders">Involving Cross-Functional Stakeholders</h2>
<p>An agentic experience is only as good as our understanding of the underlying problem. This is especially true for expert workflowsâ€”consumption-based cost estimation or SOC investigation, for exampleâ€”where product and engineering teams are rarely the domain experts. Involving architects, sales engineers, and analysts only at the prompt-iteration stage is not enough. To build agent behaviors that actually fit, we have to internalize existing workflows and best practices, then design signifiers and affordances that match practitioner expectations. Language, steps, intermediate outputs, and handoffs should mirror how experts already think and work. When the agent speaks their dialect and respects their process, adoption follows because the experience feels native rather than novel for novelty&rsquo;s sake.</p>
<p>

  <img src="/images/projects/agent_prototyping/ai-tools-comparison.png" alt="Figma Collaboration Model" loading="lazy">
 </p>
<p>This is exactly where the Figma analogyâ€”Kevin Kwok&rsquo;s point about non-linear returns from tighter collaboration loopsâ€”becomes operational. Figma did not just make drawing easier; it made critique, alignment, and decision-making happen in the same place, by the right people, at the right time. AI coding assistants catalyze a similar shift for agentic products: they collapse the distance between a domain expert&rsquo;s intent and a working prototype, making assumptions explicit, turning tacit heuristics into checkable rules, and surfacing disagreements while they are still cheap to resolve. When prototypes function as shared canvasesâ€”co-edited by PMs, engineers, and subject-matter expertsâ€”the loop tightens further: experts shape the signifiers and workflows, product sharpens the hypotheses, and engineering focuses on robustness and safety. The compounding return comes not from adding more features, but from aligning agent behavior with the realities of the domain.</p>
<h2 id="learnings-from-the-cost-estimator-agent">Learnings from the Cost Estimator Agent</h2>
<p>To ground these principles, let&rsquo;s look at an agentic implementation of a cost estimation scenario</p>
<h3 id="project-context">Project Context</h3>
<p>Customers need accurate cost estimates for budget planning and solution comparison, yet consumption-based pricing is notoriously hard to predict. We heard repeatedly from the field that this uncertainty stalls decisions and, in competitive deals, can tilt outcomes against us. Existing tools do not help enough. Web calculators feel like black boxes with coarse, inflexible inputs and little transparency. Spreadsheet models are opaque and fragile, with assumptions scattered across cells. Both often ask for inputs customers do not understand or cannot provide without heavy translation.</p>
<p>

  <img src="/images/projects/agent_prototyping/cost-estimation-agent.png" alt="The Knowledge Quadrant" loading="lazy">
 </p>
<p>In other words, this is not a known unknowns problem where a general-purpose copilot can retrieve an answer upon request. Nor is it an unknown knowns problem where the customer already has a tried-and-true estimation method and we simply need to automate it. It is often an unknown unknowns problem: customers do not know what to ask, and they do not have the raw data in the needed form. The result is planning paralysis and, ultimately, stalled or lost deals.</p>
<h3 id="design-rationale">Design Rationale</h3>
<p>Designing for &ldquo;unknown unknowns&rdquo; required optimizing along three intertwined dimensions. First, we focused on transparency and control so that users could see the reasoning behind estimatesâ€”the assumptions, intermediate calculations, and trade-offsâ€”and adjust inputs with confidence. Numbers without narrative do not build trust, and trust is the currency of estimation. Second, we embedded domain expertise directly in the experience. Instead of pushing the knowledge gap back to the user, the system translated familiar factsâ€”industry patterns, ingestion profiles, retention policiesâ€”into the metrics the pricing model requires, pre-populating where possible and teaching as it went. Third, we treated estimation as a process rather than a form, and we designed for iterative refinement. The goal was not a one-shot answer but a guided conversation that converges on confidence.</p>
<p>At a basic level, we began with an agent side-panel, similar to a Copilot, to unify product documentation, pricing schemas, and frequently asked questions. This supported conversational guidance throughout the estimation process, but it also exposed three frictions we had to solve in order to achieve fit. First, use-case discovery was weak: without strong signifiers, users did not know what to ask and often ventured beyond the agent&rsquo;s scope. Second, chat lacked context: humans are economical with effort, so expecting users to restate all the fields they had filled and the stage they were in created unnecessary friction. Third, people don&rsquo;t know what they don&rsquo;t know: there is a structural gap between what customers know about their business (for example, number of users, typical event patterns) and what we require to estimate costs (for example, daily gigabytes ingested). Simply asking, &ldquo;How many gigabytes per day?&rdquo; does not bridge that gap.</p>
<p>

  <img src="/images/projects/agent_prototyping/agent-framework-patterns.png" alt="Agent Architecture" loading="lazy">
 </p>
<p>These insights shaped a prototype with two synchronized surfaces: a pricing panel and an agent panel kept in bidirectional sync. Edits in the graphical interface updated the conversation&rsquo;s context, and the agent&rsquo;s reasoning flowed back as explanation cards anchored beside the fields they affected.</p>
<p>In brownfield scenarios, the agent could pull relevant account signals to prefill inputs and explain each value&rsquo;s provenance. In greenfield scenarios, the experience offered size recommendationsâ€”small, medium, large, enterpriseâ€”that users could apply with one click, each accompanied by clear rationales and editable assumptions.</p>
<p>When hard numbers were missingâ€”say, daily ingestion in gigabytesâ€”the agent asked questions users could answer about environment size, event rates, and retention needs, then converted those responses into derived estimates, showing the math and inviting adjustments. Under the hood, a focused knowledge base provided product and pricing facts, while three structured workflowsâ€”volume estimation, pricing estimation, and design recommendationsâ€”gave the conversation shape and kept it oriented toward decisions rather than dialogue for its own sake.</p>
<h2 id="evaluation-and-benchmarking">Evaluation and Benchmarking</h2>
<p>Agent platforms encourage generality, but effectiveness must be demonstrated on concrete tasks. We evaluate the experience by asking whether it completes representative estimation scenarios end to end, how its outputs compare to human-expert baselines, and how quickly it converges to a result stakeholders trust. Accuracy matters, but so do user effort and confidence. When building agentic experiences, we should track time to an acceptable estimate, the number of clarifying turns, and whether users report understanding and accepting the assumptions they carry forward. Scenario coverage also matters: behavior needs to hold not only in the &ldquo;happy path,&rdquo; but across brownfield and greenfield cases, high-volume and bursty workloads, and strict-retention and cost-optimized policies. When behavior degrades, it should degrade gracefully with clear explanations, ranges, or a handoff to a human expert.</p>
<p>In larger organizations, evaluation pairs with safeguards. Data validation and drift monitoring ensure that quotes reflect current pricing and product information, with alerts when underlying references change. Guardrails protect embedded expert logicâ€”estimation methods and pricing strategiesâ€”against prompt injection and leakage of system instructions, and they constrain access to sensitive APIs. Finally, bad-case handling is a first-class requirement: the system detects ambiguous inputs, surfaces low-confidence steps, and offers conservative defaults or escalation paths rather than silently producing spurious precision. Specifications and engineering plans that omit scenario walkthroughs, benchmarks, and safeguards drift toward imagined use cases and weak agentâ€“scenario fit; those that include them turn agentic ambition into reliable impact.</p>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>Choose a build path that fits your context, but always prototype to learn, not to impress. Use AI tools to shorten the distance between ideas and feedback. Bring domain experts into the design of signifiers and workflows so the agent respects reality. Make reasoning visible, embed expertise at the point of need, and shape the experience for iterative refinement. Then prove it with scenario-based evaluation and strong guardrails. This, I believe, is how you truly iterate at the pace of AI.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-social-listening-for-product-insight" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('social-listening-for-product-insight')">&times;</button>
      <h2>Social Listening for Product Insight</h2>
      <p class="project-meta">August 29, 2025</p>
      <div class="modal-content">
        <h2 id="why-we-need-more-ways-to-hear-customers">Why We Need More Ways to Hear Customers</h2>
<p>Your last customer call was three weeks ago. Your PM dashboard shows green metrics. Yet your latest feature has a 12% adoption rate, and the support tickets keep growing. Sound familiar?</p>
<p>Here&rsquo;s the trap every growing product team falls into: you start with great customer connections, but success creates distance. Suddenly you&rsquo;re building for your five power users while 95% of your market stays silent. The result? Products that feel over-engineered to newcomers and underwhelming to everyone else. We end up in what I call the &ldquo;vocal minority trap&rdquo;â€”building for the loudest voices while missing authentic needs from the broader market.</p>
<p>With AI-powered social listening tools and tighter feedback loops, we can flip from product â†’ market to market â†’ product development. Instead of building first and hoping for adoption, we can listen broadly to authentic customer discourse, form grounded hypotheses from real needs, then decide what to build. This approach lets us partner closely with key customers while staying connected to the broader market conversation.</p>
<p>

  <img src="/images/projects/crowdlistening/microsoft-sentinel-overview.png" alt="Microsoft Sentinel Analysis Overview" loading="lazy">
 </p>
<h2 id="the-ai-way-to-do-it">The AI Way to Do It</h2>
<p>Traditional social listening tells you what people think about your brand. But what if you could use the same approach to discover what products people actually needâ€”before you build them?</p>
<p>Beyond internal testers and formal user research channels, the internet already hosts rich, authentic signals about how productsâ€”yours and competitors&rsquo;â€”land in the real world. Community forums, Q&amp;A sites, GitHub issues, Reddit threads, and support communities contain unfiltered customer discourse about pain points, expectations, and the exact language users employ when describing their needs. If we capture, filter, and analyze that discourse responsibly, we get a truer view of market needs than traditional research methods often provide.</p>
<h3 id="practical-workflow">Practical workflow</h3>
<p>The system operates through two complementary approaches based on your research needs. <strong>Targeted discovery</strong> works best when you have specific product questionsâ€”like &ldquo;why do users abandon our checkout flow?&rdquo; Here, we compile a focused corpus from relevant public discussions (support forums, Reddit threads, GitHub issues), extract and normalize the text, then use large-context LLMs to identify and synthesize recurring themes with direct citations back to source material.</p>
<p><strong>Open-ended exploration</strong> suits broader market research where you&rsquo;re not sure what patterns might emerge. This approach generates embeddings at both sentence and discussion levels, applies semantic clustering to group similar concerns naturally, and labels each cluster with evidence-linked summaries. The goal isn&rsquo;t to confirm existing hypotheses but to discover what customers actually discuss when they think no one from your company is listening.</p>
<p>Every discovered theme flows into a structured decision scaffold: <strong>theme</strong> â†’ <strong>testable hypothesis</strong> â†’ <strong>bet</strong> â†’ <strong>telemetry</strong> â†’ <strong>refinement</strong>. This prevents what I call &ldquo;insight theater&rdquo;â€”beautiful dashboards that don&rsquo;t change what gets built. For example, discovering that users frequently mention &ldquo;billing confusion&rdquo; becomes the hypothesis &ldquo;clearer usage breakdowns will reduce support tickets,&rdquo; which becomes the bet &ldquo;redesign invoice layout,&rdquo; measured through &ldquo;support ticket volume&rdquo; and refined based on actual outcomes.</p>
<p>Longer-term, the system runs as an always-on Customer Insight Radar that continuously ingests external communities alongside internal customer notes (with appropriate privacy filters). It tracks theme velocityâ€”which concerns are growing versus shrinkingâ€”and maintains representative quotes so product managers and engineers can feel the human reality behind abstract metrics.</p>
<p>

  <img src="/images/projects/crowdlistening/sentiment-breakdown.png" alt="Audience Sentiment Analysis" loading="lazy">
 </p>
<h3 id="primary-vs-secondary-sources">Primary vs. secondary sources</h3>
<p>When building your corpus, it&rsquo;s crucial to distinguish between primary and secondary sources to maximize insight quality. Treat raw online discourseâ€”posts, threads, issue comments, and direct user communicationsâ€”as primary research. These represent unfiltered customer voices expressing genuine needs and frustrations in their own language. In contrast, polished content like vendor blog posts, marketing materials, and SEO-optimized pages should be considered secondary sources that often reflect corporate messaging rather than authentic user experience.</p>
<p>Many &ldquo;deep research&rdquo; tools inadvertently amplify secondary sources because they index what&rsquo;s easily crawlable and well-structured, leading to summaries of summaries rather than original insights. Our approach prioritizes going directly to the source, weighting firsthand accounts more heavily, and using secondary material only for background context or to triangulate findings from primary sources.</p>
<h2 id="a-bit-more-on-embeddings-and-why-they-help">A Bit More on Embeddings (and Why They Help)</h2>
<p>Think of embeddings as a way to teach computers what words and sentences actually mean. Instead of just matching exact keywords, AI can understand that &ldquo;billing is confusing&rdquo; and &ldquo;invoices are unclear&rdquo; are talking about the same problem, even though they use different words.</p>
<p>Early embedding methods learned one meaning per wordâ€”fast and useful for basic clustering, but they missed context. The word &ldquo;bill&rdquo; could mean an invoice or a proposed law, and the system couldn&rsquo;t tell the difference. Modern contextual encoders solve this by creating different representations for the same word depending on how it&rsquo;s used, then combining these into sentence and document-level meanings.</p>
<p>For social listening, sentence and document embeddings are the workhorses that enable semantic search (finding &ldquo;unreconcilable line items&rdquo; when someone searches for &ldquo;can&rsquo;t map charges to usage&rdquo;), organic theme discovery without predefined categories, and tracking how customer language evolves over time. In practice, we create embeddings at both sentence and full discussion levels, use ANN (Approximate Nearest Neighbor) indexing for fast retrieval, and employ clustering methods that can handle the uneven, natural shape of real community discussionsâ€”always linking themes back to actual customer quotes for transparency.</p>
<h2 id="reasoning-models-and-visualizations">Reasoning Models and Visualizations</h2>
<p>With smaller datasets, we can directly provide the extracted raw text to a large-context, strong-reasoning model. This goes beyond coarse sentiment categories or word clouds, enabling richer context and actionable insights. Curated prompts let an LLM (or agent) read the semantic neighborhoods, surface key themes, note counter-signals, and propose testable hypotheses.</p>
<h3 id="harvesting-customer-testimonials-at-scale">Harvesting Customer Testimonials at Scale</h3>
<p>One particularly powerful application involves extracting authentic customer impact stories from large collections of interview transcripts and recorded conversations. Rather than manually combing through dozens of hours of customer calls, modern AI tools can systematically identify and structure testimonial content in minutes rather than days.</p>
<p>The workflow is surprisingly straightforward: First, transcribe customer interviews, podcast episodes, or recorded calls using automated transcription services. Next, upload these transcripts to large-context reasoning models like NotebookLM or Claude. Then ask the AI to identify every quote describing specific product impact, organizing results into structured tables with columns for quote, customer name, company, and use case. Finally, use AI-powered copywriting tools to convert raw testimonials into polished, punchy customer quotes suitable for marketing materials.</p>
<p>This approach transforms 1-2 days of manual testimonial extraction into under an hour of guided AI work. More importantly, it ensures comprehensive coverageâ€”human reviewers naturally miss valuable quotes buried in lengthy conversations, while AI can systematically process every statement for relevant content. The result is a richer, more complete picture of how customers actually describe your product&rsquo;s value in their own language.</p>
<p>

  <img src="/images/projects/crowdlistening/detailed-analysis.png" alt="Key Themes Analysis" loading="lazy">
 
<em>Detailed thematic analysis reveals customer pain points through AI-powered clustering and sentiment analysis, enabling product teams to identify high-impact opportunities from authentic user discourse.</em></p>
<p>

  <img src="/images/projects/crowdlistening/product-insights.png" alt="Product Insights Dashboard" loading="lazy">
 
<em>The insights dashboard translates raw social listening data into actionable product decisions, showing priority themes, opportunity sizing, and specific customer quotes that ground each recommendation in real user needs.</em></p>
<h3 id="what-the-output-isand-isnt">What the output isâ€”and isn&rsquo;t</h3>
<p>The analysis output serves as a decision input rather than a presentation deck. Each synthesized theme should directly map to three actionable components: a testable product hypothesis that can be validated or refuted, an opportunity-size signal that helps prioritize resources, and a specific instrumentation plan that will measure success. This maintains the same decision scaffold mentioned earlier, ensuring that insights translate into concrete product decisions rather than remaining as interesting but unused research findings.</p>
<h2 id="seeing-it-in-action">Seeing It in Action</h2>
<h3 id="example-1-the-cost-experience">Example 1: The Cost Experience</h3>
<p>From ~700 candidate threads, we curated ~100 high-signal discussions (Reddit, HN, Quora, vendor/community forums), normalized text, embedded at sentence/thread level, clustered themes, and linked each to example quotes. One dominant signal emerged: billing complexity and transparency drive most cost-related UX pain. Users struggle to reconcile invoices to usage, discover overruns after month-end, and use calculators that ignore dynamic workloadsâ€”leading to surprise spikes. Strategically, users prefer predictable costs over merely lower costs. The advantage is cost-experience design (real-time transparency, proactive controls, behavior-aware forecasting) rather than discounts alone. Platform â€œflavorsâ€ vary (e.g., BigQuery pricing confusion, Snowflake credit visibility, Databricks cluster trade-offs, Splunk ingestion spikes, Redshift monitoring blind spots), but the design response is consistent: plain-English cost impact at point of action, pre-threshold alerts, safe throttles, and workload-aware forecasting.</p>
<h3 id="example-2-tier-2-soc-analyst-friction">Example 2: Tier-2 SOC Analyst Friction</h3>
<p>When we analyzed discussions from cybersecurity forums and support communities, five universal barriers emerged across different security tools:</p>
<ol>
<li><strong>Query language complexity</strong>: Analysts struggle with proprietary search syntaxes that vary across platforms</li>
<li><strong>False-positive overload</strong>: Too many alerts that turn out to be benign, creating alert fatigue</li>
<li><strong>Integration hurdles</strong>: Data silos prevent effective correlation between security tools</li>
<li><strong>Workflow friction</strong>: Constant context switching and manual processes break investigation flow</li>
<li><strong>Platform limitations</strong>: Analysts spend time troubleshooting tools rather than investigating threats</li>
</ol>
<p>The design mandate became clear: lower technical barriers, suppress noise, preserve investigative context, and streamline common workflows so analysts can focus on actual threats rather than tool mechanics. The most impactful solutions involved agentic experiences that automate routine tasks and stronger default configurations that require less customization.</p>
<p>

  <img src="/images/projects/crowdlistening/crowdlisten-homepage.png" alt="CrowdListening Platform Homepage" loading="lazy">
 
<em>Crowdlistening&rsquo;s main interface provides a clean entry point for product teams to analyze community discussions, with options for targeted discovery, open-ended exploration, and always-on insight monitoring.</em></p>
<p>

  <img src="/images/projects/crowdlistening/analyze-interface.png" alt="Analysis Interface" loading="lazy">
 
<em>The analysis workspace combines semantic search, clustering algorithms, and LLM reasoning to surface patterns in customer discourse while maintaining traceability back to original sources.</em></p>
<h2 id="crowdlistening">Crowdlistening</h2>
<p>As shown in the cost-estimation agent presentation, Crowdlistening is a tool I built to extract patterns from collective discourse without flattening individual voices. It pairs LLM reasoning with a larger-context pipeline to ingest public discussions, structure them, and tie findings back to evidence. At its core, Crowdlistening treats raw discourse as primary data, emphasizes traceability (â€œshow your workâ€), and optimizes for original insight over derivative summaries.</p>
<p>Crowdlistening&rsquo;s goal isn&rsquo;t forced consensus; it&rsquo;s to surface authentic needs, native customer language, and edge cases that formal channels miss. At enterprise scaleâ€”where the user base is large and diverseâ€”listening broadly helps us prioritize what&rsquo;s real over what&rsquo;s merely loud. (More background at Crowdlistening.com.)</p>
<p>Since the launch of MCPs, I&rsquo;ve experimented with exposing Crowdlistening capabilities as MCP serversâ€”directly accessible in clients like Copilot or Claude. Features remain similar (with some visualization limits), but inputs become more nuanced and multi-turn, making the experience far more intuitive for non-technical users.</p>
<h2 id="building-this-at-large-organizations">Building This at Large Organizations</h2>
<h3 id="a-feature-proposal">A Feature Proposal</h3>
<p>To enable the market â†’ product workflow at enterprise scale, we could develop a Copilot MCP integration that serves as a conversational guide for early specification writing. This tool would fundamentally change how product requirements are generated by starting with customer evidence rather than internal assumptions.</p>
<p>The system would ingest customer meeting transcripts alongside curated online discussions, applying the same semantic analysis techniques to identify patterns across both formal and informal feedback channels. It would run evidence-linked synthesis with clear citations to primary sources, ensuring that every product decision can trace back to specific customer voices. Most importantly, it would automatically generate the theme â†’ hypothesis â†’ bet â†’ telemetry â†’ refinement scaffolds that slot directly into product specifications, making the customer-driven development process systematic rather than ad-hoc.</p>
<h3 id="governance-and-guardrails">Governance and Guardrails</h3>
<p>Social data presents unique challenges around privacy, bias, and accuracy that require systematic safeguards. Our approach emphasizes ethical data collection through strict sourcing practices that respect user consent, aggressive deduplication to prevent over-weighting vocal users, and zero tolerance for capturing personally identifiable information. The goal is to understand collective patterns without compromising individual privacy.</p>
<p>To combat sampling biasâ€”a critical risk when community discussions may not represent your full user baseâ€”we actively measure representativeness across demographics, use cases, and engagement levels. Every synthesized claim maintains auditable links back to source material, implementing a &ldquo;show your work&rdquo; principle that allows stakeholders to verify the evidence behind recommendations. Finally, we use MCP-based connectors to ensure data processing pipelines remain inspectable and secure, with clear governance over what data flows where and how it&rsquo;s transformed.</p>
<h2 id="closing-thought">Closing Thought</h2>
<p>AI-enabled social listening doesnâ€™t replace customer calls, design research, or telemetry; it enriches themâ€”especially at the fuzzy front end. Used well, it helps us choose better problems, write crisper specs, and ship experiences that feel obvious in hindsight.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-why-people-care-what-others-think" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('why-people-care-what-others-think')">&times;</button>
      <h2>Why People Care What Others Think</h2>
      <p class="project-meta">May 12, 2025</p>
      <div class="modal-content">
        <p>Every builder I know checks the comments.</p>
<p>Not because they trust them, but because they canâ€™t help it. Beneath the noise lives a signal we all chase: proof that what weâ€™re making actually matters to other people. In a world optimized for clicks and curation, the raw pulse of collective opinion has become one of the last places where human meaning leaks through.</p>
<p>We like to imagine ourselves as rational, independent thinkers. Yet an invisible social field shapes most of our decisionsâ€”what to buy, what to build, what to believe. Even when we resist consensus, we define ourselves in relation to it. Understanding what people think, and why they think it, isn&rsquo;t optional for buildersâ€”it&rsquo;s the difference between creating something people actually want and building expensive lessons in humility.</p>
<h2 id="the-comfort-and-context-of-the-crowd">The Comfort and Context of the Crowd</h2>
<p>Thereâ€™s wisdom in crowds, but not because they always converge on truth.
Often, they donâ€™t. Truth, especially early truth, tends to start as a contrarian position. But crowds provide something just as vital: context. They tell us what people are noticing, fearing, celebrating, misunderstanding. They give us the social coordinates to make sense of our own place in the world.</p>
<p>Psychologists Baumeister and Leary once argued that belongingness is not optionalâ€”itâ€™s as fundamental as food and shelter. That drive manifests everywhere: the need to check reviews before buying, the urge to see how others reacted to a film we just watched, the compulsion to refresh the feed after posting something we care about.</p>
<p>Being part of a groupâ€”even digitallyâ€”creates a sense of safety and meaning that&rsquo;s hard to replicate in isolation. This is not mere conformity; it&rsquo;s contextual navigation. Understanding the group helps us understand ourselves.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-engagement-metrics.png" alt="Zhihu Engagement Metrics" loading="lazy">
 
<em>The visible pulse of crowd engagement: Like counts, shares, and comments become the social proof that drives the fundamental human need for belonging. These metrics transform abstract social validation into tangible, quantified feedback that fuels our participation in digital crowds.</em></p>
<h2 id="the-two-broken-mirrors-of-modern-information">The Two Broken Mirrors of Modern Information</h2>
<p>Todayâ€™s information ecosystem distorts this instinct in two opposite ways.</p>
<p>On one side, curated expertise filters the world into neat conclusionsâ€”clarity without nuance.
On the other, personalized algorithms mirror our past clicks back to us, promising relevance while quietly narrowing perspective. Scroll through any trending page and youâ€™ll see the split: polished certainty on one side, algorithmic dÃ©jÃ  vu on the other. Both claim to know what matters. Neither truly listens.</p>
<p>What&rsquo;s missing is the raw human layerâ€”the messy, unfiltered dialogue that happens in comment sections, forums, Discord servers, and threads. These spaces often turn chaotic, even hostile, but they contain the kind of pattern recognition that emerges when thousands of people react to the same thingâ€”something curated feeds can&rsquo;t replicate.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-comment-format-analysis.png" alt="Zhihu Comment Format Analysis" loading="lazy">
 
<em>Zhihu&rsquo;s &ldquo;How to evaluate&rdquo; format demonstrates how structured questioning shapes collective opinion formation. Note how the format itself becomes a lens that influences what aspects of a topic the crowd focuses on.</em></p>
<p>As Palantir&rsquo;s Alex Karp has said, &ldquo;Understanding the collective judgment of imperfect people in uncertain domains is the real frontier of intelligence.&rdquo; He was speaking about intelligence analysis, but the principle applies perfectly to social mediaâ€”real intelligence, human or artificial, emerges not from isolation but from aggregation with awareness: knowing when to listen, when to weigh, and when to dissent.</p>
<h2 id="listening-at-scale">Listening at Scale</h2>
<p>This is the paradox every product builder faces: we need crowd insight, but most tools give us crowd metrics. We get volume without meaning, sentiment without story. What if we could change that?</p>
<p>This is where Crowdlistening enters the pictureâ€”a platform designed to systematically listen to the internet the way great founders listen to their customers, not by counting mentions, but by mapping meaning.</p>
<p>Traditional social analytics measure volume, sentiment, reach. But these metrics flatten emotion into numbers. They strip away what makes human expression meaningful: tone, humor, frustration, context. Crowdlisten instead seeks to surface meaning clustersâ€”patterns of how people express emotion, share experience, and assign value.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-social-platform-nature.png" alt="Zhihu Social Platform Nature" loading="lazy">
 
<em>Chinese social media reveals how platform mechanics shape crowd behavior. This Zhihu discussion explores how social networks are inherently about human-to-human interaction, not just Q&amp;A formatsâ€”highlighting the social substrate beneath all crowd intelligence.</em></p>
<p>Imagine thousands of YouTube comments on a new AI tool. A typical dashboard might tell you itâ€™s â€œ70% positive.â€ Crowdlistening reveals something deeper:</p>
<p>that creators praise its creative control but distrust its data usage,</p>
<p>that early adopters joke about â€œprompt fatigue,â€</p>
<p>that a sub-community is already remixing it into new workflows.</p>
<p>These are not metrics. Theyâ€™re narratives.
And for product builders, narratives are where insight lives.</p>
<h2 id="from-data-to-dialogue">From Data to Dialogue</h2>
<p>Listening to the crowd doesnâ€™t mean surrendering to it. It means learning from its distributed intelligence.
The crowd may not always be right, but itâ€™s always revealing.</p>
<p>Crowds show us contradictions between what people say they want and what they actually do. They highlight emotional undercurrents that surveys missâ€”like frustration masked as humor, or excitement mixed with fear. When interpreted thoughtfully, these signals help teams design with empathy rather than ego.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-knowledge-limitation.png" alt="Zhihu Knowledge Limitation" loading="lazy">
 
<em>The humility of crowd discourse: &ldquo;Because our knowledge is limited, we can&rsquo;t make accurate judgments.&rdquo; This admission of epistemic humility creates space for collective learning rather than individual certainty.</em></p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-opinion-diversity.png" alt="Zhihu Opinion Diversity" loading="lazy">
 
<em>Platform design influences discourse quality. When questions allow diverse perspectives, the hope is that through viewing answers, people find intellectual peace rather than taking sidesâ€”a design philosophy for constructive crowd dynamics.</em></p>
<p>In this sense, crowd intelligence is a mirror, not a manual. It reflects the messy, contradictory human condition that every good product ultimately serves. The best builders donâ€™t seek consensus; they seek comprehension. They use the crowdâ€™s collective reflection as raw input for intuition and design judgment.</p>
<h2 id="the-moral-dimension-of-listening">The Moral Dimension of Listening</h2>
<p>Karp often argues that the purpose of technology isnâ€™t efficiency but preservation of human agency.
That idea reframes listening as a moral act. When we listen to the crowd, weâ€™re not just analyzing data; weâ€™re acknowledging dignity. Weâ€™re saying that every voiceâ€”no matter how chaotic or conflictedâ€”contains a fragment of the human experience worth understanding.</p>
<p>What we call &ldquo;crowd insight&rdquo; is ultimately a question of empathy at scale. It challenges us to believe that what happens when imperfect people judge uncertain situations together can, when interpreted with care, bring us closer to truth than the confidence of a few perfect algorithms.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-knowledge-discovery.png" alt="Zhihu Knowledge Discovery" loading="lazy">
 
<em>The practical value of crowd wisdom: For unknown domains, crowds help find entry points. For familiar domains, they reveal different problem-solving approaches. The layered relationship between domains mirrors how crowd intelligence operates at different levels of expertise.</em></p>
<p>

  <img src="/images/posts/crowd-thesis/google-knowledge-bottles-concept.png" alt="Google Knowledge Bottles Concept" loading="lazy">
 
<em>Google&rsquo;s &ldquo;knowledge bottles&rdquo; concept: The vision of packaging expert knowledge for on-demand access reflects the tension between curated expertise and crowd-sourced intelligenceâ€”both approaches to democratizing knowledge at scale.</em></p>
<h2 id="listening-as-design">Listening as Design</h2>
<p>The point isnâ€™t to glorify crowds or discredit experts.
Itâ€™s to remember that listeningâ€”to many, not just a fewâ€”is an act of humility.</p>
<p>The next generation of intelligent systems wonâ€™t just analyze the world; theyâ€™ll learn to listen to it. And maybe, if we design them well, so will we.
Because when we learn to hear the collective heartbeat of humanityâ€”its fears, its hopes, its humorâ€”we don&rsquo;t just build better products.
We build better mirrors of ourselves.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-monetization-idea.png" alt="Zhihu Monetization Idea" loading="lazy">
 
<em>The meta-discussion of crowd platforms: A user proposes that Zhihu could monetize by charging for &ldquo;How to evaluate&rdquo; questions, recognizing the inherent value in structured crowd inquiryâ€”demonstrating how the crowd can optimize its own intelligence-gathering mechanisms.</em></p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-question-format-analysis.png" alt="Zhihu Question Format Analysis" loading="lazy">
 
<em>The anatomy of crowd discourse: This meta-analysis of Zhihu&rsquo;s question patterns shows how format shapes thinking. The comparison to Quora reveals cultural differences in how platforms structure collective intelligenceâ€”each format creates different kinds of crowd wisdom.</em></p>
<h2 id="crowd-intelligence-in-practice">Crowd Intelligence in Practice</h2>
<p>The examples from Chinese social media platforms like Zhihu demonstrate how different cultural contexts shape crowd behavior and intelligence. The structured format of &ldquo;How to evaluate X&rdquo; questions creates a framework for systematic collective analysis, while the meta-discussions about platform design show crowds reflecting on their own intelligence-gathering processes.</p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-user-content-creation.png" alt="Zhihu User Content Creation" loading="lazy">
 
<em>Understanding platform incentives: This detailed analysis of user-generated content dynamics shows how platform economics influence the quality and nature of crowd contributionsâ€”a critical factor in designing systems that harness collective intelligence effectively.</em></p>
<p>

  <img src="/images/posts/crowd-thesis/zhihu-crowd-evaluation.png" alt="Zhihu Crowd Evaluation" loading="lazy">
 
<em>The essence of crowd evaluation distilled: &ldquo;Actually, crowd evaluation.&rdquo; Sometimes the most profound insights come in the simplest formsâ€”this minimalist comment captures the fundamental nature of what we&rsquo;re exploring.</em></p>
<p>

  <img src="/images/posts/crowd-thesis/google-labs-intelligence-service.png" alt="Google Labs Intelligence Service" loading="lazy">
 
<em>From concept to implementation: The journey from Josh Woodward&rsquo;s conversations about &ldquo;intelligence as a service&rdquo; to practical applications demonstrates how visionary ideas about crowd and artificial intelligence converge into real-world systems that can &ldquo;bottle up the knowledge of experts.&rdquo;</em></p>
<h2 id="references">References</h2>
<p>Alex Karp, Palantir CEO Letters (2023â€“2024): on collective judgment and moral responsibility in machine intelligence.</p>
<p>Baumeister, R. F., &amp; Leary, M. R. (1995). â€œThe need to belong: Desire for interpersonal attachments as a fundamental human motivation.â€ Psychological Bulletin, 117(3), 497â€“529.</p>
<p>Surowiecki, J. (2004). The Wisdom of Crowds. Anchor Books.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-value-add-of-ai-generation-as-distribution" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('value-add-of-ai-generation-as-distribution')">&times;</button>
      <h2>Value Add of AI: Generation as Distribution</h2>
      <p class="project-meta">May 8, 2025</p>
      <div class="modal-content">
        <h2 id="the-evolution-of-ai-value">The Evolution of AI Value</h2>
<p>The first wave of generative AI focused primarily on content creation - ChatGPT writing articles, Midjourney generating images, essentially replacing traditional production roles. However, as these technologies mature, their greatest value might well shift towards distribution and personalization rather than raw production.</p>
<h2 id="from-rss-to-recommender-systems">From RSS to Recommender Systems</h2>
<p>The evolution of content distribution reveals how technology repeatedly transforms information access. RSS (Really Simple Syndication) represented an early attempt to solve content discovery, providing a pull-based system where users subscribed to feeds they cared about.</p>
<blockquote>
<p><em>Personal Note: I interned at China Impact Investing Network (CINN), down the road from Huangzhuang, earning 100 RMB daily for translation and RSS-related tasks - work that would now be largely automated by GPT.</em></p>
</blockquote>
<p>As content volume exploded, the focus shifted to algorithmic distribution through recommender systems. These attempted to match existing content to user preferences through increasingly sophisticated methods, but still fundamentally operated on a &ldquo;create once, distribute many times&rdquo; model.</p>
<h2 id="content-creation-vs-distribution-costs">Content Creation vs. Distribution Costs</h2>
<p>The economics of content have always been defined by the balance between creation and distribution costs, as illustrated in our visualization. Traditional models rely on two fundamentally different approaches:</p>
<h3 id="professional-vs-user-generated-content-economics">Professional vs. User-Generated Content Economics</h3>
<p>Traditional PGC platforms invest heavily in upfront content creation ($200-500 per article) while optimizing distribution costs ($0.001 per user). This creates an economic model where high-quality, centrally produced content must reach massive scale to be sustainable. Users remain passive consumers with minimal influence on content direction.</p>
<p>UGC platforms invert this model by outsourcing creation costs to users while investing primarily in discovery infrastructure. This creates greater diversity but inconsistent quality. Both approaches increasingly allocate resources to distribution rather than creation as competition for attention intensifies.</p>
<p>The discovery paradox emerges: as content volume increases, the marginal value of new content approaches zero without effective discovery mechanisms. Users face decision fatigue from too many choices, and the market naturally shifts investment toward discovery rather than production.</p>
<h2 id="generation-as-distribution-personalized-content-at-scale">Generation as Distribution: Personalized Content at Scale</h2>
<p>AI fundamentally changes this paradigm by collapsing the distinction between production and distribution. When content can be generated at the moment of consumption, personalized for each user, the model shifts from:</p>
<p>Traditional: Create once â†’ Distribute to millions<br>
Generative: Create parameters â†’ Generate millions of variations</p>
<p>The dual-axis economics chart reveals this transformation. Traditional content scales efficiently after high initial investment but delivers mediocre value. Generative approaches provide dramatically higher user value but face linearly increasing costs that become prohibitive at scale.</p>
<p>The optimal approach emerges through content modularity - recognizing that not everything needs regeneration. By identifying which components provide the most personalization value, hybrid approaches can maintain 80% of personalization benefits at 30% of the cost.</p>
<h2 id="the-602020-rule-strategic-content-modularity">The 60/20/20 Rule: Strategic Content Modularity</h2>
<p>Most knowledge domains contain substantial core material that remains consistent across users. The 60/20/20 rule maximizes efficiency by segmenting content into:</p>
<ul>
<li>60% static core content (foundational principles, established facts)</li>
<li>20% cohort-level content (industry examples, cultural contexts)</li>
<li>20% individual personalization (connections to personal experience, learning pace)</li>
</ul>
<p>This approach creates a fundamentally different economic curve that scales more efficiently than traditional content while maintaining most personalization benefits. For a business book distributed to 50,000 professionals, this approach can deliver twice the relevance at the same cost as traditional publishing.</p>
<h2 id="multi-modal-personalization-beyond-text">Multi-modal Personalization: Beyond Text</h2>
<p>The personalization paradigm extends beyond text to encompass multiple modalities. Content can dynamically transform between formats - text summaries becoming virtual presenter videos, complex topics converting to interactive explanations, news transforming into personalized audio briefings. This multi-modal capability increases generation costs but dramatically enhances engagement and information retention.</p>
<p>These cross-modal transformations add approximately 30-40% to generation costs but can increase engagement by 200-300%, creating compelling economics despite the higher production expense. Each user&rsquo;s preferred learning style becomes another dimension for personalization.</p>
<h2 id="the-user-advocate-beyond-algorithmic-recommendation">The User Advocate: Beyond Algorithmic Recommendation</h2>
<p>The most powerful personalization emerges not from content formatting but from deeper user understanding. The User Advocate concept represents an AI persona that truly comprehends the user&rsquo;s interests, knowledge level, and perspective, then guides content creation accordingly.</p>
<p>Unlike recommendation systems that rely on sparse signals, the Advocate builds a comprehensive user model through conversation and observation. This enables exploration of &ldquo;unknown unknowns&rdquo; - valuable topics users didn&rsquo;t know to search for. The approach fundamentally changes platform economics by aligning incentives with actual user satisfaction rather than engagement metrics.</p>
<h2 id="fluid-knowledge-and-the-future">Fluid Knowledge and the Future</h2>
<p>The most significant impact of AI lies not in replacing content creators but in transforming how knowledge flows to individuals. As the internet solved information scarcity, generative AI now solves the problem of relevance through &ldquo;fluid knowledge&rdquo; (çŸ¥è¯†æ¶²åŒ–) that adapts perfectly to each person&rsquo;s context.</p>
<p>In this emerging paradigm, content becomes transformable across formats, users experience the feeling of being truly understood, and exploration replaces search as the primary discovery model. The User Advocate becomes a critical interface between vast information spaces and human understanding, fundamentally changing our relationship with knowledge acquisition.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-interesting-reads" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('interesting-reads')">&times;</button>
      <h2>Interesting Reads</h2>
      <p class="project-meta">April 1, 2025</p>
      <div class="modal-content">
        <p>Here&rsquo;s a list of articles that I founding interesting. I&rsquo;ve attached the original article / transcript for easy refernece as well.</p>
<h1 id="gary-tan-on-manus-the-new-general-purpose-ai-agent">Gary Tan on Manus: The New General-Purpose AI Agent</h1>
<p>Video URL: <a href="https://www.youtube.com/watch?v=JOYSDqJdiro">https://www.youtube.com/watch?v=JOYSDqJdiro</a></p>
<p>Usable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.</p>
<p>&ldquo;Today we&rsquo;re launching an early preview of Manus, the first general AI agent.&rdquo;</p>
<p>When Manus officially launched, the hype around it immediately took off. A Chinese startup unveiling a new AI agent that some are calling &ldquo;China&rsquo;s next DeepSeek moment,&rdquo; with people calling it &ldquo;the most impressive AI tool they&rsquo;ve ever tried&rdquo; and &ldquo;the most sophisticated computer-using AI.&rdquo;</p>
<p>Unlike some of its predecessors, Manus wasn&rsquo;t just another specialized chatbot. It promised to be a true general-purpose AI agent. With invitations rare and access limited, the question remains: has Manus truly revolutionized the AI agent landscape?</p>
<h2 id="how-manus-works">How Manus Works</h2>
<p>Behind all the excitement around Manus is something genuinely innovative: a multi-agent AI system that can seemingly complete all sorts of tasks from travel planning and financial analysis to searching over dozens of files or doing industry research.</p>
<p>Rather than relying on one big neural network, Manus works more like an executive overseeing a team of sub-agents, coordinating and guiding their every move across a shared action space. It takes in your prompt as input and gets to work figuring out what it needs to do.</p>
<p>Instead of tackling your task in one go, Manus employs a sophisticated approach. A planner agent first comes up with a master plan to follow, breaking things down into manageable subtasks. This way Manus knows precisely what needs to be done before executing and can hand off these tasks to other sub-agents. These sub-agents are like Manus&rsquo;s own in-house experts - they share the same context but each has its own delineated domain from knowledge or memory to execution.</p>
<p>Manus can call upon an extensive suite of 29 different integrated tools, whether they&rsquo;re automating web navigation, securely running code, or pulling important information from files. Manus&rsquo; sub-agents intelligently decide which tools to use.</p>
<p>Finally, when each subtask is complete, the executor agent combines the outputs together into a final synthesized output for the user.</p>
<h2 id="technical-details">Technical Details</h2>
<p>Under the hood, Manus is powered by a sophisticated dynamic task decomposition algorithm. This is what enables it to autonomously break down complex instructions into clear execution paths.</p>
<p>To ensure stability even after dozens of rounds of reasoning and tool use, the Manus team developed an original technique called &ldquo;chain of thought injection,&rdquo; enabling agents to actively reflect and update plans.</p>
<p>At its core, Manus makes use of Anthropic&rsquo;s Claude 3.7 Sonnet. Manus also features robust cross-platform execution capabilities thanks to its seamless integration with open-source tools like YC company Browser.js for advanced website interaction and startup E2B&rsquo;s secure cloud sandbox environment.</p>
<h2 id="capabilities-and-performance">Capabilities and Performance</h2>
<p>What can Manus actually accomplish? Impressively, it can take on a wide range of real-world tasks. These include creating travel itineraries, performing detailed financial analyses, developing educational content, compiling structured databases, comparing insurance policies, sourcing suppliers, and assisting with high-quality presentations.</p>
<p>To truly measure Manus&rsquo; capabilities, we can look at Gaia, a benchmark designed to challenge AI agents on reasoning, multimodal handling, web browsing, and tool proficiency. Humans typically score about 92% on this benchmark, while OpenAI&rsquo;s Deep Research scored about 74% at its best. Manus smashed the state-of-the-art on Gaia, scoring 86.5%, just a few points shy of the average human.</p>
<h2 id="the-wrapper-debate">The &ldquo;Wrapper&rdquo; Debate</h2>
<p>Despite impressive benchmark performance, Manus has reignited a broader conversation about the nature of AI startups at the application layer: &ldquo;wrappers.&rdquo;</p>
<p>Some have dismissed Manus as merely a wrapper, since it stitches together existing foundational models and various tool calls. But this dismissal overlooks an important reality: most successful AI products today could also qualify as wrappers by this logic.</p>
<p>Cursor and Warp, for example, integrate existing LLMs alongside external APIs and developer-focused tooling such as real-time code analysis and debugging utilities. Domain-specific agents like Harvey combine foundational models with legal-specific tool integrations, case law retrieval, compliance checks, and document analysis.</p>
<p>Clearly, many useful applications do fit the wrapper mold, and for many developers, it makes sense to go this route. As Manus co-founder Yizhow Peak G told us himself, &ldquo;From day one they decided to work orthogonally to model development, wanting to be excited rather than threatened by each new model release.&rdquo;</p>
<p>What distinguishes successful wrappers from their less effective counterparts is typically a bunch of things: intuitive UI, proprietary evals, much more careful fine-tuning of foundational models, and thoughtfully designed multi-agent architectures.</p>
<h2 id="strengths-and-limitations">Strengths and Limitations</h2>
<p>Manus itself illustrates these trade-offs really well:</p>
<h3 id="strengths">Strengths:</h3>
<p>Its multi-agent orchestration helps deliver significantly lower per-task costs (around $2 a task compared to integrated competitors like OpenAI&rsquo;s Deep Research). It offers greater transparency and user control, letting users directly inspect, customize, or replace individual sub-agents and tool integrations. Additionally, it provides a degree of flexibility centralized platforms rarely match.</p>
<p>One of the coolest things Manus figured out was actually exposing the file system so you could see exactly what the agents were doing. Chat GPT requires you to reprompt, and it&rsquo;s opaque what&rsquo;s happening when it&rsquo;s thinking. Manus is a glimpse into the future of Chat GPT desktop operating directly on your computer, and it will be cool to see how much more control you&rsquo;ll get when it&rsquo;s happening there instead of a browser.</p>
<h3 id="limitations">Limitations:</h3>
<p>Coordination across specialized agents becomes increasingly difficult as tasks scale or complexity grows. Its current advantages (UX refinements, targeted fine-tuning, thoughtful integrations) are vulnerable to competitors just coming along and doing that as well.</p>
<p>These strengths and weaknesses are generally shared by wrappers. They allow rapid deployment, iteration, and specialized UX at lower upfront cost. However, they&rsquo;re vulnerable to disruption such as API pricing changes or provider policy shifts, which can quickly erase any cost benefits.</p>
<h2 id="the-future-of-ai-products">The Future of AI Products</h2>
<p>Ultimately, the critical challenge isn&rsquo;t deciding whether wrappers are viable but identifying genuinely sustainable differentiation for your product.</p>
<p>For founders, this might mean investing early in proprietary evals that are expensive or time-consuming to replicate, embedding workflows deeply into specific user routines to increase switching costs, and identifying integrations with platforms or data sets competitors can&rsquo;t easily access.</p>
<p>In the end, success in AI doesn&rsquo;t hinge on reinventing the wheel but rather on who can stitch together the existing models into a product users genuinely love.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-user-needs-opportunities" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('user-needs-opportunities')">&times;</button>
      <h2>User Needs &amp; Opportunities</h2>
      <p class="project-meta">March 25, 2025</p>
      <div class="modal-content">
        <p>When you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They&rsquo;d probably have something substantial to talk about. While I don&rsquo;t usually go to bars during weekdays, I&rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I&rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.</p>
<h2 id="more-apparent-ones">More apparent ones:</h2>
<p>2025-03-24</p>
<ul>
<li>Career seeking: it&rsquo;s a difficult job market, whether it&rsquo;s doing leetcode or preparing for behavioral interviews, people want a way to cheat their way out of this difficult process, and they&rsquo;re willing to pay for it too.</li>
</ul>
<p>2025-03-27</p>
<ul>
<li>
<p>Agentic worklows: I&rsquo;m usually more skeptical towards areas more widely reported, but agentic workflows may well be here to stay. From the initial action gpts (autogpt) agent chains (devin, metagpt, cogno etc), to the more recent operators (MCPs, OpenAI operator), it would indeed be nice to have agents act on our behalf (perhaps hopefuly not the way Anton goes about ordering Hamburgers in Silcon Valley).</p>
</li>
<li>
<p>Building on the previous point concerning agentic computer use. It&rsquo;s also interesting to consider the two approaches towards web navigation: vision-based and ai-native-ui. While people like Aravind Srinivas have openly expressed their skepicism towards vision based web operations, noting the limitations imposed by operating systems, especially iOS, which restrict access to other applications, vision-based web interactions are inherently more versatile and flexible.</p>
</li>
<li>
<p>Content generation: Generative ai is at its core, generative. We have for the past three years seen numerous improvements in text/image/video/avatar generation, with applications in vairous forms of UGC (and now nearing PGC) ads, podcasts, short-form videos. I love how creative people have been with the available AI tools, but at the same time, I view these tools as an available means to scale creativity, not for enabling 0-1 creation.</p>
</li>
</ul>
<h2 id="less-apparent-and-more-interesting-ones">Less apparent (and more interesting) ones:</h2>
<p>2025-03-25</p>
<ul>
<li>Ads through LLM generated content: While I was trying to figure out the best way to add a forms feature to this website yesterday, I asked claude. And interestingly, this was what I got back at first:</li>
</ul>
<p>

  <img src="/images/posts/user-needs/formspree.png" alt="Claude Recommendation" loading="lazy">
 </p>
<ul>
<li>
<p>Note that <a href="https://formspree.io/">Formspree</a> is a paid service. This raises an interesting point about the future of AI generated content and suggestions. On one hand, the result is based on training data inputted into the model, but at the same time, one can easily manipulate it to recommend one solution over another (try Formspree vs Google Forms). This presents an interesting avenue for ads via llm generated content, and given the extent to which people are willing to trust llm results, this could even be an effective avenue. Yet in doing so, we are also risking the reliability of llm generated content.</p>
</li>
<li>
<p>SEO is already changing in the age of LLMs, with AI genearated content flooding web browsers. Another interesting observation is the amount of website traffic going to specific companies from AI-enabled search services such as Perplexity. While looking at website traffic for <a href="https://www.genspark.ai/">Genspark</a> and a few other websites, we can see both ChatGPT and Perplexity listed as the top referring websites (source: similarweb) Though the accuracy of this data is yet to be determined, it make sense intuitively that, owing to the large amount of content genearted via these AI platforms, the pages will be easily indexed by ai-browsers.</p>
</li>
</ul>
<p>

  <img src="/images/posts/user-needs/similar-web-genspark.png" alt="Similarweb Genspark" loading="lazy">
 </p>
<h2 id="general-directions">General Directions:</h2>
<p>These are some relatively well agreed upon directions that one may take:</p>
<p>2025-03-27</p>
<ul>
<li>Better customization: Traditionally, a tradeoff existed between customization and scale. Creating for wider audiences required standardized offerings. However, as LLM inference costs rapidly decrease, information retrieval and content generation are approaching fixed costs. This shift enables customization at scale across various domainsâ€”personalized news, cinematography, and other content formats. Additionally, advancements in multimodal capabilities (voice, image, video) will likely introduce greater variety in the content we consume.</li>
</ul>

      </div>
    </div>
  </div>
  
  <div id="modal-exploring-unknown-unknowns" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('exploring-unknown-unknowns')">&times;</button>
      <h2>Exploring Unknown Unknowns</h2>
      <p class="project-meta">October 5, 2024</p>
      <div class="modal-content">
        <h1 id="exploring-unknown-unknowns-the-future-of-knowledge-interfaces">Exploring Unknown Unknowns: The Future of Knowledge Interfaces</h1>
<p>We live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don&rsquo;t know what to read, and we don&rsquo;t understand what we&rsquo;ve read. These pain pointsâ€”&ldquo;not knowing how to choose&rdquo; and &ldquo;not knowing how to comprehend&rdquo;â€”represent a massive opportunity for reimagining how we interact with knowledge.</p>
<p>The core insight driving next-generation learning interfaces is simple but profound: most people don&rsquo;t know what they don&rsquo;t know. We can&rsquo;t formulate good questions about topics we&rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.</p>
<h2 id="beyond-search-learning-from-googles-experiments">Beyond Search: Learning from Google&rsquo;s Experiments</h2>
<p>Google&rsquo;s Learn About product offers a compelling glimpse of this future. Unlike traditional search, which requires users to know what to look for, Learn About allows users to &ldquo;zoom out and look at the space of questions around your question.&rdquo; It combines the information accuracy of search with the flexible, dynamic interaction of AI chat, creating an exploratory learning experience that goes far beyond simple Q&amp;A.</p>
<p>This approach represents a fundamental shift from information retrieval to knowledge discovery. Instead of returning static results, the system actively helps users explore adjacent concepts and ask better questions. Users can pursue their immediate curiosity while simultaneously discovering related topics they never thought to investigate.</p>
<p>The most innovative learning interfaces take this concept further by specializing in specific domains. Rather than trying to handle all possible queries, they focus on particular knowledge areasâ€”like literature, technical documentation, or professional developmentâ€”where they can provide genuinely superior experiences compared to general-purpose tools.</p>
<h2 id="the-llm-architecture-behind-intelligent-learning">The LLM Architecture Behind Intelligent Learning</h2>
<p>The technical foundation of these systems relies on sophisticated prompt engineering and modular content generation. Large language models serve as the cognitive engine, but their raw output must be carefully structured to create coherent learning experiences. The key innovation lies in using LLMs to generate JSON-formatted responses that populate predefined UI templates, creating consistent yet dynamic interfaces.</p>
<p>This architecture allows the system to maintain conversational flow while presenting information in learner-friendly formats. For example, instead of generating wall-of-text responses, the LLM outputs structured data that renders as interactive cards, related questions, and exploration pathways. Each response includes not just content, but also suggested next steps and connection points to related topics.</p>
<p>The prompt engineering becomes crucial here. Effective systems use detailed behavioral instructions that guide the LLM to act as a knowledgeable teacher rather than a simple question-answering service. These prompts specify tone, content depth, interaction style, and response structure, ensuring consistency across thousands of potential learning conversations.</p>
<h2 id="reducing-cognitive-friction-through-design">Reducing Cognitive Friction Through Design</h2>
<p>Traditional learning interfaces suffer from what could be called &ldquo;prompt friction&rdquo;â€”the cognitive overhead of formulating good questions and organizing complex thoughts into text. The most successful knowledge interfaces minimize this friction through several design strategies.</p>
<p>First, they embed potential questions directly into content responses. Instead of requiring users to think of follow-up questions, the system generates three or four relevant next steps that users can explore with a simple click. This transforms learning from an active questioning process into a guided exploration where curiosity can flow naturally.</p>
<p>Second, they use modular response formats that pack high knowledge density into digestible chunks. Rather than lengthy explanations, responses combine concise answers with interactive elements: reflection prompts, knowledge checks, relevance connections, and vocabulary builders. Users receive exactly the information they need while being invited to go deeper on specific aspects that interest them.</p>
<p>Third, they implement &ldquo;prompt prefills&rdquo;â€”pre-written questions and conversation starters that help users begin productive dialogues without staring at blank input fields. These aren&rsquo;t generic suggestions but contextually relevant questions based on the current topic and common learning patterns.</p>
<h2 id="personalization-through-conversational-intelligence">Personalization Through Conversational Intelligence</h2>
<p>Unlike traditional recommendation systems that rely on explicit preferences or behavioral tracking, conversational learning interfaces build user understanding organically through dialogue. Each interaction reveals information about the user&rsquo;s background knowledge, interests, learning goals, and preferred depth of explanation.</p>
<p>This conversational profiling enables increasingly sophisticated personalization. The system learns whether a user prefers concrete examples or abstract concepts, detailed explanations or high-level overviews, historical context or contemporary applications. Over time, responses become naturally calibrated to individual learning styles and knowledge levels.</p>
<p>The personalization extends beyond content delivery to include book recommendations, topic suggestions, and learning path optimization. By understanding what concepts a user struggles with and what types of explanations resonate, the system can proactively surface relevant material and adapt its teaching approach in real-time.</p>
<h2 id="technical-implementation-rag-and-knowledge-curation">Technical Implementation: RAG and Knowledge Curation</h2>
<p>Behind the conversational interface lies a sophisticated knowledge management system. Rather than relying solely on LLM training data, effective learning platforms implement Retrieval-Augmented Generation (RAG) architectures that combine real-time information retrieval with language generation.</p>
<p>This approach proves particularly valuable for specialized domains like literature analysis, where high-quality, curated knowledge sources significantly improve response accuracy and depth. Systems can draw from structured databases of book analyses, expert commentary, reader discussions, and academic sources to provide richer, more authoritative answers than general-purpose models alone.</p>
<p>The challenge lies in balancing different information sources. Community discussions from platforms like Reddit offer authentic reader perspectives and common questions, while academic sources provide authoritative analysis. Professional reviews and curated summaries add editorial quality. Effective systems learn to synthesize these different knowledge types based on the specific question and user context.</p>
<h2 id="measuring-success-beyond-engagement">Measuring Success Beyond Engagement</h2>
<p>Traditional educational metrics often miss the point of exploratory learning. While engagement metrics like session length and click-through rates provide some insight, the real value lies in knowledge acquisition and curiosity development. The most meaningful measures focus on learning outcomes: Do users ask better questions over time? Do they make novel connections between concepts? Do they pursue deeper investigation of topics that initially seemed uninteresting?</p>
<p>Advanced systems track conversation quality through several indicators: the progression from basic to sophisticated questions, the frequency of cross-topic connections, the depth of follow-up exploration, and user-generated insights that suggest genuine understanding. These metrics help optimize not just for engagement, but for actual learning effectiveness.</p>
<h2 id="the-future-of-knowledge-work">The Future of Knowledge Work</h2>
<p>As these interfaces mature, they point toward a fundamental transformation in how we approach knowledge work. Instead of consuming information passively, we&rsquo;ll increasingly collaborate with AI systems to explore ideas, test understanding, and discover unexpected connections. The goal isn&rsquo;t to replace human thinking but to augment it with better tools for curiosity and exploration.</p>
<p>The most promising applications extend beyond individual learning to collaborative knowledge building. Imagine research environments where teams can explore complex topics together, with AI facilitators helping surface relevant connections, identify knowledge gaps, and guide productive discussions. Or educational settings where students learn not just facts but how to ask increasingly sophisticated questions about any domain.</p>
<p>The technical foundation already exists. The remaining challenge is design: creating interfaces that feel natural, educational experiences that genuinely improve understanding, and systems that scale personalized learning without losing the human touch that makes great teaching transformative.</p>
<hr>
<p><em>The next time you encounter a complex topic, imagine having a knowledgeable guide who not only answers your questions but helps you discover the questions you didn&rsquo;t know to ask. That&rsquo;s the promise of intelligent knowledge interfacesâ€”and it&rsquo;s closer than you might think.</em></p>

      </div>
    </div>
  </div>
  
  <div id="modal-why-context-is-everything-in-ai-content-generation" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('why-context-is-everything-in-ai-content-generation')">&times;</button>
      <h2>Why Context is Everything in AI Content Generation</h2>
      <p class="project-meta">September 20, 2024</p>
      <div class="modal-content">
        <p>In the rapidly evolving landscape of artificial intelligence, we&rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.</p>
<p>When we expect AI to output precise, high-quality content based on a single prompt, we&rsquo;re making a fundamental mistake. For AI content to be truly valuable, we must provide sufficient context for it to reason over. The quality of AI output is directly proportional to the richness of context we provideâ€”not just about the topic itself, but about the audience, purpose, constraints, and desired outcomes.</p>
<h2 id="the-limitations-of-current-approaches">The Limitations of Current Approaches</h2>
<p>Today&rsquo;s AI content tools largely excel at taking existing information and reformatting it into different modalities. We can convert text to audio, create video summaries, or generate podcast-style conversations from written material. However, as Large Language Model context windows continue to expand, simple content summarization becomes increasingly commoditized. The real value lies not in these mechanical transformations, but in the creative synthesis and novel perspectives that emerge when AI systems understand both the content and the consumer.</p>
<p>The essence of creativity lies in finding fresh angles of approach. Quality content distinguishes itself through novel perspectives, clear structure, and genuine utility to the reader. As we move beyond basic summarization, the challenge becomes how to help AI systems discover these unique entry points that make content both engaging and personally relevant.</p>
<h2 id="knowledge-liquefaction-the-new-content-paradigm">Knowledge Liquefaction: The New Content Paradigm</h2>
<p>We&rsquo;re entering an era of &ldquo;knowledge liquefaction&rdquo; where any piece of information can be rapidly transformed into formats that match specific consumption scenarios. Whether someone needs structured learning materials for deep study or fragmentary content for casual listening during commutes, AI systems can now adapt the same core knowledge to fit these different contexts seamlessly.</p>
<p>This capability extends far beyond simple format conversion. The most compelling applications combine high-quality human-created content with AI&rsquo;s ability to find unexpected connections and generate personalized frameworks. Rather than replacing human creativity, these systems amplify it by identifying patterns and relationships that might not be immediately obvious, then presenting them through personalized lenses that resonate with individual users.</p>
<h2 id="the-personalization-challenge">The Personalization Challenge</h2>
<p>Creating truly personalized content presents a fundamental tension between scale and customization. If every piece of content requires individual adaptation for each user, the costs become prohibitive. However, knowledge fusion offers a solution through its inherent modularity. Many elements of content remain constant across audiencesâ€”core concepts, fundamental principles, and essential factsâ€”while the variable elements involve how these concepts connect to individual interests, goals, and existing knowledge.</p>
<p>The key insight is that personalization doesn&rsquo;t require generating entirely new content for each user. Instead, it involves intelligent selection and combination of existing content elements, supplemented by targeted customization that creates meaningful connections to the user&rsquo;s specific context and needs.</p>
<h2 id="dynamic-user-understanding-through-interaction">Dynamic User Understanding Through Interaction</h2>
<p>Modern AI systems have unprecedented access to rich user interaction data through natural language conversations, reading highlights, and behavioral patterns. Unlike traditional recommendation systems that rely primarily on click-through data, AI-native platforms can analyze the semantic content of user queries, the topics they explore, and the questions they ask to build sophisticated models of their interests and learning preferences.</p>
<p>This approach moves beyond simple topic matching to understand cognitive patterns and learning styles. For example, the system might recognize that one user prefers concrete examples and case studies, while another gravitates toward theoretical frameworks and abstract principles. These insights enable the generation of content that not only covers relevant topics but presents them in ways that align with how each individual processes and retains information.</p>
<h2 id="building-memory-systems-that-learn-and-adapt">Building Memory Systems That Learn and Adapt</h2>
<p>The most sophisticated AI-native learning platforms implement memory architectures inspired by human cognition, incorporating episodic memory for recent interactions, semantic memory for abstracted patterns, and procedural memory for learned preferences and behaviors. This multi-layered approach enables systems to maintain context over time while continuously refining their understanding of user needs.</p>
<p>Rather than treating each interaction as isolated, these systems build cumulative knowledge about user interests, expertise levels, and learning goals. They can recognize when someone is exploring a new domain versus deepening existing knowledge, and adjust their content generation accordingly. This longitudinal understanding becomes increasingly valuable as it enables the system to suggest unexpected but relevant connections between seemingly disparate areas of interest.</p>
<h2 id="the-promise-of-adaptive-content-creation">The Promise of Adaptive Content Creation</h2>
<p>The ultimate vision extends beyond personalized recommendation to adaptive content creation. Imagine a system that can take a classic work like Sun Tzu&rsquo;s &ldquo;The Art of War&rdquo; and generate multiple interpretations tailored to different audiences and applications. For a business professional, it might emphasize strategic planning and competitive analysis. For a parent, it could explore family dynamics and conflict resolution. For a student, it might focus on historical context and philosophical implications.</p>
<p>Each version would maintain the core insights of the original work while presenting them through frameworks and examples that resonate with the specific audience. This approach recognizes that great ideas have universal applicability, but their accessibility depends heavily on how they&rsquo;re presented and contextualized.</p>
<h2 id="technical-implementation-and-practical-considerations">Technical Implementation and Practical Considerations</h2>
<p>Building these capabilities requires sophisticated orchestration of multiple AI systems working in concert. Content generation engines must work alongside user modeling systems, recommendation algorithms, and quality control mechanisms. The challenge lies not just in generating personalized content, but in ensuring it maintains accuracy, coherence, and genuine value while adapting to individual preferences.</p>
<p>Recent advances in multimodal AI and agent-based architectures provide the technical foundation for these applications. Tools like MCP (Model Context Protocol) servers enable modular, composable AI capabilities that can be combined and recombined to address specific user needs. This architectural approach allows for the kind of flexible, adaptive content generation that personalized learning requires.</p>
<h2 id="the-road-ahead">The Road Ahead</h2>
<p>As we look toward the future of AI-native learning platforms, the focus shifts from simple automation of existing processes to the creation of entirely new forms of educational experience. The most successful applications will be those that understand the deep relationship between content, context, and individual cognition, using this understanding to create learning experiences that are not just personalized, but genuinely transformative.</p>
<p>The transition from traditional content consumption to AI-enhanced learning represents more than a technological upgrade. It&rsquo;s a fundamental reimagining of how knowledge can be packaged, presented, and absorbed in ways that honor both the richness of human understanding and the unique cognitive patterns of individual learners. In this future, every question becomes an opportunity for personalized exploration, and every piece of content becomes a starting point for deeper, more meaningful engagement with ideas.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-essence-of-creativity-future-of-creative-work" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('essence-of-creativity-future-of-creative-work')">&times;</button>
      <h2>Essence of Creativity: Future of Creative Work</h2>
      <p class="project-meta">August 25, 2024</p>
      <div class="modal-content">
        <p>Is it creative to screenshot someone else&rsquo;s video and caption it with other people&rsquo;s comments? This seemingly simple question hits every creator making rent from content: if AI can remix, analyze, and generate at scale, what&rsquo;s left that&rsquo;s genuinely yours?</p>
<p>Through building AI content tools, I&rsquo;ve discovered the real opportunity isn&rsquo;t AI replacing creatorsâ€”it&rsquo;s AI helping creators understand the massive amounts of content data around them to find genuinely fresh angles. Think of it as having a research team that can analyze millions of posts, comments, and engagement patterns in seconds, then surface the insights that lead to truly original work.</p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-1.png" alt="TikTok Video Interface" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>TikTok's content creation interface: Where creators combine video, audio, and engagement elements to build viral content</em></p>
</div>
<p><em>Note (Nov 4th, 2025): I was actually able to speak to the head of strategy for Mr.Beast and surprisingly enough (or not surprisingly) this is exactly what they are doing and what makes their content so successful - they look for outliers in mass amounts of data - finding videos that genuinely spark viewers&rsquo; interest, even if it&rsquo;s in a different domain - think a Minecraft simulation with 100 players on each island (male/female) and recreating that with actual human participants.</em></p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-2.png" alt="TikTok Comment Analysis" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Comment analytics revealing audience engagement patterns and content resonance across different demographics</em></p>
</div>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/posts/creativity/content-structure-table.png" alt="Content Component Structure Table" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Content structure breakdown: How platforms organize multimodal content across video, interaction data, and comments</em></p>
</div>
<p>Let&rsquo;s touch on how, as we can see in this breakdown, what gets abstracted away to just tabular blobs of text contains much information on how information is presented and interacted with. The visual elements and text within the video grab a user&rsquo;s attention, while the audio provides voiceover narrating the message (or sometimes relevant background music). The titles and description provide detailed information on the video, while the comments sectionâ€”something often overlooked in current processing workflowsâ€”presents a goldmine of user interaction and feedback.</p>
<p>The primary comments act as tier 1 opinions, with responses and likes serving as interaction trackers. If users have the same opinion, they&rsquo;ll usually hit the like button instead of posting the exact same thing again. By linking the content of the video to actual interactions (both tier 1 and tier 2), we get a polling of audience feedback that is timely and beyond anything we can do in the same amount of time with surveys or really any other kind of data collection.</p>
<p>AI isn&rsquo;t becoming creativeâ€”it&rsquo;s becoming the ultimate creative research assistant. While generative AI struggles to produce truly fresh perspectives, it excels at helping us understand information and generate new insights that lead to genuinely creative work.</p>
<h2 id="what-constitutes-creative-work">What Constitutes Creative Work?</h2>
<p>To understand AI&rsquo;s role in creativity, we need to establish clear boundaries around what constitutes creative work. Consider the common practice of taking screenshots from viral videos and adding captions from popular comments. While this involves some editing, it&rsquo;s essentially sophisticated copying that accelerates content diffusion while reducing the economic returns of original creationâ€”what economist Schumpeter called &ldquo;creative destruction&rdquo; in reverse.</p>
<p>Understanding how platforms structure multimodal content helps us see the complexity involved in creative work.</p>
<p>Real creativity is about choosing a unique perspective. Content with contrast or conflict naturally captures our attentionâ€”think of viral TikToks that expose workplace absurdities or Twitter threads that challenge conventional wisdom. But thoughtful, empathetic content is equally creative: the YouTube essayist who helps you understand your own anxiety, or the LinkedIn post that perfectly articulates what you&rsquo;ve been feeling about remote work.</p>
<p>Here&rsquo;s how I think about the creative ecosystem: there&rsquo;s production (generating new content) and diffusion (deriving from or spreading existing content). AI&rsquo;s sweet spot isn&rsquo;t in either category aloneâ€”it&rsquo;s in helping us understand massive amounts of information to find genuinely new angles and insights.</p>
<p>Right now, creating professional-quality video requires hours of shooting, editing, and post-production. As AI gets better at handling video, audio, and text together, we&rsquo;re heading toward a world where that same video could be produced in minutes. This changes everything about the creative economy, making tools that help you find new inspiration increasingly valuable. Through this creative assistance, we can achieve two main effects:</p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/posts/creativity/creative-assistance-effects.png" alt="Creative Assistance Effects" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Two primary effects of AI creative assistance: Inspiration acquisition and content derivation</em></p>
</div>
<ol>
<li><strong>Inspiration acquisition</strong>: Accelerating original content production by collapsing the draft â†’ iterate loop</li>
<li><strong>Content derivation</strong>: Accelerating the diffusion of quality creative work across formats and channels</li>
</ol>
<h2 id="content-understanding-for-enhanced-generation">Content Understanding for Enhanced Generation</h2>
<p>How can we make language models produce outputs that meet our expectations? This challenge breaks down into two distinct problems: (1) we don&rsquo;t know what our ideal output looks like, and (2) we know what we want, but the language model doesn&rsquo;t understand us.</p>
<p>Most teams focus on the second problem through a toolkit of techniques: model alignment (training AI to follow human preferences), prompting (crafting better instructions), few-shot learning (training AI with just a few examples), retrieval-augmented generation or RAG (helping AI access specific databases), fine-tuning (customizing AI for specific tasks), and memory systems (helping AI remember context across conversations). But companies are rapidly commoditizing these approachesâ€”many solutions are open-sourced, which explains why so many generative products deliver roughly comparable results.</p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/posts/creativity/ai-workflow-types.png" alt="AI Workflow Types and Capabilities" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Different types of AI workflows: From basic generation to iterative collaboration</em></p>
</div>
<p>The real differentiation lies in how we adapt engineering and data processing to specific business scenariosâ€”and more importantly, in solving the first problem: helping users understand what they actually want to create.</p>
<p><em>August 2025: Recently I was asked about what the role of a PM in building technical products stands, especially given how much of the model side work is handled by technical teams. I think the answer, in short, is understanding of the product, and making sense of data in relation to how they are communicated and what they express. One may deal with numbers, but it&rsquo;s harder to actually understand the people that make up those numbers. In the concrete example of multimodal content understanding, it&rsquo;s being able to preserve the very granularity that makes this data so valuable, and proposing technical solutionsâ€”modality alignment, weighted clustering, agent triage, content rewrite, etc.</em></p>
<h3 id="brand-understanding-and-ai-integration">Brand Understanding and AI Integration</h3>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-6.png" alt="Brand Intelligence Platform" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Brand intelligence platform: AI systems learning to understand brand voice, visual identity, and content guidelines for contextual generation</em></p>
</div>
<p>The evolution toward brand-aware AI represents a significant shift in content generation capabilities. Instead of producing generic output that requires extensive human editing, these systems can understand contextâ€”what works for a luxury brand versus a startup, what tone resonates with different demographics, what visual styles align with brand guidelines.</p>
<p><em>November 2025: I saw the recent release of Google Pomelli and I think this is a great example of how a general purpose technology moves from research and public beta to a more grounded and applied case that delivers actual time saving and value. Like TypeFace, it&rsquo;s essentially creating a brand kit to free users from prompting repeatedly, and often times not knowing how to most effectively describe their style.</em></p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-8.png" alt="AI Brand Training Interface" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>AI brand training interface: How multimodal brand kits teach AI systems to generate content that aligns with specific brand requirements</em></p>
</div>
<p>The training process involves feeding AI systems examples of successful brand content across multiple modalitiesâ€”text, images, videos, and audio. The system learns not just what the brand says, but how it says it, what visual elements it uses, and what emotional tone it maintains. This creates AI that can generate content that feels authentically on-brand without constant human oversight.</p>
<p>Returning to the first problemâ€”&ldquo;I don&rsquo;t know what output I want&rdquo;â€”this stems from a lack of content understanding. Good script writing requires more than just hooks (&ldquo;You won&rsquo;t believe what happens next&rdquo;), unique selling propositions (USPs), and calls-to-action (CTAs)â€”it needs a clear angle: content that resonates with the audience, fits the context, and achieves its purpose.</p>
<p>Some products are building brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will probably become standard, the real breakthrough would be connecting insight data with generation without requiring manual setup every time.</p>
<h2 id="understanding-user-needs">Understanding User Needs</h2>
<p>Looking at the creative technology landscape, every categoryâ€”ad aggregation, competitor tracking, brand insights, performance analysis, content generationâ€”has 3-4 companies offering basically the same thing. The data products feel traditional, while the AI products often just add ChatGPT integrations to existing workflows.</p>
<p>The real opportunity lies in acquiring more granular data and creating smoother interactions. Instead of isolated tools, imagine connecting the entire creative production process where you can participate and adjust at each stageâ€”from initial research through final publication.</p>
<p>Here&rsquo;s a simple way to think about product value: user value = new experience - old experience - replacement cost. Most products built on foundational language models with minor tweaks deliver limited incremental value. Users still need to craft personalized prompts, and outputs almost always require multiple rounds of editing before they&rsquo;re usable.</p>
<p>So how do we increase incremental value? The answer isn&rsquo;t just better AIâ€”it&rsquo;s better workflows.</p>
<h2 id="user-friendly-workflows">User-Friendly Workflows</h2>
<p>Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.</p>
<p>The concept of &ldquo;workflows, not skills&rdquo; addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.</p>
<p>I used to think that simply connecting AI capabilities constitutes a workflow, but that&rsquo;s like saying a toolbox is the same as knowing how to build a house. What we call &ldquo;Language UI&rdquo; is actually &ldquo;Prompt UI&rdquo;â€”it differs from true language interaction by missing the context and shared understanding present in human conversation.</p>
<p>Think about the difference: you can tell a colleague &ldquo;make this more engaging&rdquo; and they understand your brand, audience, and context. With ChatGPT, you need to write a novel-length prompt every single time explaining who you are, what you&rsquo;re building, and what &ldquo;engaging&rdquo; means in your specific context.</p>
<p>The future workflow tools will have human-like elementsâ€”they&rsquo;ll ask follow-up questions, remember previous conversations, and understand your specific goals without you having to explain everything from scratch. Current prompting is probably transitional; eventually, we&rsquo;ll eliminate the need for context-heavy prompts by building AI that understands your context and generates appropriate guidance automatically.</p>
<h2 id="multimodal-interaction-and-content-ecosystem">Multimodal Interaction and Content Ecosystem</h2>
<p>Finally, let&rsquo;s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.</p>
<h3 id="understanding-content-through-data-visualization">Understanding Content Through Data Visualization</h3>
<p>The first layer of multimodal content understanding goes beyond traditional analytics. Rather than just tracking views and likes, the most interesting insights come from clustering comments and opinion spread by categoryâ€”product feedback, creator engagement, emotional responses. This granular analysis reveals patterns in audience sentiment and helps creators understand not just what performs well, but why it resonates with different audience segments.</p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-4.png" alt="Vector Visualization Analysis" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Vector visualization analysis: AI-powered semantic mapping revealing hidden relationships between content themes and audience preferences</em></p>
</div>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-3.png" alt="Content Analytics Dashboard" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Brand intelligence extraction: Analyzing fine-grained insights from audience feedback and engagement patterns</em></p>
</div>
<p>But the real magic happens in semantic analysis. Vector embeddings can reveal hidden relationships between content themes that humans might miss. For example, videos about &ldquo;productivity tips&rdquo; might cluster surprisingly close to &ldquo;cooking tutorials&rdquo; because both satisfy the same underlying need for life optimization. This kind of insight helps creators find unexpected angles and untapped niches.</p>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/projects/tiktok/tiktok-video-5.png" alt="Content Performance Metrics" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Content performance metrics: Comprehensive analysis tracking engagement patterns, reach optimization, and conversion effectiveness across content types</em></p>
</div>
<p>The final piece is comprehensive performance analysis that connects creative decisions to business outcomes. This isn&rsquo;t just about vanity metricsâ€”it&rsquo;s about understanding which content patterns lead to sustainable audience growth, higher conversion rates, and long-term creator success. When you can see these patterns clearly, you can make more informed creative decisions.</p>
<p>Switching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren&rsquo;t just people; they&rsquo;re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.</p>
<h2 id="case-study-voice-synthesis">Case Study: Voice Synthesis</h2>
<p>Take voice synthesis as an example. Technically, this technology is already quite matureâ€”you can clone a voice with just a few minutes of audio. Yet when most people think about AI voice cloning, they imagine phone scams. Sure, there are fun projects like AI David Attenborough narrating random videos, or OpenAI&rsquo;s GPT-4o launch event that briefly simulated Samantha&rsquo;s voice from &ldquo;Her.&rdquo; But the most creative use I&rsquo;ve seen comes from short video creators.</p>
<p>I recently discovered a creator called &ldquo;Yi Tou Jue Lv&rdquo; who makes derivative content based on &ldquo;In the Name of the People&rdquo; (a 2017 Chinese political drama). Their videos consistently get 500K+ views by doing something brilliant: they take original footage but replace all the narration with AI-synthesized character voices speaking internal monologues and psychological commentary. The result feels like getting inside the characters&rsquo; heads in a way the original show never offered.</p>
<p>What makes this work is the creator&rsquo;s deep understanding of the characters combined with AI&rsquo;s ability to generate consistent, high-quality voice synthesis. They&rsquo;re not just copyingâ€”they&rsquo;re creating a completely new layer of interpretation that audiences can&rsquo;t get anywhere else.</p>
<h2 id="contrasting-audio--text">Contrasting Audio &amp; Text</h2>
<p>It&rsquo;s fascinating how differently our brains process audio and text. When we read, we&rsquo;re essentially interacting with a graphical user interfaceâ€”scanning, jumping between sections, processing information at our own pace. We&rsquo;ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text can feel less engaging than a good conversation.</p>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
  <img src="/images/posts/creativity/podcast-interface.png" alt="Podcast Interface" style="width: 100%; border-radius: 8px;">
  <img src="/images/posts/creativity/wechat-saved-audio.png" alt="WeChat Audio Saving Interface" style="width: 100%; border-radius: 8px;">
</div>
<p style="font-size: 14px; color: #666; text-align: center; margin-top: 8px;"><em>Audio interfaces evolution: From podcast consumption (left) to social audio saving (right)â€”platforms adapting to our increasingly audio-first content behaviors and the need to preserve valuable conversations</em></p>
<p>Speaking, in contrast, is inherently linear and social. There&rsquo;s something about the human voice that keeps us presentâ€”the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It&rsquo;s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.</p>
<p>This contrast reveals something deeper about how we process information. Text excels at conveying complex ideasâ€”we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of textâ€”where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.</p>
<h2 id="conclusion">Conclusion</h2>
<div style="text-align: center; margin: 20px 0;">
  <img src="/images/posts/creativity/death-of-author.png" alt="Death of the Author by Roland Barthes" style="width: 100%; border-radius: 8px;">
  <p style="font-size: 14px; color: #666; margin-top: 8px;"><em>Roland Barthes' "Death of the Author": When content is created, interpretation rights transfer to the audience</em></p>
</div>
<p>As Roland Barthes suggested with &ldquo;The Death of the Author,&rdquo; once content is created, interpretation rights transfer to the audience. We see this everywhere todayâ€”YouTube channels that analyze every Marvel movie, TikTok accounts that remix old TV shows, podcast networks that dissect every episode of popular series.</p>
<p>With improvements in AI voice synthesis, character generation, and content manipulation, we&rsquo;re approaching a future where derivative works based on original intellectual properties can achieve professional quality while satisfying different interpretations and imaginations. The &ldquo;Yi Tou Jue Lv&rdquo; example I mentioned earlier is just the beginning.</p>
<p>These perspectives might all exist in the original work, but each remix offers a different angle, providing audiences with unique experiences. There&rsquo;s still massive amounts of content that people want to see but isn&rsquo;t available on any platform. Maybe creativity&rsquo;s next evolution isn&rsquo;t about generating entirely new contentâ€”it&rsquo;s about intelligently remixing and reinterpreting what already exists to better satisfy what audiences actually want.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>While generative AI capabilities evolve rapidly, human nature changes slowly. We overestimate technology&rsquo;s short-term creative impact (AI won&rsquo;t replace human creativity next year), but underestimate how fundamentally it will change creative workflows (it&rsquo;s already happening in ways we&rsquo;re only beginning to understand).</p>
<p>Making probabilistic models truly creative remains challenging yet fascinating work. The future lies not in AI replacing human creativity, but in building systems that amplify our ability to understand, synthesize, and create meaning from the infinite streams of content around us. That&rsquo;s the creative challenge I&rsquo;ll continue working on.</p>
<hr>
<h2 id="appendix">Appendix</h2>
<p>This article was originally developed as a presentation and shared internally with TikTok team members during my time there. The content has been adapted for public publication and adjusted to remove potentially sensitive information while preserving the core insights about AI and creativity.</p>
<p>All views expressed in this article are my own and do not represent the official positions or strategies of TikTok or any other organization.</p>

      </div>
    </div>
  </div>
  
  <div id="modal-building-ai-that-actually-understands-how-students-learn" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('building-ai-that-actually-understands-how-students-learn')">&times;</button>
      <h2>Building AI That Actually Understands How Students Learn</h2>
      <p class="project-meta">August 15, 2024</p>
      <div class="modal-content">
        <p><em>Terry Chen, Allyson Lee</em></p>
<p>Your engineering team is stuck. Again.</p>
<p>It&rsquo;s not that they lack the technical skillsâ€”they can write clean code and architect complex systems. But watch them tackle ambiguous problems and you&rsquo;ll see the real bottleneck: they overthink requirements, jump to solutions too quickly, or get paralyzed trying to make everything perfect before shipping.</p>
<p>These aren&rsquo;t skill gaps. They&rsquo;re self-regulation patterns that determine whether talented people build breakthrough products or get trapped in endless cycles of &ldquo;almost ready to launch.&rdquo;</p>
<p>What if AI could understand not just <em>what</em> people are struggling with, but <em>how</em> they&rsquo;re strugglingâ€”and then connect them with others who&rsquo;ve broken through the same patterns?</p>
<p>We built an AI coaching system that does exactly this. Instead of treating learning like a checklist, it recognizes the messy reality of how high-performers actually develop breakthrough capabilities.</p>
<p>

  <img src="/images/projects/llmcoaching/coach-feedback-interface.png" alt="Coach Feedback Interface showing regulation gap analysis" loading="lazy">
 
<em>Our AI system analyzes coaching conversations to identify specific learning regulation gaps and suggests targeted interventions</em></p>
<p>

  <img src="/images/projects/llmcoaching/novice2expert.png" alt="Novice to Expert Learning System" loading="lazy">
 
<em>Novice to Expert Learning System</em></p>
<h2 id="try-the-live-prototype">Try the Live Prototype</h2>
<p><strong><a href="/llm-coaching/">âž¤ Access the LLM Coaching System Prototype</a></strong></p>
<p>Experience our AI coaching system firsthandâ€”upload a coaching conversation and see how it identifies learning patterns and suggests peer connections. The app is embedded directly in our site for seamless interaction.</p>
<hr>
<h2 id="the-real-problem-with-educational-ai">The Real Problem with Educational AI</h2>
<p>Here&rsquo;s what most people miss about learning: it&rsquo;s not about information transfer. It&rsquo;s about developing self-regulationâ€”the ability to plan, execute, overcome challenges, seek help, and reflect on your own learning process.</p>
<p>Think about the last time you got stuck on a complex problem. Maybe you procrastinated, or dove into details too early, or got paralyzed trying to make it perfect. These aren&rsquo;t character flawsâ€”they&rsquo;re specific patterns of how you regulate your learning. And they&rsquo;re predictable.</p>
<p>Traditional coaching can identify these patterns, but it doesn&rsquo;t scale. A good coach might work with 20 students max. Meanwhile, thousands of students struggle with the same regulation gaps, often in isolation.</p>
<p>We realized something: what if we could capture the expertise of great coaches and make it accessible to everyone? Not through generic advice, but through intelligent peer connections based on actual learning patterns.</p>
<h2 id="how-we-built-ai-that-actually-gets-learning">How We Built AI That Actually Gets Learning</h2>
<p>Our solution, called &ldquo;Peer Connections,&rdquo; has three core components:</p>
<ol>
<li><strong>Record and analyze</strong> coaching conversations to identify specific regulation gaps</li>
<li><strong>Match students</strong> with peers who&rsquo;ve successfully worked through similar challenges</li>
<li><strong>Facilitate meaningful conversations</strong> that help students develop concrete action plans</li>
</ol>
<p>The magic happens in how we taught our AI to recognize learning patterns that humans care about.</p>
<h3 id="the-challenge-teaching-ai-to-recognize-learning-patterns">The Challenge: Teaching AI to Recognize Learning Patterns</h3>
<p>The first challenge was getting AI to understand the difference between &ldquo;student struggles with visual representation&rdquo; and &ldquo;student fears imperfection.&rdquo; Both might result in poor deliverables, but they require completely different interventions.</p>
<p>We built a structured codebook that categorizes learning regulation into three core types:</p>
<p>

  <img src="/images/projects/llmcoaching/regulation-gap-categories.png" alt="Regulation Gap Categories showing Cognitive, Metacognitive, and Emotional patterns" loading="lazy">
 
<em>Our AI categorizes learning challenges into specific, actionable patterns that enable precise peer matching</em></p>
<p><strong>Cognitive gaps</strong>: Problems with approaching unknown challenges (like creating clear visual representations)<br>
<strong>Metacognitive gaps</strong>: Issues with planning, help-seeking, and reflection (like not slicing work effectively)<br>
<strong>Emotional gaps</strong>: Dispositions toward learning that affect motivation (like fear of imperfection)</p>
<p>The breakthrough came when we realized that pure semantic matching wasn&rsquo;t enough. Two students could describe their struggles completely differently but face the same underlying regulation pattern. Through testing on 28 manually coded CAP notes, our hybrid approach combines LLM-based pattern recognition with semantic similarity, achieving 87.5% precision and 89.3% recall in identifying regulation gapsâ€”performance that rivals human expert consistency in this domain.</p>
<h2 id="making-peer-learning-actually-work">Making Peer Learning Actually Work</h2>
<p>But identifying learning patterns was only half the challenge. The harder question was: how do you facilitate a conversation between two students that actually helps?</p>
<p>Most peer learning fails because experienced students struggle to articulate their process. They remember what worked but not why it worked, or how they figured it out. Meanwhile, novices often ask surface-level questions that don&rsquo;t get to the real learning.</p>
<p>We discovered that effective peer conversations need structure. Not scripts, but scaffolding that helps both students navigate the conversation productively.</p>
<h3 id="the-conversation-framework">The Conversation Framework</h3>
<p>We tested different approaches to peer conversation facilitation. What we found was transformative:</p>
<p><strong>Before structured conversation</strong>: &ldquo;I have no time to get things done, since my internship is starting tomorrow. There&rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it&rsquo;s even lower.&rdquo;</p>
<p><strong>After structured conversation</strong>: &ldquo;You know what, I&rsquo;m gonna send this to my partner first, and then I&rsquo;m gonna send this to Haoqi.&rdquo;</p>
<p>The shift was remarkable. The student went from resignation and self-doubt to concrete action planning. This wasn&rsquo;t just emotional supportâ€”it was a structured process that helped her move from paralysis to progress.</p>
<h2 id="system-description">System Description</h2>
<p>Our system consists of three key components. First, users upload an audio recording of their conversation with the coach. Then, our system matches the user with a student who experienced and addressed that regulation gap in the past. Finally, we help them facilitate a conversation that guides how they will plan their next sprint.</p>
<h3 id="1-speech-to-text-transcription">1. Speech-to-text Transcription</h3>
<p>The first component of our system utilizes speech-to-text transcription with NLP-based feedback, to intake a student&rsquo;s audio recording of their conversation with the coach, allowing the system to analyze the student&rsquo;s regulation gap, while reducing the time restraints required by CAP notes.</p>
<p>

  <img src="/images/projects/llmcoaching/figure1.png" alt="Speech-to-text Transcription Interface" loading="lazy">
 
<em>Figure 1: Speech-to-text Transcription System Interface</em></p>
<h3 id="2-student-regulation-gap-analysis-system">2. Student regulation gap analysis system</h3>
<p>Thereafter, we employ a student regulation gap analysis system that leverages vector embeddings and large language models to identify patterns in student regulation behaviors across learning contexts. The system provides targeted coaching suggestions based on similar historical cases, addressing the challenge of effective coaching for developing regulation skills in design, research, and STEM innovation.</p>
<p>Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach. Initially, we pre-process student regulation notes to include metadata on tier 1 and tier 2 regulation gaps, then encode them into text embeddings. The vector database retrieves the most similar historical cases, and we send these along with the original query to an LLM (Deepseek). The LLM generates a structured response including a diagnosis of potential regulation gaps, practice suggestions targeted to these gaps, and references to similar historical cases. This approach grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.</p>
<p>

  <img src="/images/projects/llmcoaching/figure2.png" alt="Student Regulation Gap Analysis" loading="lazy">
 
<em>Figure 2: Student Regulation Gap Analysis Page</em></p>
<h4 id="i-data-preprocessing">I. Data Preprocessing</h4>
<p>We exported all existing notes from the CAP (Context-Assessment-Plan) note system and encountered several challenges during preprocessing. Many notes lacked clear assessment sections identifying regulation gaps, provided insufficient context about the student&rsquo;s situation, and used inconsistent terminology to describe similar regulation gaps. To address these issues, we removed duplicated notes and filtered out entries with incomplete fields. To better structure the data for our system, we standardized the information into several key fields.</p>
<p>These fields include a unique identifier for each case, the key learning gap identified in the assessment, additional information such as project details, title, context, and plan, the complete original coaching note, the project name, high-level regulation categories (e.g., &ldquo;Cognitive,&rdquo; &ldquo;Metacognitive&rdquo;), and more specific skill categories (e.g., &ldquo;Critical thinking,&rdquo; &ldquo;Forming feasible plans&rdquo;). We generated the last two fieldsâ€”tier1_categories and tier2_categoriesâ€”using the Deepseek reasoning model with a specialized regulation skills codebook that provides the conceptual backbone for our semantic matching approach.</p>
<p>

  <img src="/images/projects/llmcoaching/figure3.png" alt="CAP Note Structure" loading="lazy">
 
<em>Figure 3: Organized CAP Note Structure</em></p>
<h4 id="ii-the-regulation-skills-codebook-structured-knowledge-for-semantic-matching">II. The Regulation Skills Codebook: Structured Knowledge for Semantic Matching</h4>
<p>Our key breakthrough was integrating a structured codebook that categorizes student regulation gaps according to a research-grounded frameworkâ€”enabling our AI to match students based on learning patterns rather than just surface-level problem descriptions. For example, a student saying &ldquo;I&rsquo;m stuck on the prototype&rdquo; and another saying &ldquo;I can&rsquo;t visualize the system architecture&rdquo; would both be matched under &ldquo;Representing problem and solution spaces,&rdquo; even though their language is completely different. The codebook provides a hierarchical organization of regulation skills across three domains, as detailed in <a href="#appendix-e">Appendix E</a>. This framework serves as the foundation for our semantic matching system.</p>
<p>The codebook defines three main categories of regulation skills: <em>Cognitive Skills</em>, which encompass abilities for approaching problems with unknown answers; <em>Metacognitive Skills</em>, which include capacities for planning, help-seeking, collaboration, and reflection; and <em>Emotional Regulation</em>, which covers dispositions toward self and learning that affect motivation.</p>
<h4 id="iii-bridging-language-and-concepts-semantic-enrichment">III. Bridging Language and Concepts: Semantic Enrichment</h4>
<p>The codebook functions as a semantic bridge between varied descriptions of similar problems. Consider examples such as &ldquo;Student struggles to create clear visual representations of system architecture&rdquo; (categorized as Cognitive &gt; Representing problem and solution spaces), &ldquo;Prototype missing key feature&rdquo; (also Cognitive &gt; Representing problem and solution spaces), &ldquo;Not slicing work to risk&rdquo; (Metacognitive &gt; Planning effective iterations), and &ldquo;Student gets stuck and then stops thinking about strategy&rdquo; (Emotional &gt; Embracing challenges/learning/independence). Even without textual similarity, the codebook categorization reveals conceptual relationships. For instance, we categorize both &ldquo;stopping too soon with examples&rdquo; and &ldquo;not thinking deeply about risks&rdquo; under &ldquo;Critical thinking and argumentation,&rdquo; enabling the system to recognize their conceptual similarity despite different phrasing.</p>
<p>This structured approach provides several advantages. It standardizes vocabulary across different coaching contexts, reveals underlying patterns in student regulation behavior, enables transfer of coaching knowledge across project domains, and prioritizes skill-based similarity over surface-level textual matching.</p>
<table>
  <thead>
      <tr>
          <th>Gap Description</th>
          <th>Tier 1</th>
          <th>Tier 2</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>&ldquo;Student struggles to create clear visual representations of system architecture&rdquo;</td>
          <td>Cognitive</td>
          <td>Representing problem and solution spaces</td>
      </tr>
      <tr>
          <td>&ldquo;Prototype missing key feature&rdquo;</td>
          <td>Cognitive</td>
          <td>Representing problem and solution spaces</td>
      </tr>
      <tr>
          <td>&ldquo;Not slicing work to risk&rdquo;</td>
          <td>Metacognitive</td>
          <td>Planning effective iterations</td>
      </tr>
      <tr>
          <td>&ldquo;Student gets stuck and then stops thinking about strategy&rdquo;</td>
          <td>Emotional</td>
          <td>Embracing challenges/learning/independence</td>
      </tr>
  </tbody>
</table>
<h4 id="iv-similarity-methods-for-regulation-gap-analysis">IV. Similarity Methods for Regulation Gap Analysis</h4>
<p>To find cases with the same regulation gaps, we started with the semantic-based approach and gradually enhanced our methods. Our initial baseline approach uses vector embeddings to find similar cases based on textual similarity. We encode the full text of student issues using OpenAI&rsquo;s ada-002 embedding model and compare them using cosine similarity. This pure semantic similarity approach treats all text equally and serves as a useful baseline. However, while straightforward, this approach often prioritizes surface-level similarities like project domain rather than underlying regulation patterns.</p>
<p>To improve matching relevance, we enhanced our approach by separating the regulation gap description from contextual information, applying higher weight to the gap text (default: 0.7 for regulation gap, 0.3 for other content). The gap text often contains the most critical information about the learning challenge, so it deserves higher weight. This weighted semantic similarity approach better identifies regulation similarities even when project contexts differ, as it emphasizes the specific regulation gap rather than surrounding information.</p>
<p>However, we also realized limitations with the semantic-based matching methods, namely that semantic similarity only worked when there were apparent keywords to match. To address this limitation, we created a comprehensive codebook containing regulation gap definitions and examples, using a reasoning model (Deepseek-reasoning) to generate metadata with tier 1 and tier 2 regulation gap tags for enhanced matching. This LLM reasoning with codebook approach assigns the highest weight (0.5) to tier 2 categories and a lower weight (0.1) to tier 1 categories, with gap text and other content each receiving 0.2 weight. This metadata represents specific regulation skills, allowing the system to match cases addressing similar skills regardless of how they&rsquo;re described or in what context they appear.</p>
<h4 id="v-applying-the-codebook-in-evaluation-scripts">V. Applying the Codebook in Evaluation Scripts</h4>
<p>The codebook plays a crucial role in the tiered similarity evaluation methods. The categorization process begins with the categorize_regulation_gaps_deepseek_v0.2.py script, which uses the codebook definitions to classify each gap text into tier 1 and tier 2 categories. Each gap is analyzed against the codebook&rsquo;s framework, identifying which cognitive, metacognitive, or emotional regulation skills are being addressed. We then store these categorizations in the tiered_weighted_cases.json file as structured metadata for later retrieval and analysis.</p>
<p>The codebook improves evaluation outcomes in several important ways. It provides a standardized vocabulary with a consistent framework of terms and concepts, helping to match cases that use different wording but address the same underlying skills. It enables contextual understanding by categorizing gaps according to the codebook, allowing the system to understand the deeper educational context beyond surface-level language similarities. The codebook&rsquo;s three-category structure (cognitive, metacognitive, emotional) aligns with research on key regulation skills central to the situated practice system, providing coaching concept integration. Even when projects differ completely, the codebook categories help identify transferable skills and learning patterns across domains, enabling cross-project relevance. The higher weight given to tier 2 categories (0.5) in the tiered similarity approach reflects their importance in precisely identifying the specific skill gaps being addressed.</p>
<p>For instance, when processing a gap like &ldquo;stopping too soon with examples,&rdquo; without the codebook approach, the system might only match cases with similar phrasing about &ldquo;stopping&rdquo; or &ldquo;examples.&rdquo; With the codebook integration, this gap is properly categorized as &ldquo;Cognitive &gt; Critical thinking and argumentation,&rdquo; allowing matches to conceptually similar gaps like &ldquo;not thinking deeply enough about risks&rdquo; even when the phrasing differs completely.</p>
<h3 id="3-facilitating-conversation-and-formulating-tailored-plans">3. Facilitating Conversation and Formulating Tailored Plans</h3>
<p>To generate helpful and contextualized conversation facilitation questions, we created a codebook based on Choi et al.&rsquo;s question methodology, then prompted an LLM  to generate responses based on conversation context (streamed) and codebook <a href="#ref20">[20]</a> (See <a href="#appendix-f">Appendix F</a>). The codebook and question generation is still within its early stages, until we conduct more user testing to discover the most pertinent questions to ask. Additionally, while not implemented yet, formulating tailored plans would take place in the form of filling out their sprint log/plan for their week, which they would be either linked to or the system will parse it into the sheet for them. This will allow them to enact their plans to address their regulation gap while still incorporating the tasks they need to complete for the week.</p>
<p>

  <img src="/images/projects/llmcoaching/figure4.png" alt="Discussion Questions Interface" loading="lazy">
 
<em>Figure 4: Suggested Discussion Questions</em></p>
<h2 id="study--experiment--deployment">Study / Experiment / Deployment</h2>
<p>We conducted manual qualitative coding on a set of 28 CAP notes utilizing our codebook definitions to use as ground truth to evaluate our model against. When comparing our tier 2 codes on these notes with the codes given by our system, we observed a precision of 0.875, and a recall of 0.893. Given the possible subjectivity and overlap of regulation gaps, these results suggest we can rely on our system to accurately categorize student casesâ€™ regulation gaps.</p>
<p>Next, we investigated how to facilitate effective conversation between the peers once they are matched. Most notably, how do we define what an â€œeffectiveâ€ conversation looks like?</p>
<p>To begin, we conducted some casual conversations with peers in the DTR program, asking them to explain about an issue they struggled with in the past, and how they were able to overcome it. Our conversations had no structure, as it was just to give us an idea of how students talked about regulation with a peer. From this preliminary research, we observed the main finding that experienced students struggle to articulate their experience in a way that is helpful to the peer. This is due to several factors:</p>
<p>A lack of project context/terminology
Not articulating their experience addressing the regulation gap in a step-by-step, clear way, making it hard for the peer to abstract their thinking process into a high-level framework they could use
Both peers have trouble remembering what they experienced and what actionable next steps to take</p>
<p>Based upon these findings, we defined an &ldquo;effective&rdquo; conversation to be one that helps them create a more concrete, actionable plan than they had before, and that they actually implement this and solve their regulation gap faster. We then identified this causal structure for which to guide our prototype and user testing:</p>
<p>

  <img src="/images/projects/llmcoaching/diagram.png" alt="Causal Structure Diagram" loading="lazy">
 
<em>Figure: Causal structure diagram showing the relationship between peer conversations and regulation gap resolution</em></p>
<p>Given this causal diagram, we set up a second, more formal user testing scenario. Given the work of Choi et al., which highlights how scaffolding peer-questioning strategies can help facilitate metacognition, we formulated a list of questions that fell under the clarification/elaboration and â â context/perspective-oriented categories suggested in the paper <a href="#ref20">[20]</a>. We created two lists of questions, one for the novice and one for the experienced peer, to guide the conversation, in the goal of assisting the experienced peer in articulating their experience in a step-by-step manner, and helping the novice understand how they can apply this to their own situation (See <a href="#appendix-g">Appendix G</a>).</p>
<p>We then conducted some preliminary user testing, by having one of us act as the experienced peer, and having another student in DTR be the novice. We simulated the conversation by having the novice ask us the questions provided in the scaffold, while we acted as the experienced peer answering the questions and providing advice. To qualitatively assess how the conversation influenced the peer, we asked them the same questions before and after (See <a href="#appendix-h">Appendix H</a>). During one of our user tests, one of our users, who struggled with the fear of imperfection, demonstrated this transformation:</p>
<table>
  <thead>
      <tr>
          <th>Before peer conversation</th>
          <th>After peer conversation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>&ldquo;I have no time to get things done, since my internship is starting tomorrow. There&rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it&rsquo;s even lower.&rdquo;</td>
          <td>&ldquo;You know what, I&rsquo;m gonna send this to my partner first, and then I&rsquo;m gonna send this to Haoqi.&rdquo;</td>
      </tr>
  </tbody>
</table>
<p>This illustrates multiple findings, first that the student shifted their mindset towards already deciding that she wasn&rsquo;t going to get what she wanted to do, despite not having started yet, to taking actionable steps towards working on her project and addressing her regulation gap. Furthermore, the student changed the steps they were going to dedicate time halfway between her tasks for the week to send deliverables over to the coach. The next step would be to follow up and see if this was accomplished, and how it affected her project outcome and regulation.</p>
<h2 id="discussion">Discussion</h2>
<p>Our system effectively facilitates AI-enhanced coaching by leveraging Large Language Models (LLMs) and structured metadata to support project-based learning environments. By integrating semantic matching with structured codebook metadata, our approach identifies relevant coaching cases, reducing cognitive load on mentors while maintaining high-quality, context-aware feedback. We also began to explore how to facilitate effective and productive conversation amongst peers about regulation gaps. This discussion outlines the generalizable design elements and techniques that contributed to the prototypeâ€™s effectiveness, highlighting key takeaways for future socio-technical system development.
<strong>Hybrid AI-Driven Case Retrieval:</strong> Our system employs a hybrid approach that combines LLM-driven metadata tagging with traditional semantic matching. While pure semantic similarity methods struggled to capture nuanced regulation gaps due to limited repetitive terminology, and LLM-based approaches returned too many broadly relevant cases, integrating both techniques enabled <em>precision</em> in retrieving the most relevant coaching cases. This demonstrates the efficacy of hybrid AI-driven retrieval in domains where contextual similarity and structured knowledge are both crucial. Future systems designed for education or expert-driven fields can benefit from this combined methodology to ensure both relevance and specificity in recommendations.</p>
<p><strong>Structured Codebooks for Domain-Specific AI Applications:</strong> A key enabler of our system&rsquo;s success was the development of a structured codebook that categorizes regulation gaps into tiered classifications, allowing advanced reasoning models to more accurately categorize regulation gaps. By leveraging <em>cognitive</em>, <em>metacognitive</em>, and <em>emotional</em> regulation categories, the system grounded LLM-based reasoning in expert-validated pedagogical frameworks and effectively addresses the issue of LLM hallucinations. The implication for broader AI applications is that structured codebooks can serve as a mechanism to guide AI reasoning, especially for test time scaled models, and in fields requiring human-like judgment and contextual understanding.</p>
<p><strong>Scaffolding productive conversations for addressing regulation gaps:</strong> Providing enough context and terminology of the other&rsquo;s project and issue is crucial to set them up for success. Additionally, adopting peer-questioning scaffolds assists in driving a conversation where the more experienced peer can better articulate and reflect upon their step-by-step strategy to addressing their regulation gap, which can be adapted and applied by the novice.</p>
<h2 id="limitations-and-future-work">Limitations and Future Work</h2>
<p>There are inherent limitations in our current approach that necessitate attention. The CAP note system is succinctly written and does not always provide sufficient context for robust reasoning. Additionally, our codebook currently focuses primarily on high-level descriptions of regulation gaps along with their examples, which makes it difficult for the language model to develop domain-specific knowledge and reasoning for effective categorization.
To address these limitations and plan for iterative improvements, we will work with CAP note developers to identify ways toward either:</p>
<ul>
<li>Improving clarity of writing in notes</li>
<li>Collecting more data through alternative data sources (SIG meeting transcripts, etc.)</li>
</ul>
<p>For future work, we propose developing a sub-categorized codebook that further segments existing regulation gaps and contains specific examples along with reasoning chains for arriving at regulation gap categorizations. With such a codebook, we can first perform a tier 1 categorization of the gap to route to a corresponding model identifying each of the subcategories for further reasoning (using two sets of language models and corresponding reasoning prompts) for few-shot learning and prompt-based LLM-enabled categorization.</p>
<p>As LLMs continue to progress, we believe there is merit in more sophisticated reasoning methods such as the use of external knowledge bases or memory systems for persistent storage of student regulation gap progression. These approaches could enable more tailored and precise gap understanding, ultimately leading to more effective coaching support for developing regulation skills in design, research, and STEM innovation contexts.</p>
<p>In terms of facilitating effective conversation between peers, we plan to continually design testing strategies of feature slices, focusing on evaluating what intervention strategies are most effective in facilitating peers to more clearly articulate and discuss their task plans in context of their regulation gaps, thereby facilitating meaningful peer to peer interaction enabling effective regulation skill learning. This involves testing whether real time generation of follow up questions for explanation elaboration or regulation skill connection allows better articulation of questions during peer to peer conversations, as well as whether more context regarding the peerâ€™s project and regulation skills should be provided in advance to foster more meaningful conversations grounded in discussion of actionable next steps for regulation skill learning.</p>
<p>Most notably, there is still more work to be done with testing these strategies, specifically figuring out how to ensure that it impacts students not just in the moment, but in the long-term. To do so, we plan to run formal user tests which both qualitatively and quantitatively examine how they will and if they are addressing their regulation gap, by surveying them before using the prototype and having the conversation, after, and the subsequent weeks.</p>
<h2 id="references-in-ieee-format">References in IEEE Format</h2>
<p><strong>[1]</strong> E. M. Gerber, J. M. Olson, and R. L. D. Komarek, &ldquo;Extracurricular design-based learning: Preparing students for careers in innovation,&rdquo; International Journal of Engineering Education, vol. 28, no. 2, p. 317, 2012. {#ref1}</p>
<p><strong>[2]</strong> J. C. Dunlap, &ldquo;Problem-based learning and self-efficacy: How a capstone course prepares students for a profession,&rdquo; Educational Technology Research and Development, vol. 53, no. 1, pp. 65â€“83, Mar. 2005. {#ref2}</p>
<p><strong>[3]</strong> B. J. Zimmerman, &ldquo;Becoming a self-regulated learner: An overview,&rdquo; Theory Into Practice, vol. 41, no. 2, pp. 64â€“70, May 2002. {#ref3}</p>
<p><strong>[4]</strong> D. R. Lewis, M. Easterday, and C. Riesbeck, &ldquo;Research slices: Core processes for effective iteration in eder,&rdquo; EDeR. Educational Design Research, vol. 8, no. 1, 2024. {#ref4}</p>
<p><strong>[5]</strong> D. G. R. Lewis, S. E. Carlson, C. K. Riesbeck, E. M. Gerber, and M. W. Easterday, &ldquo;Encouraging engineering design teams to engage in expert iterative practices with tools to support coaching in problem-based learning,&rdquo; Journal of Engineering Education, vol. 112, no. 4, pp. 1012â€“1031, 2023. {#ref5}</p>
<p><strong>[6]</strong> E. J. Huang, D. R. Lewis, S. Gaudani, M. Easterday, and E. Gerber, &ldquo;Intelligent coaching systems: Understanding one-to-many coaching for ill-defined problem solving,&rdquo; Proceedings of the ACM on Human-Computer Interaction, vol. 7, no. CSCW1, pp. 138:1â€“138:24, Apr. 2023. {#ref6}</p>
<p><strong>[7]</strong> H. Zhang, M. Easterday, and S. Shah, &ldquo;Collaborative Research: Situated Practice Systems: Supporting Coaches and Students to Develop Regulation Skills for Design, Research, and STEM Innovation,&rdquo; National Science Foundation Grant Proposal, 2024. {#ref7}</p>
<p><strong>[8]</strong> Asana. <a href="https://asana.com/product">https://asana.com/product</a>, 2025. {#ref8}</p>
<p><strong>[9]</strong> Trello. <a href="https://trello.com/">https://trello.com/</a>, 2025. {#ref9}</p>
<p><strong>[10]</strong> Z. Xiao, X. Yuan, Q. V. Liao, R. Abdelghani, and P.-Y. Oudeyer, &ldquo;Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding,&rdquo; in Companion Proceedings of the 28th International Conference on Intelligent User Interfaces, pp. 75â€“78, 2023. {#ref10}</p>
<p><strong>[11]</strong> C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang, &ldquo;Can large language models transform computational social science?&rdquo; Computational Linguistics, vol. 50, no. 1, pp. 237â€“291, 2024. {#ref11}</p>
<p><strong>[12]</strong> K. F. Shaalan, &ldquo;An Intelligent Computer Assisted Language Learning System for Arabic Learners,&rdquo; Computer Assisted Language Learning, vol. 18, no. 1-2, pp. 81-108, Feb. 2005. {#ref12}</p>
<p><strong>[13]</strong> E. Mousavinasab, M. Zarifsanaiey, S. R. Niakan Kalhori, M. R. Rakhshan, M. Keikha, and A. Ghazi Saeedi, &ldquo;Intelligent Tutoring Systems: A Systematic Review of Characteristics, Applications, and Evaluation Methods,&rdquo; Interactive Learning Environments, vol. 29, no. 1, pp. 142-163, Jan. 2021. {#ref13}</p>
<p><strong>[14]</strong> M. Zawacki-Richter, V. MarÃ­n, M. Bond, and F. Gouverneur, &ldquo;Systematic Review of Research on Artificial Intelligence Applications in Higher Education â€“ Where Are the Educators?&rdquo; International Journal of Educational Technology in Higher Education, vol. 16, no. 1, pp. 1-27, Dec. 2019. {#ref14}</p>
<p><strong>[15]</strong> P. Arnau-GonzÃ¡lez, M. Arevalillo-HerrÃ¡ez, R. Albornoz-De Luise, and D. Arnau, &ldquo;A Methodological Approach to Enable Natural Language Interaction in an Intelligent Tutoring System,&rdquo; Computer Speech &amp; Language, vol. 77, p. 101386, Jun. 2023. {#ref15}</p>
<p><strong>[16]</strong> B. Woolf, &ldquo;Building Intelligent Interactive Tutors: Student-Centered Strategies for Revolutionizing E-Learning,&rdquo; Morgan Kaufmann, 2009. {#ref16}</p>
<p><strong>[17]</strong> K. R. Koedinger and A. Corbett, &ldquo;Cognitive Tutors: Technology Bringing Learning Science to the Classroom,&rdquo; in The Cambridge Handbook of the Learning Sciences, R. K. Sawyer, Ed. Cambridge: Cambridge University Press, 2006, pp. 61-78. {#ref17}</p>
<p><strong>[18]</strong> J. Evens and J. Michael, &ldquo;One-on-One Tutoring by Humans and Computers,&rdquo; Routledge, 2006. {#ref18}</p>
<p><strong>[19]</strong> E. Wenger, &ldquo;Artificial Intelligence and Learning in Context: A Review,&rdquo; AI in Education Journal, vol. 21, no. 4, pp. 457-473, 2022. {#ref19}</p>
<p><strong>[20]</strong> I. Choi, S. M. Land, and A. J. Turgeon, &ldquo;Scaffolding peer-questioning strategies to facilitate metacognition during online small group discussion,&rdquo; Instructional Science, vol. 33, no. 5â€“6, pp. 483â€“511, 2005, doi: 10.1007/s11251-005-1277-4. {#ref20}</p>
<h2 id="appendix-b">Appendix B</h2>
<p>Improving and Scaling Coaching is part three of <em>Situated Practice Systems</em> (SPS), which offers tools to help coaches and learners understand work practices and develop self-directed innovation skills <a href="#ref7">[7]</a>.</p>
<p>

  <img src="/images/projects/llmcoaching/appendixb.png" alt="Appendix B Diagram" loading="lazy">
 
<em>Figure: Situated Practice Systems overview showing the three-part framework</em></p>
<h2 id="appendix-c">Appendix C</h2>
<p>CAP Notes helps coaches elicit information about a student&rsquo;s work issue and regulation gaps by tracking their activities, outputs, and reflections <a href="#ref7">[7]</a>.</p>
<p>

  <img src="/images/projects/llmcoaching/appendixc.png" alt="CAP Notes Structure" loading="lazy">
 
<em>Figure: CAP Notes structure showing Context, Assessment, and Plan components</em></p>
<h2 id="appendix-d">Appendix D</h2>
<p>Practice Objects contain information about a student&rsquo;s work issues, current and tracked regulation gaps, suggested practices, and practice traces <a href="#ref7">[7]</a>.</p>
<p>

  <img src="/images/projects/llmcoaching/appendixd.png" alt="Practice Objects Framework" loading="lazy">
 
<em>Figure: Practice Objects framework showing work issues, regulation gaps, and practice traces</em></p>
<h2 id="appendix-e">Appendix E</h2>
<p>Prompting of the LLM with the codebook incorporated.</p>
<p>â€œYou are an expert in analyzing student regulation gaps. You need to categorize each CAP (Context, Assessment, Plan) note into these three tier 1 categories and their corresponding tier 2 categories. Each case may be categorized into multiple tier 1 and tier 2 categories:</p>
<ol>
<li>
<p>Cognitive: The student lacks skills for approaching problems with an unknown answer, or even, knowing what the problem is exactly. This includes:
Representing problem and solution spaces: The way the student structures or presents the information is not effectively supporting reasoning, analysis, or communication.
Example: Student struggles to create a representation that would help them show a working example and an example where the system breaks.
Assessing risks: The student struggles with identifying the riskiest risks and/or prioritizing them. They may skip ahead, do unnecessary and unimportant tasks, or have impractical plans due to not properly addressing and prioritizing the risk.
Example: Student wasted time jumping ahead (they created multiple very detailed mockups) when they should&rsquo;ve been focusing on addressing the riskiest risk at hand, which was identifying their target audience.
Critical thinking and argumentation: The student struggles to construct well-reasoned arguments supported by evidence or lacks a conceptual understanding of the task at hand. They might find it difficult to identify conceptual differences (they are treating concepts as too similar when they actually have meaningful differences).
Example: Student isn&rsquo;t fully understanding what a regulation gap is and how to distinguish between the different types, and keeps taking the wrong changes to their prototypes because of that.</p>
</li>
<li>
<p>Metacognitive: The student struggles in areas of planning, help-seeking and collaboration, and reflection. This includes:
Forming feasible plans: The student struggles to develop structured, realistic, and actionable plans. This could include what their outcome should be and how to measure their outcome.
Example: Student overloaded themselves with tasks this sprint and while they got a lot of it done, it wasn&rsquo;t good work.
Planning effective iterations: The student struggles to create a deliverable that addresses the sprintâ€™s riskiest risk. The student may struggle due to problems with slicing (breaking larger problems down), prioritization, or understanding the problem.
Example: Student didn&rsquo;t incorporate the feedback the coach gave them last week, so their work this week was not effective.
Leveraging resources and seeking help: The skill of identifying and utilizing available materials, expertise from others, and information to enhance their learning and problem-solving.</p>
</li>
<li>
<p>Emotional: The student has regulation and dispositions toward self and learning that affects their motivation, cognition, and metacognition. This includes:
Fears and anxieties: The student may have a fear of imperfection which causes them to shy away from the work, and/or doesnâ€™t want to try things themselves.
Example: Student had a well-planned sprint to carry out, but got too caught up trying to perfectly design the solution rather than creating a first prototype.
Embracing challenges and learning: The student tries to brute force their way through a solution or runs away from it, rather than thinking about the strategy and approach.
Example: Student&rsquo;s system was not producing their optimal output, so they tried to overfit on one example rather than take a step back and observe what patterns are causing the system to fail.</p>
</li>
</ol>
<p>Based on the regulation gap (assessment), issue title, and context provided, categorize this case any categories (can be multiple) it applies to (Cognitive, Metacognitive, or Emotional).</p>
<p>To help you understand the notes better, here are some definitions of key terms:
Slice: A well-defined, manageable portion of a larger task or goal that can be completed within a short timeframe (typically a week), contributing to incremental progress.
Sprint: A time-boxed, iterative work cycle in agile project management, typically lasting 2 weeks, during which a team completes a set amount of work toward a project goal. Sprints emphasize rapid progress, continuous feedback, and adaptability.
Mysore: A structured learning and practice time where students work on their projects while a mentor provides feedback.
SIG: Special Interest Group meetings (SIG meetings) bring together undergraduate students, graduate students, and faculty working on different projects in the same research area. Each SIG is its own mini-studio initially led by a faculty member whose leadership fades over time as a graduate student SIG lead gains competencies in mentoring and becomes the leader of their own SIG. At the start of a sprint, teams share the outcome of their last sprint and present their current sprint plans for review. Halfway through a sprint, teams present their progress and SIG members help devise strategies for overcoming blockers.</p>
<p>Categorization should be based 80% on the â€œAssessmentâ€ part of the note, as this is the coachâ€™s perceived regulation gap. Be careful not to get confused-you should be categorizing their regulation gap, not the implications of the regulation gap. For instance, letâ€™s take a look at this CAP note:</p>
<p>&ldquo;Student: Improving and Scaling LLMs for Coaching
Improving and Scaling LLMs for Coaching | 2025-02-01
Items of Concern: Really hard to see how the conceptual examples would work / workout
Context: - Not sure why a lot of the categories are there.. not sure how they are actually relevant and what you learned from the analysis
Assessment: - Analysis not quite showing <em>why</em> the system suggested/showed what it showed?
Practice Suggestions: - [self-work] I couldn&rsquo;t tell from the output examples <em>why</em> the system was generating what it was generating. Can you think about a representation that would help you show, for an example category that you think are good and one that isn&rsquo;t, <em>why</em> the system got it right or wrong? Perhaps you can do this by showing component TF-IDF scores for each term, and also by showing some of the matching regulation gaps?&rdquo;
Looking at the note as a whole, one might identify it under the tier 2 Representing problem and solution spaces, Critical thinking and argumentation, Forming feasible plans, and Planning effective iterations. However, forming feasible plans and planning effective iterations were identified from the â€œpractice suggestionsâ€ section, meaning that this is an implication of the regulation gap rather than the regulation gap itself.</p>
<p>In this case, the correct tier 2 categorization of the regulation gap would be Representing problem and solution spaces and Critical thinking and argumentation based on the â€œassessmentâ€ section.</p>
<p>Provide your reasoning and then your final category choice in this format:
Reasoning: [your step-by-step reasoning]
Categories: [tier 1 category name] [tier 2 category name]â€</p>
<h2 id="appendix-f">Appendix F</h2>
<p>Codebook used to generate contextualized conversation facilitation questions <a href="#ref20">[20]</a>.
Generate conversation facilitation questions for the user. This scaffolding should include:</p>
<ul>
<li>Clarification/elaboration questions (seeking missing information)</li>
<li>Counter-arguments (expressing disagreement to prompt cognitive conflict)</li>
<li>Context/perspective-oriented questions (hypothetical scenarios, different viewpoints)</li>
</ul>
<h2 id="appendix-g">Appendix G</h2>
<p>Question scaffolds used for the second round of user testing.</p>
<p><strong>Questions novice might ask:</strong></p>
<ul>
<li>What was the issue you were encountering, was it recurring? How does it manifest in the tasks you did?</li>
<li>What were the steps you took to address it?</li>
<li>What made the process hard? Was there a moment where you did something that helped?</li>
<li>Were you able to utilize this framework/way of thinking in other instances where you experienced this regulation gap?</li>
<li>How were you able to break the cycle of constantly having this regulation gap?</li>
</ul>
<p><strong>Questions experienced peer might ask:</strong></p>
<ul>
<li>What is your issue and how is it manifesting in your work?</li>
<li>How can I use that framework to address my regulation gap (What analogies can you draw from my experience)?</li>
</ul>
<h2 id="appendix-h">Appendix H</h2>
<p>Question used in the before and after survey for the second round of user testing.</p>
<ul>
<li>What are the steps you will take to address your regulation gap this week?</li>
<li>What is the high-level framework for addressing this regulation gap?</li>
<li>What is your level of confidence in this plan/framework? (Scale of 1-10)</li>
<li>How comfortable are you with taking these steps? (Scale of 1-10)</li>
<li>What are your goals and project outcomes for this week?</li>
</ul>

      </div>
    </div>
  </div>
  
  <div id="modal-multi-modal-creative-ad-generation" class="modal-overlay">
    <div class="modal">
      <button class="modal-close" onclick="closeModal('multi-modal-creative-ad-generation')">&times;</button>
      <h2>Multi-modal Creative Ad Generation</h2>
      <p class="project-meta">May 20, 2024</p>
      <div class="modal-content">
        <p>The advertising industry&rsquo;s AI tools problem isn&rsquo;t about generation qualityâ€”it&rsquo;s about workflow integration. Most creative AI products today offer impressive individual capabilities but fail catastrophically when creators try to chain them together into actual work processes. A typical ad campaign might require juggling five to ten different AI tools for concept development, script writing, visual generation, voice synthesis, and editing, with creators constantly context-switching between platforms that don&rsquo;t understand each other. The result? AI tools that promise ten-fold efficiency but deliver ten-fold frustration instead.</p>
<p>During my internship building TikTok Symphony Assistant, I learned why the future of creative AI isn&rsquo;t about better models, but about better agent workflows that understand how creativity actually happens. The challenge isn&rsquo;t technicalâ€”it&rsquo;s cultural and systemic. Professional creators need to feel like they&rsquo;re directing the AI, not being replaced by it. This means building systems where AI handles the tedious execution while creators focus on strategy, brand voice, and creative direction. The goal is augmentation that preserves creative agency rather than automation that eliminates it.</p>
<p>The TikTok Symphony Assistant represents a significant step toward solving this integration problem, leveraging sophisticated agentic workflows to streamline creative processes from ideation through execution. Rather than offering another standalone tool, Symphony Assistant demonstrates how AI can enhance existing creative workflows by understanding the context and continuity that professional campaigns require. The platform is accessible at <a href="https://ads.tiktok.com/business/copilot/standalone">https://ads.tiktok.com/business/copilot/standalone</a> and serves as a practical case study for how agent-based systems can transform traditional advertising workflows.</p>
<p>Credits: TikTok Creative Team</p>
<h1 id="building-agentic-workflows">Building Agentic Workflows</h1>
<h2 id="from-llms-to-agents">From LLMs to Agents</h2>
<p>Leading AI companies now recognize that the transition from large language models to agent-based systems represents a fundamental shift in how we approach complex creative tasks. Traditional LLMs excel at generating individual pieces of content but struggle with the multi-step coordination that professional creative work demands. Agent systems solve this by introducing AI that can break down complex creative briefs, plan multi-step campaigns, and automatically route tasks to specialized tools while maintaining context throughout the entire process.</p>
<p>The key insight driving this evolution is that large language models deliver not just tools, but actual work results at specific stages of creative processes. Rather than asking creators to become prompt engineers, effective agent systems understand creative workflows and can execute specific roles within them. Application deployment becomes a matter of providing models with specific contexts and clear behavioral standards that align with professional creative workflows. The understanding and reasoning capabilities of LLMs can be applied to various creative scenarios, but success requires packaging general capabilities as abilities needed for specific positions or processes, overlaying domain expertise with general intelligence.</p>
<p>However, the promise of ten-fold efficiency gains remains largely unfulfilled because most AI tools still require creators to adapt their workflows rather than the AI adapting to how creative work actually happens. These workflows aren&rsquo;t merely a presentation of parallel capabilities running in isolation, but rather seamless integrations where creators can jump in at any step to provide feedback, make adjustments, or take creative control. The real challenge isn&rsquo;t building smarter AIâ€”it&rsquo;s building AI that preserves creative agency while eliminating the tedious, repetitive tasks that consume most creators&rsquo; time.</p>
<p>To understand what effective creative AI workflows look like in practice, let&rsquo;s examine Typefaceâ€”a billion-dollar startup that&rsquo;s closest to solving this integration problem. Their approach reveals both the promise and the remaining challenges in building AI that actually enhances creative work rather than replacing it.</p>
<p>The fundamental insight that drove our approach at TikTok was recognizing that successful AI applications require a workflow perspective that considers the entire creative process rather than optimizing individual tasks in isolation. Instead of asking &ldquo;How can AI help with script writing?&rdquo; we asked &ldquo;How can AI understand the complete journey from creative brief to final campaign delivery?&rdquo; This shift in thinking leads to very different product decisions. Rather than building another chatbot that generates scripts, we focused on building an intelligent coordinator that understands how scripts fit into broader campaigns, how they need to align with brand guidelines, and how they connect to visual concepts and distribution strategies.</p>
<p>The key questions that guided our Symphony Assistant development were practical and workflow-centered: What parts of daily creative workflows can be effectively enhanced by AI without disrupting the creative process? If AI systems need to process enterprise creative data, what value does this data provide at different stages of the creative business? Where does AI assistance sit most naturally in the creative value chain? In current creative operational models, which specific handoffs and transitions could be most effectively streamlined with intelligent automation? These questions helped us move beyond generic AI capabilities toward purpose-built creative intelligence.</p>
<h2 id="industry-consensus-task-specific-models-and-architecture-evolution">Industry Consensus: Task-Specific Models and Architecture Evolution</h2>
<p>Leading AI companies have converged on several key architectural approaches that directly informed our work on TikTok Symphony Assistant. Companies like Anthropic, A12Labs, and others now prioritize task-specific models and Mixture of Experts (MoE) architectures that represent a significant evolution from general-purpose language models. This shift reflects the recognition that creative workflows benefit more from specialized intelligence than from generalized capability.</p>
<p>Think of Mixture of Experts like a creative agency where different specialists handle different aspects of a campaign. Instead of one generalist AI doing everything poorly, you have separate &rsquo;experts&rsquo; for script writing, visual concepts, brand voice consistency, and audience targetingâ€”all coordinated by an intelligent router that knows which expert to consult for each task. This approach dramatically improves both the quality of individual outputs and the coherence of the overall campaign, while reducing the computational resources required compared to scaling a single massive model.</p>
<p>For creative applications like Symphony Assistant, MoE architecture enables the system to develop deep expertise in different aspects of content creation while maintaining overall campaign coherence. Rather than asking a general-purpose model to switch context constantly between writing scripts and understanding visual concepts, we route different creative challenges to models specifically trained for those domains.</p>
<p>Our implementation assigns input creative data to different expert networks based on the creative challenge type. Each expert returns specialized outputs optimized for their domainâ€”audience-appropriate script writing, brand-compliant visual concepts, or platform-specific content adaptations. The final output emerges as a coordinated combination that ensures both specialization and coherence throughout campaign development.</p>
<p>The key innovation lies in organizing expert networks around actual creative roles rather than technical divisions. For Symphony Assistant, we created experts that mirror how creative teams organize: audience psychology and messaging strategy, brand voice and tone consistency, platform-specific content requirements, and visual-text integration. This approach required training each expert on carefully curated datasets representing high-quality examples of their creative specialty, allowing deep domain expertise rather than shallow general competency.</p>
<h3 id="long-context-windows-enable-sophisticated-routing">Long Context Windows Enable Sophisticated Routing</h3>
<p>The development of longer context windows, exemplified by Gemini 1.5&rsquo;s one million token capacity, has fundamentally changed what&rsquo;s possible in creative AI applications. Extended context windows solve one of the most persistent problems in creative work: maintaining consistency and coherence across complex, multi-faceted campaigns. Jeff Dean&rsquo;s presentation at the Gemini 1.5 Hackathon at AGI House highlighted how these extended context windows enable more sophisticated in-context learning and more effective Mixture of Experts architectures, allowing AI systems to understand not just individual creative tasks but the broader strategic context that informs every creative decision.</p>
<p>For creative applications like Symphony Assistant, longer context windows mean the system can maintain awareness of entire creative briefs, comprehensive brand guidelines, detailed audience research, and complete campaign contexts throughout the generation process. This eliminates the frustrating experience of AI systems that &ldquo;forget&rdquo; crucial brand requirements or campaign objectives partway through content creation. Instead of forcing creators to constantly re-specify context, the system maintains a persistent understanding of the creative project&rsquo;s goals, constraints, and requirements across every interaction.</p>
<h3 id="practical-implementation-of-ai-routing-systems">Practical Implementation of AI Routing Systems</h3>
<p>Real-world implementations of intelligent routing concepts can be seen in platforms like Writesonic, which uses GPT Router for intelligent model selection during content generation. The GPT Router system demonstrates how smooth coordination of multiple specialized modelsâ€”including OpenAI&rsquo;s GPT series, Anthropic&rsquo;s Claude, Microsoft&rsquo;s Azure models, and image generation models like DALL-E and Stable Diffusionâ€”can dramatically speed up responses while ensuring reliability and consistency across different types of creative tasks.</p>
<p>This approach directly influenced our architecture decisions for Symphony Assistant, where different creative challenges benefit from different specialized models and different computational approaches. Script writing might route to a model optimized for conversational language and narrative structure, while visual concept development routes to models that understand visual composition and brand aesthetics. Platform optimization for TikTok versus LinkedIn requires entirely different understanding of audience behavior and content format requirements, so these tasks benefit from specialists trained on platform-specific data and success patterns.</p>
<h2 id="how-agents-can-help-creators-achieve-10x-efficiency">How Agents Can Help Creators Achieve 10x Efficiency</h2>
<p>The advertising and marketing industry represents one of the most promising applications for AI-driven workflow automation. Currently, creators typically juggle eight to ten different AI tools to produce a complete video campaign. This fragmented approach creates significant friction and context-switching overhead that negates many of the efficiency benefits AI should provide.</p>
<p>A typical video creation workflow demonstrates this challenge perfectly. Creators start with concept design in Midjourney, move to script and storyboard development in ChatGPT, generate visual assets using multiple image generation platforms, create video content through services like Runway or Pika, add dialogue and narration via Eleven Labs, incorporate sound effects and music from platforms like SUNO, enhance video quality through Topaz Video, and finally handle subtitles and editing in CapCut or similar tools. Each transition requires re-establishing context and manually ensuring consistency across platforms.</p>
<h2 id="improving-agent-user-experience">Improving Agent User Experience</h2>
<p>Effective creative AI systems must address four fundamental user experience challenges that consistently emerge in professional creative workflows.</p>
<p><strong>Personalized Memory &amp; Style Customization</strong> becomes essential because adjusting generation style through prompts before each generation is both time-consuming and unpredictable. Professional creators need comprehensive generation rules that ensure consistent output quality without repeated manual adjustments. Typeface&rsquo;s Brand Kit exemplifies this approach by allowing creators to establish persistent brand guidelines that inform every generation.</p>
<p><strong>Rewind &amp; Edit</strong> functionality addresses the reality that agent chaining accuracy decreases progressively through multi-step workflows. Human-in-the-loop processes allow creators to regenerate or fine-tune content at each step, ensuring final generation quality meets professional standards. Typeface&rsquo;s Projects feature demonstrates this principle by including Magic Prompt assistance and seamless regeneration capabilities.</p>
<p><strong>Choose from Variations</strong> recognizes that creators require options to make informed decisions about their content. Traditional generation processes force users to refresh entirely when dissatisfied with outputs, creating inefficiency. Providing multiple variations in single generations significantly improves user experience and creative flexibility.</p>
<p><strong>Workflows, Not Skills</strong> addresses the core problem that creators currently need five to ten disconnected AI capabilities to complete advertising video creation. Most tools require frequent platform switching and context re-establishment. Effective creative AI systems present all capabilities at appropriate workflow stages, enabling efficient tool invocation without breaking creative flow.</p>
<h2 id="typeface-blueprint-for-integrated-creative-ai">Typeface: Blueprint for Integrated Creative AI</h2>
<p>Typeface serves as the closest current example of effective creative AI workflow integration, having raised $165 million to reach a $1 billion valuation by solving the fundamental coordination problem in creative AI tools.</p>
<p>The platform demonstrates successful implementation of the four essential user experience principles. Their <strong>Brand Kit</strong> system allows creators to establish comprehensive brand guidelines including image styles, color palettes, and brand voice analysis. The <strong>Projects</strong> interface provides a Google Doc-like experience where creators can seamlessly invoke different AI capabilities without losing context. Their <strong>Template Library</strong> offers workflow-specific starting points that understand creative intent rather than just generating generic content.</p>
<p>Most significantly, Typeface&rsquo;s <strong>integration strategy</strong> eliminates cross-platform collaboration friction through native connections with Microsoft Dynamics 365, Salesforce Marketing, Google BigQuery, Google Workspace, and Microsoft Teams. This approach recognizes that effective creative AI must work within existing professional workflows rather than requiring creators to adopt entirely new platforms.</p>
<h2 id="summary">Summary</h2>
<p>The evolution of AI-powered creative tools reveals a clear trajectory from isolated capabilities toward integrated workflow solutions. Current marketing-focused products successfully integrate multiple stages of the creation process, providing workflow-like experiences that reduce cross-platform collaboration friction through strategic external integrations. However, the most successful implementations go beyond simply chaining various capabilities togetherâ€”they require thoughtful GUI process specifications that understand how creative work actually happens.</p>
<p>The key insight from analyzing platforms like Typeface, Symphony Assistant, and similar tools is that workflows must be designed around creative intent rather than technical capability. Effective creative AI systems understand the relationships between different creative decisions, maintain context across complex campaigns, and preserve creative agency while automating repetitive tasks. The future of creative AI lies not in building more powerful individual models, but in building more intelligent coordination systems that understand how different types of creative intelligence need to work together to produce professional-quality campaigns.</p>
<hr>
<h2 id="added-nov-11th-case-study-of-pomelli---progress-and-limitations-in-brand-kit-creation">Added Nov 11th: Case Study of Pomelli - Progress and Limitations in Brand Kit Creation</h2>
<p>

  <img src="/images/posts/copilot/pomelli-landing-page.png" alt="Pomelli Landing Page" loading="lazy">
 
<em>Pomelli&rsquo;s landing page showcases a visually appealing interface for generating on-brand content, positioned as &ldquo;Google Labs&rdquo; experimental project for business content creation.</em></p>
<p>Pomelli represents a good step forward in creating a brand kit framework for creative content generation, demonstrating several advances in user experience design for AI-powered marketing tools. However, the platform reveals key limitations that highlight ongoing challenges in the space: lacking context awareness and an overfocus on generalization at the expense of domain-specific optimization.</p>
<p>

  <img src="/images/posts/copilot/pomelli-campaign-interface.png" alt="Pomelli Campaign Interface" loading="lazy">
 
<em>The campaign creation interface emphasizes simplicity with a central prompt area and &ldquo;Suggest Ideas&rdquo; functionality, but the disclaimer &ldquo;Pomelli can make mistakes, so double-check it&rdquo; reveals underlying reliability concerns.</em></p>
<h3 id="strengths-brand-identity-framework">Strengths: Brand Identity Framework</h3>
<p>Pomelli&rsquo;s most significant contribution lies in its systematic approach to brand identity capture and application. The platform successfully implements several key principles we identified in our analysis of effective creative AI tools:</p>
<p><strong>Personalized Memory &amp; Style Customization</strong>: Like Typeface&rsquo;s Brand Kit, Pomelli allows users to establish comprehensive brand guidelines that persist across content generation sessions. This addresses the fundamental user frustration of having to re-specify brand requirements for each creative task.</p>
<p><strong>Workflow Integration</strong>: The platform demonstrates understanding that effective creative AI tools must integrate seamlessly into existing creative processes rather than requiring users to adapt their workflows to the tool&rsquo;s limitations.</p>
<p>

  <img src="/images/posts/copilot/pomelli-business-dna-setup.png" alt="Pomelli Business DNA Setup" loading="lazy">
 
<em>The &ldquo;Business DNA&rdquo; setup process captures comprehensive brand information including logos, fonts, color palettes, taglines, and brand values, demonstrating a systematic approach to brand identity integration.</em></p>
<p>Despite these strengths, Pomelli doesn&rsquo;t seem to support consistent generation of poster content. [Section to be continued]</p>
<p>The evolution from tools like Typeface through Pomelli to platforms like Symphony Assistant demonstrates the rapid maturation of creative AI, but also reveals that the most significant challenges lie not in generating content, but in generating the <em>right</em> content for specific contexts, audiences, and objectives.</p>

      </div>
    </div>
  </div>
  
</div>

<script>

document.addEventListener('DOMContentLoaded', function() {
  const slides = document.querySelectorAll('.theme-slide');
  const nav = document.getElementById('themeNav');
  let currentSlide = 0;

  
  slides.forEach((_, index) => {
    const button = document.createElement('button');
    button.onclick = () => goToSlide(index);
    nav.appendChild(button);
  });

  function updateNav() {
    const buttons = nav.querySelectorAll('button');
    buttons.forEach((button, index) => {
      button.classList.toggle('active', index === currentSlide);
    });
  }

  function goToSlide(index) {
    const slideWidth = slides[0].offsetWidth;
    document.getElementById('themeSlides').scrollLeft = slideWidth * index;
    currentSlide = index;
    updateNav();
  }

  
  updateNav();

  
  setInterval(() => {
    currentSlide = (currentSlide + 1) % slides.length;
    goToSlide(currentSlide);
  }, 5000);
});


function openModal(projectId) {
  const modal = document.getElementById(`modal-${projectId}`);
  if (modal) {
    modal.classList.add('active');
    document.body.style.overflow = 'hidden';
  }
}

function closeModal(projectId) {
  const modal = document.getElementById(`modal-${projectId}`);
  if (modal) {
    modal.classList.remove('active');
    document.body.style.overflow = 'auto';
  }
}


document.addEventListener('click', function(event) {
  if (event.target.classList.contains('modal-overlay')) {
    event.target.classList.remove('active');
    document.body.style.overflow = 'auto';
  }
});


document.addEventListener('keydown', function(event) {
  if (event.key === 'Escape') {
    const activeModal = document.querySelector('.modal-overlay.active');
    if (activeModal) {
      activeModal.classList.remove('active');
      document.body.style.overflow = 'auto';
    }
  }
});
</script>

    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://chenterry.com/">Terry Chen</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    
    
    <div class="archived-link">
        <span style="font-size: 0.85em; color: #888; margin-top: 8px; display: block;">
            To view archived content, <a href="/archived/" style="color: #666; text-decoration: underline;">click here</a>
        </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>



<div id="subscribe-system">
  
  <div id="subscription-modal" class="subscription-modal hidden">
    <div class="modal-overlay"></div>
    <div class="modal-content">
      <button id="modal-close" class="modal-close" title="Close">Ã—</button>
      <div class="modal-body">
        <h2>Stay close to the frontier</h2>
        <p>Get updates when I publish new insights about design, technology, and human interaction patterns.</p>
        <form id="subscription-form" class="centered-subscribe-form">
          <input type="email" id="subscription-email" placeholder="your@email.com" required>
          <button type="submit" class="submit-btn">Subscribe</button>
        </form>
        <div id="subscription-message" class="form-message"></div>
        <div class="modal-footer">
          <button id="modal-no-thanks" class="no-thanks-btn">No thanks</button>
        </div>
      </div>
    </div>
  </div>

  
  <div id="user-reminder" class="returning-reminder hidden">
    
    <div id="reminder-collapsed" class="reminder-collapsed clickable">
      <span class="reminder-text">Explore More Content</span>
      <div class="expand-btn">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="m18 15-6-6-6 6"/>
        </svg>
      </div>
    </div>
    
    
    <div id="reminder-expanded" class="reminder-expanded hidden">
      <div class="expanded-header">
        <h4>Stay close to the frontier</h4>
        <button id="reminder-close" class="reminder-close" title="Minimize">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M6 9l6 6 6-6"/>
          </svg>
        </button>
      </div>
      <p>Get updates when I publish new insights about design, technology, and human interaction patterns.</p>
      
      <div class="action-options">
        <a href="/posts/" class="action-item">
          <div class="action-icon">ðŸ“–</div>
          <span class="action-text">Read an article</span>
          <div class="action-arrow">â€º</div>
        </a>
        
        <a href="https://crowdlisten.com" target="_blank" class="action-item">
          <div class="action-icon">ðŸ‘¥</div>
          <span class="action-text">Insights with Crowdlisten</span>
          <div class="action-arrow">â€º</div>
        </a>
        
        <div class="action-item subscribe-item">
          <div class="action-icon">âœ‰ï¸</div>
          <span class="action-text">Subscribe for updates</span>
          <div class="action-arrow">â€º</div>
        </div>
      </div>
    </div>
  </div>
</div>

<script>
class SimplifiedSubscribe {
  constructor() {
    this.init();
  }

  init() {
    this.setupEventListeners();
    
    
    if (this.isNewVisitor()) {
      setTimeout(() => this.showCenteredModal(), 1000); 
    }
    
    
    this.showUserReminder();
  }

  
  isNewVisitor() {
    const hasVisited = localStorage.getItem('has-visited');
    if (!hasVisited && !this.hasSubscribed()) {
      localStorage.setItem('has-visited', 'true');
      return true;
    }
    return false;
  }

  
  hasSubscribed() {
    return localStorage.getItem('newsletter-subscribed') === 'true';
  }

  hasPermanentlyDismissed() {
    return localStorage.getItem('newsletter-never-show') === 'true';
  }

  
  showUserReminder() {
    document.getElementById('user-reminder').classList.remove('hidden');
  }

  hideUserReminder() {
    document.getElementById('user-reminder').classList.add('hidden');
  }

  
  expandReminder() {
    document.getElementById('reminder-collapsed').classList.add('hidden');
    document.getElementById('reminder-expanded').classList.remove('hidden');
  }

  collapseReminder() {
    const expanded = document.getElementById('reminder-expanded');
    const collapsed = document.getElementById('reminder-collapsed');
    
    
    expanded.style.animation = 'collapseVertical 0.2s ease-in forwards';
    
    setTimeout(() => {
      expanded.classList.add('hidden');
      expanded.style.animation = '';
      collapsed.classList.remove('hidden');
    }, 200);
  }

  
  showCenteredModal() {
    document.getElementById('subscription-modal').classList.remove('hidden');
  }

  
  hideCenteredModal() {
    document.getElementById('subscription-modal').classList.add('hidden');
  }

  
  async submitSubscription(email, source) {
    try {
      const pageUrl = window.location.href;
      
      const response = await fetch('https://script.google.com/macros/s/AKfycbxF7czbmo2JEeATzFoMnHIfGASoSTzogbjJcuQm30sjBXFqmDOqISWkIHBr64GdmURS/exec', {
        method: 'POST',
        mode: 'no-cors',
        headers: {
          'Content-Type': 'application/x-www-form-urlencoded',
        },
        body: new URLSearchParams({
          'email': email,
          'pageUrl': pageUrl,
          'source': source
        })
      });

      
      localStorage.setItem('newsletter-subscribed', 'true');
      
      
      setTimeout(() => {
        this.resetToActionOptions();
      }, 2000);

      return true;
    } catch (error) {
      console.error('Subscription error:', error);
      return false;
    }
  }

  
  resetToActionOptions() {
    this.hideCenteredModal();
    
    document.getElementById('subscription-email').value = '';
    document.getElementById('subscription-message').textContent = '';
    document.getElementById('subscription-message').classList.remove('success', 'error');
  }

  
  setupEventListeners() {
    
    document.getElementById('reminder-collapsed')?.addEventListener('click', () => {
      this.expandReminder();
    });

    document.getElementById('reminder-close')?.addEventListener('click', () => {
      this.collapseReminder();
    });

    
    document.querySelector('.subscribe-item')?.addEventListener('click', () => {
      this.showCenteredModal();
    });

    document.getElementById('user-form')?.addEventListener('submit', async (e) => {
      e.preventDefault();
      const email = document.getElementById('user-email').value;
      const messageEl = document.getElementById('user-message');
      const submitBtn = e.target.querySelector('button[type="submit"]');
      
      submitBtn.disabled = true;
      submitBtn.textContent = 'Subscribing...';
      
      const success = await this.submitSubscription(email, 'user-reminder');
      
      if (success) {
        messageEl.textContent = 'Thanks for subscribing!';
        messageEl.classList.add('success');
        setTimeout(() => this.hideUserReminder(), 2000);
      } else {
        messageEl.textContent = 'Something went wrong. Please try again.';
        messageEl.classList.add('error');
      }
      
      submitBtn.disabled = false;
      submitBtn.textContent = 'Subscribe';
    });


    
    document.getElementById('modal-close')?.addEventListener('click', () => {
      this.hideCenteredModal();
    });

    document.querySelector('.modal-overlay')?.addEventListener('click', () => {
      this.hideCenteredModal();
    });

    document.getElementById('subscription-form')?.addEventListener('submit', async (e) => {
      e.preventDefault();
      const email = document.getElementById('subscription-email').value;
      const messageEl = document.getElementById('subscription-message');
      const submitBtn = e.target.querySelector('button[type="submit"]');
      
      submitBtn.disabled = true;
      submitBtn.textContent = 'Subscribing...';
      
      const success = await this.submitSubscription(email, 'centered-modal');
      
      if (success) {
        messageEl.textContent = 'Thanks for subscribing!';
        messageEl.classList.add('success');
        setTimeout(() => this.hideCenteredModal(), 2000);
      } else {
        messageEl.textContent = 'Something went wrong. Please try again.';
        messageEl.classList.add('error');
      }
      
      submitBtn.disabled = false;
      submitBtn.textContent = 'Subscribe';
    });

    document.getElementById('modal-no-thanks')?.addEventListener('click', () => {
      this.hideCenteredModal();
    });
  }
}


document.addEventListener('DOMContentLoaded', () => {
  new SimplifiedSubscribe();
});
</script>

<style>
 
.hidden {
  display: none !important;
}

 
.subscription-modal {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 9999;
  display: flex;
  align-items: center;
  justify-content: center;
  animation: fadeIn 0.4s ease-out;
}

.modal-overlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: rgba(0,0,0,0.8);
  backdrop-filter: blur(4px);
}

.subscription-modal .modal-content {
  position: relative;
  background: var(--entry);
  border: 1px solid var(--border);
  border-radius: var(--radius);
  max-width: 680px;
  width: 95%;
  max-height: 90vh;
  overflow-y: auto;
  box-shadow: var(--shadow);
  animation: modalSlideIn 0.5s ease-out;
  font-family: inherit;
}

.modal-close {
  position: absolute;
  top: 20px;
  right: 20px;
  background: none;
  border: none;
  font-size: 24px;
  cursor: pointer;
  color: var(--secondary);
  padding: 8px;
  border-radius: var(--radius);
  transition: all 0.2s ease;
  opacity: 0.6;
}

.modal-close:hover {
  opacity: 1;
  background: var(--theme);
  color: var(--primary);
}

.modal-body {
  padding: calc(var(--gap) * 2);
  text-align: center;
}

.modal-body h2 {
  margin: 0 0 24px 0;
  font-size: 32px;
  font-weight: 700;
  color: var(--primary);
  font-family: inherit;
  line-height: 1.2;
}

.modal-body p {
  margin: 0 0 32px 0;
  color: var(--secondary);
  line-height: 1.6;
  font-size: 16px;
  font-family: inherit;
  max-width: 480px;
  margin-left: auto;
  margin-right: auto;
}

 
.centered-subscribe-form {
  display: flex;
  flex-direction: column;
  gap: 16px;
  margin-bottom: 24px;
  max-width: 400px;
  margin-left: auto;
  margin-right: auto;
}

.centered-subscribe-form input {
  padding: 16px 20px;
  border: 1px solid var(--border);
  border-radius: var(--radius);
  font-size: 16px;
  background: var(--theme);
  color: var(--primary);
  font-family: inherit;
  transition: border-color 0.2s ease;
  text-align: center;
}

.centered-subscribe-form input::placeholder {
  color: var(--secondary);
  opacity: 0.7;
}

.centered-subscribe-form input:focus {
  outline: none;
  border-color: var(--primary);
}

.centered-subscribe-form .submit-btn {
  background: var(--primary);
  color: var(--theme);
  border: none;
  padding: 16px 24px;
  border-radius: var(--radius);
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s ease;
  font-family: inherit;
}

.centered-subscribe-form .submit-btn:hover {
  opacity: 0.9;
  transform: translateY(-1px);
}

.centered-subscribe-form .submit-btn:disabled {
  opacity: 0.6;
  transform: none;
  cursor: not-allowed;
}

.modal-footer {
  text-align: center;
  margin-top: 16px;
}

 

 
.subscribe-form {
  display: flex;
  flex-direction: column;
  gap: 12px;
  margin-bottom: 16px;
  width: 100%;
}

.subscribe-form input {
  padding: 14px 16px;
  border: 1px solid var(--border);
  border-radius: var(--radius);
  font-size: 14px;
  background: var(--theme);
  color: var(--primary);
  font-family: inherit;
  transition: border-color 0.2s ease;
  box-sizing: border-box;
  width: 100%;
  text-align: left;
}

.subscribe-form input::placeholder {
  color: var(--secondary);
  opacity: 0.7;
}

.subscribe-form input:focus {
  outline: none;
  border-color: var(--primary);
}

.submit-btn {
  background: var(--primary);
  color: var(--theme);
  border: none;
  padding: 14px 16px;
  border-radius: var(--radius);
  font-size: 14px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s ease;
  font-family: inherit;
  box-sizing: border-box;
  width: 100%;
}

.no-thanks-btn {
  background: none;
  border: none;
  color: var(--secondary);
  font-size: 14px;
  cursor: pointer;
  text-decoration: underline;
  font-family: inherit;
  transition: color 0.2s ease;
  opacity: 0.8;
}

.no-thanks-btn:hover {
  color: var(--primary);
  opacity: 1;
}

 
.returning-reminder {
  position: fixed;
  bottom: 0;
  right: 0;
  z-index: 999;
  font-family: inherit;
}

 
.reminder-collapsed {
  background: var(--entry);
  border: 1px solid var(--border);
  border-radius: calc(var(--radius) * 2) calc(var(--radius) * 2) 0 0;
  padding: 12px 20px;
  box-shadow: var(--shadow);
  display: flex;
  align-items: center;
  justify-content: space-between;
  cursor: pointer;
  transition: all 0.3s ease;
  user-select: none;
  width: 360px;
  margin: 0;
}

.reminder-collapsed.clickable {
  cursor: pointer;
}

.reminder-collapsed:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 16px rgba(0,0,0,0.15);
}

.reminder-text {
  font-size: 14px;
  color: var(--secondary);
  font-weight: 500;
  white-space: nowrap;
}

.expand-btn {
  background: var(--theme);
  border: 1px solid var(--border);
  border-radius: 50%;
  padding: 6px;
  transition: all 0.2s ease;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--secondary);
  pointer-events: none;
}

.reminder-collapsed:hover .expand-btn {
  background: var(--border);
  color: var(--primary);
}

 
.reminder-expanded {
  background: var(--entry);
  border: 1px solid var(--border);
  border-radius: var(--radius) var(--radius) 0 0;
  padding: 24px;
  box-shadow: var(--shadow);
  width: 360px;
  animation: expandVertical 0.3s ease-out;
  margin: 0;
  box-sizing: border-box;
}

.expanded-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 16px;
}

.expanded-header h4 {
  margin: 0;
  font-size: 18px;
  font-weight: 600;
  color: var(--primary);
  font-family: inherit;
}

.reminder-close {
  background: none;
  border: none;
  cursor: pointer;
  color: var(--secondary);
  padding: 4px;
  transition: all 0.2s ease;
  border-radius: 4px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.reminder-close:hover {
  color: var(--primary);
  background: var(--theme);
}

.reminder-expanded p {
  margin: 0 0 20px 0;
  font-size: 14px;
  color: var(--secondary);
  line-height: 1.6;
  font-family: inherit;
}

 
.action-options {
  display: flex;
  flex-direction: column;
  gap: 8px;
  margin-bottom: 16px;
}

.action-item {
  display: flex;
  align-items: center;
  padding: 12px 16px;
  border: 1px solid var(--border);
  border-radius: var(--radius);
  background: var(--theme);
  color: var(--primary);
  text-decoration: none;
  transition: all 0.2s ease;
  cursor: pointer;
}

.action-item:hover {
  background: var(--code-bg);
  border-color: var(--primary);
}

.action-icon {
  font-size: 16px;
  margin-right: 12px;
  min-width: 20px;
}

.action-text {
  flex: 1;
  font-size: 14px;
  font-weight: 500;
}

.action-arrow {
  font-size: 18px;
  color: var(--secondary);
  margin-left: 8px;
}

.action-item:hover .action-arrow {
  color: var(--primary);
}

.reminder-actions {
  text-align: center;
  margin-top: 16px;
}

 
.form-message {
  margin-top: 16px;
  padding: 12px 16px;
  border-radius: var(--radius);
  font-size: 14px;
  text-align: center;
  font-family: inherit;
}

.form-message.success {
  background: var(--code-bg);
  color: var(--primary);
  border: 1px solid var(--border);
}

.form-message.error {
  background: var(--code-bg);
  color: var(--primary);
  border: 1px solid var(--border);
}

 
@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

@keyframes modalSlideIn {
  from {
    transform: scale(0.95) translateY(-10px);
    opacity: 0;
  }
  to {
    transform: scale(1) translateY(0);
    opacity: 1;
  }
}

@keyframes expandVertical {
  from {
    transform: scaleY(0);
    transform-origin: bottom;
    opacity: 0;
  }
  to {
    transform: scaleY(1);
    transform-origin: bottom;
    opacity: 1;
  }
}

@keyframes collapseVertical {
  from {
    transform: scaleY(1);
    transform-origin: bottom;
    opacity: 1;
  }
  to {
    transform: scaleY(0);
    transform-origin: bottom;
    opacity: 0;
  }
}

 
@media (max-width: 768px) {
  .new-user-modal .modal-content {
    margin: 20px;
    width: calc(100% - 40px);
  }
  
  .modal-body {
    padding: var(--gap);
  }
  
  .modal-body h2 {
    font-size: 28px;
  }
  
  .returning-reminder {
    right: 0;
    bottom: 0;
  }
  
  .reminder-expanded {
    width: 100vw;
    max-width: 360px;
    padding: 16px;
  }
  
  .reminder-collapsed {
    padding: 10px 16px;
    width: 100vw;
    max-width: 360px;
  }
  
  .reminder-text {
    font-size: 13px;
  }
}

 
:root {
  --main-width: 720px;
  --gap: 24px;
  --radius: 8px;
  --shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}
</style>

<script>
    
    window.addEventListener('error', function(e) {
        console.error('Page error:', e.error);
        console.error('Error details:', {
            message: e.message,
            filename: e.filename,
            lineno: e.lineno,
            colno: e.colno
        });
    });

    
    if (history.scrollRestoration) {
        history.scrollRestoration = 'manual';
    }
    
    
    window.addEventListener('load', function() {
        window.scrollTo(0, 0);
    });
    
    
    window.addEventListener('pageshow', function(event) {
        if (event.persisted) {
            window.scrollTo(0, 0);
        }
    });

    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script> </body>

</html>
