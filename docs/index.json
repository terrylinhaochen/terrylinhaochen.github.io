[{"content":"Extracting actionable insights from large-scale distributed online conversations. Engineered content optimization features that generate variations and simulate engagement through agentic crowds.\n","permalink":"https://chenterry.com/about/projects/crowdlistening/","summary":"\u003cp\u003eExtracting actionable insights from large-scale distributed online conversations. Engineered content optimization features that generate variations and simulate engagement through agentic crowds.\u003c/p\u003e","title":"CrowdListening | Evanston, Illinois"},{"content":"Agent AI for security.\n","permalink":"https://chenterry.com/about/experience/microsoft/","summary":"\u003cp\u003eAgent AI for security.\u003c/p\u003e","title":"Microsoft | Redmond, Washington"},{"content":"Built LLM system that performs real-time conversation processing and detects conceptual misconceptions. Facilitated Socratic learning through synchronous voice interactions.\n","permalink":"https://chenterry.com/about/research/c3lab/","summary":"\u003cp\u003eBuilt LLM system that performs real-time conversation processing and detects conceptual misconceptions. Facilitated Socratic learning through synchronous voice interactions.\u003c/p\u003e","title":"Northwestern C3 Lab | Evanston, Illinois"},{"content":"Developed LLM system that identifies and connects students with similar regulation gaps, achieving 88% precision and 89% recall in categorization.\n","permalink":"https://chenterry.com/about/research/deltalab/","summary":"\u003cp\u003eDeveloped LLM system that identifies and connects students with similar regulation gaps, achieving 88% precision and 89% recall in categorization.\u003c/p\u003e","title":"Northwestern Delta Lab | Evanston, Illinois"},{"content":"Making lifelong learning engaging and accessible. Research agent ai and generation workflows. Designed and implemented an agentic LLM learning companion, working on search, recommendation, etc.\n","permalink":"https://chenterry.com/about/experience/ouraca/","summary":"\u003cp\u003eMaking lifelong learning engaging and accessible. Research agent ai and generation workflows. Designed and implemented an agentic LLM learning companion, working on search, recommendation, etc.\u003c/p\u003e","title":"Ouraca | Palo Alto, California"},{"content":"Leveraging billion parameter data to provide actionable trends and insights for key agencies. Designed keyword clustering methods for insight extraction, worked on creative video ad creation.\n","permalink":"https://chenterry.com/about/experience/tiktok/","summary":"\u003cp\u003eLeveraging billion parameter data to provide actionable trends and insights for key agencies. Designed keyword clustering methods for insight extraction, worked on creative video ad creation.\u003c/p\u003e","title":"TikTok | San Jose, California"},{"content":"Summary OpenAI\u0026rsquo;s rapid product cadence isn\u0026rsquo;t just releasing tools—it\u0026rsquo;s consolidating power. By integrating models, infrastructure, and interfaces into a single AI operating system, the company is reshaping where startups can compete and how value accrues across the AI stack.\nOpenAI isn\u0026rsquo;t just launching new products; it\u0026rsquo;s redefining where startups can safely operate. In less than three years, it has evolved from a research lab into a full-stack AI platform whose reach now spans infrastructure, models, applications, and compliance. Each release expands its gravitational field, redrawing the boundaries of opportunity for founders and investors. Understanding OpenAI\u0026rsquo;s strategic expansion is therefore not about tracking a single company; it\u0026rsquo;s about mapping the blast radius of a platform that is systematically consolidating multiple layers of the AI value chain.\nAnalysis of OpenAI\u0026rsquo;s recent product releases reveals an ambitious three-pillar strategy that goes far beyond language models. The company is positioning itself as the operating system for AI-powered work, combining a unified assistant surface with heavy multimodal research and development, all supported by hyperscale infrastructure. This isn\u0026rsquo;t just about building better models—it\u0026rsquo;s about creating an integrated platform that captures value across the entire AI stack.\nThe Operating System for Work and Life Based on their product release patterns and hiring focus, OpenAI appears to be executing a clear progression from ChatGPT as a chat interface toward a comprehensive AI operating system. This evolution involves three critical components: positioning ChatGPT as the primary interface for AI interactions, developing advanced reasoning capabilities with safety guardrails, and building massive infrastructure scale to support global deployment. Their enterprise strategy emphasizes data sovereignty and compliance, particularly in regulated industries where local data residency becomes a competitive necessity.\nTogether, these moves reveal that OpenAI is no longer competing at the level of models, but at the level of workflows. ChatGPT, Atlas, and related products form a single interface through which users think, search, and act. The company’s differentiation now lies less in model quality than in coordination—how seamlessly its products orchestrate tasks across text, voice, and visual contexts. For startups, that means the competitive frontier has shifted: value now accrues not to who builds the smartest model, but to who controls the user’s entry point into intelligent work.\nThe Architecture of Dominance OpenAI\u0026rsquo;s approach mirrors the logic of an operating system rather than an application suite. The assistant surface is the user shell; multimodal models are the compute kernel; and enterprise infrastructure provides the permissions, policies, and data flows that make the system safe and scalable. This architectural cohesion is what gives OpenAI its durability. Each new feature—Agents, Memory, or Company Knowledge—plugs into the same orchestration layer, reinforcing a feedback loop between capability and distribution that becomes increasingly difficult for smaller players to break.\nThe Strategic Timeline: From Foundation to Platform OpenAI\u0026rsquo;s roadmap reveals a methodical approach to market expansion across five distinct layers: Product, Research, Infrastructure, Enterprise, and Human. Each layer follows a deliberate progression from foundational capabilities to advanced platform features.\nIn the immediate term, OpenAI is consolidating its product offering around Atlas browser integration and search capabilities, while simultaneously advancing deliberative alignment and o-series reasoning models. The infrastructure focus remains on online storage and data movement, supporting enterprise residency requirements across Japan, India, Singapore, and South Korea. For human-centered features, Study Mode and parental controls establish OpenAI\u0026rsquo;s presence in regulated environments.\nThe next phase introduces agentic workflows and memory-aware user experiences, supported by robustness research against attacks and grounded reasoning capabilities. Infrastructure scaling continues with real-time streaming and multimodal training capabilities, while enterprise features expand to include pricing platforms and administrative controls. Well-being guardrails and expert councils for youth and health contexts demonstrate OpenAI\u0026rsquo;s commitment to responsible deployment.\nThe longer-term vision encompasses OS-level assistant integration and vertical solution playbooks, supported by generalizable agent capabilities and world-modeling research. Stargate build-outs will provide exascale orchestration capabilities, while industry-specific playbooks and partner ecosystems will address specialized market needs. Personalized pedagogy and trust benchmarks will complete the human-centered AI platform.\nCompetitive Positioning: Understanding the Battlefield If the timeline shows how OpenAI expands, the competitive landscape shows what resistance it meets along the way. OpenAI\u0026rsquo;s footprint now spans nearly every tier of the AI stack—from the chips that power training to the browsers where users interact. What\u0026rsquo;s notable is not just the breadth of competition, but the pace at which OpenAI enters new domains once they become strategically adjacent to its assistant experience.\nThe multimodal battleground is particularly intense, with OpenAI\u0026rsquo;s video capabilities through Sora competing against established players like Google (Veo), Luma, Kling, Hailuo, ElevenLabs, and Cartesia. Voice and text-to-speech represent another competitive front against Google, Meta, and Bytedance. At the application layer, ChatGPT, Atlas, Agents, and Study Mode compete against Anthropic, Google, Perplexity, Cohere, and various other companies with AI product offerings.\nThe Supply Chain Reality: Dependencies and Leverage Points Understanding OpenAI\u0026rsquo;s industry position requires examining the supply chain dynamics that constrain and enable its growth. At the supply level, OpenAI depends heavily on semiconductor providers including NVIDIA, AMD, Intel, TSMC, and Samsung Foundry for the computational infrastructure that powers its models. While primarily NVIDIA-dependent, potential alternative accelerator systems from Google TPU, AWS Trainium, Microsoft, and Cerebras represent possible diversification options, though OpenAI\u0026rsquo;s actual usage of these alternatives remains limited. Cloud and datacenter infrastructure from Azure, AWS, Google Cloud,etc support their deployment requirements.\nThe channel relationships reveal OpenAI\u0026rsquo;s distribution strategy. Enterprise platforms including Salesforce, ServiceNow, and Workday provide pathways to business customers, while collaboration and productivity tools like Slack, Teams, Google Workspace, Microsoft 365, Notion, and Figma represent potential integration opportunities. Browser surface integration through Atlas creates a direct consumer touchpoint, competing with traditional web search and productivity workflows.\nComplement relationships highlight critical dependencies for OpenAI\u0026rsquo;s platform strategy. Data and licensing partnerships with AP, Financial Times, Le Monde, Reddit, and Stack Overflow provide the content foundation for training and response generation. Safety and evaluation frameworks from Scale AI, ARC, and Metaprompt help ensure responsible deployment. Identity and compliance solutions from Okta, Auth0, Microsoft Entra, and Stripe Identity handle the enterprise security requirements that make large-scale deployment possible.\nThese dependencies also signal where OpenAI directs capital and hiring—particularly in online storage, data movement, and distributed systems that underpin its enterprise ambitions. OpenAI\u0026rsquo;s hiring patterns and organizational focus suggest deliberate prioritization of infrastructure scaling and platform consolidation. The company appears to be emphasizing online storage and data movement capabilities, distributed systems for enterprise deployment, and agent platforms that enable browser-native workflows. These focus areas, evidenced through job postings and team expansions, align directly with the platform strategy of becoming the coordination layer for AI-powered work.\nCritical Milestones: When the Platform Consolidates Based on their announced roadmap and product release patterns, several key inflection points will likely determine OpenAI\u0026rsquo;s platform success. The 2023 launch of ChatGPT Plus established consumer monetization and early plugin ecosystem momentum, creating the foundation for platform expansion. The 2024 releases of GPT-4o, Search, and Canvas created a unified assistant surface with real-time multimodal capabilities, positioning OpenAI to capture more complex user workflows.\nLooking ahead, the Summer 2025 rollout of Study Mode, Atlas, and Agent Mode appears designed to move OpenAI beyond conversation into browser-native agentic workflows, changing how users interact with AI systems.\nThese milestones matter because they represent platform lock-in moments. Once enterprises commit to data residency infrastructure and users adopt agentic workflows, switching costs increase dramatically. The browser-native experiences create new interaction patterns that become increasingly difficult for competitors to displace.\nGlobal Scaling: The Infrastructure Imperative OpenAI\u0026rsquo;s global hiring footprint reveals the scale of its platform ambitions. With 86.33% of hiring concentrated in the United States across San Francisco, Remote-US, NYC, Seattle, and Washington DC, OpenAI maintains strong coordination around its core product and research development. However, the international distribution across Japan (3.49%), Ireland (2.68%), Singapore (2.14%), India (1.61%), Australia (1.61%), South Korea (1.34%), Germany (0.54%), and France (0.27%) indicates strategic positioning for regional expansion and compliance requirements.\nThis geographic distribution suggests a strategic approach to international expansion, with local presence in key regulatory jurisdictions potentially supporting enterprise adoption in international markets. The concentration in specific cities—Tokyo, Dublin, Singapore, Delhi, Sydney, Seoul, Munich, and Paris—indicates a hub-based approach to regional scaling rather than distributed expansion.\nOpenAI’s geographic distribution underscores how infrastructure strategy and regulatory positioning converge. Concentration in U.S. hubs allows for tight coordination, while targeted expansion into Asia and Europe aligns with data residency requirements and enterprise trust. The result is a hub-and-spoke model of compliance—regional enough to meet local regulation, centralized enough to maintain product velocity. For startups, this creates both clarity and constraint: regions where OpenAI lacks presence may offer short-term white space, but the window narrows quickly once compliance infrastructure lands.\nStrategic Implications for AI Startups OpenAI\u0026rsquo;s trajectory signals a decisive end to the era of thin AI wrappers. The company\u0026rsquo;s integration of model, interface, and infrastructure has collapsed what used to be a multi-layer market into a vertically unified platform. For founders, defensibility now depends on depth—specialization, proprietary data, or domain-specific regulation—rather than breadth. Some will thrive as complements, building tools that extend the platform\u0026rsquo;s reach into verticals OpenAI can\u0026rsquo;t or won\u0026rsquo;t prioritize. Others will seek independence through novel architectures or community-owned ecosystems.\nOpenAI\u0026rsquo;s expansion marks the normalization of AI as infrastructure. By controlling how people write, search, and learn, it\u0026rsquo;s setting behavioral defaults that future builders will either align with or challenge. The blast radius isn\u0026rsquo;t destruction—it\u0026rsquo;s redefinition. The companies that survive will be those that understand where OpenAI\u0026rsquo;s platform stops and differentiated value begins.\nAppendix: Analytical Methodology and Limitations This analysis synthesizes over one hundred OpenAI product releases and four hundred job postings using automated web crawling and strategic content synthesis. It favors breadth over depth—identifying cross-layer patterns in OpenAI\u0026rsquo;s expansion rather than case-level detail. While this reveals the company\u0026rsquo;s overarching trajectory, it should be complemented with vertical-specific research for actionable insight.\nThe Missing Layer of Collective Semantics Current assistants compress vast human discourse into atomic answers, overlooking the higher-order structures of collective meaning—clusters of opinion, degrees of conviction, and the evolution of public narratives. The Reddit partnership brings social data into OpenAI\u0026rsquo;s responses but stops short of indexing group intelligence. This gap defines a white space: translating collective semantics into measurable indicators for decision-making in investing, branding, or governance. Whoever builds that interface between crowd cognition and AI reasoning will occupy the next layer beyond OpenAI\u0026rsquo;s platform.\nCredits Thanks to Livia and Richard for keeping me company amid rambling, ChatGPT for helping with the visualizations, and Gemini for the synthesis.\n","permalink":"https://chenterry.com/posts/openai-blast-radius/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI\u0026rsquo;s rapid product cadence isn\u0026rsquo;t just releasing tools—it\u0026rsquo;s consolidating power.\u003c/strong\u003e By integrating models, infrastructure, and interfaces into a single AI operating system, the company is reshaping where startups can compete and how value accrues across the AI stack.\u003c/p\u003e\n\u003cp\u003eOpenAI isn\u0026rsquo;t just launching new products; it\u0026rsquo;s redefining where startups can safely operate. In less than three years, it has evolved from a research lab into a full-stack AI platform whose reach now spans infrastructure, models, applications, and compliance. Each release expands its gravitational field, redrawing the boundaries of opportunity for founders and investors. Understanding OpenAI\u0026rsquo;s strategic expansion is therefore not about tracking a single company; it\u0026rsquo;s about mapping the blast radius of a platform that is systematically consolidating multiple layers of the AI value chain.\u003c/p\u003e","title":"OpenAI's Blast Radius"},{"content":"From Indexing to Understanding Intent in Discovery Systems Discovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoning—delivering fast, contextually relevant answers while offering deeper AI-powered synthesis when users explore unfamiliar or complex topics.\nWhy Traditional Search Falls Short Traditional keyword search works best when the user knows exactly what to ask. However, most discovery today begins with uncertainty: a half-remembered quote, a general theme, or a desire for inspiration. Users often don\u0026rsquo;t know what they\u0026rsquo;re looking for until they see it. The problem is both cognitive and technical—users\u0026rsquo; mental models evolve as they explore. Modern discovery tools must anticipate that evolution, interpret vague intent, and surface meaningfully related ideas rather than exact word matches.\nFrom Indexing to Intent-Based Query Completion The transition from indexing to intent-driven discovery mirrors a broader movement from databases that store information to systems that reason about context. In practice, this takes two complementary forms: fast responses and deep responses.\nFast responses are built for immediate clarity. They combine multiple recall routes—lexical, semantic, behavioral, and social—to provide accurate answers quickly. This approach excels in known-item retrieval and factual questions. For instance, Fable\u0026rsquo;s new search experience goes beyond simple keyword matching to understand emotional and stylistic dimensions. When a reader searches \u0026ldquo;books that feel like autumn evenings,\u0026rdquo; the system infers ambience and tone rather than relying solely on metadata tags. This design transforms search from literal matching to contextual association.\nFable\u0026rsquo;s enhanced search showing predictive completion for incomplete queries like \u0026ldquo;heaven and earth g\u0026rdquo;\nThe system demonstrates superior intent understanding by providing relevant suggestions before users complete their queries. The \u0026ldquo;After\u0026rdquo; version shows how modern search anticipates user needs, surfacing \u0026ldquo;The Heaven \u0026amp; Earth Grocery Store\u0026rdquo; and related titles when users type partial queries.\nFable\u0026rsquo;s improved relevance matching for author searches like \u0026ldquo;james\u0026rdquo;\nSimilarly, when searching for \u0026ldquo;james,\u0026rdquo; the enhanced system prioritizes contextually relevant results like \u0026ldquo;James (Pulitzer Prize Winner)\u0026rdquo; rather than generic matches, demonstrating how intent-aware systems understand query context.\nDeep responses, in contrast, serve users exploring open-ended questions. They rely on large language models to synthesize information, drawing connections across content sources and explaining the reasoning behind conclusions. Red (Xiaohongshu) offers a compelling example through its AskNow feature, where AI-generated insights are grounded on verified community posts, maintaining both authenticity and transparency. Similarly, Reddit Answers (currently in beta) uses real user discussions as evidence for its responses, ensuring that generated insights remain rooted in human context rather than abstract data patterns.\nRed\u0026rsquo;s AskNow feature providing AI-generated company analysis with structured insights on market competition, privacy policies, and legal disputes\nThe AskNow interface demonstrates sophisticated content synthesis, taking user queries about companies like AppLovin and generating comprehensive analyses that include market positioning (competition with Meta, Google), regulatory challenges (privacy policies like IDFA), and risk factors (legal disputes affecting stock prices)—all grounded in community discussions rather than abstract data.\nThis dual architecture—fast for confidence, deep for curiosity—captures how systems can dynamically adapt to user intent during a single session.\nThe Two-Loop Discovery Engine Intent-aware systems operate through a continuous feedback structure combining retrieval and reasoning. The retrieval loop aggregates relevant results using hybrid signals such as keyword relevance, vector similarity, and recency. The reasoning loop interprets ongoing behavior—clicks, refinements, and skips—to update an Intent State that guides subsequent outputs. Each iteration yields one of three outcomes: a direct answer, a curated content set, or an AI-generated synthesis.\nMapping Intent and Context Different user goals demand different discovery experiences. The table below illustrates how systems can tailor responses according to intent and specificity.\nUser Goal Specificity Ideal Response Interface Type Example Retrieve Exact Concise factual snippet with citation Inline summary \u0026ldquo;Release date of Dune Part Two.\u0026rdquo; Learn Fuzzy Conceptual overview with examples and follow-ups Accordion-style cards \u0026ldquo;How do neural embeddings improve search?\u0026rdquo; Decide Mid Structured comparison with trade-offs Comparison grid with rationale notes \u0026ldquo;Which AI search tools balance transparency and cost?\u0026rdquo; Explore Open Serendipitous content spanning adjacent ideas Visual knowledge map or gallery \u0026ldquo;Books that inspire design thinking.\u0026rdquo; Design Principles for Intent-Aware Search An effective discovery system prioritizes clarity, adaptability, and transparency. Each result should show why it was retrieved—through cues such as \u0026ldquo;popular in similar sessions\u0026rdquo; or \u0026ldquo;matches your theme and tone.\u0026rdquo; Systems should learn within each session, refining their understanding as the user interacts. They must balance novelty with relevance, maintaining an exploratory rhythm without overwhelming the user. Transparency matters most: showing data sources, confidence levels, and offering manual control over personalization builds long-term trust. Lastly, discovery should feel seamless across formats—books, posts, videos, and conversations—as part of a single cognitive journey.\nWhen the Corpus Falls Short An intent-aware system recognizes when no direct answer exists. Instead of ending in failure, it synthesizes a grounded response that discloses its sources and confidence level. These generated insights transform information gaps into moments of learning—highlighting missing viewpoints, summarizing scattered data, or surfacing counterexamples that broaden understanding.\nMeasuring Success Through Comprehension Success in intent-based discovery extends beyond engagement metrics. Systems should measure Time to Insight (TTI) to understand how quickly users reach clarity, Exploration Depth to gauge how broadly users traverse concepts, and Trust Indicators reflecting how often users expand citations or rely on AI explanations. Another key metric, Intent Alignment, measures how closely the system\u0026rsquo;s inferred goal matches the user\u0026rsquo;s evolving intent. These metrics move focus from clicks to comprehension, rewarding designs that help people think more efficiently.\nWhy Intent Awareness Matters Search and recommendation are converging into a unified discipline focused on intent understanding. Platforms like Fable, Red, and Reddit show how discovery is shifting from indexing content to interpreting context. The future of search will belong to systems that bridge speed and synthesis—those that recognize a user\u0026rsquo;s goal, respond in real time, and expand understanding with each interaction.\n","permalink":"https://chenterry.com/posts/intent-driven-discovery/","summary":"\u003ch1 id=\"from-indexing-to-understanding-intent-in-discovery-systems\"\u003eFrom Indexing to Understanding Intent in Discovery Systems\u003c/h1\u003e\n\u003cp\u003eDiscovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoning—delivering fast, contextually relevant answers while offering deeper AI-powered synthesis when users explore unfamiliar or complex topics.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-traditional-search-falls-short\"\u003eWhy Traditional Search Falls Short\u003c/h2\u003e\n\u003cp\u003eTraditional keyword search works best when the user knows exactly what to ask. However, most discovery today begins with uncertainty: a half-remembered quote, a general theme, or a desire for inspiration. Users often don\u0026rsquo;t know what they\u0026rsquo;re looking for until they see it. The problem is both cognitive and technical—users\u0026rsquo; mental models evolve as they explore. Modern discovery tools must anticipate that evolution, interpret vague intent, and surface meaningfully related ideas rather than exact word matches.\u003c/p\u003e","title":"From Indexing to Understanding Intent in Discovery Systems"},{"content":"When Knowledge Becomes Fluid For most of history, knowledge has been bound — fixed in books, trapped in formats, and constrained by how it could be consumed. You could read a page, listen to a lecture, or watch a documentary, but each existed in isolation. With generative models, that boundary begins to dissolve. Knowledge itself becomes fluid — able to reshape, reframe, and re-express itself across contexts and mediums.\nDistilling and communicating ideas is now easier and more capable than ever. What once required manual summarization and editing can now be synthesized in real time. A single idea can expand into many forms: a summary for clarity, a dialogue for perspective, a visual story for intuition. Instead of treating knowledge as static, we can begin to design it as something alive — capable of adapting to how, when, and where we learn.\nScaling Knowledge Most modern learning apps do a good job of compressing information, but they still rely on manual curation. Platforms like Blinkist summarize a few thousand books, turning complex ideas into short, digestible snippets. Useful, but limited. With large language models, we can go far beyond that. We can synthesize millions of books, using public-domain sources and model-based abstraction to avoid infringement while expanding reach.\nImagine a content pipeline that can transform any text into multiple formats — summaries, debates, timelines, or storyboards — automatically. This isn\u0026rsquo;t just about quantity; it\u0026rsquo;s about accessibility. Anyone could instantly learn from any book ever written, reformatted into whatever experience fits their context best. The scale of knowledge expands not by adding editors, but by giving knowledge itself the ability to self-express.\nFluid Learning Learning is deeply contextual. Reading a dense essay might work at a desk, but not while commuting or cooking. The same knowledge can take different forms depending on where we are and what we\u0026rsquo;re doing. Generative models make this flexibility possible. A book chapter can transform into a short podcast for a train ride, a video summary during a break, or an interactive chat when you want to explore ideas more deeply.\nWhen the system understands context — the time of day, the device, your focus level, or even your past learning behavior — it can dynamically serve the right modality for the moment. Instead of forcing you to adapt to the medium, the medium adapts to you. The result is a more continuous, natural learning flow — where engagement and retention rise because the format matches your state of mind.\nConnecting Ideas The next step is not just summarizing knowledge, but connecting it. A truly generative library doesn\u0026rsquo;t only compress information — it discovers relationships. It can find the echoes that cut across books, fields, and centuries. It might reveal that The Art of War and Measure What Matters both explore alignment under uncertainty — one in ancient warfare, the other in modern management.\nThis kind of synthesis transforms learning from retrieval to insight. Instead of static archives, we begin to build creative constellations of ideas. Knowledge becomes something that grows through its connections, not just its content. We move from consuming isolated summaries to experiencing patterns of thought that evolve as we explore them.\nInteractive Understanding Summaries can tell you what to think, but they rarely teach you how to think. A new generation of learning systems can make that process interactive. Rather than passively reading, you engage with structured knowledge cards — each guiding you through definition, context, application, reflection, and connection. You can ask questions, compare ideas, and trace how concepts evolve across books.\nOver time, these systems learn from your curiosity. If you\u0026rsquo;ve been exploring Stoicism, they might suggest how its ideas overlap with Taoism or cognitive psychology. The experience becomes conversational — a collaboration between human intuition and machine reasoning. Learning feels less like consumption and more like construction, where understanding is built, not delivered.\nA Living Medium Generative AI changes what a book even is. It turns knowledge into a living medium — fluid, adaptive, and co-creative. Instead of locking ideas into fixed containers, we can let them flow. A book becomes a conversation, a lecture becomes an experience, and learning becomes something that moves with us.\nWhen knowledge becomes fluid, understanding no longer depends on how much we can read or memorize. It depends on how well our tools can help ideas move — across contexts, across formats, and ultimately, across minds.\n","permalink":"https://chenterry.com/posts/fluid-knowledge-ai-transforms-learning/","summary":"\u003ch1 id=\"when-knowledge-becomes-fluid\"\u003eWhen Knowledge Becomes Fluid\u003c/h1\u003e\n\u003cp\u003eFor most of history, knowledge has been bound — fixed in books, trapped in formats, and constrained by how it could be consumed. You could read a page, listen to a lecture, or watch a documentary, but each existed in isolation. With generative models, that boundary begins to dissolve. Knowledge itself becomes fluid — able to reshape, reframe, and re-express itself across contexts and mediums.\u003c/p\u003e\n\u003cp\u003eDistilling and communicating ideas is now easier and more capable than ever. What once required manual summarization and editing can now be synthesized in real time. A single idea can expand into many forms: a summary for clarity, a dialogue for perspective, a visual story for intuition. Instead of treating knowledge as static, we can begin to design it as something alive — capable of adapting to how, when, and where we learn.\u003c/p\u003e","title":"When Knowledge Becomes Fluid: How AI Transforms Learning"},{"content":"Understanding Google\u0026rsquo;s Product Ecosystem Google has evolved from a simple search engine into a comprehensive ecosystem of interconnected products and services that power much of the modern internet experience. This analysis examines Google\u0026rsquo;s core products and their strategic evolution into AI-powered services that define contemporary technology investment opportunities. From traditional consumer applications to cutting-edge AI experiments, Google\u0026rsquo;s product portfolio demonstrates a coherent strategy of data collection, user engagement, and technological advancement.\nCore Product Portfolio Google\u0026rsquo;s product ecosystem spans multiple categories, each serving different user needs while contributing to the company\u0026rsquo;s overall data and advertising strategy. The core products include consumer staples like Search, Gmail, Chrome, and YouTube, productivity tools such as Google Docs, Google Calendar, and Google Drive, platform services like Android and Google Play, and emerging technologies through Pixel devices and Gemini AI. This diversified portfolio creates multiple touchpoints with users throughout their digital lives, generating valuable data that powers Google\u0026rsquo;s advertising business and AI development.\nProduct Portfolio and Market Position Segment Flagship products Market position / scale (latest reliable figures) Monthly Active Users / User Base Search \u0026amp; Ads (Google Services) Google Search, YouTube Ads, Google Ads/Ad Manager, Shopping Search: ~ninety percent worldwide share (Statcounter, Sept 2025). (StatCounter Global Stats) 4.97 billion global users; 8.5 billion daily searches YouTube YouTube, Shorts, Premium/Music MAUs: ~two-and-a-half to two-point-seven billion; Shorts: ~two hundred billion daily views; Premium: one hundred twenty-five million subscribers. (DemandSage) 2.54 billion MAU; 125 million Premium subscribers Cloud Google Cloud Platform (GCP), Workspace for enterprise Cloud IaaS/PaaS share: ~thirteen percent (Q2 2025), behind AWS (~thirty percent) and Azure (~twenty percent). Revenue: $13.6B in Q2 2025, +32% YoY; operating income ~$2.8B. (Statista) $50+ billion annual run rate; 28% QoQ customer growth Platforms \u0026amp; OS Android, Chrome/ChromeOS, Play Android: ~seventy-four percent global mobile OS share. Chrome: ~seventy-two percent global browser share (Sept 2025). (DemandSage) Android: 3-4.2 billion devices; Chrome: 3.45 billion users; Play: 2.5 billion users Productivity (consumer \u0026amp; edu) Gmail, Drive, Docs/Sheets/Slides, Meet, Classroom Email client share (opens): Gmail ~twenty-four to twenty-six percent, second to Apple Mail (Litmus/industry panels, 2025). Workspace scale: billions of users; paying customers in the single-digit millions (public figures are older). (Litmus) Gmail: 1.8-2.5 billion users; Drive: 3 billion MAU; Workspace: 6+ million paying customers Maps \u0026amp; Local Google Maps, Maps Platform APIs Usage: widely cited at one-plus billion MAUs; third-party estimates range higher; Google continues deep integration (AI route summaries, business info). (Center AI) 2+ billion MAU (Q3 2024); projected 2.2 billion by Q1 2025 Hardware Pixel phones/tablets, Nest (home), Chromecast Complements services; revenue included in \u0026ldquo;Subscriptions, Platforms \u0026amp; Devices\u0026rdquo; inside Google Services. (Breakouts not separately disclosed.) (Q4 Inc.) Integration with ecosystem; exact user counts not disclosed Google Search leads with nearly 5 billion global users conducting 8.5 billion searches daily, while Chrome browser reaches 3.45 billion users worldwide. The productivity suite, anchored by Gmail\u0026rsquo;s 1.8-2.5 billion users and Drive\u0026rsquo;s 3 billion monthly active users, demonstrates Google\u0026rsquo;s success in transitioning from search to comprehensive digital services. YouTube\u0026rsquo;s 2.54 billion monthly active users and 125 million Premium subscribers showcase the platform\u0026rsquo;s dominance in video content and subscription services.\nAI-Powered Search Evolution Google Search has transformed into a multimodal AI platform. Circle to Search enables gesture-based queries on Android devices, while AI Mode provides conversational search with follow-up suggestions. Google Lens extends visual search beyond object recognition to complex tasks like solving handwritten math problems and real-time translation, demonstrating Google\u0026rsquo;s push toward intuitive, context-aware interfaces.\nGemini: Google\u0026rsquo;s AI Assistant Platform Gemini serves as Google\u0026rsquo;s flagship AI assistant and comprehensive thinking partner for complex reasoning, creative projects, and analytical work. Unlike standalone AI platforms, Gemini\u0026rsquo;s integration with Google\u0026rsquo;s ecosystem provides unique advantages: real-time Search access, Google Workspace integration, and personalized responses based on user data. This ecosystem approach positions Gemini as a direct competitor to ChatGPT while leveraging Google\u0026rsquo;s existing platform advantages.\nNotebookLM: Research and Analysis Platform NotebookLM represents Google\u0026rsquo;s approach to AI-powered research and knowledge management, positioning itself as a research and thinking partner grounded in trusted information sources. The platform is built on the latest Gemini models and designed to work with user-provided documents, creating a personalized knowledge base that can be queried and analyzed through natural language interactions.\nThe \u0026ldquo;Understand Anything\u0026rdquo; tagline reflects NotebookLM\u0026rsquo;s capability to process and synthesize information from multiple sources, making it particularly valuable for academic research, business analysis, and content creation. Unlike general-purpose AI assistants that draw from broad internet knowledge, NotebookLM focuses on understanding and analyzing specific documents uploaded by users, ensuring that responses are grounded in trusted, user-selected sources. This approach addresses concerns about AI hallucination and provides users with more reliable research assistance.\nGoogle Labs: Experimental AI Features Google Labs serves as Google\u0026rsquo;s experimental platform for testing cutting-edge AI features before mainstream deployment. The platform enables rapid iteration and user feedback collection for emerging technologies.\nFlow represents a breakthrough in AI filmmaking, using Veo for video generation to create cinematic clips with visual consistency. This tool democratizes professional video production through intelligent automation.\nDaily Listen showcases another experimental direction: AI-generated personalized audio content that curates topics from across the web, demonstrating Google\u0026rsquo;s exploration of audio-first AI experiences.\nGoogle\u0026rsquo;s Position in the AI Search Era The transition to AI-powered search represents perhaps the most significant shift in Google\u0026rsquo;s business model since its founding. Recent changes to Google\u0026rsquo;s search infrastructure reveal a strategic repositioning that has profound implications for both the company\u0026rsquo;s competitive moat and the broader internet ecosystem. Last month, Google quietly removed the num=100 search parameter — the small flag that let users view up to 100 results at once. The maximum is now ten. It sounds trivial, but it\u0026rsquo;s a massive shift in how the web works. By collapsing access to the \u0026ldquo;long tail\u0026rdquo; of search, Google just reduced the visible internet by 90 percent.\nThis strategic partnership with Reddit exemplifies Google\u0026rsquo;s approach to expanding content access while maintaining search dominance.\nThat long tail has always mattered. It\u0026rsquo;s where niche knowledge lives — community posts, independent blogs, GitHub issues, Reddit threads. It\u0026rsquo;s also the layer most large language models rely on, directly or indirectly, through Google\u0026rsquo;s indexed ranking of relevance. Even when OpenAI, Perplexity, or Anthropic crawl the web themselves, Google\u0026rsquo;s structure guides what they find and prioritize. Removing access to deep results means those models — and the startups that depend on them — now see a much smaller portion of the web.\nThe impact has been immediate and measurable. According to Search Engine Land, 88 percent of websites reported a drop in impressions after the change. Reddit, which often ranks in positions 11–100, saw its visibility collapse; its mentions in LLM outputs plunged, and its stock fell roughly 15 percent, wiping out around $5 billion in market value. What looked like a minor search tweak turned out to be a profound re-wiring of online discovery. This example illustrates the interconnected nature of Google\u0026rsquo;s influence — changes to search parameters don\u0026rsquo;t just affect Google, they reshape the entire information ecosystem that other AI companies depend upon.\nFor startups, the implications are brutal. Visibility just got harder. The open-web assumption — that a good product will eventually be found — no longer holds. If your site doesn\u0026rsquo;t rank in the top 10, it may as well not exist. In an AI-driven ecosystem, discoverability is no longer distributed; it\u0026rsquo;s gated by a few dominant indexes and interfaces. This represents a fundamental shift from the democratized web of the early 2000s to a curated, AI-mediated information environment where Google\u0026rsquo;s algorithmic decisions determine what knowledge exists in practical terms.\nThe deeper story is about power and distribution. Google\u0026rsquo;s decision protects its data moat and limits how easily AI competitors can piggyback on its index. But it also accelerates a larger shift: from an open web to a closed network of curated answers. As search turns into synthesis, Google becomes not just the map of the internet — but its gatekeeper. This transformation positions Google uniquely in the AI era, where access to high-quality training data becomes increasingly valuable and scarce.\nFor builders, the takeaway is simple but sobering. Great products don\u0026rsquo;t guarantee reach anymore; distribution does. If no model or platform can see you, users can\u0026rsquo;t either. The future of the internet isn\u0026rsquo;t about publishing to be found — it\u0026rsquo;s about integrating to be surfaced. This shift fundamentally alters the startup landscape, making Google\u0026rsquo;s ecosystem integration not just advantageous but essential for visibility in an AI-mediated world.\nStrategic Implications and Investment Thesis Google\u0026rsquo;s product ecosystem reveals a coherent strategy of building an AI-powered platform that touches every aspect of digital life. The integration of AI capabilities across traditional products like Search and new experimental platforms like NotebookLM demonstrates Google\u0026rsquo;s commitment to maintaining technological leadership in the AI era. This comprehensive approach creates multiple competitive advantages: extensive data collection for model training, diverse distribution channels for AI capabilities, and integrated user experiences that increase platform stickiness.\nFrom an investment perspective, Google\u0026rsquo;s product evolution suggests several key trends. First, the company is successfully transitioning from advertising-dependent revenue models to AI-powered service offerings that could command premium pricing. Second, the integration of AI across the product portfolio creates new monetization opportunities and strengthens competitive moats. Third, the experimental approach through Google Labs enables rapid innovation cycles and risk mitigation for emerging technologies.\nThe breadth of Google\u0026rsquo;s product portfolio also provides resilience against competitive threats. While individual products may face direct competition, the interconnected nature of the ecosystem creates switching costs and network effects that protect Google\u0026rsquo;s market position. As AI capabilities become more central to user interactions, Google\u0026rsquo;s head start in both AI research and product integration positions the company well for sustained growth in the evolving technology landscape.\nReferences StatCounter Global Stats. \u0026ldquo;Search Engine Market Share Worldwide.\u0026rdquo; StatCounter. Accessed October 2025. https://gs.statcounter.com/search-engine-market-share\nDemandSage. \u0026ldquo;How Many People Use YouTube? (2025 Active Users Stats).\u0026rdquo; DemandSage. 2025. https://www.demandsage.com/youtube-stats/\nStatista. \u0026ldquo;The Big Three Stay Ahead in Ever-Growing Cloud Market.\u0026rdquo; Statista. 2025. https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/\nDemandSage. \u0026ldquo;Android Usage Statistics (2025) - Global Market Share.\u0026rdquo; DemandSage. 2025. https://www.demandsage.com/android-statistics/\nLitmus. \u0026ldquo;Email Client Market Share and Popularity.\u0026rdquo; Litmus. 2025. https://www.litmus.com/email-client-market-share/\nCenter AI. \u0026ldquo;37 Google Maps Statistics and Interesting Facts.\u0026rdquo; Center AI. 2025. https://center.ai/blog/google-maps-statistics-and-interesting-facts/\nAlphabet Inc. \u0026ldquo;Alphabet Announces Second Quarter 2025 Results.\u0026rdquo; SEC Filing. Q4 Inc. 2025. https://s206.q4cdn.com/479360582/files/doc_financials/2025/q2/2025q2-alphabet-earnings-release.pdf\nGoogle Labs. \u0026ldquo;The home for AI experiments at Google.\u0026rdquo; Google. Accessed October 2025. https://labs.google.com\nAlphabet Inc. \u0026ldquo;Alphabet Inc. Form 10-Q for the quarterly period ended June 30, 2025.\u0026rdquo; Securities and Exchange Commission. 2025. https://www.sec.gov/Archives/edgar/data/1652044/000165204425000062/goog-20250630.htm\nSundar Pichai. \u0026ldquo;Q3 2024 Alphabet Earnings Call Transcript.\u0026rdquo; Alphabet Inc. October 2024. https://seekingalpha.com/article/4730692-alphabet-inc-goog-q3-2024-earnings-call-transcript\nTechCrunch. \u0026ldquo;Google\u0026rsquo;s AI and Machine Learning Advances in 2025.\u0026rdquo; TechCrunch. 2025. https://techcrunch.com/2025/09/24/it-isnt-your-imagination-google-cloud-is-flooding-the-zone/\nGoogle Blog. \u0026ldquo;Google\u0026rsquo;s Product Strategy and AI Integration.\u0026rdquo; Google. 2024. https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/\nBloomberg Technology. \u0026ldquo;Google Cloud Platform Growth and Market Position.\u0026rdquo; Bloomberg. 2024. https://www.bloomberg.com/news/videos/2024-10-30/bloomberg-technology-10-30-2024-video\nGartner. \u0026ldquo;Magic Quadrant for Strategic Cloud Platform Services 2024.\u0026rdquo; Gartner Research. 2024. https://www.gartner.com/en/documents/5851847\nIDC. \u0026ldquo;Worldwide Public Cloud Services Spending Guide.\u0026rdquo; International Data Corporation. 2025. https://my.idc.com/getdoc.jsp?containerId=prUS52460024\nSearch Engine Land. \u0026ldquo;Google Search Parameter Changes and Website Visibility Impact.\u0026rdquo; Search Engine Land. 2025.\nAppendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/posts/google-primer/","summary":"\u003ch2 id=\"understanding-googles-product-ecosystem\"\u003eUnderstanding Google\u0026rsquo;s Product Ecosystem\u003c/h2\u003e\n\u003cp\u003eGoogle has evolved from a simple search engine into a comprehensive ecosystem of interconnected products and services that power much of the modern internet experience. This analysis examines Google\u0026rsquo;s core products and their strategic evolution into AI-powered services that define contemporary technology investment opportunities. From traditional consumer applications to cutting-edge AI experiments, Google\u0026rsquo;s product portfolio demonstrates a coherent strategy of data collection, user engagement, and technological advancement.\u003c/p\u003e","title":"Understanding Google: A Primer"},{"content":"Content coming soon\u0026hellip;\n","permalink":"https://chenterry.com/posts/built_to_last/","summary":"\u003cp\u003eContent coming soon\u0026hellip;\u003c/p\u003e","title":"Built to last"},{"content":"OpenAI\u0026rsquo;s announcement that developers can build apps and tools directly inside ChatGPT isn\u0026rsquo;t just another feature drop; it\u0026rsquo;s a distribution shift. When AI becomes a canvas, the winners are the coordination layers that turn ideas into shipped product. The market\u0026rsquo;s immediate reaction—Figma jumping nearly fifteen percent—signals that investors increasingly view design collaboration platforms as the natural aggregation points for AI-generated work.\nFigma and Lovable illustrate two paths to that future. Lovable compresses ideation into working UI quickly; Figma converts individual creativity into team progress at enterprise scale. The question isn\u0026rsquo;t which tool \u0026ldquo;has more AI,\u0026rdquo; but who best translates AI\u0026rsquo;s raw generation into reliable, multi-stakeholder workflows.\nWedge vs. Workflow: The Lovable Challenge Lovable is a terrific wedge: it transforms ambiguous PRDs into working UI and code with startling speed. But wedges must graduate into workflows to hold value in teams. High-fidelity nuance still benefits from direct manipulation; complex data flows still require versioned review, access control, and code governance. Until AI-first generators own those moments of accountability, they amplify Figma\u0026rsquo;s role as the coordination substrate rather than displace it.\nSequencing Loops → Platform Gravity Figma\u0026rsquo;s early loop—real-time, browser-first collaboration—pulled in designers. The second loop—shared libraries, specs, and comments—pulled in PMs and engineers. The next loop is AI actions embedded in those same surfaces: generate variants, auto-redline, bind to live data, and export code with guardrails. Each loop recruits a new cohort, increases retention for the previous cohort, and raises switching costs. AI doesn\u0026rsquo;t replace these loops; it accelerates them.\nAs others have argued about Figma\u0026rsquo;s \u0026lsquo;browser-first\u0026rsquo; bet and cross-side effects, the platform advantage becomes clear when considering the full design lifecycle. Figma doesn\u0026rsquo;t just enable individual creativity; it orchestrates the entire collaborative process that turns ideas into shipped products. This includes design system maintenance, component libraries, developer handoff specifications, and stakeholder review processes.\nToday, roughly one-third of Figma\u0026rsquo;s users are professional designers—a group the platform has almost fully captured. But as AI continues to lower the barriers to design and creation, the remaining two-thirds—non-designers—represent the next wave of growth. The very market Lovable is nurturing today could eventually flow toward Figma, since Figma already integrates seamlessly into existing product and design ecosystems.\nWhat to Watch Next Three signals will reveal whether Figma turns AI into durable advantage. First, the mix shift toward non-designer actives, measured by viewer-to-editor conversion and time-to-first-comment on files created with AI features. Second, design-to-deployment cycle time, captured by reduction in handoff defects and PR-to-ship latency on files sourced from Figma Make. Third, ecosystem velocity, reflected in monthly active plugins, enterprise-grade plugin adoption, and AI actions invoked per file. If these curves bend up together, Figma\u0026rsquo;s collaboration moat is compounding.\nThe Coordination Layer Thesis Critics argue that if AI collapses the distance between prompt and production code, the design surface could be bypassed entirely. But in practice, accountability moves toward shared surfaces when stakes rise. Compliance, accessibility, localization, and performance budgets require artifacts that non-designers can review and approve. The more AI generates, the more organizations need a legible, collaborative spine—which is Figma\u0026rsquo;s native terrain.\nWhere Value Accrues As foundation models commoditize, differentiation shifts to integration quality, governance, and cross-functional velocity. Platforms that already mediate conversations among designers, PMs, and engineers are positioned to convert generic model output into organization-specific, reviewable change. That is where budgets live.\nAI increases the volume of drafts, variants, and micro-changes. Without a shared system, that creates chaos; within Figma, it creates momentum. The same surface that shortened idea → design now shortens design → implementation—and the delta is monetizable.\nThe lesson of today\u0026rsquo;s announcement isn\u0026rsquo;t that AI will crown a new category king. It\u0026rsquo;s that AI amplifies whichever layer already coordinates work. Lovable shows how quickly AI can turn intent into interface. Figma shows how teams turn interface into impact. If the next decade of design looks more like engineering—faster, more legible, and more automated—the platform that standardizes those feedback loops will capture the bulk of the value. Right now, that center of gravity is Figma.\n","permalink":"https://chenterry.com/posts/figma-ai-era/","summary":"\u003cp\u003eOpenAI\u0026rsquo;s announcement that developers can build apps and tools directly inside ChatGPT isn\u0026rsquo;t just another feature drop; it\u0026rsquo;s a distribution shift. When AI becomes a canvas, the winners are the coordination layers that turn ideas into shipped product. The market\u0026rsquo;s immediate reaction—Figma jumping nearly fifteen percent—signals that investors increasingly view design collaboration platforms as the natural aggregation points for AI-generated work.\u003c/p\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/investing/figma-ai-era/chatgpt-figma-integration.png\" alt=\"ChatGPT-Figma Integration\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eFigma and Lovable illustrate two paths to that future. Lovable compresses ideation into working UI quickly; Figma converts individual creativity into team progress at enterprise scale. The question isn\u0026rsquo;t which tool \u0026ldquo;has more AI,\u0026rdquo; but who best translates AI\u0026rsquo;s raw generation into reliable, multi-stakeholder workflows.\u003c/p\u003e","title":"Why Figma Wins (In the AI Era Too)"},{"content":"Inspiring insights, amplifying voices. (crowdlisten.com) From Content Aggregation to Original Research Crowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. While being able to quantatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\nDeep research features provide professional-looking research reports, yet the contents are far from original, as they\u0026rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet\u0026rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they\u0026rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.\nI\u0026rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlistening, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn\u0026rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows.\nI paused Crowdlistening in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I\u0026rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).\nWhat I\u0026rsquo;m particularly interested in exploring and building with Crowdlistening is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.\nThe Insight Paradox Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can\u0026rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlistening bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in \u0026ldquo;Essence of Creativity\u0026rdquo; - helping users understand massive amounts of information and generate meaningful insights when they \u0026ldquo;don\u0026rsquo;t know what output they want.\u0026rdquo;\nTechnical Architecture: Multi-Modal by Design The rationale behind Crowdlistening\u0026rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video\u0026rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.\nCrowdlistening\u0026rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM\u0026rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities.\nThis dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlistening can generate insights that reflect not just what is being said, but how it\u0026rsquo;s being said, the surrounding context, and the audience\u0026rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the \u0026ldquo;insight paradox\u0026rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlistening: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.\nDetailed Analysis Capabilities The platform provides granular breakdowns of content performance and audience reactions. Users can explore specific themes, track sentiment over time, and identify the most engaging content types across different categories and industries. This helps brands understand not just what is being said, but why certain content resonates with their audience.\nThe opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.\nThe MCP Advantage: Accessible Functional Calls We have integrated Model Context Protocols (MCPs) - an emerging standard that simplifies how LLMs interact with specialized tools and data sources. Rather than simple API calls, MCPs provide structured interfaces for LLMs to access specialized capabilities while maintaining context awareness throughout the analysis process.\nAs shown above, when a user submits a research question, the system dynamically determines which analytical capabilities to deploy. The Claude interface serves as the orchestration layer, identifying relevant MCP tools to activate and calling them sequentially:\nFirst gathering baseline information through web search Then performing targeted data collection via specialized TikTok MCP tools Following with multi-layered analysis of videos and comments Finally synthesizing everything into coherent, actionable insights This MCP-driven approach creates a dramatic efficiency improvement, reducing complex social media analysis from weeks to minutes while maintaining remarkable analytical depth.\nCase Study - Trump Tariffs To demonstrate Crowdlistening\u0026rsquo;s capabilities, we conducted a comprehensive analysis of public sentiment regarding Trump\u0026rsquo;s tariff policies. This serves as an excellent test case due to its complexity, polarizing nature, and economic impact.\nWhen a user inputs the query about Trump\u0026rsquo;s tariff policies, our system activates the appropriate MCP tools in sequence. First, it gathers factual background information on the policies themselves, as shown below:\nThis background research provides context on what the current tariff policies are, including the 10% baseline tariff on all imports that took effect in April 2025, plus the higher \u0026ldquo;reciprocal\u0026rdquo; tariffs on countries with which the US has trade deficits (34% for China, 20% for the EU, and 24% for Japan).\nNext, the system analyzes public opinion on these policies by examining social media content. The analysis reveals highly polarized reactions, categorized into three main perspectives:\nThe sentiment analysis dashboard shows that opinions on Trump\u0026rsquo;s tariff policies are distributed as 38% supportive, 42% critical, and 20% neutral or mixed. This visualization helps brands and researchers quickly understand the overall public response landscape.\nOne of the most valuable outputs is our projected economic impact analysis. This data visualization clearly presents the concrete financial implications of these policies across multiple domains:\nThe analysis shows an estimated $1,300 annual cost increase per US household, a projected 0.8% reduction in long-run US GDP, significant auto price increases ($3,000 for US vehicles, $6,000 for imports), and warnings about market volatility.\nBeyond simple pro/con sentiment, our opinion analysis feature categorizes specific viewpoints with remarkable granularity. For instance, when examining comments on related content, we can identify nuanced perspectives and their prevalence:\nThis example shows how our system can identify several different comment themes, including positive views of content creators (37.5%), appreciation for intelligent discussion (25%), and concerns about media echo chambers (12.5%). This level of nuanced understanding would be impossible through traditional keyword or basic sentiment analysis.\nValidation and Impact Our solution has been validated through interviews with major brands like L\u0026rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlistening enables:\nRapid response to emerging trends Deep understanding of consumer sentiment across demographics Identification of microtrends before they become mainstream Competitive intelligence at unprecedented scale The Future of MCP-Driven Research We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.\nThis approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude\u0026rsquo;s interface) emerge as this paradigm gains traction.\nFor social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they\u0026rsquo;re covered in traditional reporting.\nCredits This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Roy, Ruiwan, Qiping, Tongming and other members on the Creative team at TikTok, who I\u0026rsquo;ve discussed early conceptions of this idea with.\nOn Social Intelligence Crowdlistening represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we\u0026rsquo;re helping brands make more informed decisions faster than ever before.\nAs noted in \u0026ldquo;Essence of Creativity,\u0026rdquo; the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we\u0026rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.\n📋 Version History v1.1 • Oct 25, 2025 • View changes • Updated Title\n💡 Click \u0026ldquo;View changes\u0026rdquo; to see exactly what changed between versions\n","permalink":"https://chenterry.com/posts/crowdlistening/","summary":"\u003ch2 id=\"inspiring-insights-amplifying-voices-crowdlistencom\"\u003eInspiring insights, amplifying voices. (crowdlisten.com)\u003c/h2\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/projects/crowdlistening/crowdlisten-homepage.png\" alt=\"CrowdListen Homepage\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003ch2 id=\"from-content-aggregation-to-original-research\"\u003eFrom Content Aggregation to Original Research\u003c/h2\u003e\n\u003cp\u003eCrowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. While being able to quantatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\u003c/p\u003e","title":"From Raw Social Data to Real Research"},{"content":"There are two credible paths to building agentic experiences. The first is platform-first: stand up a unified agent framework with the core capabilities—multi-turn conversation, a knowledge base, and memory—and then layer in signifiers and affordances that fit your environment. The second is scenario-first: begin with the thinnest viable surface and add only the features that demonstrably create value beyond what ChatGPT or Copilot already provide, bringing in memory and other \u0026ldquo;platform\u0026rdquo; features only once they have earned their keep. The platform-first approach yields a consistent engineering experience and lets teams reuse prior agent work, but it risks poor agent–scenario fit. The scenario-first approach can feel messier and demands more from product managers, yet it validates real-world use cases faster. I don\u0026rsquo;t claim one approach is universally better—startups and large companies face different constraints—but I do believe there is only one way to prototype: ship quickly, test explicit hypotheses, and iterate without delay.\nA clarifying question keeps this cadence honest: what is the minimum version of the product that lets us learn whether the solution can find product–market fit? Counterintuitively, you often do not need a working prototype to answer that. Walking through end-to-end customer scenarios frequently reveals whether a proposed feature fits existing workflows and where it will break. That said, some questions hinge on new engineering—experiences that are hard to reason about in the abstract. In those cases, the objective is not to \u0026ldquo;build the demo,\u0026rdquo; but to surface and test the assumptions that matter. Each design choice should map to the outcome it seeks and to the user challenge it addresses. The simpler the stack, the more learning cycles you can run with less effort, which is the real engine of progress.\nModern AI coding tools make this possible. Cursor, GitHub Copilot, and Claude Code compress build time by generating boilerplate, suggesting common patterns, and helping troubleshoot. A single engineer can now produce a functional MVP in a fraction of the time that used to require a small team. Much like Figma tightened the collaboration loop in design, these tools narrow the gap between product intent and implementation. The result is not merely faster engineering; it is broader participation. Product managers, designers, even sales and customer success teams can test ideas more directly, while engineers concentrate on production-grade systems and reliability concerns that truly benefit from their specialization.\nInvolving Cross-Functional Stakeholders An agentic experience is only as good as our understanding of the underlying problem. This is especially true for expert workflows—consumption-based cost estimation or SOC investigation, for example—where product and engineering teams are rarely the domain experts. Involving architects, sales engineers, and analysts only at the prompt-iteration stage is not enough. To build agent behaviors that actually fit, we have to internalize existing workflows and best practices, then design signifiers and affordances that match practitioner expectations. Language, steps, intermediate outputs, and handoffs should mirror how experts already think and work. When the agent speaks their dialect and respects their process, adoption follows because the experience feels native rather than novel for novelty\u0026rsquo;s sake.\nThis is exactly where the Figma analogy—Kevin Kwok\u0026rsquo;s point about non-linear returns from tighter collaboration loops—becomes operational. Figma did not just make drawing easier; it made critique, alignment, and decision-making happen in the same place, by the right people, at the right time. AI coding assistants catalyze a similar shift for agentic products: they collapse the distance between a domain expert\u0026rsquo;s intent and a working prototype, making assumptions explicit, turning tacit heuristics into checkable rules, and surfacing disagreements while they are still cheap to resolve. When prototypes function as shared canvases—co-edited by PMs, engineers, and subject-matter experts—the loop tightens further: experts shape the signifiers and workflows, product sharpens the hypotheses, and engineering focuses on robustness and safety. The compounding return comes not from adding more features, but from aligning agent behavior with the realities of the domain.\nLearnings from the Cost Estimator Agent To ground these principles, let\u0026rsquo;s look at an agentic implementation of a cost estimation scenario\nProject Context Customers need accurate cost estimates for budget planning and solution comparison, yet consumption-based pricing is notoriously hard to predict. We heard repeatedly from the field that this uncertainty stalls decisions and, in competitive deals, can tilt outcomes against us. Existing tools do not help enough. Web calculators feel like black boxes with coarse, inflexible inputs and little transparency. Spreadsheet models are opaque and fragile, with assumptions scattered across cells. Both often ask for inputs customers do not understand or cannot provide without heavy translation.\nIn other words, this is not a known unknowns problem where a general-purpose copilot can retrieve an answer upon request. Nor is it an unknown knowns problem where the customer already has a tried-and-true estimation method and we simply need to automate it. It is often an unknown unknowns problem: customers do not know what to ask, and they do not have the raw data in the needed form. The result is planning paralysis and, ultimately, stalled or lost deals.\nDesign Rationale Designing for \u0026ldquo;unknown unknowns\u0026rdquo; required optimizing along three intertwined dimensions. First, we focused on transparency and control so that users could see the reasoning behind estimates—the assumptions, intermediate calculations, and trade-offs—and adjust inputs with confidence. Numbers without narrative do not build trust, and trust is the currency of estimation. Second, we embedded domain expertise directly in the experience. Instead of pushing the knowledge gap back to the user, the system translated familiar facts—industry patterns, ingestion profiles, retention policies—into the metrics the pricing model requires, pre-populating where possible and teaching as it went. Third, we treated estimation as a process rather than a form, and we designed for iterative refinement. The goal was not a one-shot answer but a guided conversation that converges on confidence.\nAt a basic level, we began with an agent side-panel, similar to a Copilot, to unify product documentation, pricing schemas, and frequently asked questions. This supported conversational guidance throughout the estimation process, but it also exposed three frictions we had to solve in order to achieve fit. First, use-case discovery was weak: without strong signifiers, users did not know what to ask and often ventured beyond the agent\u0026rsquo;s scope. Second, chat lacked context: humans are economical with effort, so expecting users to restate all the fields they had filled and the stage they were in created unnecessary friction. Third, people don\u0026rsquo;t know what they don\u0026rsquo;t know: there is a structural gap between what customers know about their business (for example, number of users, typical event patterns) and what we require to estimate costs (for example, daily gigabytes ingested). Simply asking, \u0026ldquo;How many gigabytes per day?\u0026rdquo; does not bridge that gap.\nThese insights shaped a prototype with two synchronized surfaces: a pricing panel and an agent panel kept in bidirectional sync. Edits in the graphical interface updated the conversation\u0026rsquo;s context, and the agent\u0026rsquo;s reasoning flowed back as explanation cards anchored beside the fields they affected.\nIn brownfield scenarios, the agent could pull relevant account signals to prefill inputs and explain each value\u0026rsquo;s provenance. In greenfield scenarios, the experience offered size recommendations—small, medium, large, enterprise—that users could apply with one click, each accompanied by clear rationales and editable assumptions.\nWhen hard numbers were missing—say, daily ingestion in gigabytes—the agent asked questions users could answer about environment size, event rates, and retention needs, then converted those responses into derived estimates, showing the math and inviting adjustments. Under the hood, a focused knowledge base provided product and pricing facts, while three structured workflows—volume estimation, pricing estimation, and design recommendations—gave the conversation shape and kept it oriented toward decisions rather than dialogue for its own sake.\nEvaluation and Benchmarking Agent platforms encourage generality, but effectiveness must be demonstrated on concrete tasks. We evaluate the experience by asking whether it completes representative estimation scenarios end to end, how its outputs compare to human-expert baselines, and how quickly it converges to a result stakeholders trust. Accuracy matters, but so do user effort and confidence. When building agentic experiences, we should track time to an acceptable estimate, the number of clarifying turns, and whether users report understanding and accepting the assumptions they carry forward. Scenario coverage also matters: behavior needs to hold not only in the \u0026ldquo;happy path,\u0026rdquo; but across brownfield and greenfield cases, high-volume and bursty workloads, and strict-retention and cost-optimized policies. When behavior degrades, it should degrade gracefully with clear explanations, ranges, or a handoff to a human expert.\nIn larger organizations, evaluation pairs with safeguards. Data validation and drift monitoring ensure that quotes reflect current pricing and product information, with alerts when underlying references change. Guardrails protect embedded expert logic—estimation methods and pricing strategies—against prompt injection and leakage of system instructions, and they constrain access to sensitive APIs. Finally, bad-case handling is a first-class requirement: the system detects ambiguous inputs, surfaces low-confidence steps, and offers conservative defaults or escalation paths rather than silently producing spurious precision. Specifications and engineering plans that omit scenario walkthroughs, benchmarks, and safeguards drift toward imagined use cases and weak agent–scenario fit; those that include them turn agentic ambition into reliable impact.\nClosing Thoughts Choose a build path that fits your context, but always prototype to learn, not to impress. Use AI tools to shorten the distance between ideas and feedback. Bring domain experts into the design of signifiers and workflows so the agent respects reality. Make reasoning visible, embed expertise at the point of need, and shape the experience for iterative refinement. Then prove it with scenario-based evaluation and strong guardrails. This, I believe, is how you truly iterate at the pace of AI.\n","permalink":"https://chenterry.com/posts/agent_prototyping/","summary":"\u003cp\u003eThere are two credible paths to building agentic experiences. The first is platform-first: stand up a unified agent framework with the core capabilities—multi-turn conversation, a knowledge base, and memory—and then layer in signifiers and affordances that fit your environment. The second is scenario-first: begin with the thinnest viable surface and add only the features that demonstrably create value beyond what ChatGPT or Copilot already provide, bringing in memory and other \u0026ldquo;platform\u0026rdquo; features only once they have earned their keep. The platform-first approach yields a consistent engineering experience and lets teams reuse prior agent work, but it risks poor agent–scenario fit. The scenario-first approach can feel messier and demands more from product managers, yet it validates real-world use cases faster. I don\u0026rsquo;t claim one approach is universally better—startups and large companies face different constraints—but I do believe there is only one way to prototype: ship quickly, test explicit hypotheses, and iterate without delay.\u003c/p\u003e","title":"Iterating at the Pace of AI"},{"content":"Why We Need More Ways to Hear Customers As product teams scale, direct exposure to customers narrows. We drift into a product → market pattern: build for a vocal few, iterate with them, and accumulate complexity that alienates everyone else. With agentic tools and tighter loops, we can flip to market → product: still partner closely with a subset of customers, but also listen more broadly, form grounded hypotheses from authentic needs, then decide what to build.\nThe AI Way to Do It Traditional social listening tracks competitors and brand chatter. We extend it to enterprise product building. Beyond internal testers and formal channels, the internet already hosts rich, first-party signals about how products—ours and others—land in the wild: community forums, Q\u0026amp;A sites, GitHub issues, Reddit/HN threads, and support communities. If we capture, filter, and reason over that discourse responsibly, we get a truer view of pain points, expectations, and the language users actually use.\nPractical workflow 1.Targeted discovery (for specific questions): compile a tight corpus from public discussions, extract and normalize text, then reason over it with a large-context LLM to label and synthesize themes. 2.Open-ended exploration (for fuzzy spaces): generate sentence/thread embeddings, cluster semantically, and label themes with short, evidence-linked summaries.\nEvery theme flows into a decision scaffold—theme → testable hypothesis → bet → telemetry → refinement—to prevent “insight theater.” Longer-term, run an always-on Customer Insight Radar that ingests external communities plus internal notes (with privacy filters), tracks theme velocity, and attaches representative quotes so PMs and engineers can feel the evidence behind the numbers.\nPrimary vs. secondary sources Treat raw online discourse (posts, threads, issue comments) as primary research and polished content (blogs, vendor write-ups, SEO pages) as secondary. “Deep research” tools often paraphrase what’s indexed. We optimize for original insight, not summaries of summaries: go to the source, weight firsthand accounts more heavily, and use secondary material only for background or triangulation.\nA Bit More on Embeddings (and Why They Help) Embeddings convert messy text into dense vectors where closeness reflects meaning. Early static methods (skip-gram/CBOW, GloVe) learn one vector per word—fast and useful for clustering, but they blur senses (“bill” the invoice vs. “bill” the law). Contextual encoders produce different vectors for the same word depending on context and can be pooled to sentence/document embeddings.\nFor social listening, sentence/document embeddings are the workhorse: they enable semantic search (“unreconcilable line items” ↔ “can’t map charges to usage”), organic theme discovery without hand-built taxonomies, and drift tracking as language shifts. In practice, we embed at sentence and thread granularity, use ANN indexing for retrieval, and favor density-based or hierarchical clustering to fit the uneven shape of real discussions—always anchoring themes to verbatim quotes for auditability.\nReasoning Models and Visualizations With smaller datasets, we can directly provide the extracted raw text to a large-context, strong-reasoning model. This goes beyond coarse sentiment categories or word clouds, enabling richer context and actionable insights. Curated prompts let an LLM (or agent) read the semantic neighborhoods, surface key themes, note counter-signals, and propose testable hypotheses.\nWhat the output is—and isn\u0026rsquo;t The output isn’t a slide; it’s a decision input. Each synthesized theme should map to a testable product hypothesis, an opportunity-size signal, and an instrumentation plan—the same scaffold above that keeps the loop tight.\nSeeing It in Action Example 1: The Cost Experience From ~700 candidate threads, we curated ~100 high-signal discussions (Reddit, HN, Quora, vendor/community forums), normalized text, embedded at sentence/thread level, clustered themes, and linked each to example quotes. One dominant signal emerged: billing complexity and transparency drive most cost-related UX pain. Users struggle to reconcile invoices to usage, discover overruns after month-end, and use calculators that ignore dynamic workloads—leading to surprise spikes. Strategically, users prefer predictable costs over merely lower costs. The advantage is cost-experience design (real-time transparency, proactive controls, behavior-aware forecasting) rather than discounts alone. Platform “flavors” vary (e.g., BigQuery pricing confusion, Snowflake credit visibility, Databricks cluster trade-offs, Splunk ingestion spikes, Redshift monitoring blind spots), but the design response is consistent: plain-English cost impact at point of action, pre-threshold alerts, safe throttles, and workload-aware forecasting.\nExample 2: Tier-2 SOC Analyst Friction Across tools, five universal barriers appear: 1.query language/parsing complexity; 2.false-positive overload; 3.correlation/integration hurdles across silos; 4.workflow friction from context switching and ad-hoc processes; 5.platform limitations that pull analysts into tool troubleshooting.\nDesign mandate: lower technical barriers, suppress noise, preserve investigative context, and streamline common paths so analysts spend time on threats—not tool mechanics. Agentic experiences and stronger defaults move the needle fastest.\nCrowdlistening As shown in the cost-estimation agent presentation, Crowdlistening is a tool I built to extract patterns from collective discourse without flattening individual voices. It pairs LLM reasoning with a larger-context pipeline to ingest public discussions, structure them, and tie findings back to evidence. At its core, Crowdlistening treats raw discourse as primary data, emphasizes traceability (“show your work”), and optimizes for original insight over derivative summaries.\nCrowdlistening\u0026rsquo;s goal isn\u0026rsquo;t forced consensus; it\u0026rsquo;s to surface authentic needs, native customer language, and edge cases that formal channels miss. At enterprise scale—where the user base is large and diverse—listening broadly helps us prioritize what\u0026rsquo;s real over what\u0026rsquo;s merely loud. (More background at Crowdlistening.com.)\nSince the launch of MCPs, I\u0026rsquo;ve experimented with exposing Crowdlistening capabilities as MCP servers—directly accessible in clients like Copilot or Claude. Features remain similar (with some visualization limits), but inputs become more nuanced and multi-turn, making the experience far more intuitive for non-technical users.\nBuilding This at Large Organizations A Feature Proposal To enable the market → product workflow (listen broadly → form grounded hypotheses → decide what to build), we can ship a Copilot MCP integration as a conversational guide for early spec writing. It would:\nIngest customer meeting transcripts and selected online discussions, Run evidence-linked synthesis with clear citations to primary sources, Produce theme → hypothesis → bet → telemetry → refinement scaffolds that slot directly into specs. Governance and Guardrails Social data is messy and sensitive. We will:\nApply strict sourcing, deduplication, and consent practices; avoid PII capture. Measure representativeness to reduce sampling bias. Link every synthesized claim to auditable evidence (\u0026ldquo;show your work\u0026rdquo;). Use MCP-based connectors so data plumbing remains inspectable and secure. Closing Thought AI-enabled social listening doesn’t replace customer calls, design research, or telemetry; it enriches them—especially at the fuzzy front end. Used well, it helps us choose better problems, write crisper specs, and ship experiences that feel obvious in hindsight.\n","permalink":"https://chenterry.com/posts/need_validation/","summary":"\u003ch2 id=\"why-we-need-more-ways-to-hear-customers\"\u003eWhy We Need More Ways to Hear Customers\u003c/h2\u003e\n\u003cp\u003eAs product teams scale, direct exposure to customers narrows. We drift into a product → market pattern: build for a vocal few, iterate with them, and accumulate complexity that alienates everyone else. With agentic tools and tighter loops, we can flip to market → product: still partner closely with a subset of customers, but also listen more broadly, form grounded hypotheses from authentic needs, then decide what to build.\u003c/p\u003e","title":"Social Listening for Product Insight"},{"content":"Understanding the Security Data Lake and SIEM Business Work in progress for understanding the security data lake and SIEM business.\nDefining the Business The Security Information and Event Management (SIEM) and data lake business centers on platforms that collect, store, analyze, and correlate security telemetry to detect threats, ensure compliance, and facilitate response. SIEMs focus on real-time alerting and investigation, while data lakes provide scalable, cost-effective storage for raw data, enabling advanced analytics and long-term retention. This solves escalating problems: exploding data volumes from cloud/IoT/tools (e.g., 90% of orgs use 40+ security tools), unsustainable SIEM costs (ingestion-based pricing), format inconsistencies impeding correlation, and regulatory needs for auditable logs (e.g., SEC/GDPR). Efficiency gains come via preprocessing (filtering 40-65% noise, normalizing to OCSF), enrichment (threat intel), and tiered routing, cutting MTTD/MTTR and costs. The market evolves from monolithic SIEMs to modular architectures with Security Data Pipeline Platforms (SDPPs) as intelligent layers, projected at $10.78B in 2025, growing to $19.13B by 2030 (12.16% CAGR), fueled by AI adoption and cloud shifts.\nKey Players \u0026amp; Competitive Landscape The landscape pits legacy SIEM vendors against innovative SDPPs and data lake specialists, with convergence blurring lines. Microsoft (Sentinel) leads in cloud-native growth, Datadog bridges observability-security, Databricks powers analytics-heavy lakes, Cribl dominates pipelines ($200M+ ARR), and Wiz (post-Google $32B acquisition) bolsters cloud security integrations. AI adoption accelerates, with 43% of orgs centralizing data strategies for ML-driven insights.\nPlayer Product Offerings Differentiation Market Position \u0026amp; Evolution Microsoft (Sentinel) Cloud SIEM; data connectors, ML analytics, Copilot for Security; integrates with Azure lakes. AI-powered threat hunting, multi-tenant management; updates in 2025 include enhanced visibility, AI insights for intel. Cloud leader; evolving to AI-SOC hub, 60%+ Fortune 500 adoption; partnerships boost education/training. Datadog Cloud SIEM, Observability Pipelines; log management, threat detection. Unified sec/ops; AI parsing/quota mgmt; 2025 updates: Code Security, data protection enhancements. Observability-security convergence; SIEM migration aid; strong in DevSecOps. Databricks Lakehouse Platform; Unity Catalog for governance, Delta Lake for storage. AI-driven analytics; 2025: serverless multicloud security, cybersecurity lakehouse for threats (e.g., State Street use). Data intelligence leader; evolving for sec lakes, 100+ use cases including AI risk mitigation. Cribl Stream/Edge/Search/Lake; data routing, reduction, lakehouse. Vendor-agnostic; AI copilot; 2025: tiered storage, SIEM integration (e.g., CrowdStrike Falcon). SDPP pioneer; $200M+ ARR; enables migrations, next-gen SIEM evolution. Wiz (Google) CNAPP; cloud security scanning, risk prioritization. Post-$32B acquisition: Enhances Google Cloud sec; integrates with lakes/SIEMs for vuln mgmt. Cloud sec disruptor; bolsters Google\u0026rsquo;s CNAPP, impacts multicloud strategies. Splunk (Cisco) Enterprise Security; federated search, data mgmt. Hybrid support; deep analytics. Legacy leader; evolving with pipelines for cost control. Elastic ELK Stack; data tiering, search. Open-source scalability. Versatile; lakehouse convergence. Abstract Security Streaming analytics; AI enrichment. Real-time detection; no-code UI. Emerging; SOC efficiency focus. Anomali Cloud SIEM + pipeline; threat intel. Converged TIP/SIEM; AI copilot. Migration ease; intel-driven. Stellar Cyber Open XDR + SDPP; multi-layer AI. Unified SecOps; mid-market. Integrated platform; agentic AI. The Technology \u0026amp; Strategy Tech includes log aggregation, ML anomaly detection, and scalable lakes (e.g., S3/Snowflake with Athena queries). Strategies shift to modular SIEMs (decoupling storage/analytics), SDPP preprocessing (filtering 80%+, OCSF normalization), and AI adoption (43% centralized data for ML; copilots like Sentinel\u0026rsquo;s for queries). Serves efficiently by enabling real-time streaming, cutting costs 50%+, speeding MTTR to minutes via agentic AI. Future: AI data engineers automating parsing/enrichment, data fabrics unifying layers, observability-sec convergence.\nFinding the Edge Differentiation: Microsoft excels in ecosystem integration/AI (Copilot boosts hunting); Datadog unifies sec/ops with pipelines (50%+ savings, AI parsing); Databricks leverages lakehouses for AI analytics (serverless sec, threat products); Cribl leads SDPP with tiered storage/SIEM evo (migrations in weeks); Wiz enhances CNAPP post-acquisition (Google Cloud sec, multicloud risk). Edges from AI copilots (natural queries), agentic systems (auto-optimization), hybrid support. Field heads to AI-SOCs (MTTR minutes), fabrics, convergence.\nReferences: Software Analyst: Market Guide 2025: The Rise of Security Data Pipelines - Market Guide 2025: The Rise of Security Data Pipelines \u0026amp; How SIEMs Must Evolve Mordor Intelligence: SIEM Market Analysis - Security Information and Event Management Market Size \u0026amp; Share Analysis IDC: Worldwide SIEM Forecast - Worldwide Security Information and Event Management Forecast, 2025–2029 Expert Insights: SIEM Market Overview 2025 - SIEM Market Overview: Key Stats And Insights For 2025 Detection at Scale: Transition from Monolithic SIEMs - The Transition from Monolithic SIEMs to Data Lakes for Security Analytics Omdia: Cybersecurity Data Fabrics 2025 - Market Landscape: Cybersecurity Data Fabrics 2025 SentinelOne: Data Lake Solutions - Singularity™ Data Lake overview Cribl: RSAC 2025 Insights - Five Non-Obvious Insights Shaping IT and Security from RSAC 2025 Contrary Research: Cribl Business Breakdown - Cribl Business Breakdown \u0026amp; Founding Story Microsoft: Sentinel Updates - What\u0026rsquo;s new in Microsoft Sentinel Datadog: DASH 2025 Features - DASH 2025: Guide to Datadog\u0026rsquo;s newest announcements Databricks: Security and Compliance Updates - What\u0026rsquo;s new in security and compliance at Data + AI Summit 2025 Wiz: Google Acquisition Analysis - Analysis: No Matter How Google Deal Turns Out, Wiz Wins Appendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/posts/ai-powered-security-data-pipelines/","summary":"\u003ch2 id=\"understanding-the-security-data-lake-and-siem-business\"\u003eUnderstanding the Security Data Lake and SIEM Business\u003c/h2\u003e\n\u003cp\u003eWork in progress for understanding the security data lake and SIEM business.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eThe Security Information and Event Management (SIEM) and data lake business centers on platforms that collect, store, analyze, and correlate security telemetry to detect threats, ensure compliance, and facilitate response. SIEMs focus on real-time alerting and investigation, while data lakes provide scalable, cost-effective storage for raw data, enabling advanced analytics and long-term retention. This solves escalating problems: exploding data volumes from cloud/IoT/tools (e.g., 90% of orgs use 40+ security tools), unsustainable SIEM costs (ingestion-based pricing), format inconsistencies impeding correlation, and regulatory needs for auditable logs (e.g., SEC/GDPR). Efficiency gains come via preprocessing (filtering 40-65% noise, normalizing to OCSF), enrichment (threat intel), and tiered routing, cutting MTTD/MTTR and costs. The market evolves from monolithic SIEMs to modular architectures with Security Data Pipeline Platforms (SDPPs) as intelligent layers, projected at $10.78B in 2025, growing to $19.13B by 2030 (12.16% CAGR), fueled by AI adoption and cloud shifts.\u003c/p\u003e","title":"AI-Powered Security Data Pipelines: The Future of Enterprise Cybersecurity"},{"content":"Agentic Workforce Our current rate of adoption for agentic workforces has significant room for improvement. AI coding is mainly for developers, but the true value unlock is when everyday people can integrate entire workflows (think assembly lines for repetitive work). All the work that one can conceive of how to do but needs to sit through should be delegated.\nDefining the Business An agentic workforce involves autonomous AI agents—systems that reason, plan, act, learn, and adapt—to handle complex tasks and workflows, augmenting or replacing human labor in repetitive or decision-heavy roles. This business solves inefficiencies in traditional work structures, such as high labor costs, error-prone manual processes, and scalability limits, by deploying AI agents that operate as \u0026ldquo;digital teammates\u0026rdquo; for tasks like data analysis, customer service, and automation. Efficiency is achieved through hyperautomation (e.g., 30% productivity gains), personalized experiences, and reduced MTTR in operations, with adoption projected to jump 327% by 2027. The market, part of broader AI, sees agentic AI driving $4.4T in value, but faces challenges like 40% project cancellations by 2027 due to costs and risks.\nKey Players \u0026amp; Competitive Landscape The landscape features AI leaders building agentic tools, with $33.9B in GenAI investments (2024-2025) and acquisitions like Capgemini-WNS ($3.4B) for agentic ops. Startups like Gradient Labs ($13M) target regulated sectors. Competition focuses on enterprise vs. consumer, with stocks like UiPath, NVIDIA rising 20-50% on agentic bets.\nPlayer Key Offerings Differentiation Investments/Acquisitions OpenAI GPT agents; o1 model for reasoning. Advanced reasoning; agentic frameworks for workflows. $157B valuation; io Products acquisition for hardware. Microsoft Copilot agents in Dynamics/365; Azure AI Studio. Enterprise integration; hybrid human-AI decisions. $1.3B AI; OpenAI partnership. Google Gemini agents; Project Astra. Decision intelligence; Android ecosystem. $75B data centers; AI acquisitions. Anthropic Claude for agentic tasks; constitutional AI. Ethical alignment; safe automation. $61.5B valuation; Amazon investments. UiPath RPA with agentic AI for processes. Hyperautomation; workflow orchestration. Stock focus; partnerships. Gradient Labs Agentic AI for customer support in regulated industries. Compliance-focused; reskilling integration. $13M raised (Monzo alums). The Technology \u0026amp; Strategy Tech: Agentic AI uses LLMs with tools/memory for autonomous actions (e.g., reasoning/planning in o1 models); multi-agent systems coordinate tasks. Strategies: Hybrid workforces (AI-human collaboration), governance frameworks; 2025 trends: Reasoning models, MoE, synthetic data. AI adoption: 70% orgs operationalize by 2025; productivity +30%, but 40% cancellations.\nFinding the Edge Edges: Ethical AI (Anthropic), enterprise scale (Microsoft), reasoning (OpenAI). Field heads to cognitive enterprises, hybrid workforces; investments like RSM\u0026rsquo;s $1B signal maturity. Differentiation via data governance, multi-agent orchestration.\nPrototyping \u0026amp; Explorations Prototypes: Multi-agent systems (Chain-of-Agents); explorations: AI data engineers, agentic L\u0026amp;D for upskilling. VC memos: Focus on agentic for ROI, but caution costs.\nRemaining Questions How will agentic AI reshape traditional job roles and responsibilities? Can organizations effectively manage the transition to hybrid human-AI workforces? What regulatory frameworks are needed for autonomous AI agents in the workplace? References: A2A Catalog: Agentic AI Tools Directory - Comprehensive directory of agentic AI tools and platforms McKinsey: AI in the Workplace - AI\u0026rsquo;s impact on workplace productivity and agentic systems Gartner: Agentic AI Trends - Agentic AI market trends and adoption forecasts Forrester: Workforce Automation - Automation platforms and agentic workforce solutions IDC: AI Workforce Market - AI workforce market analysis and projections Appendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/archived/agentic_workforce/","summary":"\u003ch2 id=\"agentic-workforce\"\u003eAgentic Workforce\u003c/h2\u003e\n\u003cp\u003eOur current rate of adoption for agentic workforces has significant room for improvement. AI coding is mainly for developers, but the true value unlock is when everyday people can integrate entire workflows (think assembly lines for repetitive work). All the work that one can conceive of how to do but needs to sit through should be delegated.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eAn agentic workforce involves autonomous AI agents—systems that reason, plan, act, learn, and adapt—to handle complex tasks and workflows, augmenting or replacing human labor in repetitive or decision-heavy roles. This business solves inefficiencies in traditional work structures, such as high labor costs, error-prone manual processes, and scalability limits, by deploying AI agents that operate as \u0026ldquo;digital teammates\u0026rdquo; for tasks like data analysis, customer service, and automation. Efficiency is achieved through hyperautomation (e.g., 30% productivity gains), personalized experiences, and reduced MTTR in operations, with adoption projected to jump 327% by 2027. The market, part of broader AI, sees agentic AI driving $4.4T in value, but faces challenges like 40% project cancellations by 2027 due to costs and risks.\u003c/p\u003e","title":"Human-Mediated Agentic Workflows"},{"content":"Understanding the Business of Search Ads Work in progress for understanding the search ads business.\nDefining the Business Search ads appear on Search Engine Results Pages (SERPs) for keyword queries, part of Pay-Per-Click (PPC) marketing. Ads display based on bids, at top/bottom of results or alongside organics. This auction model uses bid, quality, and relevance for placement. It connects advertisers to high-intent users, charging per click for engagement-based efficiency.\nAdvertisements Text ads include headlines, descriptions, site links; extensions add calls or locations. Formats like shopping ads feature images/prices. Revenue efficiency uses Click-Through Rate (CTR) (impressions to clicks), Cost Per Click (CPC) (cost per click), Return on Ad Spend (ROAS) (revenue/ad spend). High CTR (e.g., 6% in dating) shows relevance; ROAS \u0026gt;4:1 signals e-commerce success. Metrics guide optimization for lower costs, higher conversions.\n","permalink":"https://chenterry.com/archived/search_advertising/","summary":"\u003ch2 id=\"understanding-the-business-of-search-ads\"\u003eUnderstanding the Business of Search Ads\u003c/h2\u003e\n\u003cp\u003eWork in progress for understanding the search ads business.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eSearch ads appear on Search Engine Results Pages (SERPs) for keyword queries, part of Pay-Per-Click (PPC) marketing. Ads display based on bids, at top/bottom of results or alongside organics. This auction model uses bid, quality, and relevance for placement. It connects advertisers to high-intent users, charging per click for engagement-based efficiency.\u003c/p\u003e\n\u003ch4 id=\"advertisements\"\u003eAdvertisements\u003c/h4\u003e\n\u003cp\u003eText ads include headlines, descriptions, site links; extensions add calls or locations. Formats like shopping ads feature images/prices. Revenue efficiency uses Click-Through Rate (CTR) (impressions to clicks), Cost Per Click (CPC) (cost per click), Return on Ad Spend (ROAS) (revenue/ad spend). High CTR (e.g., 6% in dating) shows relevance; ROAS \u0026gt;4:1 signals e-commerce success. Metrics guide optimization for lower costs, higher conversions.\u003c/p\u003e","title":"Search Advertisement"},{"content":"Opportunity Costs It\u0026rsquo;s never easy to discover that a product you\u0026rsquo;ve poured your heart, sweat, and tears into isn\u0026rsquo;t working out. Startups operate in constant ambiguity, and sometimes you can\u0026rsquo;t see light at the end of the tunnel after toiling away for what feels like an eternity. Sometimes there simply is no light.\nI\u0026rsquo;ve heard the phrase \u0026ldquo;Take more market risk when you are young, and more execution risk when you are older.\u0026rdquo; As I understand it, this suggests that people early in their careers should bet on markets and opportunities, even contrarian ones. I\u0026rsquo;m reflecting on this because I\u0026rsquo;ve been thinking deeply about how to best allocate my time and energy on the most promising projects. This isn\u0026rsquo;t about diversifying—I recognize my limited attention span, and pursuing everything simultaneously leads to burnout and mediocre results. Hence this post: an attempt to provide clarity.\nThe first sunsetted products - Cogno (Multi-agent Sales Assistant) and Marrrket (AI Enabled Secondhand Marketplace)\nAs AI tools mature, I\u0026rsquo;ve accelerated my shipping velocity dramatically. However, speed doesn\u0026rsquo;t guarantee success. Over the past two years, I\u0026rsquo;ve launched five products: Cogno (Cognogpt.com), Marrrket (Marrrket.com), Crowdlistening (Crowdlistening.com), and A2A Catalog (a2acatalog.com). I also helped ship three 0-1 products: Symphony Assistant (https://ads.tiktok.com/business/copilot/standalone), Insights Spotlight (https://ads.tiktok.com/business/en-US/blog/insights-spotlight-trends-tool), and Aibrary (aibrary.ai). Among these products, some worked, while some didn\u0026rsquo;t. Some worked, others didn\u0026rsquo;t. For transparency, I\u0026rsquo;ll focus only on projects I directly led.\nMarketing matters enormously, but converting traffic into revenue remains the critical challenge. Amid AI hype, one differentiation strategy involves controversy and viral marketing, as demonstrated by Cluely. While we await that outcome, Crowdtest.ai offers a retrospective case study. Founded by a freshman with significant Twitter following, Crowdtest.ai claimed $30k revenue within 24 hours of launch (with generous refund policies). Despite initial buzz, it maintained only ~1,500 MAU two months post-launch, with declining metrics since. This wasn\u0026rsquo;t success—the value proposition remained unclear. Who pays $100 to optimize Twitter posts? Does AI genuinely outperform human intuition here?\nCrowdtest.ai - A case study in viral marketing\nI replicated this approach for Crowdlistening\u0026rsquo;s predict feature, inspired by a16z\u0026rsquo;s social simulation investment thesis. The concept of agent-based testing grounds for ideas seemed compelling, but too many hypotheses remained unvalidated. Agents likely represent generalizations of language (as do human crowds), and the willingness-to-pay question persisted. While not my proudest feature launch, it provided valuable learning. Key lessons: build genuine Twitter following and scrutinize marketing claims—data reveals truth.\nEarly Catalog Products The \u0026ldquo;app store for everything\u0026rdquo; phenomenon continues proliferating. FlowGPT exemplifies 2022 success: an early Discord community that evolved into a digital prompt catalog, now perhaps hosting agent experiences. They maintain over two million monthly active users—an impressive achievement. Timing proved crucial: right product, early market entry, sufficient success factors (though monetization remains unclear). Building similar products today faces significantly higher competition given ChatGPT\u0026rsquo;s November 2022 launch impact.\nWebsite traffic of FlowGPT (June 2025)\nThe A2A Agents catalog concept originated from domain name exploration. I sought high-value MCP-related domains, assuming their long-term value, but discovered all premium options were taken. A2A Agents seemed the logical alternative. My reasoning: (1) .com domains retain maximum value, (2) I could compile the web\u0026rsquo;s most comprehensive A2A Agents directory, and (3) adding MCP Servers would prove easier than adding A2A Agents, given the agent-tool hierarchy. These hypotheses largely hold true, though I underestimated competitive scale and pace. I\u0026rsquo;ve observed at least five competing catalog websites that, despite lacking premium domain names, have compiled impressive directories.\nThis remains an active project, but I\u0026rsquo;ll maintain complete transparency about competitive dynamics. Revisiting my three assumptions: (1) While a2acatalog.com offers excellent branding, domain alone cannot capture and retain users—aggressive SEO optimization and content marketing remain essential (areas where I\u0026rsquo;m still developing expertise). Alternative channels like TikTok, Reddit, and Twitter seem suboptimal for non-viral content. (2) I can efficiently compile agent and server lists, but so can competitors. If sufficient profit exists, competitors will inevitably crawl catalogs and replicate content. Differentiation must focus on user experience—either simplified deployment for non-technical users or hyper-personalization for technical ones. (3) While I can integrate MCP servers, it\u0026rsquo;s premature to establish lasting moats, connecting to point two.\nMCP Server integration example\nI\u0026rsquo;m curious about catalog product differentiation strategies. I\u0026rsquo;ve observed websites with 150k+ traffic, though revenue models appear suboptimal. For a2acatalog.com and future products, my goal is either maximum reach or niche vertical dominance enabling consistent revenue streams. The latter requires entering verticals with marketing potential—not general-purpose platforms, but solutions addressing concrete user needs. For maximum reach, significant progress remains necessary.\nGeographic distribution of catalog users\nWork in progress, thoughts in process. (6.25) Analyzing sunsetted products reveals a pattern: nearly all failed due to customer acquisition challenges. This insight proves illuminating. Successful product launches demonstrate that the best product doesn\u0026rsquo;t always win—distribution channels do. Perhaps I should prioritize developing distribution channels, using products as traffic capture mechanisms.\nEarly Products in General Recently surveying agent products reveals persistent patterns—multi-agent systems for various applications, agentic experiences across domains. As markets have evolved from viewing agents as chatbots (2022) to potential employees, I question whether I should have persisted with certain projects and how to rapidly test ideas for advancing domain understanding.\n[ Reusing a Previous Blog Post] Reflecting on 2024 (and 2023 for the context of Cogno), among my list of failed projects, very few failed due to lack of innovation. Since I began working with LLMs in fall 2022, there has been an abundance of interesting GenAI technologies to experiment with. It started with “domain specific prompting/finetuning” and data flywheels (thou not even now does anyone know what this looks like in action). By spring 2023, the focus shifted to LLMs as agents, exemplified by the Generative Agents paper, Microsoft AutoGen, and a few opensource projects like MetaGPT. At Cogno, we also built multi-agent systems, integrating various function calling features and agent collaboration for complex task reasoning. Everyone built, few created value (Glean focused on enterprise search, while Moveworks created value through api actions, neither of which I believe agents to have mattered). Founders encouraged each other’s enthusiasm, while investors rushed to learn the latest buzzwords in LLM technology (‘prompt engineering’ and ‘function calls’ sounded less sexy compared to’agents’).\nBeing first to market rarely matters - people won’t remember you. What matters is creating defensible moats or developing critical elements that lead to unfair advantages. While Google’s technology investment in Android can be considered ’not just building a moat, but scorching the earth for 250 miles around the castle,’ most companies’ self-described technological differentiation is merely self-flattery and a feeble attempt to impress tech-enthusiast investors. Technology truly matters only when it can solve seemingly insurmountable challenges or optimize costs and operations. In every other situation, the focus should be on building sustainable advantages that ensure long-term survival. [Thoughts WIP]\nThoughts Going Forward I\u0026rsquo;m intrigued by results-based monetization models. We began with tokens (chat products)—measuring LLM word output—then shifted to conversations (Copilot) as intent interpretation and generation quality improved. Next evolution: charging based on results? Exa and Sierra are already proposing this, though significant improvements remain necessary. What happens when heavily filtered queries return no results? Extensive compute goes uncompensated—is this sustainable? Similarly, for MCPs and A2As, when users engage with agents through third-party clients, who receives payment? An interesting market system awaits development here.\n","permalink":"https://chenterry.com/posts/when-to-sunset-a-product/","summary":"\u003ch2 id=\"opportunity-costs\"\u003eOpportunity Costs\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s never easy to discover that a product you\u0026rsquo;ve poured your heart, sweat, and tears into isn\u0026rsquo;t working out. Startups operate in constant ambiguity, and sometimes you can\u0026rsquo;t see light at the end of the tunnel after toiling away for what feels like an eternity. Sometimes there simply is no light.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve heard the phrase \u0026ldquo;Take more market risk when you are young, and more execution risk when you are older.\u0026rdquo; As I understand it, this suggests that people early in their careers should bet on markets and opportunities, even contrarian ones. I\u0026rsquo;m reflecting on this because I\u0026rsquo;ve been thinking deeply about how to best allocate my time and energy on the most promising projects. This isn\u0026rsquo;t about diversifying—I recognize my limited attention span, and pursuing everything simultaneously leads to burnout and mediocre results. Hence this post: an attempt to provide clarity.\u003c/p\u003e","title":"When to Sunset a Product: A Decision Framework for Entrepreneurs"},{"content":"Every now and then I come across some article or discussion that just feels plain and mundane. All the words seem to make sense, yet at the same time, they feel almost predictable. Despite how well articulated these ideas were - be it in carefully formatted slide decks or confidently delivered proses - they fail to amaze. Ever since November of 2022, the ability to articulate words cohesively (I\u0026rsquo;m purposefully not using the word coherently) has become table stakes. In a society where frankly most work is evaluated on completion and length, LLMs have led to a rapid advancement of productivity. Yet I think we should make certain clarifications here - productivity gain is in automating repetitive and redundant tasks, this does not apply to all tasks, in fact, using GPT for sophisticated reasoning is almost guarenteed to produce mediocore results.\nLet\u0026rsquo;s admit it, a big chunck of the work we do everyday some one else can do. The tedius, repetitive, and standard operating procedure tasks don\u0026rsquo;t require drastic innovation, they just need a criteria to be evaluated on and human hours, lots of it. This is work that AI could automate. However, an issue I\u0026rsquo;ve been seeing recently is people using AI as a catch all for tasks that should involve a level of reasoning and for a lack of better word, taste. Product managers go asking LLMs for user pain points, product features, and even feedback for products. However, the thing to note here is that a LLM is probablistic - it\u0026rsquo;s trained on generalization - when you build for all, you build for none. This is why I caution my self and take a step back each time an LLM produces a lengthy blob of text that I don\u0026rsquo;t see obvious issues with through the first run - do I have enough knowledge in the field to have good taste?\nThis to me is a fascinating topic. Although I have some ideas of how to best use LLMs. I now ask my self to read more before formulating a response. At worse, this would be developing enough text corpus to develop probablistic predictions. At best, I\u0026rsquo;d even be able to reason and build on some good ideas and push the field a bit further. Here are some of the papers / books I plan to be reading in the coming weeks:\nMagic Link - Information Software and the Graphical Interface by Bret Victor Man-Computer Symbiosis by J.C.R Licklider Augmenting Human Intellect: A Conceptual Framework by D.C. Engelbart. 6.25 Something I\u0026rsquo;ve just recently started to acknowledge (I\u0026rsquo;ve heard this repeated many times, but it sort of just sunk in today) is that we are actually fast approaching a period where ai moves beyond the traditional \u0026ldquo;copilot\u0026rdquo; and human-in-the-loop dynmaic, moving to fully autonomous teams capable to aligning goals and executing multi-step tasks over a long horizon. This would mean a dramatic shift in our workforce, where we would actually have a significant portion of workers be non-human entities. The industrial revolution and rise of the internet has led to specialization in the work we do: for internet products, we\u0026rsquo;d have software engineers (Research, QA, ML) , PMs, Designers, Marketers, but I see the line to become increasingly blurred as we progress.\n","permalink":"https://chenterry.com/posts/poor_thinking/","summary":"\u003cp\u003eEvery now and then I come across some article or discussion that just feels plain and mundane. All the words seem to make sense, yet at the same time, they feel almost predictable. Despite how well articulated these ideas were - be it in carefully formatted slide decks or confidently delivered proses - they fail to amaze. Ever since November of 2022, the ability to articulate words cohesively (I\u0026rsquo;m purposefully not using the word coherently) has become table stakes. In a society where frankly most work is evaluated on completion and length, LLMs have led to a rapid advancement of productivity. Yet I think we should make certain clarifications here - productivity gain is in automating repetitive and redundant tasks, this does not apply to all tasks, in fact, using GPT for sophisticated reasoning is almost guarenteed to produce mediocore results.\u003c/p\u003e","title":"Using LLMs as a Cover up for Poor Thinking"},{"content":"Introduction We live in an era of unprecedented access to information. The web contains almost all the knowledge needed to complete virtually any task, yet many of us still struggle to learn effectively. Our ability to ask the right questions has become the limiting factor in unlocking knowledge acquisition. This fundamental shift is transforming how we learn, build expertise, and might revolutionize education itself.\nThe Traditional Knowledge Landscape Historically, human conversations have been the default method of acquiring knowledge. We seek out doctors for medical advice, mechanics for car problems, and teachers for academic subjects. These experts are valuable not just for their knowledge, but for their ability to understand questions we may not be able to formulate ourselves.\nWhat makes human-to-human teaching so effective is an expert\u0026rsquo;s ability to address our \u0026ldquo;unknown unknowns.\u0026rdquo; A good teacher has seen countless students facing similar challenges and can explain concepts at the appropriate level of understanding. They can conceptualize and respond to gaps in knowledge that students themselves might not recognize.\nThe Changing Nature of Expertise The traditional path to expertise has been structured and comprehensive. School builds foundational knowledge across multiple disciplines—calculus, linear algebra, and other fundamentals that eventually lead to specialized topics like machine learning or language models.\nHowever, a new paradigm is emerging. With language models (and the theoretical ability to ask perfect questions), one could develop targeted slices of knowledge directly related to specific tasks—whether building an electric boat, a rocket, or any other complex project. This approach enables a more direct path to practical knowledge.\nSchool still offers invaluable benefits beyond the curriculum itself. It creates structure, provides community, and compels us to explore topics we might otherwise avoid. It exposes us to applications we wouldn\u0026rsquo;t have discovered independently. But the rigid structure has limitations in an age of personalized learning.\nHistorical Barriers to Self-Directed Learning What has prevented us from simply learning everything we need from the web? Several key barriers have existed:\nInstitutional confinement: Knowledge was traditionally locked within institutions, requiring physical presence to access resources and expertise.\nContent variability: Even with open courseware, YouTube, and similar platforms, not every lecture or video guarantees the information you specifically need.\nDifficulty calibration: Content creators don\u0026rsquo;t know your expertise level. When material underestimates your knowledge, it becomes boring. When it overestimates your background, you risk getting lost and losing motivation.\nThe fundamental question becomes: Can we design systems that meet learners where they are?\nThe Potential of Question-Driven Learning Language models have compiled vast amounts of knowledge (excluding proprietary company data). However, this knowledge isn\u0026rsquo;t truly at our fingertips because:\nWe don\u0026rsquo;t always know what to ask Systems aren\u0026rsquo;t familiar enough with us as individuals to present information in easily digestible ways Theoretically, driven by precisely calibrated questions with adequate depth and breadth, we could achieve the quickest mastery of the minimal knowledge needed to complete any task. The ability to integrate this capability into our learning process will differentiate those who can ride this wave of knowledge transformation.\nConclusion This transformation creates tremendous opportunities for both individuals and companies. For learners, developing the skill to ask excellent questions becomes as valuable as the knowledge itself. For companies, there\u0026rsquo;s an opportunity to build products that facilitate this question-driven, personalized learning approach.\nAs we move forward, the limiting factor in knowledge acquisition will increasingly be our ability to ask the right questions rather than access to information itself. Those who master the art of asking will unlock potential beyond what traditional educational systems could provide.\n","permalink":"https://chenterry.com/posts/questions-new-bottleneck-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe live in an era of unprecedented access to information. The web contains almost all the knowledge needed to complete virtually any task, yet many of us still struggle to learn effectively. Our ability to ask the right questions has become the limiting factor in unlocking knowledge acquisition. This fundamental shift is transforming how we learn, build expertise, and might revolutionize education itself.\u003c/p\u003e\n\u003ch2 id=\"the-traditional-knowledge-landscape\"\u003eThe Traditional Knowledge Landscape\u003c/h2\u003e\n\u003cp\u003eHistorically, human conversations have been the default method of acquiring knowledge. We seek out doctors for medical advice, mechanics for car problems, and teachers for academic subjects. These experts are valuable not just for their knowledge, but for their ability to understand questions we may not be able to formulate ourselves.\u003c/p\u003e","title":"Questions as the New Bottleneck in Learning"},{"content":"In a world dominated by expert opinions and algorithm-driven content, there\u0026rsquo;s something fundamentally human about wanting to know what others think. Whether we admit it or not, we\u0026rsquo;re drawn to understand the collective mindset.\nThere\u0026rsquo;s wisdom in crowds. While large groups may not always converge on absolute truths (in fact, many truthful views begin as contrarian positions), they provide something equally valuable: comfort and context. Being part of a group, understanding its thoughts and values, creates a sense of safety and belonging that\u0026rsquo;s deeply wired into our social nature. Even when we disagree with mainstream opinions, understanding them helps us navigate social landscapes and provides reference points for our own thinking. This isn\u0026rsquo;t mere conformity—it\u0026rsquo;s about contextualizing our experiences within the broader human narrative.\nOur information ecosystem has evolved in two problematic directions. On one side, mainstream media delivers curated \u0026ldquo;expert views\u0026rdquo; that often miss nuance. On the other, recommendation algorithms trap us in personalized echo chambers that reinforce existing beliefs.\nWhat\u0026rsquo;s missing? The authentic, unfiltered perspective of the crowd.\nComment sections, forums, and face-to-face conversations provide windows into what people actually think—unmediated by gatekeepers or algorithms. These spaces, though sometimes chaotic, offer genuine insights that both experts and algorithms frequently miss.\nThis is where Crowdlistening enters the picture. Rather than filtering out the noise of crowd perspectives, Crowdlistening aims to extract meaningful patterns and insights from collective thought. It\u0026rsquo;s about amplifying voices without homogenizing them. By understanding what people collectively think—their concerns, insights, and experiences—we can build products, services, and communities that truly resonate. The crowd isn\u0026rsquo;t always right, but it\u0026rsquo;s always worth listening to.\nWhen we learn to listen to crowds effectively, we gain access to a type of distributed intelligence that no single expert or algorithm can match. In our increasingly fragmented information landscape, this skill becomes not just valuable but essential.\n","permalink":"https://chenterry.com/posts/crowd_thesis/","summary":"\u003cp\u003eIn a world dominated by expert opinions and algorithm-driven content, there\u0026rsquo;s something fundamentally human about wanting to know what others think. Whether we admit it or not, we\u0026rsquo;re drawn to understand the collective mindset.\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s wisdom in crowds. While large groups may not always converge on absolute truths (in fact, many truthful views begin as contrarian positions), they provide something equally valuable: comfort and context. Being part of a group, understanding its thoughts and values, creates a sense of safety and belonging that\u0026rsquo;s deeply wired into our social nature. Even when we disagree with mainstream opinions, understanding them helps us navigate social landscapes and provides reference points for our own thinking. This isn\u0026rsquo;t mere conformity—it\u0026rsquo;s about contextualizing our experiences within the broader human narrative.\u003c/p\u003e","title":"Why People Care What Others Think"},{"content":"The Evolution of AI Value The first wave of generative AI focused primarily on content creation - ChatGPT writing articles, Midjourney generating images, essentially replacing traditional production roles. However, as these technologies mature, their greatest value might well shift towards distribution and personalization rather than raw production.\nFrom RSS to Recommender Systems The evolution of content distribution reveals how technology repeatedly transforms information access. RSS (Really Simple Syndication) represented an early attempt to solve content discovery, providing a pull-based system where users subscribed to feeds they cared about.\nPersonal Note: I interned at China Impact Investing Network (CINN), down the road from Huangzhuang, earning 100 RMB daily for translation and RSS-related tasks - work that would now be largely automated by GPT.\nAs content volume exploded, the focus shifted to algorithmic distribution through recommender systems. These attempted to match existing content to user preferences through increasingly sophisticated methods, but still fundamentally operated on a \u0026ldquo;create once, distribute many times\u0026rdquo; model.\nContent Creation vs. Distribution Costs The economics of content have always been defined by the balance between creation and distribution costs, as illustrated in our visualization. Traditional models rely on two fundamentally different approaches:\nProfessional vs. User-Generated Content Economics Traditional PGC platforms invest heavily in upfront content creation ($200-500 per article) while optimizing distribution costs ($0.001 per user). This creates an economic model where high-quality, centrally produced content must reach massive scale to be sustainable. Users remain passive consumers with minimal influence on content direction.\nUGC platforms invert this model by outsourcing creation costs to users while investing primarily in discovery infrastructure. This creates greater diversity but inconsistent quality. Both approaches increasingly allocate resources to distribution rather than creation as competition for attention intensifies.\nThe discovery paradox emerges: as content volume increases, the marginal value of new content approaches zero without effective discovery mechanisms. Users face decision fatigue from too many choices, and the market naturally shifts investment toward discovery rather than production.\nGeneration as Distribution: Personalized Content at Scale AI fundamentally changes this paradigm by collapsing the distinction between production and distribution. When content can be generated at the moment of consumption, personalized for each user, the model shifts from:\nTraditional: Create once → Distribute to millions\nGenerative: Create parameters → Generate millions of variations\nThe dual-axis economics chart reveals this transformation. Traditional content scales efficiently after high initial investment but delivers mediocre value. Generative approaches provide dramatically higher user value but face linearly increasing costs that become prohibitive at scale.\nThe optimal approach emerges through content modularity - recognizing that not everything needs regeneration. By identifying which components provide the most personalization value, hybrid approaches can maintain 80% of personalization benefits at 30% of the cost.\nThe 60/20/20 Rule: Strategic Content Modularity Most knowledge domains contain substantial core material that remains consistent across users. The 60/20/20 rule maximizes efficiency by segmenting content into:\n60% static core content (foundational principles, established facts) 20% cohort-level content (industry examples, cultural contexts) 20% individual personalization (connections to personal experience, learning pace) This approach creates a fundamentally different economic curve that scales more efficiently than traditional content while maintaining most personalization benefits. For a business book distributed to 50,000 professionals, this approach can deliver twice the relevance at the same cost as traditional publishing.\nMulti-modal Personalization: Beyond Text The personalization paradigm extends beyond text to encompass multiple modalities. Content can dynamically transform between formats - text summaries becoming virtual presenter videos, complex topics converting to interactive explanations, news transforming into personalized audio briefings. This multi-modal capability increases generation costs but dramatically enhances engagement and information retention.\nThese cross-modal transformations add approximately 30-40% to generation costs but can increase engagement by 200-300%, creating compelling economics despite the higher production expense. Each user\u0026rsquo;s preferred learning style becomes another dimension for personalization.\nThe User Advocate: Beyond Algorithmic Recommendation The most powerful personalization emerges not from content formatting but from deeper user understanding. The User Advocate concept represents an AI persona that truly comprehends the user\u0026rsquo;s interests, knowledge level, and perspective, then guides content creation accordingly.\nUnlike recommendation systems that rely on sparse signals, the Advocate builds a comprehensive user model through conversation and observation. This enables exploration of \u0026ldquo;unknown unknowns\u0026rdquo; - valuable topics users didn\u0026rsquo;t know to search for. The approach fundamentally changes platform economics by aligning incentives with actual user satisfaction rather than engagement metrics.\nFluid Knowledge and the Future The most significant impact of AI lies not in replacing content creators but in transforming how knowledge flows to individuals. As the internet solved information scarcity, generative AI now solves the problem of relevance through \u0026ldquo;fluid knowledge\u0026rdquo; (知识液化) that adapts perfectly to each person\u0026rsquo;s context.\nIn this emerging paradigm, content becomes transformable across formats, users experience the feeling of being truly understood, and exploration replaces search as the primary discovery model. The User Advocate becomes a critical interface between vast information spaces and human understanding, fundamentally changing our relationship with knowledge acquisition.\n","permalink":"https://chenterry.com/posts/generation_distribution/","summary":"\u003ch2 id=\"the-evolution-of-ai-value\"\u003eThe Evolution of AI Value\u003c/h2\u003e\n\u003cp\u003eThe first wave of generative AI focused primarily on content creation - ChatGPT writing articles, Midjourney generating images, essentially replacing traditional production roles. However, as these technologies mature, their greatest value might well shift towards distribution and personalization rather than raw production.\u003c/p\u003e\n\u003ch2 id=\"from-rss-to-recommender-systems\"\u003eFrom RSS to Recommender Systems\u003c/h2\u003e\n\u003cp\u003eThe evolution of content distribution reveals how technology repeatedly transforms information access. RSS (Really Simple Syndication) represented an early attempt to solve content discovery, providing a pull-based system where users subscribed to feeds they cared about.\u003c/p\u003e","title":"Value Add of AI: Generation as Distribution"},{"content":"SEO Guide: Implementation \u0026amp; Best Practices Table of Contents Search Engine Basics Technical Implementation On-Page Optimization Off-Page Strategies Modern Approaches Analytics \u0026amp; Tools What is SEO? Search Engine Optimization improves website visibility in organic search results. Three core processes determine rankings:\nCrawling Process and Timeframes Search engines discover pages by following links. New websites typically take 4-6 weeks for complete indexing. Factors affecting speed include site structure, server response time, and internal linking. The more efficiently your site is structured, the faster search engines can discover and index your content.\nWebsite builders handle this process differently. Wix automatically submits sitemaps to Google with pages typically indexed within 2-4 weeks. Squarespace generates and submits sitemaps automatically as well, usually getting indexed within 1-3 weeks. WordPress requires manual submission through Search Console or plugins like Yoast SEO. Shopify automatically submits sitemaps but benefits from manual Search Console verification for faster indexing.\nTo accelerate the crawling process, ensure your robots.txt file allows crawlers access to important pages, implement thorough internal linking connecting your important pages, and use Google Search Console to request indexing for priority pages. These techniques create clearer paths for search engine bots to discover and process your content efficiently.\nIndexing Process After crawling, search engines store page information in their index. They primarily index text and HTML elements, plus structured data, image alt text, and metadata. JavaScript content requires special handling for proper indexing, often needing client-side rendering to be properly processed. Search engines analyze both the content itself and its context within your site architecture.\nFor optimal indexing, use keyword-rich title tags that accurately describe page content, create compelling meta descriptions that encourage clicks, structure content with semantic HTML that signals content hierarchy, and provide descriptive image alt text for visual elements. Keeping important content in HTML rather than embedded in JavaScript improves indexing reliability and completeness.\nRanking Factors That Matter Content relevance and quality serve as fundamental ranking signals, with comprehensive content addressing user search intent receiving preference. Search engines analyze how thoroughly content answers likely user questions and provides value beyond basic information. The depth and originality of content directly influence ranking potential across competitive keywords.\nBacklinks from authoritative sources act as trust signals, with quality links from relevant, authoritative sites carrying significant ranking weight. The overall link profile, including diversity of sources and naturalness of acquisition, influences domain authority and ranking capability. Strategic link building focusing on relevance over quantity produces sustainable ranking improvements.\nUser experience metrics, including page speed, mobile-friendliness, and intuitive navigation, increasingly impact rankings through Core Web Vitals and other engagement signals. Search engines monitor how users interact with search results, factoring metrics like bounce rate and time-on-site into ranking decisions. Sites providing seamless, helpful experiences across devices generally outperform those with technical or usability issues.\nOn-page optimization through strategic keyword usage in titles, headings, and content helps search engines understand relevance and topic focus. The intelligent use of schema markup, HTTPS security, and proper crawlability provides additional signals that influence ranking decisions across competitive search landscapes.\nTechnical SEO Implementation Implementing a Proper HTML Structure \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;!-- Critical SEO elements --\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Primary Keyword - Secondary Keyword | Brand Name\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;A compelling 150-160 character description that includes keywords and encourages clicks.\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;https://www.example.com/page-url/\u0026#34;\u0026gt; \u0026lt;!-- Structured Data --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Article\u0026#34;, \u0026#34;headline\u0026#34;: \u0026#34;Article Title\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Author Name\u0026#34; }, \u0026#34;datePublished\u0026#34;: \u0026#34;2025-05-01T08:00:00+08:00\u0026#34; } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Semantic HTML Structure --\u0026gt; \u0026lt;header\u0026gt; \u0026lt;nav\u0026gt; \u0026lt;a href=\u0026#34;/about/\u0026#34;\u0026gt;About Us\u0026lt;/a\u0026gt; \u0026lt;!-- Additional navigation links --\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;main\u0026gt; \u0026lt;!-- Proper heading hierarchy --\u0026gt; \u0026lt;h1\u0026gt;Main Page Title with Primary Keyword\u0026lt;/h1\u0026gt; \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;Subheading with Related Keywords\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Content with natural keyword usage. Avoid keyword stuffing and focus on providing value to users.\u0026lt;/p\u0026gt; \u0026lt;!-- Internal linking --\u0026gt; \u0026lt;h3\u0026gt;Another Sub-section\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;More content with internal links to \u0026lt;a href=\u0026#34;/related-page/\u0026#34;\u0026gt;related pages\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;!-- Image optimization --\u0026gt; \u0026lt;img src=\u0026#34;image.jpg\u0026#34; alt=\u0026#34;Descriptive alt text with keywords if relevant\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;600\u0026#34;\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;!-- Footer links and content --\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; XML Sitemaps: Creation and Submission What Are Sitemaps?\nXML files that list all important URLs on your website Help search engines discover and crawl your content Include metadata about pages (last updated, change frequency, priority) Creating a Sitemap:\nAutomated Tools:\nWordPress plugins (Yoast SEO, Rank Math) Online generators (XML-Sitemaps.com) CMS features (Shopify, Wix automatically generate sitemaps) Manual Creation for Small Sites:\n\u0026lt;!-- Basic sitemap structure (simplified) --\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34;\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://example.com/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2025-05-01\u0026lt;/lastmod\u0026gt; \u0026lt;priority\u0026gt;1.0\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;!-- Add more URLs as needed --\u0026gt; \u0026lt;/urlset\u0026gt; Submitting Your Sitemap to Google:\nVia Google Search Console:\nCreate/verify your property in Search Console Navigate to \u0026ldquo;Sitemaps\u0026rdquo; section Enter your sitemap URL (typically \u0026ldquo;sitemap.xml\u0026rdquo;) Click \u0026ldquo;Submit\u0026rdquo; Via robots.txt:\n# Add to robots.txt file Sitemap: https://example.com/sitemap.xml Direct Indexing Request: Enter your sitemap URL directly in a browser Use Search Console\u0026rsquo;s URL Inspection for important pages Sitemap Best Practices:\nKeep under 50,000 URLs and 50MB per sitemap file Use sitemap index files for larger sites Update when adding significant content Only include canonical, indexable URLs Submitting Your Site to Google The process of submitting your site to Google when building a website yourself:\nCreate and Verify Google Search Console Property:\nGo to https://search.google.com/search-console Add your property (domain or URL prefix) Verify ownership through: HTML tag (add to section) DNS record (add to domain settings) HTML file upload (to root directory) Google Analytics linking (if already set up) Initial Indexing Methods:\nSubmit your sitemap via Search Console Create and share high-quality backlinks Interlink your content properly Share new content on social media (creates crawl paths) Monitor Initial Indexing:\nCheck Coverage reports in Search Console Use URL Inspection tool for specific pages Set up Google Alerts for your domain Implementing Structured Data Schema.org markup helps search engines understand your content better:\n\u0026lt;!-- Product schema example --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Example Product Name\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;https://example.com/photos/product.jpg\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is an example product description.\u0026#34; } \u0026lt;/script\u0026gt; \u0026lt;!-- Additional product details --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;brand\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Brand\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Example Brand\u0026#34; }, \u0026#34;offers\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Offer\u0026#34;, \u0026#34;price\u0026#34;: \u0026#34;99.99\u0026#34;, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;availability\u0026#34;: \u0026#34;https://schema.org/InStock\u0026#34; } } \u0026lt;/script\u0026gt; Implementing Technical SEO with JavaScript // Example 1: Lazy loading for images document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { const lazyImages = document.querySelectorAll(\u0026#34;img.lazy\u0026#34;); if (\u0026#34;IntersectionObserver\u0026#34; in window) { const imageObserver = new IntersectionObserver(function(entries, observer) { entries.forEach(function(entry) { if (entry.isIntersecting) { const image = entry.target; image.src = image.dataset.src; image.classList.remove(\u0026#34;lazy\u0026#34;); imageObserver.unobserve(image); } }); }); lazyImages.forEach(function(image) { imageObserver.observe(image); }); } }); // Example 2: Performance optimization for external resources const head = document.getElementsByTagName(\u0026#39;head\u0026#39;)[0]; const preconnectLinks = [ \u0026#39;https://fonts.googleapis.com\u0026#39;, \u0026#39;https://cdn.example.com\u0026#39; ]; preconnectLinks.forEach(url =\u0026gt; { const linkElement = document.createElement(\u0026#39;link\u0026#39;); linkElement.rel = \u0026#39;preconnect\u0026#39;; linkElement.href = url; head.appendChild(linkElement); }); On-Page SEO Factors Content \u0026amp; Keyword Strategy Finding target keywords requires using specialized tools like Google Keyword Planner, Ahrefs, or SEMrush to identify terms with appropriate search volume and competition levels for your site\u0026rsquo;s current authority. The research process should balance opportunity with realistic ranking potential, particularly for newer domains that may struggle against established competitors for high-volume terms.\nStrategic keyword placement remains essential for signaling relevance to search engines. Your primary keyword should appear in the title tag, H1 heading, URL structure, and within the first 100 words of content to establish topic focus early. Secondary keywords should be distributed throughout H2 and H3 subheadings and naturally incorporated in body content. The focus should remain on creating valuable, readable content rather than keyword stuffing, which can trigger penalties from search algorithms designed to detect manipulation.\nKeyword competition varies significantly by type, creating different opportunities based on your site\u0026rsquo;s current authority. Head terms consisting of single words typically offer high search volume but face extreme competition from established domains with substantial backlink profiles. Body keywords containing 2-3 word phrases present moderate volume with manageable competition levels for sites with some established authority. Long-tail keywords comprised of 4+ words show lower individual volume but much less competition, making them ideal starting points for newer websites.\nNew websites should strategically target long-tail keywords first to build topical authority and demonstrate relevance before expanding to more competitive terms as domain authority grows. Assessing competition requires analyzing current top-ranking pages for your target keywords. When search results are dominated by major brands with thousands of quality backlinks and extensive content depth, newer sites should choose less competitive alternatives that offer realistic ranking potential while building authority in their niche.\nPage Speed Optimization Image optimization plays a critical role in both page speed and layout stability metrics. Implementing responsive image solutions with modern formats like WebP (while providing appropriate fallbacks for browser compatibility) significantly reduces load times. Always specifying width and height attributes prevents layout shifts during loading that negatively impact user experience and Core Web Vitals scores. The loading=\u0026ldquo;lazy\u0026rdquo; attribute further improves performance by deferring off-screen images until they\u0026rsquo;re needed.\nCritical CSS implementation separates essential styling needed for above-the-fold content from non-critical styles that can load after initial rendering. This technique delivers dramatically faster perceived loading experiences by prioritizing visible content styling. Loading non-critical CSS asynchronously through the preload pattern prevents render-blocking while ensuring all styles eventually apply. Including a noscript fallback maintains functionality for users with JavaScript disabled, ensuring universal accessibility.\nJavaScript optimization through strategic deferral prevents render-blocking issues and improves time to interactive metrics. Creating a systematic approach to loading non-essential scripts only after core page content becomes usable significantly improves user experience. Implementing script loading prioritization ensures critical functionality appears first while deferring analytics, advertising, and enhancement scripts until after the initial page load completes. This balanced approach maintains functionality while optimizing loading sequence for performance.\nAds and SEO Balance Google\u0026rsquo;s ranking algorithms penalize sites with excessive advertisements, particularly those creating poor user experiences or pushing important content below the fold. The Page Experience update specifically targets intrusive interstitials and aggressive ad implementations that interfere with content consumption. Finding the appropriate balance between monetization and search performance requires implementing ads in ways that complement rather than dominate the user experience.\nImplementation best practices include lazy loading advertisements using the Intersection Observer API to defer ad loading until users scroll near their position. This approach dramatically improves initial page load metrics while still effectively monetizing content. Positioning ads strategically between content sections rather than interrupting paragraphs mid-thought preserves reading flow and engagement. Maintaining a reasonable ad-to-content ratio under 30% signals to both users and search engines that content value remains the primary focus.\nOngoing monitoring of Core Web Vitals metrics following ad implementation provides critical feedback on performance impact. The Search Console Core Web Vitals report offers insights into real-world user experience metrics across your site, highlighting any pages where ad implementation negatively affects performance thresholds. Conducting regular A/B testing with different ad placements and loading strategies helps identify optimal approaches that balance revenue needs with search performance requirements in your specific content niche.\nOff-Page SEO Factors Backlink Quality vs. Quantity Backlinks serve as \u0026ldquo;votes of confidence\u0026rdquo; in Google\u0026rsquo;s ranking algorithm. A single high-quality link typically outweighs numerous low-quality links. The key factors determining backlink quality include domain authority (the overall trustworthiness of the linking site), topical relevance (how closely related the linking site\u0026rsquo;s content is to yours), anchor text (descriptive text containing target keywords), and link placement (with in-content links carrying more value than those in footers or sidebars).\nGoogle identifies low-quality sites through several signals including excessive ads with minimal content, poor engagement metrics like high bounce rates, thin or duplicated content, technical issues and poor mobile experience, manipulative linking practices, and limited expertise signals. Understanding these factors helps explain why competitors often block backlinks to other sites in their industry through nofollow attributes—they\u0026rsquo;re preventing the passing of \u0026ldquo;link equity\u0026rdquo; that would help your rankings for competitive keywords.\nOutbound Linking Strategy Outbound links can benefit SEO when implemented properly by establishing topical relevance for search engines, building content credibility by citing authoritative sources, improving user experience with supplementary information, and creating opportunities for relationship building with other sites in your industry. The implementation should follow best practices for both standard reference links and sponsored content.\nThe most beneficial linking targets include academic institutions with .edu domains, government resources with .gov domains, established industry publications and research papers, Wikipedia for general information that doesn\u0026rsquo;t require specialized expertise, and complementary businesses that offer services related to but not directly competing with yours. This strategic approach to outbound links signals to search engines that your content is well-researched and connected to authoritative sources in your field.\nLink Building Tactics Content-driven link acquisition focuses on creating assets that naturally attract links from other websites. This includes developing original research studies, comprehensive guides, or unique tools that serve as reference points for others in your industry. Visual content like infographics or diagrams with embed codes can also generate links, as can publishing expert interviews with recognized industry figures who may share the content with their audiences.\nGuest posting remains effective when done properly by targeting relevant, quality publications in your field. The key is providing genuinely valuable content rather than thin promotional pieces, including natural contextual links where appropriate, and focusing on establishing expertise rather than maximizing link quantity. This approach builds sustainable link profiles that withstand algorithm updates.\nThe broken link building process involves finding resource pages in your industry, checking for broken links using tools like Ahrefs or Screaming Frog, creating replacement content for the broken resource, and contacting the site owner with a helpful offer to provide your content as a solution. This technique provides immediate value to the site owner while securing relevant backlinks for your domain.\nModern SEO Approaches AI Content Creation Pipeline Effective AI-to-SEO process begins with thorough topic research, including identifying target keywords and search intent, analyzing competitors\u0026rsquo; content structure and coverage, and mapping content to appropriate sales funnel stages. This research foundation guides the AI-assisted drafting phase, where tools like Claude, ChatGPT, or Coze generate initial content drafts incorporating proper heading structure and addressing common queries through FAQ sections.\nHuman enhancement represents the critical differentiating step, where experts add unique insights and examples, incorporate proprietary data or exclusive information, and ensure the content matches brand voice and depth requirements. This human expertise layer transforms basic AI output into valuable content that demonstrates genuine domain knowledge. The SEO optimization stage then involves formatting with proper HTML structure, implementing metadata and schema markup, and applying Core Web Vitals best practices to ensure technical excellence.\nThe final publication and indexing process includes submitting to Google Search Console, building strategic internal and external links, and establishing ongoing monitoring to refine content based on performance data. Search engines evaluate AI content based on demonstrated expertise and original insights rather than generic information, the value provided to readers through specific problem-solving, and clear E-E-A-T signals that establish credibility and authority within the subject matter.\nFor user-generated content (UGC) indexing, implementing proper HTML structure is essential. This includes using appropriate schema markup to signal content type and authorship, providing clear content structure with proper heading hierarchy, and establishing content transparency through source attribution and timestamps. These technical elements help search engines properly categorize and index content generated through AI assistants or user contributions.\nCore Web Vitals Optimization Core Web Vitals optimization focuses on three critical metrics that directly impact search rankings. Largest Contentful Paint (LCP) measures loading performance with a target under 2.5 seconds, improved by optimizing server response times, preloading essential resources, properly compressing and formatting images, and utilizing content delivery networks for static assets. This metric directly affects user perception of site speed and initial engagement.\nFirst Input Delay (FID) measures interactivity with a target under 100ms, enhanced by minimizing JavaScript execution time, breaking up long tasks into smaller processes, optimizing event handlers for efficiency, and employing web workers for complex background operations. This metric ensures users can interact with your page quickly after it visually appears loaded, reducing frustration and improving engagement metrics.\nCumulative Layout Shift (CLS) measures visual stability with a target under 0.1, improved by setting explicit dimensions for all media elements, reserving appropriate space for dynamic content before it loads, avoiding insertion of new content above existing elements, and using transform animations instead of layout-triggering ones. This metric prevents frustrating experiences where page elements move unexpectedly as the user attempts to interact with them.\nE-E-A-T Implementation Experience, Expertise, Authoritativeness, and Trustworthiness (E-E-A-T) signals have become central to quality content evaluation. Effective author credentials display includes professional photos, relevant educational qualifications and certifications, specific industry experience metrics, and professional organization memberships. These credentials establish the content creator\u0026rsquo;s qualification to address the topic comprehensively and accurately.\nContent transparency markers reinforce trustworthiness through clear publication and update timestamps, fact-checking attribution when appropriate, and sources for statistics or claims. Additional E-E-A-T enhancements include linking to authoritative external sources, incorporating original research and proprietary data, providing comprehensive rather than surface-level topic coverage, maintaining regular content updates, prominently displaying relevant credentials and certifications, and implementing appropriate schema markup for author and content type.\nThese E-E-A-T signals collectively build credibility with both users and search algorithms, particularly important for YMYL (Your Money, Your Life) topics where accuracy and trustworthiness carry even greater weight in ranking decisions. Implementing these elements systematically across your content creates a strong foundation for both current rankings and resilience against future algorithm updates.\nSEO Best Practices Technical SEO Checklist Implement HTTPS across your site Ensure mobile responsiveness Fix crawl errors and broken links Optimize page speed Create and submit XML sitemaps Implement proper redirects (301 for permanent, 302 for temporary) Fix duplicate content issues with canonical tags Implement proper hreflang tags for international sites Content SEO Checklist Conduct thorough keyword research Create comprehensive, high-quality content Optimize title tags and meta descriptions Use descriptive alt text for images Include internal links to relevant pages Update content regularly Implement schema markup where appropriate Monitoring and Analytics // Example of setting up custom SEO tracking in Google Analytics 4 // This could be implemented in your main.js or analytics.js file // Track outbound links document.addEventListener(\u0026#39;click\u0026#39;, function(e) { const link = e.target.closest(\u0026#39;a\u0026#39;); if (link \u0026amp;\u0026amp; link.hostname !== window.location.hostname) { gtag(\u0026#39;event\u0026#39;, \u0026#39;outbound_link_click\u0026#39;, { \u0026#39;destination\u0026#39;: link.href, \u0026#39;link_text\u0026#39;: link.innerText || link.textContent, \u0026#39;page_location\u0026#39;: window.location.href }); } }); // Track scroll depth let scrollDepthTracked = { \u0026#39;25\u0026#39;: false, \u0026#39;50\u0026#39;: false, \u0026#39;75\u0026#39;: false, \u0026#39;100\u0026#39;: false }; window.addEventListener(\u0026#39;scroll\u0026#39;, function() { const scrollPercent = Math.round((window.scrollY / (document.body.offsetHeight - window.innerHeight)) * 100); if (scrollPercent \u0026gt;= 25 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;25\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;25%\u0026#39;}); scrollDepthTracked[\u0026#39;25\u0026#39;] = true; } if (scrollPercent \u0026gt;= 50 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;50\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;50%\u0026#39;}); scrollDepthTracked[\u0026#39;50\u0026#39;] = true; } if (scrollPercent \u0026gt;= 75 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;75\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;75%\u0026#39;}); scrollDepthTracked[\u0026#39;75\u0026#39;] = true; } if (scrollPercent \u0026gt;= 90 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;100\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;100%\u0026#39;}); scrollDepthTracked[\u0026#39;100\u0026#39;] = true; } }); Tools and Resources Google Analytics \u0026amp; Search Console Google Search Console essentials:\nPerformance Report: Shows clicks, impressions, CTR, and rankings Coverage Report: Identifies indexing issues Mobile Usability: Flags responsiveness problems Core Web Vitals: Monitors user experience metrics Setup process:\n\u0026lt;!-- Verification meta tag for Search Console --\u0026gt; \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;YOUR_CODE\u0026#34; /\u0026gt; Optimization opportunities:\nFind queries driving traffic to create similar content Identify pages with high impressions but low CTR for title/description improvements Fix crawl errors promptly Monitor mobile experience issues Track Core Web Vitals compliance Google Analytics implementation:\n\u0026lt;!-- GA4 base installation --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;G-XXXXXXXX\u0026#39;); \u0026lt;/script\u0026gt; Key metrics to monitor:\nOrganic traffic sources and growth Landing page performance Conversion rates from organic visitors Engagement metrics (time on page, bounce rate) Exit pages and drop-off points Connect Analytics with Search Console to analyze which keywords drive not just traffic but meaningful engagement and conversions.\n","permalink":"https://chenterry.com/archived/guide_seo/","summary":"\u003ch1 id=\"seo-guide-implementation--best-practices\"\u003eSEO Guide: Implementation \u0026amp; Best Practices\u003c/h1\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#what-is-seo\"\u003eSearch Engine Basics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#implementing-a-proper-html-structure\"\u003eTechnical Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#on-page-seo-factors\"\u003eOn-Page Optimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#off-page-seo-factors\"\u003eOff-Page Strategies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#modern-seo-approaches\"\u003eModern Approaches\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tools-and-resources\"\u003eAnalytics \u0026amp; Tools\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"what-is-seo\"\u003eWhat is SEO?\u003c/h2\u003e\n\u003cp\u003eSearch Engine Optimization improves website visibility in organic search results. Three core processes determine rankings:\u003c/p\u003e\n\u003ch3 id=\"crawling-process-and-timeframes\"\u003eCrawling Process and Timeframes\u003c/h3\u003e\n\u003cp\u003eSearch engines discover pages by following links. New websites typically take 4-6 weeks for complete indexing. Factors affecting speed include site structure, server response time, and internal linking. The more efficiently your site is structured, the faster search engines can discover and index your content.\u003c/p\u003e","title":"A Practical Guide to SEO with Claude"},{"content":" Tobi Lutke\u0026rsquo;s Shopify internal memo\nSmaller and More Efficient Teams When should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\nCompared to late 2022, I\u0026rsquo;ve shifted more energy toward building and testing ideas directly, rather than hiring large teams to delegate to. Early-stage startups now need fewer product managers, especially when founders can build and validate ideas over a weekend. The marginal productivity increase associated with an extra headcount might now be lower than that of employing AI agent(s). In practice, this means smaller, more agile teams with outstanding capabilities from ICs. I believe the future belongs to ultra-small teams, for startups, this means any where from 2-5 people, with each member handling a job function while employing multiple AI agents to execute tasks. The benefits are clear: less time spent in meetings, quicker iterations, and the positive ripple effects of tight-knit teams. Rather than building large departments with specialized roles, companies can now assemble small, versatile teams augmented by AI capabilities. These teams can move faster and with greater autonomy than traditional corporate structures allow.\nAI Tools Enabling Rapid Prototyping Recent AI coding tools like Cursor have expedited the prototyping process, allowing developers (and non-technical people too) to quickly build functional MVPs in record time. These tools excel at generating boilerplate code, implementing common patterns, and even troubleshooting errors. A single engineer with Cursor can accomplish what previously required multiple developers working in tandem, or achieve in around one fifth the time they\u0026rsquo;d spend working on it alone.\nCollaboration workflows enabled by Figma, Credit to Kevin Kwok\nThis acceleration in prototyping parallels what we saw with Figma in design. As Kevin Kwok noted, \u0026ldquo;Tightening the feedback loop of collaboration allows for non-linear returns on the process\u0026rdquo; (Kwok, 2020). Just as Figma collapsed the barriers between designers and their collaborators, AI coding tools are now breaking down the technical barriers that previously separated product visionaries from implementation.\nWhile much of the attention has been on AI helping engineers be more efficient, I\u0026rsquo;m more interested in how its ripple effect on tangential roles - such as product managers and ui/ux designers. Just like how Figma allowed faster iteration cycles between designers and engineers/product managers, would this reduction in prototype testing allow for more innovative workflows where product people (with knowledge of engineering possibilities) could quickly test ideas and engineers focus more on the implementation of production scale systems?\nThis dynamic raises an interesting question for founders and engineering leaders: when does it make sense to hire additional team members versus investing in improving the AI capabilities of existing team members? The calculus now involves comparing the marginal cost of onboarding and training a new hire against the potential productivity gains from enhancing your current team\u0026rsquo;s AI workflow mastery.\nThe Rise of Product Engineers Software Engineers should become \u0026ldquo;Product Engineers\u0026rdquo; as LLMs have commoditized routine coding tasks. What\u0026rsquo;s truly valuable now are generalists who can code but also have an eye for UI/UX design, good product taste, and deep market understanding. Startups should increasingly seek engineers who talk directly with users, make decisions on what to build, and then build with AI assistance. This approach works because engineers inherently understand both technical constraints and opportunities better than anyone else. The traditional roles of \u0026ldquo;product manager\u0026rdquo; and \u0026ldquo;engineer\u0026rdquo; are merging into a hybrid that combines technical expertise with product sensibility.\n\u0026ldquo;Design is all of the conversations between designers and PMs about what to build\u0026rdquo; (Kwok, 2020). Similarly, product development is now larger than just engineers or product managers—it encompasses the entire collaborative process of identifying, designing, and implementing solutions. Too often, people view product management as just a career path rather than a mindset. This limited perspective overlooks the deep curiosity and problem-solving drive that true product managers embody—a drive that goes beyond mere job titles and organizational structures. The best product engineers embody this product mindset, focusing on solving real problems rather than just building features. Super ICs don\u0026rsquo;t wait for dev support—they build what they need when they need it, combining technical skills with product thinking to deliver complete solutions.\nShifting Emphasis: From Implementation to Idea Generation As AI handles more of the routine implementation work, the traits we look for in product engineers should also evolve. The most valuable skills now center around workflow generation and new idea creation rather than implementation details. The ability to conceptualize solutions, design effective workflows, and identify the right problems to solve becomes paramount.\nProduct engineers who excel in this new environment should demonstrate:\nStrong systems thinking across the entire product lifecycle The ability to rapidly iterate and test hypotheses Comfort with ambiguity and exploration over rigid planning Skill in designing human-AI collaboration workflows An understanding of when to leverage AI and when to apply human judgment This shift means that strong ICs with AI fluency can now accomplish what previously required teams of specialists. For early-stage startups, this dramatically changes the calculus around hiring, especially for traditional product management roles.\nShopify\u0026rsquo;s AI-first Approach Shopify\u0026rsquo;s CEO Tobi Lütke\u0026rsquo;s internal memo illustrates this shift perfectly. He writes:\n\u0026ldquo;We are entering a time where more merchants and entrepreneurs could be created than any other in history\u0026hellip; Having AI alongside the journey and increasingly doing not just the consultation, but also doing the work for our merchants is a mindblowing step function change here.\u0026rdquo;\nLütke emphasizes that \u0026ldquo;reflexive AI usage is now a baseline expectation at Shopify.\u0026rdquo; He notes that using AI well is a skill that needs to be learned through frequent use, and that AI acts as a multiplier for already high-performing individuals. He compares Shopify to a \u0026ldquo;red queen race\u0026rdquo; from Alice in Wonderland—you must keep running just to stay still. In a company growing 20-40% year over year, everyone must improve at that rate just to re-qualify for their position. With AI tools, this previously daunting expectation now seems achievable.\nThe Critical Role of Taste in Product Development As AI handles more of the execution, the differentiating factor for product engineers becomes \u0026ldquo;taste\u0026rdquo; - the ability to discern what makes for excellent product design, user experience, and strategic direction. Product taste is what separates adequate solutions from exceptional ones. In a world where AI can generate competent designs and functional code, the human with superior taste becomes indispensable. \u0026ldquo;Just like how the constraints on design at companies is often not a problem of pixels, but of people\u0026rdquo; (Kwok, 2020). As technical constraints dissolve through AI assistance, the human factors around judgment, taste, and strategic direction become the primary differentiators.\nTaste involves the ability to:\nIdentify which problems are worth solving Determine the appropriate level of complexity for solutions Recognize when simplicity serves users better than feature-richness Balance aesthetic appeal with functional needs Anticipate user needs before they\u0026rsquo;re explicitly requested These skills can\u0026rsquo;t be easily replicated by AI systems. While AI can execute with increasing competence, it still lacks the intuitive understanding of human needs and experiences that informs good taste.\nWhat This Means for Organizations Shopify\u0026rsquo;s approach includes several key principles organizations should consider:\nAI as a fundamental expectation - Not an optional tool but a core competency AI integration from the prototype phase - Using AI throughout the development process Headcount requests must demonstrate why AI can\u0026rsquo;t do the job - Teams must explore AI solutions first Universal application - This applies to all levels, including executives Organizations following these principles will likely outperform those treating AI as merely an optional tool. The principles represent a fundamental rethinking of how work gets done, not just a technological upgrade.\nWhile some companies are hiring for AI product managers nowadays, this might be a transitionary position, just as prompting is an intermediary step in human-LLM interaction—similar to the arrival of GUIs before the personal computer age became mainstream. Product managers now need to learn when to dive into detailed work and when to step back for strategic oversight, a balance that will continue to evolve as AI capabilities expand.\nThe Path Forward The future belongs to smaller, more nimble teams of highly capable individuals who leverage AI to achieve what previously required entire departments. This might be the new normal of work. For ICs, learning to effectively collaborate with AI tools is no longer optional—it\u0026rsquo;s essential for remaining competitive. For organizations, the challenge is creating environments where these super ICs can thrive, with processes and cultures that maximize the human+AI partnership rather than treating them as separate domains. Most importantly, this shift will enable more creative exploration and novel problem-solving, as routine tasks become increasingly automated. The most exciting products and services of the coming years will likely emerge from these small, AI-augmented teams combining human taste with AI-powered execution.\nRelated Reading A2A Catalog: A Human-Mediated Agentic Workforce - Deep dive into autonomous AI agents and workforce automation User Needs \u0026amp; Opportunities - Identifying product opportunities in the AI-first world When do I Sunset a Product? - Making strategic decisions in fast-moving AI product development Works Cited\nGoodspeed, Elizabeth. \u0026ldquo;Design Taste vs. Technical Skills in the Era of AI.\u0026rdquo; Nielsen Norman Group, 10 May 2024.\nKwok, K. (2020, June 19). Why Figma Wins. https://kwokchain.com/2020/06/19/why-figma-wins/\nLütke, Tobi. \u0026ldquo;Internal Company Memo on AI Usage.\u0026rdquo; Shopify, 2025.\n","permalink":"https://chenterry.com/posts/product_engineers/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/product_engineer/shopify_ai.png\" alt=\"Product Engineers\" loading=\"lazy\"\u003e\n \n\u003cem\u003eTobi Lutke\u0026rsquo;s Shopify internal memo\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"smaller-and-more-efficient-teams\"\u003eSmaller and More Efficient Teams\u003c/h2\u003e\n\u003cp\u003eWhen should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\u003c/p\u003e","title":"Product Engineers and AI Multipliers"},{"content":"Here\u0026rsquo;s a list of articles that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\nGary Tan on Manus: The New General-Purpose AI Agent Video URL: https://www.youtube.com/watch?v=JOYSDqJdiro\nUsable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\n\u0026ldquo;Today we\u0026rsquo;re launching an early preview of Manus, the first general AI agent.\u0026rdquo;\nWhen Manus officially launched, the hype around it immediately took off. A Chinese startup unveiling a new AI agent that some are calling \u0026ldquo;China\u0026rsquo;s next DeepSeek moment,\u0026rdquo; with people calling it \u0026ldquo;the most impressive AI tool they\u0026rsquo;ve ever tried\u0026rdquo; and \u0026ldquo;the most sophisticated computer-using AI.\u0026rdquo;\nUnlike some of its predecessors, Manus wasn\u0026rsquo;t just another specialized chatbot. It promised to be a true general-purpose AI agent. With invitations rare and access limited, the question remains: has Manus truly revolutionized the AI agent landscape?\nHow Manus Works Behind all the excitement around Manus is something genuinely innovative: a multi-agent AI system that can seemingly complete all sorts of tasks from travel planning and financial analysis to searching over dozens of files or doing industry research.\nRather than relying on one big neural network, Manus works more like an executive overseeing a team of sub-agents, coordinating and guiding their every move across a shared action space. It takes in your prompt as input and gets to work figuring out what it needs to do.\nInstead of tackling your task in one go, Manus employs a sophisticated approach. A planner agent first comes up with a master plan to follow, breaking things down into manageable subtasks. This way Manus knows precisely what needs to be done before executing and can hand off these tasks to other sub-agents. These sub-agents are like Manus\u0026rsquo;s own in-house experts - they share the same context but each has its own delineated domain from knowledge or memory to execution.\nManus can call upon an extensive suite of 29 different integrated tools, whether they\u0026rsquo;re automating web navigation, securely running code, or pulling important information from files. Manus\u0026rsquo; sub-agents intelligently decide which tools to use.\nFinally, when each subtask is complete, the executor agent combines the outputs together into a final synthesized output for the user.\nTechnical Details Under the hood, Manus is powered by a sophisticated dynamic task decomposition algorithm. This is what enables it to autonomously break down complex instructions into clear execution paths.\nTo ensure stability even after dozens of rounds of reasoning and tool use, the Manus team developed an original technique called \u0026ldquo;chain of thought injection,\u0026rdquo; enabling agents to actively reflect and update plans.\nAt its core, Manus makes use of Anthropic\u0026rsquo;s Claude 3.7 Sonnet. Manus also features robust cross-platform execution capabilities thanks to its seamless integration with open-source tools like YC company Browser.js for advanced website interaction and startup E2B\u0026rsquo;s secure cloud sandbox environment.\nCapabilities and Performance What can Manus actually accomplish? Impressively, it can take on a wide range of real-world tasks. These include creating travel itineraries, performing detailed financial analyses, developing educational content, compiling structured databases, comparing insurance policies, sourcing suppliers, and assisting with high-quality presentations.\nTo truly measure Manus\u0026rsquo; capabilities, we can look at Gaia, a benchmark designed to challenge AI agents on reasoning, multimodal handling, web browsing, and tool proficiency. Humans typically score about 92% on this benchmark, while OpenAI\u0026rsquo;s Deep Research scored about 74% at its best. Manus smashed the state-of-the-art on Gaia, scoring 86.5%, just a few points shy of the average human.\nThe \u0026ldquo;Wrapper\u0026rdquo; Debate Despite impressive benchmark performance, Manus has reignited a broader conversation about the nature of AI startups at the application layer: \u0026ldquo;wrappers.\u0026rdquo;\nSome have dismissed Manus as merely a wrapper, since it stitches together existing foundational models and various tool calls. But this dismissal overlooks an important reality: most successful AI products today could also qualify as wrappers by this logic.\nCursor and Warp, for example, integrate existing LLMs alongside external APIs and developer-focused tooling such as real-time code analysis and debugging utilities. Domain-specific agents like Harvey combine foundational models with legal-specific tool integrations, case law retrieval, compliance checks, and document analysis.\nClearly, many useful applications do fit the wrapper mold, and for many developers, it makes sense to go this route. As Manus co-founder Yizhow Peak G told us himself, \u0026ldquo;From day one they decided to work orthogonally to model development, wanting to be excited rather than threatened by each new model release.\u0026rdquo;\nWhat distinguishes successful wrappers from their less effective counterparts is typically a bunch of things: intuitive UI, proprietary evals, much more careful fine-tuning of foundational models, and thoughtfully designed multi-agent architectures.\nStrengths and Limitations Manus itself illustrates these trade-offs really well:\nStrengths: Its multi-agent orchestration helps deliver significantly lower per-task costs (around $2 a task compared to integrated competitors like OpenAI\u0026rsquo;s Deep Research). It offers greater transparency and user control, letting users directly inspect, customize, or replace individual sub-agents and tool integrations. Additionally, it provides a degree of flexibility centralized platforms rarely match.\nOne of the coolest things Manus figured out was actually exposing the file system so you could see exactly what the agents were doing. Chat GPT requires you to reprompt, and it\u0026rsquo;s opaque what\u0026rsquo;s happening when it\u0026rsquo;s thinking. Manus is a glimpse into the future of Chat GPT desktop operating directly on your computer, and it will be cool to see how much more control you\u0026rsquo;ll get when it\u0026rsquo;s happening there instead of a browser.\nLimitations: Coordination across specialized agents becomes increasingly difficult as tasks scale or complexity grows. Its current advantages (UX refinements, targeted fine-tuning, thoughtful integrations) are vulnerable to competitors just coming along and doing that as well.\nThese strengths and weaknesses are generally shared by wrappers. They allow rapid deployment, iteration, and specialized UX at lower upfront cost. However, they\u0026rsquo;re vulnerable to disruption such as API pricing changes or provider policy shifts, which can quickly erase any cost benefits.\nThe Future of AI Products Ultimately, the critical challenge isn\u0026rsquo;t deciding whether wrappers are viable but identifying genuinely sustainable differentiation for your product.\nFor founders, this might mean investing early in proprietary evals that are expensive or time-consuming to replicate, embedding workflows deeply into specific user routines to increase switching costs, and identifying integrations with platforms or data sets competitors can\u0026rsquo;t easily access.\nIn the end, success in AI doesn\u0026rsquo;t hinge on reinventing the wheel but rather on who can stitch together the existing models into a product users genuinely love.\n","permalink":"https://chenterry.com/posts/interesting_reads/","summary":"\u003cp\u003eHere\u0026rsquo;s a list of articles that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\u003c/p\u003e\n\u003ch1 id=\"gary-tan-on-manus-the-new-general-purpose-ai-agent\"\u003eGary Tan on Manus: The New General-Purpose AI Agent\u003c/h1\u003e\n\u003cp\u003eVideo URL: \u003ca href=\"https://www.youtube.com/watch?v=JOYSDqJdiro\"\u003ehttps://www.youtube.com/watch?v=JOYSDqJdiro\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\u003c/p\u003e","title":"Interesting Reads"},{"content":" Hayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\nTo put this dedication in perspective: a single 4 second crowd scene from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\nYet as technologies like GPT-4o image generation and and \u0026ldquo;Ghibli-style\u0026rdquo; AI art taking the internet by storm, I can\u0026rsquo;t help wondering about the future. When anyone can transform an image into a Ghibli-esque cartoon nearly indistinguishable from Miyazaki\u0026rsquo;s hand-drawn work, we must reconsider what constitutes true craftsmanship and whether we can appreciate his art in the same way.\nMiyazaki himself once expressed after viewing AI generated animations: \u0026ldquo;I am utterly disgusted. I would never wish to incorporate this technology into my work at all. I strongly feel this is an insult to life itself.\u0026rdquo; However, I hesitate to apply this statement too broadly, as it was made under different circumstances—specifically in response to Dwango AI Lab\u0026rsquo;s demonstration of a 3D zombie animation of substantially lower quality than today\u0026rsquo;s AI art. Objectively speaking, current AI-generated Ghibli-style images demonstrate impressive technical quality, yet for longtime fans, they lack the same emotional resonance.\nThe genius of Ghibli\u0026rsquo;s work transcends time. Released in 1997, \u0026ldquo;Princess Mononoke\u0026rdquo; explores humanity\u0026rsquo;s complex relationship with nature through a meticulously crafted world. With a budget of 2.35 billion yen—making it the most expensive animated film of its era—the movie was created just before the digital animation revolution. Approximately 144,000 cells were hand-drawn to build its vision of Muromachi-era feudal Japan. The result was a timeless masterpiece that represented more than a decade of thoughtful development in plot, character, and animation. It was the pinnacle of its time and remains so today.\nAs we process the technological shift that now enables machines to replicate human art at scale, we must consider how this new dynamic will shape creativity\u0026rsquo;s future. When the marginal cost of producing high-quality work approaches zero, what defines creativity? Can creative works be protected, or can anyone replicate a piece after merely seeing it? In an era where everyday users generate sophisticated content with minimal input, how do we distinguish truly exceptional work?\nWhile I don\u0026rsquo;t have definitive answers, I believe our appreciation for content will evolve—perhaps valuing human-created work more highly or discovering new experiences through previously inconceivable creative expressions. Copyright laws, though often inadequate and slow to adapt, will eventually catch up to protect original works and incentivize innovation. On a positive note, creativity will no longer be limited to those with technical artistic skills. It may ultimately center on taste and the ability to understand and abstract experiences that evoke universal empathy—expressions of our collective human experience.\nHowever, as creation costs approach zero, we risk the collapse of our collective imagination while maintaining only the illusion of creativity. The dynamic we\u0026rsquo;ve observed in content consumption—preferring cheap, easy, addictive material—now extends to creation. Why face the challenging reality of making something yourself when you can skip to the result? Our aesthetics might become constrained by AI models\u0026rsquo; training data, reducing creativity to mere copying and merging of established works. We risk being left with nothing but juxtaposition.\nThe difficult process of creating and thinking deeply is where genuine ideas emerge. Technology evolves rapidly; humans do not. Even in the AI age, there remains space for quality and discernment. The labor, patience, and intention behind Miyazaki\u0026rsquo;s work embody values that transcend technological convenience—values we would be wise to preserve, even as our creative tools transform.\nReferences: https://openai.com/index/introducing-4o-image-generation/, https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/, https://www.youtube.com/watch?v=Pi2rHOhPZZ4, https://en.wikipedia.org/wiki/Spirited_Away\n","permalink":"https://chenterry.com/posts/craft_miyakazi/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/hayao-miyakazi/dust-balls.png\" alt=\"Dust Balls\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eHayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\u003c/p\u003e\n\u003cp\u003eTo put this dedication in perspective: a single \u003ca href=\"https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/\"\u003e4 second crowd scene\u003c/a\u003e from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\u003c/p\u003e","title":"The Craft of Miyazaki in an AI-Generated World"},{"content":"When you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\nMore apparent ones: 2025-03-24\nCareer seeking: it\u0026rsquo;s a difficult job market, whether it\u0026rsquo;s doing leetcode or preparing for behavioral interviews, people want a way to cheat their way out of this difficult process, and they\u0026rsquo;re willing to pay for it too. 2025-03-27\nAgentic worklows: I\u0026rsquo;m usually more skeptical towards areas more widely reported, but agentic workflows may well be here to stay. From the initial action gpts (autogpt) agent chains (devin, metagpt, cogno etc), to the more recent operators (MCPs, OpenAI operator), it would indeed be nice to have agents act on our behalf (perhaps hopefuly not the way Anton goes about ordering Hamburgers in Silcon Valley).\nBuilding on the previous point concerning agentic computer use. It\u0026rsquo;s also interesting to consider the two approaches towards web navigation: vision-based and ai-native-ui. While people like Aravind Srinivas have openly expressed their skepicism towards vision based web operations, noting the limitations imposed by operating systems, especially iOS, which restrict access to other applications, vision-based web interactions are inherently more versatile and flexible.\nContent generation: Generative ai is at its core, generative. We have for the past three years seen numerous improvements in text/image/video/avatar generation, with applications in vairous forms of UGC (and now nearing PGC) ads, podcasts, short-form videos. I love how creative people have been with the available AI tools, but at the same time, I view these tools as an available means to scale creativity, not for enabling 0-1 creation.\nLess apparent (and more interesting) ones: 2025-03-25\nAds through LLM generated content: While I was trying to figure out the best way to add a forms feature to this website yesterday, I asked claude. And interestingly, this was what I got back at first: Note that Formspree is a paid service. This raises an interesting point about the future of AI generated content and suggestions. On one hand, the result is based on training data inputted into the model, but at the same time, one can easily manipulate it to recommend one solution over another (try Formspree vs Google Forms). This presents an interesting avenue for ads via llm generated content, and given the extent to which people are willing to trust llm results, this could even be an effective avenue. Yet in doing so, we are also risking the reliability of llm generated content.\nSEO is already changing in the age of LLMs, with AI genearated content flooding web browsers. Another interesting observation is the amount of website traffic going to specific companies from AI-enabled search services such as Perplexity. While looking at website traffic for Genspark and a few other websites, we can see both ChatGPT and Perplexity listed as the top referring websites (source: similarweb) Though the accuracy of this data is yet to be determined, it make sense intuitively that, owing to the large amount of content genearted via these AI platforms, the pages will be easily indexed by ai-browsers.\nGeneral Directions: These are some relatively well agreed upon directions that one may take:\n2025-03-27\nBetter customization: Traditionally, a tradeoff existed between customization and scale. Creating for wider audiences required standardized offerings. However, as LLM inference costs rapidly decrease, information retrieval and content generation are approaching fixed costs. This shift enables customization at scale across various domains—personalized news, cinematography, and other content formats. Additionally, advancements in multimodal capabilities (voice, image, video) will likely introduce greater variety in the content we consume. ","permalink":"https://chenterry.com/posts/user-needs/","summary":"\u003cp\u003eWhen you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\u003c/p\u003e","title":"User Needs \u0026 Opportunities"},{"content":"The Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\nSystem Architecture The application uses a serverless architecture centered around AWS Lambda functions and Amazon RDS. This design ensures scalability, reliability, and cost efficiency by leveraging AWS managed services.\nThe Client Application provides a web/mobile interface for user interactions. AWS API Gateway exposes the backend as a RESTful API and routes client requests to appropriate Lambda functions. Four AWS Lambda functions handle all backend operations, with a custom datatier module managing database interactions. Amazon RDS (MySQL) serves as the persistent data store for users, transactions, and receipt metadata. AWS S3 securely stores uploaded receipt images, while AWS Textract processes these images with OCR to extract transaction details. An AI service (AWS Bedrock) automatically categorizes expenses based on merchant and description. Finally, Amazon SNS sends budget alerts via email notifications when spending exceeds predefined thresholds.\nComponent Interactions In the receipt processing flow, a user uploads a receipt image through the frontend application. The API Gateway routes this request to a dedicated Lambda function, which stores the receipt in S3 and creates metadata in RDS. The system then leverages AWS Textract to analyze the image and extract key details such as merchant name, date, and amount. This extracted information is passed to an AI model that categorizes the transaction based on the merchant and description. The complete transaction details are then stored in the RDS database for future reference and analysis.\nThe budget monitoring process begins when a user sets category-specific budget limits via the frontend. A Lambda function updates these settings in the RDS database. Later, when new transactions are processed, the system evaluates current spending against the budget thresholds. If spending exceeds a predefined limit, the SNS service automatically sends a notification to the user\u0026rsquo;s registered email address, providing timely awareness of their financial situation.\nFor financial reporting, users can request customized reports through the frontend. A Lambda function queries the relevant transaction data from RDS based on user-specified parameters. This data is then processed and formatted according to the user\u0026rsquo;s preferences, such as report type (transaction details or category summaries) and format (CSV or PDF). The final report is delivered back to the client via the API response, providing valuable insights for financial planning and tax preparation.\nAPI Specification The application exposes a RESTful API through AWS API Gateway with the following endpoints:\n1. Receipt Upload and Analysis POST /upload\nPurpose: Upload a receipt image for processing Request Body: Multipart form data with image file userid (required): User identifier filename: Name of the receipt image data: Base64-encoded image data Process: The system validates if the user exists, stores the receipt image in S3, creates a receipt record in the database, and initiates asynchronous processing. The function also checks whether the user\u0026rsquo;s current monthly spending exceeds the monthly budget, and sends an email alert to the user through SNS. Response: Status 200: Success, returns transactionid, userid, amount, category, time Status 400: Bad request (missing parameters) Status 500: Server error 2. Expense Retrieval GET /expenses\nPurpose: Retrieve user expenses with optional filtering Query Parameters: userid (required): User identifier start_date (optional): Filter transactions after this date (YYYY-MM-DD) end_date (optional): Filter transactions before this date (YYYY-MM-DD) category (optional): Filter by expense category Response: Status 200: Success, returns list of transactions and summary Status 400: Bad request (missing parameters) Status 500: Server error 3. Budget Management POST /budget_and_alert\nPurpose: Set monthly spending budget Process: The system updates the user\u0026rsquo;s budget in the database, checks current month\u0026rsquo;s spending against the budget, and sends an SNS notification if spending exceeds the budget. Response: Status 200: Budget set successfully Status 400: Bad request (missing parameters) Status 500: Server error 4. Financial Reports GET /report\nPurpose: Generate and export financial report Query Parameters: userid (required): User identifier format (optional): \u0026ldquo;csv\u0026rdquo; or \u0026ldquo;pdf\u0026rdquo; (default: \u0026ldquo;csv\u0026rdquo;) report_type (optional): \u0026ldquo;transactions\u0026rdquo; or \u0026ldquo;summary\u0026rdquo; (default: \u0026ldquo;transactions\u0026rdquo;) start_date (optional): Starting date for report end_date (optional): Ending date for report category (optional): Filter by category Report Types: The \u0026ldquo;transactions\u0026rdquo; type lists all individual transactions with details, while the \u0026ldquo;summary\u0026rdquo; type provides aggregated statistics by category (count, total, average, max, min). Output Formats: The system supports CSV (standard comma-separated values) and PDF (HTML structure that would be converted to PDF) formats. Process: The system retrieves filtered transaction data from the database, generates the report in the requested format, and encodes the report data as base64 for transmission. Response: Status 200: Success, returns report data Status 400: Bad request (missing parameters) Status 500: Server error Database Schema The application uses a MySQL database on Amazon RDS with the following schema:\nThe Users table stores basic user information. The userid field serves as the primary key and unique identifier for each user. The email field contains the user\u0026rsquo;s email address for notifications. The budget field is a JSON object storing category-specific budget limits (e.g., {\u0026ldquo;Food\u0026rdquo;: 500, \u0026ldquo;Transport\u0026rdquo;: 200}). The sns_topic_arn field contains the Amazon SNS topic ARN for sending notifications to this user.\nThe Transactions table records all user expenditures. The transactionid field is an auto-incrementing primary key. The userid field is a foreign key reference to the Users table. The amount field stores the transaction amount as a decimal with two places of precision. The category field contains the expense category (e.g., Food, Transport, Entertainment) assigned by the AI. The time field records when the transaction occurred.\nThe Receipts table tracks uploaded receipt images and their processing status. The receiptid field is an auto-incrementing primary key. The userid field references the Users table. The status field indicates the current processing status (e.g., \u0026ldquo;uploaded\u0026rdquo;, \u0026ldquo;processing\u0026rdquo;, \u0026ldquo;completed\u0026rdquo;, \u0026ldquo;error\u0026rdquo;). The s3_location field contains the S3 bucket path to the stored receipt image. The upload_time field captures when the receipt was uploaded, while analysis_time records when processing was completed.\nLambda Functions The system is implemented using four main AWS Lambda functions, each dedicated to a specific aspect of the application.\nThe Receipt Upload and Analysis function processes receipt images and extracts transaction details. It handles receipt uploads and storage in S3, uses AWS Textract for OCR processing, employs an AI model for merchant recognition and expense categorization, creates transaction records from extracted data, and updates receipt status through processing stages.\nThe Get Past Expenses function retrieves and filters transaction records. It supports filtering by date range and category, generates transaction summaries and category breakdowns, calculates spending statistics (totals, counts), and returns a formatted JSON response.\nThe Set Budget and Trigger Alerts function updates budget settings and sends alerts. It updates user budget settings in the database, checks current spending against budget limits, sends notifications via SNS when thresholds are exceeded, calculates budget utilization percentages, and supports category-specific budgets.\nThe Export Financial Reports function generates financial reports in different formats. It supports multiple report types (transactions, summary), generates reports in CSV or PDF format, applies filtering options (date range, category), and encodes reports as base64 for transmission.\nConclusion The Smart Expense Tracker with Auto-Categorization represents a significant improvement over traditional expense tracking applications by leveraging AWS cloud services and artificial intelligence. By automating the tedious aspects of expense management, the system allows users to gain financial insights with minimal effort.\n","permalink":"https://chenterry.com/archived/smart-expense-tracker/","summary":"\u003cp\u003eThe Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\u003c/p\u003e","title":"Smart Expense Tracker"},{"content":"Authors: Terry Chen, Kaiwen Che, Matthew Song\nAbstract Despite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\nIn comparison to traditional retrieval-augmented generation (RAG) techniques that store verbatim conversation segments, our system employs strategic memory consolidation procedures, abstracting key information into structured forms. Performance testing on the GoodAI Long-Term Memory benchmark demonstrates significant improvements in performance, with our memory-augmented GPT-4o achieving scores of up to 6.9/11 over the baseline 4.6/11. Additional testing across multi-agent domains demonstrates enhanced persistence and updating capacity of information.\nIntroduction State-of-the-art large language models (LLMs) possess remarkable natural language comprehension and generation. However, their architecture imposes tight constraints on memory retention and contextual comprehension during long-term interaction. Most existing LLMs operate within fixed context windows, typically ranging from 32,000 to 128,000 tokens, which impose inherent constraints on long-term conversation and complex reasoning tasks that span multiple turns.\nThe Baddeley and Hitch (1974, 2000) model of working memory provides a robust theoretical account of human information processing. The model presents memory as a multi-component system with central executive control of information flow, an episodic buffer of assembling memories into temporary experiences, a phonological loop of controlling verbal content, and a visuospatial sketchpad of controlling visual and spatial information.\nCurrent approaches to increasing LLM memory capacity heavily rely on embedding-based retrieval-augmented generation (RAG). While the approach can deliver rapid access to previous data, it suffers greatly from issues like vector explosion, the unsustainable proliferation of embeddings as conversation history grows, lack of semantic structure in stored shreds, and difficulties in maintaining relations among relevant facts.\nThis work introduces a novel biomimetic approach to LLM memory extension that more accurately models the cognitive architecture of humans, with a three-tiered memory system distinguishing between immediate context, episodic memories, and semantic facts.\nSystem Architecture Our memory improvement system utilizes a three-layer architecture inspired by human cognition:\nWorking Memory (LLM Context Window) We divide the context window into two distinct segments:\nMulti-Round Conversation History (MCH): Stores current conversation context, maintaining flow up to a defined token limit. Retrieval Memory Buffer (RMB): Provides dedicated space for injecting remembered memories from long-term storage, maintaining a balance of short-term and long-term remembered data. Long-Term Memory Store Implemented as a vector database storing two forms of memory:\nSemantic Memory: Stores factual knowledge gained from conversations as subject-predicate-object triples with optional contextual referencing. Episodic Memory: Stores complete interaction episodes by a formal schema with contextual initialization, reasoning operations, actions taken, and outcomes observed. Memory Processes There are specialized components for:\nMemory Consolidation: Operations for capturing and formalizing memories when conversation history reaches token thresholds. Retrieval Mechanisms: Multi-step operations that determine context adequacy before retrieving from external memory stores. Memory Schema Implementation Semantic Memory Triple We implemented the semantic memory schema as a structured class:\nclass SematicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Store all new facts, preferences, and relationships as triples.\u0026#34;\u0026#34;\u0026#34; subject: str predicate: str object: str context: str | None = None Episodic Memory Schema Our episodic memory implementation stores experiential information with temporal context:\nclass EpisodicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Write the episode from the perspective of the agent within it.\u0026#34;\u0026#34;\u0026#34; observation: str = Field(..., description=\u0026#34;The context and setup - what happened\u0026#34;) thoughts: str = Field( ..., description=\u0026#34;Internal reasoning process and observations of the agent\u0026#34; ) action: str = Field( ..., description=\u0026#34;What was done, how, and in what format.\u0026#34; ) result: str = Field( ..., description=\u0026#34;Outcome and retrospective.\u0026#34; ) Memory Consolidation Process The foundation of our strategy lies in sophisticated memory consolidation mechanisms that convert raw conversational information into structured memory representations:\nSemantic Memory Extraction Our semantic memory schema makes use of subject-predicate-object triples that eliminate episodic detail without sacrificing core relationships. Implementation follows several guiding principles:\nPrioritization of high-frequency accessed information Merging of redundant knowledge into a single representation Upgrading existing triples whenever new contradicting data exist Adding contextual linking to render situationally responsive retrieval Episodic Memory Extraction Episodic memory stores full interactions in an ordered schema consisting of four main components:\nObservation: Stores contextual setup and what transpired Thoughts: Stores internal reasoning processes and deliberations Action: Stores particular interventions and methodologies used Result: Stores outcome and subsequent analysis Evaluation Results GoodAI LTM benchmark results indicated radically better performance with our memory augmentation approach:\nConfiguration Score Performance Baseline GPT-4o 4.6/11 41.8% GPT-4o + Semantic Memory 6.8/11 61.8% GPT-4o + Episodic Memory 6.9/11 62.7% GPT-4o + Semantic \u0026amp; Episodic Memory 6.0/11 54.5% These results reflect a general 20-percentage-point improvement in memory performance by our augmentation method. The differential performance aligns with the corresponding functional roles these types of memory serve in human cognition, wherein semantic memory enables fact recall and episodic memory enables experiential reasoning.\nDiscussion and Future Work Our research provides empirical evidence for cognitive-inspired LLM memory enhancement methods. The witnessed performance improvements with three-tier memory architecture show that human memory systems offer valuable design concepts for overcoming inherent limitations in current AI designs.\nThe unexpected finding was the slightly worse performance of integrated memory systems compared to single implementations. This suggests complex interaction effects, which may mirror interference phenomena observed in human memory systems, where various forms of memory sometimes vie for mental resources.\nFuture research directions include:\nMulti-agent Memory Dynamics: How memory transfers between agents and how social dynamics influence memory consolidation Advanced Retrieval Strategies: Exploring spatially organized memory architectures and hierarchical memory organization Optimization of Consolidation Thresholds: Investigating dynamic thresholds that adapt based on conversation characteristics Conclusion This paper presents a novel biomimetic approach to enhancing LLM memory that addresses intrinsic limitations in current architectures. By embracing a three-level memory structure inspired by human cognitive processes, we demonstrate significant improvements in information retention, update, and context recall.\nAs LLMs advance towards more general intelligence capabilities, structured memory systems will play a larger role in enabling coherent long-term interactions, homogeneous knowledge states, and contextually appropriate information access. Our research contributes both pragmatic approaches for deploying this aspect of AI progress and theoretical frameworks to continue advancing this critical component of AI work.\n","permalink":"https://chenterry.com/archived/human-inspired-llm-memory/","summary":"\u003cp\u003e\u003cstrong\u003eAuthors: Terry Chen, Kaiwen Che, Matthew Song\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eDespite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\u003c/p\u003e","title":"LLM Memory Consolidation and Augmentation"},{"content":"2024 passed quickly, anchored by work that was equal parts challenging and rewarding. Early in the year, after Cogno gained some traction, momentum slowed. We continued to code and talk with customers, but progress came in uneven cycles. Our focus on multi-agent systems felt directionally right, yet we never found a defensible niche in sales conversion.\nOne lesson stood out: it isn’t enough to oversee product development—you need to be hands-on with the code. Direct involvement not only accelerates iteration but also sharpens your sense of what’s truly risky about shipping. I realized this later than I’d have liked, but it reshaped how I think about building.\nThat realization also clarified another decision. I had once thought about taking time off school to pursue “the next big thing,” but this year I chose to stay. I still wanted the full college experience—structure, humanities, friendships, and Chicago’s unique energy. Dropping out wasn’t the path I needed right now.\nIn March, after more than a year on Cogno, I joined TikTok to build agent systems for Creative Copilots and Insights. I’m grateful to Zhengjin and Caoye for the opportunity—even though it cut short a long-anticipated Puerto Rico trip. The experience was worth it: building at scale, learning from colleagues across countries, and attending Nvidia GTC and Google Next in Vegas. The work on multimodal insight extraction opened new ways of thinking about how LLMs interact with content. Whether probabilistic models can be “creative” remains debated, but their ability to surprise is undeniable. I’ve come to believe the future lies in combining reasoning with synthesis—likely with greater weight on synthesis—and in multimodal output.\nReturning to Beijing and then back to school later in the year felt like a blur, but I enjoyed academics more than expected. Great teammates and LLM-focused project-based classes brought a level of intensity often missing in industry. This period also sparked Crowdlistening, an exploration of how to extract meaning from the flood of unstructured, multimodal social content. While I’m now focused on AI features for a stealth startup, I still see that problem as deeply important.\nLooking back on Cogno and my earlier projects, I realize most didn’t fail for lack of innovation. Since I began with LLMs in late 2022, the pace of GenAI experimentation has been relentless—domain-specific prompting, data flywheels, agents, multi-agent orchestration. Everyone was building; few created real value. Being first to market rarely matters. What matters is building moats—durable advantages that outlast hype. Technology only matters when it solves hard problems or drives efficiency at scale. In every other case, survival comes from patient, sustainable advantage, even if it means being last.\nI’m still figuring things out, but 2024 was a year of exploration and growth—one I’m deeply grateful for. I’m excited to see how these lessons evolve in 2025.\n","permalink":"https://chenterry.com/posts/2024-year-review-startup-lessons/","summary":"\u003cp\u003e2024 passed quickly, anchored by work that was equal parts challenging and rewarding. Early in the year, after Cogno gained some traction, momentum slowed. We continued to code and talk with customers, but progress came in uneven cycles. Our focus on multi-agent systems felt directionally right, yet we never found a defensible niche in sales conversion.\u003c/p\u003e\n\u003cp\u003eOne lesson stood out: it isn’t enough to oversee product development—you need to be hands-on with the code. Direct involvement not only accelerates iteration but also sharpens your sense of what’s truly risky about shipping. I realized this later than I’d have liked, but it reshaped how I think about building.\u003c/p\u003e","title":"2024 in Review: Lessons from Founding Cogno and Building at TikTok"},{"content":"Advised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\nGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\nOur approach is centered around understanding effective study group learning for educational purposes. Understanding Learning Barriers: Traditional Q\u0026amp;A systems often provide direct answers, which may limit deeper understanding. Groupal emphasizes learning through inquiry, leveraging Socratic questioning techniques proven to improve knowledge retention and critical thinking.\nDecomposing the Problem: Groupal integrates real-time speech processing, intent routing, and contextual retrieval of relevant background information to support learning. The frontend-backend pipeline connects key components such as document parsing, relevance checks, and question generation that adapts according to the flow of group discussions. Real-Time Interaction: Groupal listens to student discussions in real time, converts speech to text, identifies misconceptions, and generates insightful socratic questions. The system retrieves relevant content through a RAG process to supplement the discussion effectively.\nGroupal provides a comprehensive set of features to enhance the effectiveness of collaborative study sessions. Through real-time speech analysis, Groupal transcribes live group discussions into text, identifies misconceptions, and stores them for further review. It detects potential misconceptions in the discussion and generates Socratic-style questions when relevant to encourage further discussions among students. With its background knowledge retrieval capability, Groupal accesses relevant materials through a RAG process, retrieving the top 3 documents from a vector database, and using a Socratic Questioning Model to generate and output the socratic question, giving students immediate access to supporting information and guidance during their sessions. The platform allows users to upload documents, form or join study groups, and explore relevant content in real time, creating a well-organized and interactive learning experience. Groupal ensures discussions remain focused and productive by providing adaptive and context-aware guidance.\nCredits: Tina Liu, Yihang Du, Doohwan Kim.\n","permalink":"https://chenterry.com/archived/groupal/","summary":"\u003cp\u003eAdvised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\u003c/p\u003e\n\u003cp\u003eGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\u003c/p\u003e","title":"Realtime Conversational Learning Aid"},{"content":"PepTalk: AI Journaling Tool Realtime conversation with aI companion to help you note down feelings and journals for the day. (Prototype: https://peptalk-navy.web.app/)\nWhat2Do: AI Trip Planning Tool A trip planning tool for generating itinearies based on article url input and content extraction. (Prototype: what2do-51224.web.app)\nOHours: Office Hour Scheduling Tool An office hour queuing system to improve student experience and help TAs manage questions more efficiently. (Prototype: ohours.web.app/)\nCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\n","permalink":"https://chenterry.com/archived/prototyping/","summary":"\u003ch3 id=\"peptalk-ai-journaling-tool\"\u003ePepTalk: AI Journaling Tool\u003c/h3\u003e\n\u003cp\u003eRealtime conversation with aI companion to help you note down feelings and journals for the day.\n(Prototype: \u003ca href=\"https://peptalk-navy.web.app/\"\u003ehttps://peptalk-navy.web.app/\u003c/a\u003e)\u003c/p\u003e\n\u003ch3 id=\"what2do-ai-trip-planning-tool\"\u003eWhat2Do: AI Trip Planning Tool\u003c/h3\u003e\n\u003cp\u003eA trip planning tool for generating itinearies based on article url input and content extraction.\n(Prototype: what2do-51224.web.app)\u003c/p\u003e\n\u003ch3 id=\"ohours-office-hour-scheduling-tool\"\u003eOHours: Office Hour Scheduling Tool\u003c/h3\u003e\n\u003cp\u003eAn office hour queuing system to improve student experience and help TAs manage questions more efficiently.\n(Prototype: ohours.web.app/)\u003c/p\u003e\n\u003cp\u003eCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\u003c/p\u003e","title":"Rapid Prototyping of LLM Enabled Webapps"},{"content":"Exploring Unknown Unknowns: The Future of Knowledge Interfaces We live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don\u0026rsquo;t know what to read, and we don\u0026rsquo;t understand what we\u0026rsquo;ve read. These pain points—\u0026ldquo;not knowing how to choose\u0026rdquo; and \u0026ldquo;not knowing how to comprehend\u0026rdquo;—represent a massive opportunity for reimagining how we interact with knowledge.\nThe core insight driving next-generation learning interfaces is simple but profound: most people don\u0026rsquo;t know what they don\u0026rsquo;t know. We can\u0026rsquo;t formulate good questions about topics we\u0026rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.\nBeyond Search: Learning from Google\u0026rsquo;s Experiments Google\u0026rsquo;s Learn About product offers a compelling glimpse of this future. Unlike traditional search, which requires users to know what to look for, Learn About allows users to \u0026ldquo;zoom out and look at the space of questions around your question.\u0026rdquo; It combines the information accuracy of search with the flexible, dynamic interaction of AI chat, creating an exploratory learning experience that goes far beyond simple Q\u0026amp;A.\nThis approach represents a fundamental shift from information retrieval to knowledge discovery. Instead of returning static results, the system actively helps users explore adjacent concepts and ask better questions. Users can pursue their immediate curiosity while simultaneously discovering related topics they never thought to investigate.\nThe most innovative learning interfaces take this concept further by specializing in specific domains. Rather than trying to handle all possible queries, they focus on particular knowledge areas—like literature, technical documentation, or professional development—where they can provide genuinely superior experiences compared to general-purpose tools.\nThe LLM Architecture Behind Intelligent Learning The technical foundation of these systems relies on sophisticated prompt engineering and modular content generation. Large language models serve as the cognitive engine, but their raw output must be carefully structured to create coherent learning experiences. The key innovation lies in using LLMs to generate JSON-formatted responses that populate predefined UI templates, creating consistent yet dynamic interfaces.\nThis architecture allows the system to maintain conversational flow while presenting information in learner-friendly formats. For example, instead of generating wall-of-text responses, the LLM outputs structured data that renders as interactive cards, related questions, and exploration pathways. Each response includes not just content, but also suggested next steps and connection points to related topics.\nThe prompt engineering becomes crucial here. Effective systems use detailed behavioral instructions that guide the LLM to act as a knowledgeable teacher rather than a simple question-answering service. These prompts specify tone, content depth, interaction style, and response structure, ensuring consistency across thousands of potential learning conversations.\nReducing Cognitive Friction Through Design Traditional learning interfaces suffer from what could be called \u0026ldquo;prompt friction\u0026rdquo;—the cognitive overhead of formulating good questions and organizing complex thoughts into text. The most successful knowledge interfaces minimize this friction through several design strategies.\nFirst, they embed potential questions directly into content responses. Instead of requiring users to think of follow-up questions, the system generates three or four relevant next steps that users can explore with a simple click. This transforms learning from an active questioning process into a guided exploration where curiosity can flow naturally.\nSecond, they use modular response formats that pack high knowledge density into digestible chunks. Rather than lengthy explanations, responses combine concise answers with interactive elements: reflection prompts, knowledge checks, relevance connections, and vocabulary builders. Users receive exactly the information they need while being invited to go deeper on specific aspects that interest them.\nThird, they implement \u0026ldquo;prompt prefills\u0026rdquo;—pre-written questions and conversation starters that help users begin productive dialogues without staring at blank input fields. These aren\u0026rsquo;t generic suggestions but contextually relevant questions based on the current topic and common learning patterns.\nPersonalization Through Conversational Intelligence Unlike traditional recommendation systems that rely on explicit preferences or behavioral tracking, conversational learning interfaces build user understanding organically through dialogue. Each interaction reveals information about the user\u0026rsquo;s background knowledge, interests, learning goals, and preferred depth of explanation.\nThis conversational profiling enables increasingly sophisticated personalization. The system learns whether a user prefers concrete examples or abstract concepts, detailed explanations or high-level overviews, historical context or contemporary applications. Over time, responses become naturally calibrated to individual learning styles and knowledge levels.\nThe personalization extends beyond content delivery to include book recommendations, topic suggestions, and learning path optimization. By understanding what concepts a user struggles with and what types of explanations resonate, the system can proactively surface relevant material and adapt its teaching approach in real-time.\nTechnical Implementation: RAG and Knowledge Curation Behind the conversational interface lies a sophisticated knowledge management system. Rather than relying solely on LLM training data, effective learning platforms implement Retrieval-Augmented Generation (RAG) architectures that combine real-time information retrieval with language generation.\nThis approach proves particularly valuable for specialized domains like literature analysis, where high-quality, curated knowledge sources significantly improve response accuracy and depth. Systems can draw from structured databases of book analyses, expert commentary, reader discussions, and academic sources to provide richer, more authoritative answers than general-purpose models alone.\nThe challenge lies in balancing different information sources. Community discussions from platforms like Reddit offer authentic reader perspectives and common questions, while academic sources provide authoritative analysis. Professional reviews and curated summaries add editorial quality. Effective systems learn to synthesize these different knowledge types based on the specific question and user context.\nMeasuring Success Beyond Engagement Traditional educational metrics often miss the point of exploratory learning. While engagement metrics like session length and click-through rates provide some insight, the real value lies in knowledge acquisition and curiosity development. The most meaningful measures focus on learning outcomes: Do users ask better questions over time? Do they make novel connections between concepts? Do they pursue deeper investigation of topics that initially seemed uninteresting?\nAdvanced systems track conversation quality through several indicators: the progression from basic to sophisticated questions, the frequency of cross-topic connections, the depth of follow-up exploration, and user-generated insights that suggest genuine understanding. These metrics help optimize not just for engagement, but for actual learning effectiveness.\nThe Future of Knowledge Work As these interfaces mature, they point toward a fundamental transformation in how we approach knowledge work. Instead of consuming information passively, we\u0026rsquo;ll increasingly collaborate with AI systems to explore ideas, test understanding, and discover unexpected connections. The goal isn\u0026rsquo;t to replace human thinking but to augment it with better tools for curiosity and exploration.\nThe most promising applications extend beyond individual learning to collaborative knowledge building. Imagine research environments where teams can explore complex topics together, with AI facilitators helping surface relevant connections, identify knowledge gaps, and guide productive discussions. Or educational settings where students learn not just facts but how to ask increasingly sophisticated questions about any domain.\nThe technical foundation already exists. The remaining challenge is design: creating interfaces that feel natural, educational experiences that genuinely improve understanding, and systems that scale personalized learning without losing the human touch that makes great teaching transformative.\nThe next time you encounter a complex topic, imagine having a knowledgeable guide who not only answers your questions but helps you discover the questions you didn\u0026rsquo;t know to ask. That\u0026rsquo;s the promise of intelligent knowledge interfaces—and it\u0026rsquo;s closer than you might think.\n","permalink":"https://chenterry.com/posts/learning_interface/","summary":"\u003ch1 id=\"exploring-unknown-unknowns-the-future-of-knowledge-interfaces\"\u003eExploring Unknown Unknowns: The Future of Knowledge Interfaces\u003c/h1\u003e\n\u003cp\u003eWe live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don\u0026rsquo;t know what to read, and we don\u0026rsquo;t understand what we\u0026rsquo;ve read. These pain points—\u0026ldquo;not knowing how to choose\u0026rdquo; and \u0026ldquo;not knowing how to comprehend\u0026rdquo;—represent a massive opportunity for reimagining how we interact with knowledge.\u003c/p\u003e\n\u003cp\u003eThe core insight driving next-generation learning interfaces is simple but profound: most people don\u0026rsquo;t know what they don\u0026rsquo;t know. We can\u0026rsquo;t formulate good questions about topics we\u0026rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.\u003c/p\u003e","title":"Exploring Unknown Unknowns"},{"content":"Observing and understanding the strange quirks of individuals and crowds What makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\n","permalink":"https://chenterry.com/main-themes/human-quirks/","summary":"\u003ch2 id=\"observing-and-understanding-the-strange-quirks-of-individuals-and-crowds\"\u003eObserving and understanding the strange quirks of individuals and crowds\u003c/h2\u003e\n\u003cp\u003eWhat makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\u003c/p\u003e","title":"Human Quirks"},{"content":"In the rapidly evolving landscape of artificial intelligence, we\u0026rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.\nWhen we expect AI to output precise, high-quality content based on a single prompt, we\u0026rsquo;re making a fundamental mistake. For AI content to be truly valuable, we must provide sufficient context for it to reason over. The quality of AI output is directly proportional to the richness of context we provide—not just about the topic itself, but about the audience, purpose, constraints, and desired outcomes.\nThe Limitations of Current Approaches Today\u0026rsquo;s AI content tools largely excel at taking existing information and reformatting it into different modalities. We can convert text to audio, create video summaries, or generate podcast-style conversations from written material. However, as Large Language Model context windows continue to expand, simple content summarization becomes increasingly commoditized. The real value lies not in these mechanical transformations, but in the creative synthesis and novel perspectives that emerge when AI systems understand both the content and the consumer.\nThe essence of creativity lies in finding fresh angles of approach. Quality content distinguishes itself through novel perspectives, clear structure, and genuine utility to the reader. As we move beyond basic summarization, the challenge becomes how to help AI systems discover these unique entry points that make content both engaging and personally relevant.\nKnowledge Liquefaction: The New Content Paradigm We\u0026rsquo;re entering an era of \u0026ldquo;knowledge liquefaction\u0026rdquo; where any piece of information can be rapidly transformed into formats that match specific consumption scenarios. Whether someone needs structured learning materials for deep study or fragmentary content for casual listening during commutes, AI systems can now adapt the same core knowledge to fit these different contexts seamlessly.\nThis capability extends far beyond simple format conversion. The most compelling applications combine high-quality human-created content with AI\u0026rsquo;s ability to find unexpected connections and generate personalized frameworks. Rather than replacing human creativity, these systems amplify it by identifying patterns and relationships that might not be immediately obvious, then presenting them through personalized lenses that resonate with individual users.\nThe Personalization Challenge Creating truly personalized content presents a fundamental tension between scale and customization. If every piece of content requires individual adaptation for each user, the costs become prohibitive. However, knowledge fusion offers a solution through its inherent modularity. Many elements of content remain constant across audiences—core concepts, fundamental principles, and essential facts—while the variable elements involve how these concepts connect to individual interests, goals, and existing knowledge.\nThe key insight is that personalization doesn\u0026rsquo;t require generating entirely new content for each user. Instead, it involves intelligent selection and combination of existing content elements, supplemented by targeted customization that creates meaningful connections to the user\u0026rsquo;s specific context and needs.\nDynamic User Understanding Through Interaction Modern AI systems have unprecedented access to rich user interaction data through natural language conversations, reading highlights, and behavioral patterns. Unlike traditional recommendation systems that rely primarily on click-through data, AI-native platforms can analyze the semantic content of user queries, the topics they explore, and the questions they ask to build sophisticated models of their interests and learning preferences.\nThis approach moves beyond simple topic matching to understand cognitive patterns and learning styles. For example, the system might recognize that one user prefers concrete examples and case studies, while another gravitates toward theoretical frameworks and abstract principles. These insights enable the generation of content that not only covers relevant topics but presents them in ways that align with how each individual processes and retains information.\nBuilding Memory Systems That Learn and Adapt The most sophisticated AI-native learning platforms implement memory architectures inspired by human cognition, incorporating episodic memory for recent interactions, semantic memory for abstracted patterns, and procedural memory for learned preferences and behaviors. This multi-layered approach enables systems to maintain context over time while continuously refining their understanding of user needs.\nRather than treating each interaction as isolated, these systems build cumulative knowledge about user interests, expertise levels, and learning goals. They can recognize when someone is exploring a new domain versus deepening existing knowledge, and adjust their content generation accordingly. This longitudinal understanding becomes increasingly valuable as it enables the system to suggest unexpected but relevant connections between seemingly disparate areas of interest.\nThe Promise of Adaptive Content Creation The ultimate vision extends beyond personalized recommendation to adaptive content creation. Imagine a system that can take a classic work like Sun Tzu\u0026rsquo;s \u0026ldquo;The Art of War\u0026rdquo; and generate multiple interpretations tailored to different audiences and applications. For a business professional, it might emphasize strategic planning and competitive analysis. For a parent, it could explore family dynamics and conflict resolution. For a student, it might focus on historical context and philosophical implications.\nEach version would maintain the core insights of the original work while presenting them through frameworks and examples that resonate with the specific audience. This approach recognizes that great ideas have universal applicability, but their accessibility depends heavily on how they\u0026rsquo;re presented and contextualized.\nTechnical Implementation and Practical Considerations Building these capabilities requires sophisticated orchestration of multiple AI systems working in concert. Content generation engines must work alongside user modeling systems, recommendation algorithms, and quality control mechanisms. The challenge lies not just in generating personalized content, but in ensuring it maintains accuracy, coherence, and genuine value while adapting to individual preferences.\nRecent advances in multimodal AI and agent-based architectures provide the technical foundation for these applications. Tools like MCP (Model Context Protocol) servers enable modular, composable AI capabilities that can be combined and recombined to address specific user needs. This architectural approach allows for the kind of flexible, adaptive content generation that personalized learning requires.\nThe Road Ahead As we look toward the future of AI-native learning platforms, the focus shifts from simple automation of existing processes to the creation of entirely new forms of educational experience. The most successful applications will be those that understand the deep relationship between content, context, and individual cognition, using this understanding to create learning experiences that are not just personalized, but genuinely transformative.\nThe transition from traditional content consumption to AI-enhanced learning represents more than a technological upgrade. It\u0026rsquo;s a fundamental reimagining of how knowledge can be packaged, presented, and absorbed in ways that honor both the richness of human understanding and the unique cognitive patterns of individual learners. In this future, every question becomes an opportunity for personalized exploration, and every piece of content becomes a starting point for deeper, more meaningful engagement with ideas.\n","permalink":"https://chenterry.com/posts/personalized_content/","summary":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, we\u0026rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.\u003c/p\u003e\n\u003cp\u003eWhen we expect AI to output precise, high-quality content based on a single prompt, we\u0026rsquo;re making a fundamental mistake. For AI content to be truly valuable, we must provide sufficient context for it to reason over. The quality of AI output is directly proportional to the richness of context we provide—not just about the topic itself, but about the audience, purpose, constraints, and desired outcomes.\u003c/p\u003e","title":"Why Context is Everything in AI Content Generation"},{"content":"Every now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\nPatrick Collison: Go deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\nDon\u0026rsquo;t stress out too much about how valuable the things you\u0026rsquo;re going deep on are\u0026hellip; but don\u0026rsquo;t ignore it either. It should be a factor you weigh but not by itself dispositive.\nTo the extent that you enjoy working hard, do. Subject to that constraint, it\u0026rsquo;s not clear that the returns to effort ever diminish substantially. If you\u0026rsquo;re lucky enough to enjoy it a lot, be grateful and take full advantage!\nMake friends over the internet with people who are great at things you\u0026rsquo;re interested in. The internet is one of the biggest advantages you have over prior generations. Leverage it.\nAim to read a lot.\nIf you think something is important but people older than you don\u0026rsquo;t hold it in high regard, there\u0026rsquo;s a reasonable chance that you\u0026rsquo;re right and they\u0026rsquo;re wrong. Status lags by a generation or more.\nAbove all else, don\u0026rsquo;t make the mistake of judging your success based on your current peer group. By all means make friends but being weird as a teenager is generally good.\nBut having good social skills confers life-long benefits. So, don\u0026rsquo;t write them off. Get good at making a good first impression, being funny (if possible\u0026hellip; this author still working on it\u0026hellip;), speaking publicly.\nMake things. Operating in a space with a lot of uncertainty is a very different experience to learning something.\nMore broadly, nobody is going to teach you to think for yourself. A large fraction of what people around you believe is mistaken. Internalize this and practice coming up with your own worldview. The correlation between it and those around you shouldn\u0026rsquo;t be too strong unless you think you were especially lucky in your initial conditions.\nIf you\u0026rsquo;re in the US and go to a good school, there are a lot of forces that will push you towards following traintracks laid by others rather than charting a course yourself. Make sure that the things you\u0026rsquo;re pursuing are weird things that you want to pursue, not whatever the standard path is. Heuristic: do your friends at school think your path is a bit strange? If not, maybe it\u0026rsquo;s too normal.\nFigure out a way to travel to San Francisco and to meet other people who\u0026rsquo;ve moved there to pursue their dreams. Why San Francisco? San Francisco is the Schelling point for high-openness, smart, energetic, optimistic people. Global Weird HQ. Take advantage of opportunities to travel to other places too, of course.\nFind vivid examples of success in the domains you care about. If you want to become a great scientist, try to find ways to spend time with good (or, ideally, great) scientists in person. Watch YouTube videos of interviews. Follow some on Twitter.\nPeople who did great things often did so at very surprisingly young ages. (They were grayhaired when they became famous\u0026hellip; not when they did the work.) So, hurry up! You can do great things.\nPaul Graham - How to do Great Work I found Paul Graham\u0026rsquo;s \u0026ldquo;How to Do Great Work\u0026rdquo; compelling because it tackles something most career advice glosses over: how do you actually figure out what to work on when you\u0026rsquo;re young and have no idea what\u0026rsquo;s good?\n\u0026ldquo;The way to figure out what to work on is by working. If you\u0026rsquo;re not sure what to work on, guess. But pick something and get going.\u0026rdquo; You learn by doing, not by endless planning.\nThe work needs three qualities: \u0026ldquo;it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\u0026rdquo; The tricky part is most people focus only on the first two and ignore scope entirely.\nDon\u0026rsquo;t get trapped by educational systems. \u0026ldquo;The educational systems in most countries pretend it\u0026rsquo;s easy. They expect you to commit to a field long before you could know what it\u0026rsquo;s really like.\u0026rdquo; I see this constantly - people picking majors at 18 and feeling locked into paths they\u0026rsquo;ve barely explored.\nProject-level procrastination is way more dangerous than daily procrastination. \u0026ldquo;You put off starting that ambitious project from year to year because the time isn\u0026rsquo;t quite right.\u0026rdquo;\nThe \u0026ldquo;staying upwind\u0026rdquo; approach: \u0026ldquo;At each stage do whatever seems most interesting and gives you the best options for the future.\u0026rdquo; This feels much more sustainable than trying to optimize for some distant goal you\u0026rsquo;re not even sure you want.\nGreat work emerges through iteration, not grand planning. \u0026ldquo;Great things are almost always made in successive versions. You start with something small and evolve it, and the final version is both cleverer and more ambitious than anything you could have planned.\u0026rdquo;\nConsistency beats intensity. \u0026ldquo;People who do great things don\u0026rsquo;t get a lot done every day. They get something done, rather than nothing.\u0026rdquo; The key insight: \u0026ldquo;Great work usually entails spending what would seem to most people an unreasonable amount of time on a problem.\u0026rdquo;\nQuestions are underrated. \u0026ldquo;Many discoveries have come from asking questions about things that everyone else took for granted.\u0026rdquo; This connects to something I\u0026rsquo;ve noticed - the best conversations often start with someone asking an obvious question that nobody had bothered to ask.\nOriginality comes from genuine curiosity, not trying to be different. \u0026ldquo;Originality isn\u0026rsquo;t a process, but a habit of mind. Original thinkers throw off new ideas about whatever they focus on, like an angle grinder throwing off sparks.\u0026rdquo;\nBe comfortable with intellectual confusion. \u0026ldquo;You have to be comfortable enough with the world being full of puzzles that you\u0026rsquo;re willing to see them, but not so comfortable that you don\u0026rsquo;t want to solve them.\u0026rdquo;\nWhat struck me most was his point about broken models: \u0026ldquo;Broken models of the world leave a trail of clues where they bash against reality.\u0026rdquo; The idea that you can find opportunities by being stricter about truth than other people is something I want to think about more. Be authentic and driven by truth.\nDiego Berdakin Resilience, Intense Curiosity, and Deep Customer Empathy\n","permalink":"https://chenterry.com/posts/how-to-work-on-what-you-love/","summary":"\u003cp\u003eEvery now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\u003c/p\u003e\n\u003ch2 id=\"patrick-collison\"\u003ePatrick Collison:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGo deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\u003c/p\u003e","title":"How to Work on What You Love: Career Advice from Top Entrepreneurs"},{"content":"This week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I\u0026rsquo;d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn\u0026rsquo;t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.\nWhat Constitutes Creative Work? Let\u0026rsquo;s discuss what kind of creativity AI can enable, establishing the capability boundaries of AI applications. Is it creative to \u0026ldquo;take screenshots of someone else\u0026rsquo;s video and caption them with other people\u0026rsquo;s comments on your own account\u0026rdquo;? Though this involves some editing rather than direct reposting, it\u0026rsquo;s difficult to call this creative work. Simple copying only accelerates content diffusion within an ecosystem while reducing the excess returns of original creation, as economist Schumpeter noted.\nI believe creativity is more about choosing a unique perspective. Content with contrast or conflict naturally captures our attention, but thoughtful, empathetic content is equally creative. From an ecosystem perspective, creativity can be divided into production and diffusion – the former generating new content, the latter deriving from or spreading existing content.\nAs for AI\u0026rsquo;s value in this process, generative AI as a probabilistic model struggles to produce content with fresh perspectives. However, it can help us efficiently understand massive amounts of information and generate insights (perspectives). As multimodal AI capabilities (text, image, video) improve, content reproduction costs will rapidly decrease, making products that help users find new inspiration increasingly valuable. Through such creative assistance, we can achieve two main effects:\nInspiration acquisition: Accelerating original content production Content derivation: Accelerating the diffusion of quality creative work Content Understanding for Enhanced Generation How can we make language models produce outputs that meet our expectations? This challenging question can be further divided into: (1) we don\u0026rsquo;t know what our ideal output looks like, and (2) we know what we want, but the language model doesn\u0026rsquo;t understand us. Most people are trying to solve the latter problem (through model alignment, prompting, few-shot learning, RAG, fine-tuning, memory and caching methods). However, the approaches in this space are increasingly similar, with many solutions being open-sourced, which is why many generative products deliver roughly comparable results. The real differentiation lies in how to adapt engineering and data processing to specific business scenarios.\nReturning to the first problem - \u0026ldquo;I don\u0026rsquo;t know what output I want\u0026rdquo; - this stems more from a lack of content understanding. Good script writing requires more than just hooks, USPs, and CTAs; it needs a clear angle: content that resonates with the audience, is appropriate for the context, and achieves its purpose. Some products are creating brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will likely become common, finding ways to connect insight data with generation without manual setup could be a breakthrough.\nUnderstanding User Needs Looking at the creative ecosystem, each creative area (ad aggregation, competitor tracking, brand insights, performance analysis, content generation) has 3-4 companies with minimal data or interaction differences. Data products tend to be traditional, while AI products often rely on simple language model adjustments. A potential differentiator would be acquiring more granular data and creating smoother interactions. Connecting upstream and downstream tasks (complete creative production process with participation/adjustment at each stage) could be an ideal product form.\nIf we calculate product value as \u0026ldquo;user value = new experience - old experience - replacement cost,\u0026rdquo; we find that most products built on foundational language models (old experience) with fine-tuning or prompting adjustments (incremental new experience) deliver very limited incremental value. From an interaction perspective, users still need to input personalized prompts, and outputs almost always require multiple rounds of editing before use. The question becomes: how do we increase incremental value?\nUser-Friendly Workflows Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.\nThe concept of \u0026ldquo;workflows, not skills\u0026rdquo; addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.\nI previously had a misconception that simply connecting capabilities constitutes a workflow, but deeper design is needed. What we consider Language UI is actually Prompt UI, which differs from true language interaction by missing the context and shared understanding present in human-to-human communication. Introducing these elements through features like detailed follow-up questions and future cross-container reference relationships could enhance user experience. Current prompting is likely a transitional form; eventually, we should eliminate the need for context-specific prompts by enabling LLMs to understand context and generate appropriate guidance.\nMultimodal Interaction and Content Ecosystem Finally, let\u0026rsquo;s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.\nSwitching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren\u0026rsquo;t just people; they\u0026rsquo;re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.\nCase Study: Voice Synthesis Take voice synthesis as an example. From a technical perspective, this technology is already quite mature, yet the first application that comes to mind might be phone scams. Other applications include David Attenborough\u0026rsquo;s wildlife narration in open-source projects or OpenAI\u0026rsquo;s GPT-4o launch event simulating Samantha\u0026rsquo;s voice (originally Scarlett Johansson) from the movie \u0026ldquo;Her.\u0026rdquo; However, I believe short video creators are truly making the best use of this technology.\nI recently saw a high-quality derivative work based on \u0026ldquo;In the Name of the People\u0026rdquo; (a 2017 Chinese TV drama). The creator (called \u0026ldquo;Yi Tou Jue Lv\u0026rdquo;) made remarkably deep portrayals of character psychology and inner monologues. Only after reading the comments did I learn that the creator used voice synthesis combined with their understanding of the characters to create this derivative work. Looking through this creator\u0026rsquo;s content, I found they cleverly used the original footage but replaced the narration with autobiographical scripts using AI-synthesized voices, making their work deeper and satisfying more viewer perspectives than their peers – essentially creating a new creative genre.\nContrasting Audio \u0026amp; Text It\u0026rsquo;s fascinating how differently our brains process audio and text. When we read, we\u0026rsquo;re essentially interacting with a graphical user interface - scanning, jumping between sections, processing information at our own pace. We\u0026rsquo;ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text may at times feel less engaging than a good conversation. Speaking, in contrast, is inherently linear and social. There\u0026rsquo;s something about the human voice that keeps us present - the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It\u0026rsquo;s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.\nThis contrast reveals something deeper about how we process information. Text excels at conveying complex ideas - we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of text - where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.\nConclusion As Roland Barthes suggested with \u0026ldquo;The Death of the Author,\u0026rdquo; once an author creates a work, the interpretation rights transfer to the readers. Video platforms feature various edited compilations, summaries, and analyses of film and television works. With improvements in constrained generative AI (voice synthesis, anime IP unification, future realistic character generation), we may see numerous derivative works based on original IPs, approaching professional quality and satisfying different interpretations and imaginations about the original work. These perspectives might all exist in the original work, but each short video offers a different angle, providing users with unique experiences. There remains much content that people want to see but isn\u0026rsquo;t yet available on platforms. Another interpretation of creativity could be how to push the supply curve outward to meet the demand curve at a new equilibrium point, better satisfying user needs.\nFinal Thoughts While generative AI effects are evolving rapidly, human nature changes slowly. The innovation opportunities brought by technology are often overestimated in the short term but underestimated in the long term. Making probabilistic models creative is challenging yet fascinating work, it\u0026rsquo;s probably something I will continue working on.\n","permalink":"https://chenterry.com/posts/essense_of_creativity/","summary":"\u003cp\u003eThis week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I\u0026rsquo;d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn\u0026rsquo;t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.\u003c/p\u003e","title":"Essence of Creativity: Future of Creative Work"},{"content":"Terry Chen, Allyson Lee\nAbstract Effective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. Coaches struggle to track students\u0026rsquo; progress across multiple teams and projects, making it difficult to provide targeted feedback and adapt recommendations based on evolving student needs. Existing AI-based project management tools facilitate task tracking but fail to capture the nuanced ways students approach their work. Large Language Models (LLMs) have shown promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.\nThis paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with a regulation gap to a peer who has addressed it in the past, and extracting personalized step-by-step frameworks for them to follow. Our system allows the user to record their conversation with a coach, then integrates our novel codebook, which includes regulation gap and key term definitions gathered across learning science literature, with the LLM to generate the user’s regulation gap. Utilizing the codebook and semantic matching, the system pairs one student who is currently facing that regulation gap and the other having addressed it in the past. This quarter, we explored how to facilitate conversations that will allow the novice to gain a new understanding of how to address their regulation gap, and created a tailored plan that keeps them accountable towards achieving it.\nBy utilizing LLMs throughout this system, this work advances AI-driven coaching methodologies, providing a scalable, adaptable solution for improving self-regulated learning in project-based environments. Our findings contribute to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.\nNovice to Expert Learning System\n🚀 Try the Live Prototype ➤ Access the LLM Coaching System Prototype\nExperience our LLM-enhanced coaching system firsthand through this interactive prototype.\nIntroduction Training college students to tackle complex, open-ended innovation work in a variety of fields such as design, entrepreneurship, and research is integral to preparing them for implementing innovative solutions upon graduation [1]. Project-based learning environments provide a stepping stone for preparing students to tackle complex, open-ended problems in design and research [2], yet are insufficient in helping them develop the needed regulation skills to self-direct innovation work [3]. Previous research has found that college students who effectively regulate their learning have stronger problem solving skills, allowing them to succeed in innovation work [4], [5]. Coaches help guide the development of these regulation skills, helping them develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach their desired outcomes [3]. This includes assessing risks, help-seeking, understanding one’s fears and anxieties, dealing with failure, and more.\nEffective coaching requires understanding students\u0026rsquo; work practices, identifying gaps in their problem-solving approaches, and suggesting targeted improvements. However, coaches face significant challenges in providing personalized guidance to multiple student teams. It is difficult for coaches to assess the effectiveness of their coaching, which can result in neglected alternative practice strategies or regulation gaps. Additionally, this process of recognizing and recording students’ regulation gaps and subsequent practice strategies is time consuming, resulting in coaches becoming increasingly overwhelmed as the number of student teams grows [6].\nExisting AI-based project management tools like Asana and Trello help coaches track tasks and feedback through functions such as goal setting, scheduling, task monitoring, reflections, and visualizing workflows [8], [9]. However, these tools do not capture the nuanced ways students approach their work or help coaches provide targeted, context-based guidance. Recent advances in Large Language Models (LLMs) have shown promise in analyzing text-based interactions and generating outputs based on multi-step reasoning, but their application to educational coaching through contextual suggestions remains unexplored. LLMs can assist in understanding unstructured input to provide feedback to coaches by analyzing student practices, identifying recurring issues, and formulating practice suggestions based on similar cases. They can automate the generation of tailored practice strategies, thus reducing the cognitive load on coaches while ensuring consistent quality of feedback [7]. Furthermore, LLMs can bridge the gap between coaching sessions by delivering scaffolds and feedback to students, fostering continuous improvement in regulation skills.\nTo address these issues, we propose utilizing LLMs to develop and integrate the web application: Peer Connections, which pairs one student who is currently facing that regulation gap and the other having addressed it in the past, helps them to facilitate effective conversation which builds upon their understanding of the regulation gap and how to address it, then assist them creating a tailored plan that keeps them accountable towards achieving it.\nTo achieve this, we used a knowledge base of historical coaching cases documented through the Context Assessment Plan (CAP) notes from Northwestern\u0026rsquo;s Design, Technology, and Research (DTR) Program (See Appendix C). The core idea behind these solutions is creating a system model that identifies similar notes by regulation gaps and also contextual similarity. By doing so, we can match people with similar problems for Peer Connections, as well as identify similar contextually similar cases to the one inputted and return commonly found regulation gaps and practice suggestions to achieve the Coaching Reflections and Practice Suggestions applications. We tested three different systems. First, we tested basic semantic matching using word embeddings, evaluating one version that took in the whole note, and another that weighted the semantic similarity of the regulation gap as 70%, and the rest of the note as the remaining 30%. Next, we tested the matching of notes using LLMs. We created a novel codebook consisting of tier 1 and tier 2 tags to categorize each note (See Appendix E). The codebook was created using a taxonomy of regulation skills and corresponding practice suggestions gathered across learning science literature [10], [11]. It also features definitions for key terms often used in the CAP notes, such as “canvas”, “slice”, “design argument”, “sprint”, and more. Utilizing prompting techniques such as few-shot learning, we utilized the LLM to identify the tier 1 and tier 2 categories of all the notes, and matched notes on the basis of having the most similar tags. Lastly, we tested a hybrid of these two systems, utilizing the LLM to incorporate the tier 1 and tier 2 tags of each as metadata to our notes, then using semantic matching to identify the most similar cases.\nTo evaluate the effectiveness of these systems, we ran each model with the same three notes, and evaluated the k=5 returned similar notes. We utilized our codebook as a rubric to manually identify the tier 1 and tier 2 regulation gaps of each returned note. If the returned note had the same regulation gap as the original note, then we marked it as correct. We found that the semantic matching had significant errors when addressing emotional regulation gaps, since the terms used are often less repetitive than that used when identifying cognitive and metacognitive gaps. While the codebook prompted the LLM model showed promise in accurately identifying the regulation gaps of models, it required that we input all the notes, causing longer processing times and at times exceeding model context window. Our last model, which is a hybrid of LLMs using the codebook and semantic matching, was able to consistently and efficiently identify notes that had the same regulation gap, as well as were contextually similar. This indicates that while some changes still need to be made, we have established a working baseline model to achieve our goal of matching similar notes.\nThis quarter, we explored how to facilitate conversations that will allow the novice to gain a new understanding of how to address their regulation gap, and how to create actionable plans to hold them accountable towards doing it.\nThis paper makes the following contributions:\nWe created a codebook consisting of regulation gap definitions and examples, drawing upon work gathered across learning science literature. Unlike existing educational taxonomies, this codebook provides structured categorization of regulation gaps through tiers 1 and 2. It synthesizes research self-regulation, scaffolding, and instructional coaching that can be utilized across a variety of use cases, such as AI-driven classification.\nWe utilized LLMs and semantic matching to create a metadata-based note-matching system to match students with similar regulation gaps, and provide coaches with regulation gaps and practice suggestions given the issue and context of their students.\nWe provided an empirical comparison of word embedding similarity, LLM-generated metadata, and a combined system for note retrieval, underscoring the need for a codebook.\nWe define what facilitating effective conversations between a novice peer and an experienced peer would look like, and begin to explore ways towards facilitating such conversations.\nThis paper proceeds as follows. First, we introduce related work about AI-based coaching, Intelligent Tutoring Systems, LLMs in education, and structured coaching documentation. Then, we describe and justify our system designs, followed by our evaluation and the results of each model we tested, our preliminary user testing and results, culminating in final words about our limitations and future directions for the project.\nRelated Work Effective coaching in project-based learning environments requires identifying students\u0026rsquo; self-regulation gaps, providing targeted feedback, and facilitating continuous improvement. Research has explored AI-driven coaching, self-regulated learning, and human-AI collaboration in education. However, existing approaches struggle to provide scalable, context-specific coaching support. Our work builds upon these research areas by integrating Large Language Model (LLM)-based analysis with structured cognitive models to enhance coaching efficacy.\nIntelligent Tutoring Systems (ITS) have been extensively studied for delivering automated feedback in structured learning domains such as mathematics and programming. These systems employ artificial intelligence techniques to provide personalized and adaptive instruction, modeling students\u0026rsquo; psychological states, prior knowledge, skills, and preferences [12]. ITS have been widely applied in computer science education, medical training, and mathematics learning, demonstrating their ability to provide structured, task-level feedback [13]. However, while ITS excel in well-defined domains with clear problem-solving steps, they often struggle in contexts where strategies are not easily codified, such as research coaching and innovation work. AI-driven coaching systems that extend beyond traditional ITS have been developed to support open-ended learning tasks, such as entrepreneurship coaching, where metacognitive processes like problem articulation, risk assessment, and strategic planning are essential [14]. Our approach builds on these advancements by integrating LLM-driven semantic matching with structured coaching documentation, ensuring that feedback is personalized and context-aware.\nLarge language models have shown promise in augmenting human expertise across various domains, including education and coaching. LLMs can generate reflective prompts, assist with problem diagnosis, and provide personalized guidance based on historical data [15]. However, a major challenge in applying LLMs to coaching is ensuring reliability, as these models are prone to hallucinations, lack domain specificity, and struggle with contextual reasoning [16]. Recent research has sought to mitigate these challenges by grounding LLM outputs in structured cognitive models that encode domain knowledge, allowing AI-generated feedback to align more closely with expert coaching strategies [17]. Our system builds on this approach by combining LLM-driven semantic matching with structured metadata for coaching notes, ensuring that generated suggestions are not only relevant but also informed by prior cases.\nStructured reflection tools, such as Context Assessment Plan (CAP) notes, have been widely used to document coaching insights and track student progress over time [18]. Prior research has shown that these tools enhance coaching effectiveness by making students\u0026rsquo; learning processes more visible to mentors, facilitating more targeted and data-driven feedback. However, manually analyzing CAP notes is time-intensive and cognitively demanding, limiting the extent to which insights can be effectively leveraged across multiple coaching sessions. Existing efforts to automate aspects of coaching documentation have primarily focused on extracting key themes or summarizing student progress but have not fully addressed the challenge of identifying recurring regulation gaps and providing tailored recommendations at scale [19]. Our work builds upon CAP notes by automating similarity analysis using LLMs to identify common regulation challenges and retrieve relevant past cases that align with a student\u0026rsquo;s current needs. This approach reduces the cognitive burden on coaches while ensuring that recommendations are grounded in expert-validated coaching strategies, making feedback more actionable and contextually relevant.\nWhen having a conversation with a peer, there has also been some discourse about how to scaffold peer-questioning strategies to facilitate metacognition. The research suggests scaffolding questions that include: clarification/elaboration questions, counter-arguments, and context/perspective-oriented questions [20]. However, the most significant gap in this research was that there was no statistically significant improvement in quality of the peer-generated questions, which they attribute to two possible reasons \u0026ldquo;(a) the scaffolds themselves may have been ineffective in supporting students to have better questioning skills; and (b) students may have needed additional training or modeling on how to use the provided scaffolds\u0026rdquo; [20]. Thus, leaving space to explore different methods towards facilitating effective questioning, prompting, and conversation.\nBy synthesizing research on AI-based coaching, ITS, LLMs in education, and structured coaching documentation, our work introduces a hybrid LLM-coaching system that enhances scalability, adaptability, and feedback quality in project-based learning environments. Unlike prior work that focuses on either structured tutoring or broad AI-driven feedback generation, our system integrates LLM-driven semantic matching with structured cognitive models to provide adaptive, context-aware coaching support. In doing so, we extend the capabilities of AI-driven coaching by dynamically retrieving and adapting practice suggestions from historical coaching cases. We also aim to facilitate better conversations between peers, in order to allow them to address their regulation gap faster. By addressing these limitations in existing research, our work contributes to the broader field of AI-enhanced education and human-AI collaboration, demonstrating a scalable approach to improving coaching efficacy in complex, project-based learning environments.\nSystem Description Our system consists of three key components. First, users will be able to upload an audio recording of their conversation with the coach. Then, the user will be matched with a student who had experienced and addressed that regulation gap in the past through our student regulation gap analysis system. Lastly, our system will assist them in facilitating a conversation that guides how they will plan their next sprint.\n1. Speech-to-text Transcription The first component of our system utilizes speech-to-text transcription with NLP-based feedback, to intake a student\u0026rsquo;s audio recording of their conversation with the coach, allowing the system to analyze the student\u0026rsquo;s regulation gap, while reducing the time restraints required by CAP notes.\nFigure 1: Speech-to-text Transcription System Interface\n2. Student regulation gap analysis system Thereafter, we employ a student regulation gap analysis system that leverages vector embeddings and large language models to identify patterns in student regulation behaviors across learning contexts. The system provides targeted coaching suggestions based on similar historical cases, addressing the challenge of effective coaching for developing regulation skills in design, research, and STEM innovation.\nOur system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach. Initially, student regulation notes are pre-processed without a reasoning model to include metadata on tier 1 and tier 2 regulation gaps, then encoded into text embeddings. The vector database retrieves the most similar historical cases, which are then sent along with the original query to an LLM (Deepseek). The LLM generates a structured response including a diagnosis of potential regulation gaps, practice suggestions targeted to these gaps, and references to similar historical cases. This approach grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.\nFigure 2: Student Regulation Gap Analysis Page\nI. Data Preprocessing We exported all existing notes from the CAP (Context-Assessment-Plan) note system and encountered several challenges during preprocessing. Many notes lacked clear assessment sections identifying regulation gaps, provided insufficient context about the student\u0026rsquo;s situation, and used inconsistent terminology to describe similar regulation gaps. To address these issues, we removed duplicated notes and filtered out entries with incomplete fields. To better structure the data for our system, we standardized the information into several key fields.\nThese fields include a unique identifier for each case, the key learning gap identified in the assessment, additional information such as project details, title, context, and plan, the complete original coaching note, the project name, high-level regulation categories (e.g., \u0026ldquo;Cognitive,\u0026rdquo; \u0026ldquo;Metacognitive\u0026rdquo;), and more specific skill categories (e.g., \u0026ldquo;Critical thinking,\u0026rdquo; \u0026ldquo;Forming feasible plans\u0026rdquo;). The last two fields—tier1_categories and tier2_categories—were generated using the Deepseek reasoning model with a specialized regulation skills codebook that provides the conceptual backbone for our semantic matching approach.\nFigure 3: Organized CAP Note Structure\nII. The Regulation Skills Codebook: Structured Knowledge for Semantic Matching The core innovation of our system is the integration of a structured codebook that categorizes student regulation gaps according to a research-grounded framework. The codebook provides a hierarchical organization of regulation skills across three domains, as detailed in Appendix E. This framework serves as the foundation for our semantic matching system.\nThe codebook defines three main categories of regulation skills: Cognitive Skills, which encompass abilities for approaching problems with unknown answers; Metacognitive Skills, which include capacities for planning, help-seeking, collaboration, and reflection; and Emotional Regulation, which covers dispositions toward self and learning that affect motivation.\nIII. Bridging Language and Concepts: Semantic Enrichment The codebook functions as a semantic bridge between varied descriptions of similar problems. Consider examples such as \u0026ldquo;Student struggles to create clear visual representations of system architecture\u0026rdquo; (categorized as Cognitive \u0026gt; Representing problem and solution spaces), \u0026ldquo;Prototype missing key feature\u0026rdquo; (also Cognitive \u0026gt; Representing problem and solution spaces), \u0026ldquo;Not slicing work to risk\u0026rdquo; (Metacognitive \u0026gt; Planning effective iterations), and \u0026ldquo;Student gets stuck and then stops thinking about strategy\u0026rdquo; (Emotional \u0026gt; Embracing challenges/learning/independence). Even without textual similarity, the codebook categorization reveals conceptual relationships. For instance, \u0026ldquo;stopping too soon with examples\u0026rdquo; and \u0026ldquo;not thinking deeply about risks\u0026rdquo; would both be categorized under \u0026ldquo;Critical thinking and argumentation,\u0026rdquo; enabling the system to recognize their conceptual similarity despite different phrasing.\nThis structured approach provides several advantages. It standardizes vocabulary across different coaching contexts, reveals underlying patterns in student regulation behavior, enables transfer of coaching knowledge across project domains, and prioritizes skill-based similarity over surface-level textual matching.\nGap Description Tier 1 Tier 2 \u0026ldquo;Student struggles to create clear visual representations of system architecture\u0026rdquo; Cognitive Representing problem and solution spaces \u0026ldquo;Prototype missing key feature\u0026rdquo; Cognitive Representing problem and solution spaces \u0026ldquo;Not slicing work to risk\u0026rdquo; Metacognitive Planning effective iterations \u0026ldquo;Jiayi gets stuck and then stops thinking about strategy\u0026rdquo; Emotional Embracing challenges/learning/independence IV. Similarity Methods for Regulation Gap Analysis To find cases with the same regulation gaps, we started with the semantic-based approach and gradually enhanced our methods. Our initial baseline approach uses vector embeddings to find similar cases based on textual similarity. We encode the full text of student issues using OpenAI\u0026rsquo;s ada-002 embedding model and compare them using cosine similarity. This pure semantic similarity approach treats all text equally and serves as a useful baseline. However, while straightforward, this approach often prioritizes surface-level similarities like project domain rather than underlying regulation patterns.\nTo improve matching relevance, we enhanced our approach by separating the regulation gap description from contextual information, applying higher weight to the gap text (default: 0.7 for regulation gap, 0.3 for other content). The gap text often contains the most critical information about the learning challenge, so it deserves higher weight. This weighted semantic similarity approach better identifies regulation similarities even when project contexts differ, as it emphasizes the specific regulation gap rather than surrounding information.\nHowever, we also realized limitations with the semantic-based matching methods, namely that semantic similarity only worked when there were apparent keywords to match. To address this limitation, we created a comprehensive codebook containing regulation gap definitions and examples, using a reasoning model (Deepseek-reasoning) to generate metadata with tier 1 and tier 2 regulation gap tags for enhanced matching. This LLM reasoning with codebook approach assigns the highest weight (0.5) to tier 2 categories and a lower weight (0.1) to tier 1 categories, with gap text and other content each receiving 0.2 weight. This metadata represents specific regulation skills, allowing the system to match cases addressing similar skills regardless of how they\u0026rsquo;re described or in what context they appear.\nV. Applying the Codebook in Evaluation Scripts The codebook plays a crucial role in the tiered similarity evaluation methods. The categorization process begins with the categorize_regulation_gaps_deepseek_v0.2.py script, which uses the codebook definitions to classify each gap text into tier 1 and tier 2 categories. Each gap is analyzed against the codebook\u0026rsquo;s framework, identifying which cognitive, metacognitive, or emotional regulation skills are being addressed. These categorizations are then stored in the tiered_weighted_cases.json file as structured metadata for later retrieval and analysis.\nThe codebook improves evaluation outcomes in several important ways. It provides a standardized vocabulary with a consistent framework of terms and concepts, helping to match cases that use different wording but address the same underlying skills. It enables contextual understanding by categorizing gaps according to the codebook, allowing the system to understand the deeper educational context beyond surface-level language similarities. The codebook\u0026rsquo;s three-category structure (cognitive, metacognitive, emotional) aligns with research on key regulation skills central to the situated practice system, providing coaching concept integration. Even when projects differ completely, the codebook categories help identify transferable skills and learning patterns across domains, enabling cross-project relevance. The higher weight given to tier 2 categories (0.5) in the tiered similarity approach reflects their importance in precisely identifying the specific skill gaps being addressed.\nFor instance, when processing a gap like \u0026ldquo;stopping too soon with examples,\u0026rdquo; without the codebook approach, the system might only match cases with similar phrasing about \u0026ldquo;stopping\u0026rdquo; or \u0026ldquo;examples.\u0026rdquo; With the codebook integration, this gap is properly categorized as \u0026ldquo;Cognitive \u0026gt; Critical thinking and argumentation,\u0026rdquo; allowing matches to conceptually similar gaps like \u0026ldquo;not thinking deeply enough about risks\u0026rdquo; even when the phrasing differs completely.\n3. Facilitating Conversation and Formulating Tailored Plans To generate helpful and contextualized conversation facilitation questions, we created a codebook based on Choi et al.\u0026rsquo;s question methodology, then prompted an LLM to generate responses based on conversation context (streamed) and codebook [20] (See Appendix F). The codebook and question generation is still within its early stages, until we conduct more user testing to discover the most pertinent questions to ask. Additionally, while not implemented yet, formulating tailored plans would take place in the form of filling out their sprint log/plan for their week, which they would be either linked to or the system will parse it into the sheet for them. This will allow them to enact their plans to address their regulation gap while still incorporating the tasks they need to complete for the week.\nFigure 4: Suggested Discussion Questions\nStudy / Experiment / Deployment We conducted manual qualitative coding on a set of 28 CAP notes utilizing our codebook definitions to use as ground truth to evaluate our model against. When comparing our tier 2 codes on these notes with the codes given by our system, we observed a precision of 0.875, and a recall of 0.893. Given the possible subjectivity and overlap of regulation gaps, these results suggest we can rely on our system to accurately categorize student cases’ regulation gaps.\nNext, we investigated how to facilitate effective conversation between the peers once they are matched. Most notably, how do we define what an “effective” conversation looks like?\nTo begin, we conducted some casual conversations with peers in the DTR program, asking them to explain about an issue they struggled with in the past, and how they were able to overcome it. Our conversations had no structure, as it was just to give us an idea of how students talked about regulation with a peer. From this preliminary research, we observed the main finding that experienced students struggle to articulate their experience in a way that is helpful to the peer. This is due to several factors:\nA lack of project context/terminology Not articulating their experience addressing the regulation gap in a step-by-step, clear way, making it hard for the peer to abstract their thinking process into a high-level framework they could use Both peers have trouble remembering what they experienced and what actionable next steps to take\nBased upon these findings, we defined an \u0026ldquo;effective\u0026rdquo; conversation to be one that helps them create a more concrete, actionable plan than they had before, and that they actually implement this and solve their regulation gap faster. We then identified this causal structure for which to guide our prototype and user testing:\nFigure: Causal structure diagram showing the relationship between peer conversations and regulation gap resolution\nGiven this causal diagram, we set up a second, more formal user testing scenario. Given the work of Choi et al., which highlights how scaffolding peer-questioning strategies can help facilitate metacognition, we formulated a list of questions that fell under the clarification/elaboration and ⁠⁠context/perspective-oriented categories suggested in the paper [20]. We created two lists of questions, one for the novice and one for the experienced peer, to guide the conversation, in the goal of assisting the experienced peer in articulating their experience in a step-by-step manner, and helping the novice understand how they can apply this to their own situation (See Appendix G).\nWe then conducted some preliminary user testing, by having one of us act as the experienced peer, and having another student in DTR be the novice. We simulated the conversation by having the novice ask us the questions provided in the scaffold, while we acted as the experienced peer answering the questions and providing advice. To qualitatively assess how the conversation influenced the peer, we asked them the same questions before and after (See Appendix H). During one of our user tests, one of our users, who struggled with the fear of imperfection, demonstrated this transformation:\nBefore peer conversation After peer conversation \u0026ldquo;I have no time to get things done, since my internship is starting tomorrow. There\u0026rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it\u0026rsquo;s even lower.\u0026rdquo; \u0026ldquo;You know what, I\u0026rsquo;m gonna send this to my partner first, and then I\u0026rsquo;m gonna send this to Haoqi.\u0026rdquo; This illustrates multiple findings, first that the student shifted their mindset towards already deciding that she wasn\u0026rsquo;t going to get what she wanted to do, despite not having started yet, to taking actionable steps towards working on her project and addressing her regulation gap. Furthermore, the student changed the steps they were going to dedicate time halfway between her tasks for the week to send deliverables over to the coach. The next step would be to follow up and see if this was accomplished, and how it affected her project outcome and regulation.\nDiscussion Our system effectively facilitates AI-enhanced coaching by leveraging Large Language Models (LLMs) and structured metadata to support project-based learning environments. By integrating semantic matching with structured codebook metadata, our approach identifies relevant coaching cases, reducing cognitive load on mentors while maintaining high-quality, context-aware feedback. We also began to explore how to facilitate effective and productive conversation amongst peers about regulation gaps. This discussion outlines the generalizable design elements and techniques that contributed to the prototype’s effectiveness, highlighting key takeaways for future socio-technical system development. Hybrid AI-Driven Case Retrieval: Our system employs a hybrid approach that combines LLM-driven metadata tagging with traditional semantic matching. While pure semantic similarity methods struggled to capture nuanced regulation gaps due to limited repetitive terminology, and LLM-based approaches returned too many broadly relevant cases, integrating both techniques enabled precision in retrieving the most relevant coaching cases. This demonstrates the efficacy of hybrid AI-driven retrieval in domains where contextual similarity and structured knowledge are both crucial. Future systems designed for education or expert-driven fields can benefit from this combined methodology to ensure both relevance and specificity in recommendations.\nStructured Codebooks for Domain-Specific AI Applications: A key enabler of our system\u0026rsquo;s success was the development of a structured codebook that categorizes regulation gaps into tiered classifications, allowing advanced reasoning models to more accurately categorize regulation gaps. By leveraging cognitive, metacognitive, and emotional regulation categories, the system grounded LLM-based reasoning in expert-validated pedagogical frameworks and effectively addresses the issue of LLM hallucinations. The implication for broader AI applications is that structured codebooks can serve as a mechanism to guide AI reasoning, especially for test time scaled models, and in fields requiring human-like judgment and contextual understanding.\nScaffolding productive conversations for addressing regulation gaps: Providing enough context and terminology of the other\u0026rsquo;s project and issue is crucial to set them up for success. Additionally, adopting peer-questioning scaffolds assists in driving a conversation where the more experienced peer can better articulate and reflect upon their step-by-step strategy to addressing their regulation gap, which can be adapted and applied by the novice.\nLimitations and Future Work There are inherent limitations in our current approach that necessitate attention. The CAP note system is succinctly written and does not always provide sufficient context for robust reasoning. Additionally, our codebook currently focuses primarily on high-level descriptions of regulation gaps along with their examples, which makes it difficult for the language model to develop domain-specific knowledge and reasoning for effective categorization. To address these limitations and plan for iterative improvements, we will work with CAP note developers to identify ways toward either:\nImproving clarity of writing in notes Collecting more data through alternative data sources (SIG meeting transcripts, etc.) For future work, we propose developing a sub-categorized codebook that further segments existing regulation gaps and contains specific examples along with reasoning chains for arriving at regulation gap categorizations. With such a codebook, we can first perform a tier 1 categorization of the gap to route to a corresponding model identifying each of the subcategories for further reasoning (using two sets of language models and corresponding reasoning prompts) for few-shot learning and prompt-based LLM-enabled categorization.\nAs LLMs continue to progress, we believe there is merit in more sophisticated reasoning methods such as the use of external knowledge bases or memory systems for persistent storage of student regulation gap progression. These approaches could enable more tailored and precise gap understanding, ultimately leading to more effective coaching support for developing regulation skills in design, research, and STEM innovation contexts.\nIn terms of facilitating effective conversation between peers, we plan to continually design testing strategies of feature slices, focusing on evaluating what intervention strategies are most effective in facilitating peers to more clearly articulate and discuss their task plans in context of their regulation gaps, thereby facilitating meaningful peer to peer interaction enabling effective regulation skill learning. This involves testing whether real time generation of follow up questions for explanation elaboration or regulation skill connection allows better articulation of questions during peer to peer conversations, as well as whether more context regarding the peer’s project and regulation skills should be provided in advance to foster more meaningful conversations grounded in discussion of actionable next steps for regulation skill learning.\nMost notably, there is still more work to be done with testing these strategies, specifically figuring out how to ensure that it impacts students not just in the moment, but in the long-term. To do so, we plan to run formal user tests which both qualitatively and quantitatively examine how they will and if they are addressing their regulation gap, by surveying them before using the prototype and having the conversation, after, and the subsequent weeks.\nReferences in IEEE Format [1] E. M. Gerber, J. M. Olson, and R. L. D. Komarek, \u0026ldquo;Extracurricular design-based learning: Preparing students for careers in innovation,\u0026rdquo; International Journal of Engineering Education, vol. 28, no. 2, p. 317, 2012. {#ref1}\n[2] J. C. Dunlap, \u0026ldquo;Problem-based learning and self-efficacy: How a capstone course prepares students for a profession,\u0026rdquo; Educational Technology Research and Development, vol. 53, no. 1, pp. 65–83, Mar. 2005. {#ref2}\n[3] B. J. Zimmerman, \u0026ldquo;Becoming a self-regulated learner: An overview,\u0026rdquo; Theory Into Practice, vol. 41, no. 2, pp. 64–70, May 2002. {#ref3}\n[4] D. R. Lewis, M. Easterday, and C. Riesbeck, \u0026ldquo;Research slices: Core processes for effective iteration in eder,\u0026rdquo; EDeR. Educational Design Research, vol. 8, no. 1, 2024. {#ref4}\n[5] D. G. R. Lewis, S. E. Carlson, C. K. Riesbeck, E. M. Gerber, and M. W. Easterday, \u0026ldquo;Encouraging engineering design teams to engage in expert iterative practices with tools to support coaching in problem-based learning,\u0026rdquo; Journal of Engineering Education, vol. 112, no. 4, pp. 1012–1031, 2023. {#ref5}\n[6] E. J. Huang, D. R. Lewis, S. Gaudani, M. Easterday, and E. Gerber, \u0026ldquo;Intelligent coaching systems: Understanding one-to-many coaching for ill-defined problem solving,\u0026rdquo; Proceedings of the ACM on Human-Computer Interaction, vol. 7, no. CSCW1, pp. 138:1–138:24, Apr. 2023. {#ref6}\n[7] H. Zhang, M. Easterday, and S. Shah, \u0026ldquo;Collaborative Research: Situated Practice Systems: Supporting Coaches and Students to Develop Regulation Skills for Design, Research, and STEM Innovation,\u0026rdquo; National Science Foundation Grant Proposal, 2024. {#ref7}\n[8] Asana. https://asana.com/product, 2025. {#ref8}\n[9] Trello. https://trello.com/, 2025. {#ref9}\n[10] Z. Xiao, X. Yuan, Q. V. Liao, R. Abdelghani, and P.-Y. Oudeyer, \u0026ldquo;Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding,\u0026rdquo; in Companion Proceedings of the 28th International Conference on Intelligent User Interfaces, pp. 75–78, 2023. {#ref10}\n[11] C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang, \u0026ldquo;Can large language models transform computational social science?\u0026rdquo; Computational Linguistics, vol. 50, no. 1, pp. 237–291, 2024. {#ref11}\n[12] K. F. Shaalan, \u0026ldquo;An Intelligent Computer Assisted Language Learning System for Arabic Learners,\u0026rdquo; Computer Assisted Language Learning, vol. 18, no. 1-2, pp. 81-108, Feb. 2005. {#ref12}\n[13] E. Mousavinasab, M. Zarifsanaiey, S. R. Niakan Kalhori, M. R. Rakhshan, M. Keikha, and A. Ghazi Saeedi, \u0026ldquo;Intelligent Tutoring Systems: A Systematic Review of Characteristics, Applications, and Evaluation Methods,\u0026rdquo; Interactive Learning Environments, vol. 29, no. 1, pp. 142-163, Jan. 2021. {#ref13}\n[14] M. Zawacki-Richter, V. Marín, M. Bond, and F. Gouverneur, \u0026ldquo;Systematic Review of Research on Artificial Intelligence Applications in Higher Education – Where Are the Educators?\u0026rdquo; International Journal of Educational Technology in Higher Education, vol. 16, no. 1, pp. 1-27, Dec. 2019. {#ref14}\n[15] P. Arnau-González, M. Arevalillo-Herráez, R. Albornoz-De Luise, and D. Arnau, \u0026ldquo;A Methodological Approach to Enable Natural Language Interaction in an Intelligent Tutoring System,\u0026rdquo; Computer Speech \u0026amp; Language, vol. 77, p. 101386, Jun. 2023. {#ref15}\n[16] B. Woolf, \u0026ldquo;Building Intelligent Interactive Tutors: Student-Centered Strategies for Revolutionizing E-Learning,\u0026rdquo; Morgan Kaufmann, 2009. {#ref16}\n[17] K. R. Koedinger and A. Corbett, \u0026ldquo;Cognitive Tutors: Technology Bringing Learning Science to the Classroom,\u0026rdquo; in The Cambridge Handbook of the Learning Sciences, R. K. Sawyer, Ed. Cambridge: Cambridge University Press, 2006, pp. 61-78. {#ref17}\n[18] J. Evens and J. Michael, \u0026ldquo;One-on-One Tutoring by Humans and Computers,\u0026rdquo; Routledge, 2006. {#ref18}\n[19] E. Wenger, \u0026ldquo;Artificial Intelligence and Learning in Context: A Review,\u0026rdquo; AI in Education Journal, vol. 21, no. 4, pp. 457-473, 2022. {#ref19}\n[20] I. Choi, S. M. Land, and A. J. Turgeon, \u0026ldquo;Scaffolding peer-questioning strategies to facilitate metacognition during online small group discussion,\u0026rdquo; Instructional Science, vol. 33, no. 5–6, pp. 483–511, 2005, doi: 10.1007/s11251-005-1277-4. {#ref20}\nAppendix B Improving and Scaling Coaching is part three of Situated Practice Systems (SPS), which offers tools to help coaches and learners understand work practices and develop self-directed innovation skills [7].\nFigure: Situated Practice Systems overview showing the three-part framework\nAppendix C CAP Notes helps coaches elicit information about a student\u0026rsquo;s work issue and regulation gaps by tracking their activities, outputs, and reflections [7].\nFigure: CAP Notes structure showing Context, Assessment, and Plan components\nAppendix D Practice Objects contain information about a student\u0026rsquo;s work issues, current and tracked regulation gaps, suggested practices, and practice traces [7].\nFigure: Practice Objects framework showing work issues, regulation gaps, and practice traces\nAppendix E Prompting of the LLM with the codebook incorporated.\n“You are an expert in analyzing student regulation gaps. You need to categorize each CAP (Context, Assessment, Plan) note into these three tier 1 categories and their corresponding tier 2 categories. Each case may be categorized into multiple tier 1 and tier 2 categories:\nCognitive: The student lacks skills for approaching problems with an unknown answer, or even, knowing what the problem is exactly. This includes: Representing problem and solution spaces: The way the student structures or presents the information is not effectively supporting reasoning, analysis, or communication. Example: Jacob struggles to create a representation that would help him show a working example and an example where the system breaks. Assessing risks: The student struggles with identifying the riskiest risks and/or prioritizing them. They may skip ahead, do unnecessary and unimportant tasks, or have impractical plans due to not properly addressing and prioritizing the risk. Example: John wasted time jumping ahead (he created multiple very detailed mockups) when he should’ve been focusing on addressing the riskiest risk at hand, which was identifying his target audience. Critical thinking and argumentation: The student struggles to construct well-reasoned arguments supported by evidence or lacks a conceptual understanding of the task at hand. They might find it difficult to identify conceptual differences (they are treating concepts as too similar when they actually have meaningful differences). Example: Eloise isn’t fully understanding what a regulation gap is and how to distinguish between the different types, and keeps taking the wrong changes to her prototypes because of that.\nMetacognitive: The student struggles in areas of planning, help-seeking and collaboration, and reflection. This includes: Forming feasible plans: The student struggles to develop structured, realistic, and actionable plans. This could include what their outcome should be and how to measure their outcome. Example: Penelope overloaded herself with tasks this sprint and while she got a lot of it done, it wasn’t good work. Planning effective iterations: The student struggles to create a deliverable that addresses the sprint’s riskiest risk. The student may struggle due to problems with slicing (breaking larger problems down), prioritization, or understanding the problem. Example: David didn’t incorporate the feedback the coach gave him last week, so his work this week was not effective. Leveraging resources and seeking help: The skill of identifying and utilizing available materials, expertise from others, and information to enhance their learning and problem-solving.\nEmotional: The student has regulation and dispositions toward self and learning that affects their motivation, cognition, and metacognition. This includes: Fears and anxieties: The student may have a fear of imperfection which causes them to shy away from the work, and/or doesn’t want to try things themselves. Example: Jennifer had a well-planned sprint to carry out, but got too caught up trying to perfectly design the solution rather than creating a first prototype. Embracing challenges and learning: The student tries to brute force their way through a solution or runs away from it, rather than thinking about the strategy and approach. Example: Riley\u0026rsquo;s system was not producing her optimal output, so she tried to overfit on one example rather than take a step back and observe what patterns are causing the system to fail.\nBased on the regulation gap (assessment), issue title, and context provided, categorize this case any categories (can be multiple) it applies to (Cognitive, Metacognitive, or Emotional).\nTo help you understand the notes better, here are some definitions of key terms: Slice: A well-defined, manageable portion of a larger task or goal that can be completed within a short timeframe (typically a week), contributing to incremental progress. Sprint: A time-boxed, iterative work cycle in agile project management, typically lasting 2 weeks, during which a team completes a set amount of work toward a project goal. Sprints emphasize rapid progress, continuous feedback, and adaptability. Mysore: A structured learning and practice time where students work on their projects while a mentor provides feedback. SIG: Special Interest Group meetings (SIG meetings) bring together undergraduate students, graduate students, and faculty working on different projects in the same research area. Each SIG is its own mini-studio initially led by a faculty member whose leadership fades over time as a graduate student SIG lead gains competencies in mentoring and becomes the leader of their own SIG. At the start of a sprint, teams share the outcome of their last sprint and present their current sprint plans for review. Halfway through a sprint, teams present their progress and SIG members help devise strategies for overcoming blockers.\nCategorization should be based 80% on the “Assessment” part of the note, as this is the coach’s perceived regulation gap. Be careful not to get confused-you should be categorizing their regulation gap, not the implications of the regulation gap. For instance, let’s take a look at this CAP note:\n\u0026ldquo;Student: Improving and Scaling LLMs for Coaching Improving and Scaling LLMs for Coaching | 2025-02-01 Items of Concern: Really hard to see how the conceptual examples would work / workout Context: - Not sure why a lot of the categories are there.. not sure how they are actually relevant and what you learned from the analysis Assessment: - Analysis not quite showing why the system suggested/showed what it showed? Practice Suggestions: - [self-work] I couldn\u0026rsquo;t tell from the output examples why the system was generating what it was generating. Can you think about a representation that would help you show, for an example category that you think are good and one that isn\u0026rsquo;t, why the system got it right or wrong? Perhaps you can do this by showing component TF-IDF scores for each term, and also by showing some of the matching regulation gaps?\u0026rdquo; Looking at the note as a whole, one might identify it under the tier 2 Representing problem and solution spaces, Critical thinking and argumentation, Forming feasible plans, and Planning effective iterations. However, forming feasible plans and planning effective iterations were identified from the “practice suggestions” section, meaning that this is an implication of the regulation gap rather than the regulation gap itself.\nIn this case, the correct tier 2 categorization of the regulation gap would be Representing problem and solution spaces and Critical thinking and argumentation based on the “assessment” section.\nProvide your reasoning and then your final category choice in this format: Reasoning: [your step-by-step reasoning] Categories: [tier 1 category name] [tier 2 category name]”\nAppendix F Codebook used to generate contextualized conversation facilitation questions [20]. Generate conversation facilitation questions for the user. This scaffolding should include:\nClarification/elaboration questions (seeking missing information) Counter-arguments (expressing disagreement to prompt cognitive conflict) Context/perspective-oriented questions (hypothetical scenarios, different viewpoints) Appendix G Question scaffolds used for the second round of user testing.\nQuestions novice might ask:\nWhat was the issue you were encountering, was it recurring? How does it manifest in the tasks you did? What were the steps you took to address it? What made the process hard? Was there a moment where you did something that helped? Were you able to utilize this framework/way of thinking in other instances where you experienced this regulation gap? How were you able to break the cycle of constantly having this regulation gap? Questions experienced peer might ask:\nWhat is your issue and how is it manifesting in your work? How can I use that framework to address my regulation gap (What analogies can you draw from my experience)? Appendix H Question used in the before and after survey for the second round of user testing.\nWhat are the steps you will take to address your regulation gap this week? What is the high-level framework for addressing this regulation gap? What is your level of confidence in this plan/framework? (Scale of 1-10) How comfortable are you with taking these steps? (Scale of 1-10) What are your goals and project outcomes for this week? ","permalink":"https://chenterry.com/posts/llmcoaching/","summary":"\u003cp\u003e\u003cem\u003eTerry Chen, Allyson Lee\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eEffective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. Coaches struggle to track students\u0026rsquo; progress across multiple teams and projects, making it difficult to provide targeted feedback and adapt recommendations based on evolving student needs. Existing AI-based project management tools facilitate task tracking but fail to capture the nuanced ways students approach their work. Large Language Models (LLMs) have shown promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.\u003c/p\u003e","title":"Improving and Scaling Coaching through LLMs"},{"content":"Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\nFrom Google Next to Baidu Create Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.\nFrom a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.\nFrom a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.\nLLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.\nWe should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?\n1.1 Consensus: Task Specific, MoE, Agents, Routing Content that has reached consensus:\nMost companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).\nThe MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.\nThe core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.\nImplementing an MoE architecture typically requires the following steps:\nDefine expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.\nTrain expert networks: Use labeled training data to train each expert network to obtain weights and parameters.\nAllocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.\nSummarize results: Weight and sum the output results of each expert network to get the final output.\nTrain the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.\nLonger Context Window -\u0026gt; LLM Routing At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.\nAI Routing Uses Writesonic (https://writesonic.com) uses GPT Router for LLM Routing during AI Model Selection.\nGPT Router (https://github.com/Writesonic/GPTRouter) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.\nfrom gpt_router.client import GPTRouterClient from gpt_router.models import ModelGenerationRequest, GenerationParams from gpt_router.enums import ModelsEnum, ProvidersEnum client = GPTRouterClient(base_url=\u0026#39;your_base_url\u0026#39;, api_key=\u0026#39;your_api_key\u0026#39;) messages = [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write me a short poem\u0026#34;}, ] prompt_params = GenerationParams(messages=messages) claude2_request = ModelGenerationRequest( model_name=ModelsEnum.CLAUDE_INSTANT_12, provider_name=ProvidersEnum.ANTHROPIC.value, order=1, prompt_params=prompt_params, ) response = client.generate(ordered_generation_requests=[claude2_request]) print(response.choices[0].text) 1.2 Non-Consensus: Scenarios, Market, Differentiation Content that is still not determined:\nWhat constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge\nMany companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures\nHow Agents Can Help Creators Achieve 10x Efficiency 2.1 Agent Application Cases AutoGPT AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters. https://github.com/Significant-Gravitas/AutoGPT\nGPT Researcher A GPT-based autonomous agent that conducts comprehensive online research on any given topic. https://github.com/assafelovic/gpt-researcher\nThe advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.\nWorkflow Building - Video Creation Example Concept Design: Midjourney Script + Storyboard: ChatGPT AI Image Generation: Midjourney, Stable Diffusion, D3 AI Video: Runway, Pika, Pixverse, Morph Studio Dialogue + Narration: Eleven Labs, Ruisheng Sound Effects + Music: SUNO, UDIO, AUDIOGEN Video Enhancement: Topaz Video Subtitles + Editing: CapCut, JianYing\n2.2 Improving Agent User Experience Personalized Memory \u0026amp; Style Customization User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments. Example: Typeface Brand Kit\nRewind \u0026amp; Edit User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality. Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)\nChoose from Variations User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience. Example: Typeface Image Generator (also supports favoriting)\nWorkflows, Not Skills User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation. Example: Typeface Workflow (all capabilities presented at the appropriate stages)\nTypeface - Product Reference from Former Adobe CTO https://www.typeface.ai\nTypeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft\u0026rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)\n3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time 3.2 Feature Breakdown - Customized Content Generation for Brands Multiple Agent calls centered around the core document editing experience.\nWhen users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.\nFeatures: Brands When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:\nImage Styles: Users can upload existing images for subsequent generation style adjustment. Color Palettes: Users can upload brand color palettes to standardize generated image colors. Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image. Projects When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:\nCreate a new image Create a product shot Generate text Create from template Additionally, users can select Refine to adjust generation language and tone (fixed options).\nCreate an Image After clicking Create an image, users enter the image editing page with six integrated functions on the left: \u0026ldquo;Add, select, extend, lighting, color, effects, adobe express.\u0026rdquo; Users can generate and adjust images directly and favorite preferred generations.\nCreate a Product Shot The difference from Create an image is that Product shot includes specific products, while image isn\u0026rsquo;t necessarily product-related.\nGenerate Text After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.\nTemplates Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.\nAudiences When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).\nIntegrations These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.\nhttps://www.typeface.ai/product/integrations\nMicrosoft Dynamics 365 Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.\nSalesforce Marketing Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.\nGoogle BigQuery Users can define audience segments with customer intelligence from BigQuery\u0026rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.\nGoogle Workspace Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.\nMicrosoft Teams Create content in Teams using Typeface\u0026rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.\nSummary Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.\n","permalink":"https://chenterry.com/posts/copilot/","summary":"\u003cp\u003eLeverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)\n\u003ca href=\"https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\"\u003ehttps://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCredits: TikTok Creative Team\u003c/p\u003e\n\u003ch1 id=\"building-agentic-workflows\"\u003eBuilding Agentic Workflows\u003c/h1\u003e\n\u003ch2 id=\"from-llms-to-agents\"\u003eFrom LLMs to Agents\u003c/h2\u003e\n\u003cp\u003eThe transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\u003c/p\u003e","title":"Multi-modal Creative Ad Generation"},{"content":"Analyze thousands of tiktoks to provide actionable trends \u0026amp; insights for key agencies. (Worked on multi-modal content understanding) To be released on TikTok Creative Center (https://ads.tiktok.com/business/creativecenter/pc/en)\nCredits: TikTok Creative Team\nTikTok Insight Spotlight Launches at Advertiser Summit Jun 3 - Excited to share that TikTok Insight Spotlight, a product I worked on during my time at TikTok, was officially unveiled at the company\u0026rsquo;s annual advertiser summit on June 3rd, 2025. The Verge covered the launch extensively, highlighting the AI-driven capabilities I helped develop.\nTikTok\u0026rsquo;s Insight Spotlight interface showing AI-generated analysis - a product I contributed to while at TikTok\nAs The Verge reported: \u0026ldquo;TikTok introduced a slew of new advertiser tools at the company\u0026rsquo;s annual advertiser summit on June 3rd. The new products range from AI-powered ad tools to new features connecting creators and brands, but the overall picture is clear: advertiser content on TikTok is about to become much more tailored and specific.\u0026rdquo;\nThe article specifically highlighted the core functionality I worked on: \u0026ldquo;The company will give brands precise details about how their target audience is using the platform — including AI-generated suggestions on ads to run. Using a tool called Insight Spotlight, advertisers will be able to sort by user demographics and industry to see what videos users in the target group are watching and what keywords are associated with popular content.\u0026rdquo;\nBehind the Product: During my time at TikTok, I worked specifically on the multi-modal content understanding capabilities that power Insight Spotlight\u0026rsquo;s core functionality. Here\u0026rsquo;s how the features I helped develop work in practice:\nAI-Generated Content Recommendations The Verge highlighted a key example of the system in action: \u0026ldquo;In an example provided by TikTok, an AI-generated suggestion recommends that a brand \u0026lsquo;produce video content focused on \u0026lsquo;hormonal health\u0026rsquo; for female, English-speaking users\u0026rsquo; and to include a specific keyword.\u0026rdquo; This type of precise, data-driven recommendation is made possible by the multi-modal analysis systems I contributed to.\nTrend Prediction Through Content Analysis As The Verge noted: \u0026ldquo;Another feature in Insight Spotlight analyzes users\u0026rsquo; viewing history to identify types of content that are bubbling up.\u0026rdquo; This capability stems from the advanced content understanding work I did, which enables the system to analyze both visual and audio elements of videos to predict emerging trends before they become mainstream.\nThe article captures the significance of this shift: \u0026ldquo;TikTok rose in prominence partly because of its spin-the-wheel, seemingly random quality: anything or anyone could go viral overnight. Brands did their best to keep up by jumping on trends, using popular formats and songs, and partnered with influencers who seemed to be at the center. The new tools will give advertisers even more ways to specifically tailor their content toward what is already happening on the platform and what people are searching for and watching.\u0026rdquo;\nMy work on multi-modal content understanding was focused on making this transition from random virality to predictable, data-driven insights possible. By analyzing thousands of TikToks across visual, audio, and textual dimensions, we created a system that can identify patterns and provide actionable recommendations to advertisers.\nPlatform-as-Search-Engine Vision The Verge perfectly captured the broader vision: \u0026ldquo;In this way, advertiser tools are TikTok\u0026rsquo;s equivalent of search engine optimization (SEO) — flooding the zone with content that attempts to capture organic user behaviors. The idea that TikTok can be used as a search engine has been around for several years, and the company says that one in four users search for something within 30 seconds of opening the app.\u0026rdquo;\nThis transformation from entertainment platform to search-driven insights tool represents our team\u0026rsquo;s work - using AI to understand not just what content performs, but why it performs and how brands can leverage those insights.\nBeyond Data: The Evolution of AI-Driven Insight Products for Content Creation Introduction: The Shifting Landscape of Creative AI Tools In the rapidly evolving space of AI-driven creative tools, we\u0026rsquo;re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.\nWhile many current GenAI applications focus heavily on content generation capabilities, the true creative bottleneck often isn\u0026rsquo;t in the generation step itself. Rather, it lies in the quality of insights that inform and guide the creative process. Without meaningful data and analysis, even the most sophisticated generation tools produce generic, uninspired content.\nThis blog explores how data insight products are evolving alongside generative AI technologies, and how their integration could fundamentally transform content creation workflows.\nThe Evolution of AI Capabilities The limitations of general-purpose large language models have become increasingly apparent when handling complex creative tasks. To address issues like hallucinations and improve task completion capabilities, the industry has largely reached a consensus around agent-based approaches.\nDifferent types of agent workflows have emerged to address specific needs. Non-agentic workflows generate content linearly without backtracking, suitable for straightforward tasks. Reflection-based systems introduce iterative improvement cycles where the AI criticizes and refines its own outputs. Tool use capabilities enable function calls and web browsing for enhanced research capabilities.\nMore advanced systems implement planning algorithms that decompose complex tasks into manageable steps, similar to how human creators break down projects. At the frontier, multi-agent collaboration enables specialized AI agents to work together, each handling different aspects of a complex creative process.\nThis evolution toward more sophisticated agent architectures reflects a growing understanding that creative work isn\u0026rsquo;t linear—it requires iteration, refinement, and the ability to leverage different capabilities at different stages of the process.\nThe Workflow Challenge One of the key limitations in current AI creative products is their focus on isolated capabilities rather than integrated workflows. In the advertising and marketing industry, there\u0026rsquo;s a high concentration of AI tools, but most provide only single functions or partial capabilities.\nA content creator typically needs to move through multiple stages: gathering insights, analyzing competitors, developing concepts, generating scripts, creating visual assets, and optimizing the final product. Currently, this requires juggling multiple disconnected tools, manually transferring context between them, and piecing together a cohesive workflow.\nUsers don\u0026rsquo;t simply need better individual tools—they need comprehensive workflows that connect these steps seamlessly. The value proposition shifts from \u0026ldquo;what can this AI do?\u0026rdquo; to \u0026ldquo;how does this AI fit into my creative process?\u0026rdquo; This represents a fundamental shift in how we should design and evaluate AI creative systems.\nMarket Analysis of Insight Products The current market for insight products shows several distinct categories, each addressing different aspects of the creative process. Here\u0026rsquo;s a structured analysis of the landscape:\nAd Compilation Tools Products in this category focus on collecting, organizing, and analyzing existing advertisements across platforms. Pipi Ads maintains a library of over 20 million TikTok ads with extensive filtering capabilities, allowing users to study successful campaigns and identify trending approaches. Foreplay offers a more workflow-oriented solution, enabling users to save ads from multiple platforms, organize them with custom tags, and build creative briefs based on existing successful content.\nThe value proposition of these tools centers on learning from what already works. By studying high-performing ads, creators can identify patterns and strategies that resonate with specific audiences. However, most of these tools stop at the analysis stage without directly connecting insights to content generation.\nCompetitor Analysis Tools Tools like Social Peta, Big Spy, and Story Clash provide deeper analysis of competitive activities. Social Peta offers insights into content distribution across 69 countries and 70 networks, analyzing multimedia types and dimensions. Big Spy enables cross-network ad searching with multiple filters, while Story Clash specializes in TikTok influencer tracking and performance analysis.\nThe competitive analysis market has grown substantially with the rise of social media advertising, with new players continuously entering the space to address specialized niches and platforms. These tools typically provide dashboard interfaces with various filters for monitoring competitor strategies, but most lack direct integration with content creation workflows.\nBrand Insight Tools Social listening platforms like Springklr, Exolyt, and Keyhole monitor brand mentions and sentiment across social channels. These tools analyze both posts and comments, providing valuable data on how audiences perceive brands and their content. Springklr offers comprehensive post and comment analysis with sentiment tracking, while Exolyt specializes in TikTok-specific insights, comparing brand content with user-generated content.\nKeyhole delivers profile analytics, social trend monitoring, and campaign tracking. These tools excel at capturing the audience\u0026rsquo;s voice and identifying shifts in perception, but typically require significant manual analysis to translate these insights into actionable creative strategies.\nPerformance Analysis Tools Platforms such as Social Insider, Motion App, and RivalQ focus on analyzing ad performance metrics. These tools help marketers understand what content performs best, with detailed analytics on engagement, conversion, and return on investment. By identifying high-performing content patterns, these tools can inform future creative decisions.\nHowever, there remains a significant gap between identifying what works and automatically generating new content based on those insights. Most performance analysis tools remain separated from content creation workflows, requiring manual interpretation and application of insights.\nDeep Dive: Notable Products Several standout products illustrate different approaches to the insight-generation challenge:\nTikBuddy: Platform-Specific Analysis TikBuddy focuses exclusively on TikTok analytics, offering creator rankings by category, follower count, and growth rate. The tool provides comprehensive account performance monitoring and video data analysis through a convenient Chrome extension.\nIts specialized focus allows for deeper platform-specific insights, but its utility is limited to a single platform and doesn\u0026rsquo;t extend to content creation. Users must manually apply any insights gained to their creative process.\nForeplay: Workflow Integration Foreplay stands out for its more integrated workflow approach. The platform enables users to collect ad content across platforms, preserve it even after platform deletion, and organize it with tags and categories. Its brief creation tools facilitate the transition from insight to execution, with support for brand information and specific generation requirements.\nThe platform\u0026rsquo;s AI storyboard generator creates hooks and develops scripts based on collected insights. Foreplay also integrates discovery features organized by community, brand, and experts, alongside competitor monitoring capabilities.\nThis approach begins to bridge the gap between insights and creation, though the integration remains partial rather than fully automated.\nKeyhole: Deep Analytics Keyhole exemplifies the analytics-focused approach, tracking keywords and brand mentions with temporal context. The platform offers detailed post analysis, influencer identification, trending topics visualization, and profile analytics with optimization recommendations.\nIts strength lies in comprehensive data collection and visualization, but like many analytics platforms, it requires significant human interpretation to translate insights into creative decisions.\nEmerging Innovations in Data Insight Products The data insight landscape continues to evolve rapidly, with several notable innovations emerging:\nOpen source projects like Vanna are revolutionizing text-to-SQL capabilities, making database querying more accessible to non-technical users. These tools enable creators to extract specific insights from complex datasets without specialized database knowledge.\nRecent startups are developing interactive data dashboards that visualize complex datasets in more intuitive ways, allowing for easier pattern identification and insight extraction. These tools employ advanced visualization techniques to make data more accessible and actionable.\nUser feedback aggregation tools are also gaining traction, automatically summarizing and categorizing customer sentiment from reviews and comments. These systems can identify common themes and concerns, providing valuable input for content creators looking to address audience needs.\nThe most promising innovations focus on reducing the cognitive load required to extract meaningful insights from data, making the path from analysis to action more direct and intuitive.\nThe Future: Insight-Driven Content Generation The next evolution in creative AI tools will likely center on high-quality content generation based on data insights. Current GenAI applications often produce unnecessary content redundancy—different from hallucinations, but equally problematic for effective communication.\nThe real creative barrier isn\u0026rsquo;t typically in the generation process itself, but in the prompts—the insights that inform decision-making. When using agent-based systems, the quality of instructions and background information directly impacts the output quality.\nFor example, advanced AI systems can now decompose complex goals like \u0026ldquo;How can a lifestyle channel creator get 1,000 subscribers on YouTube?\u0026rdquo; into specific tasks: analyzing successful channels, generating targeted content ideas, and implementing optimization strategies. However, the quality of these recommendations depends entirely on the AI\u0026rsquo;s access to relevant, accurate data about what actually works.\nBy leveraging sophisticated content analysis, we can identify truly effective patterns in high-performing content. Multimodal understanding can reveal why certain creative approaches resonate with specific audiences, providing creators with concrete, unique insights rather than generic advice.\nThe future lies in connecting these insights directly to the generation process—using what we know works as the foundation for creating new content that maintains brand uniqueness while leveraging proven patterns.\nConclusion: The Integration Opportunity Compared to other GenAI creative tools, insight products place greater emphasis on data quality and quantity. The next leap in AI-generated content quality will likely come from precise generation guided by robust insight data.\nThe most promising opportunity lies in creating systems that can automatically analyze successful content across platforms, extract meaningful patterns from this analysis, and directly translate these insights into generation guidance. This approach would produce highly targeted content that leverages proven patterns while maintaining brand distinctiveness.\nAs we move forward, the focus will shift from mere information generation toward sophisticated information synthesis—providing not just content, but content informed by actionable insights derived from real-world performance data. Organizations that successfully integrate insight gathering with content generation will gain a significant competitive advantage in an increasingly crowded digital landscape.\nThe future belongs not to those with the most powerful generative models, but to those who can effectively transform data into creative insight, and insight into compelling content.\n","permalink":"https://chenterry.com/posts/ai-powered-tiktok-trend-analysis/","summary":"\u003cp\u003eAnalyze thousands of tiktoks to provide actionable trends \u0026amp; insights for key agencies. (Worked on multi-modal content understanding)\nTo be released on TikTok Creative Center (\u003ca href=\"https://ads.tiktok.com/business/creativecenter/pc/en\"\u003ehttps://ads.tiktok.com/business/creativecenter/pc/en\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eCredits: TikTok Creative Team\u003c/p\u003e\n\u003ch2 id=\"tiktok-insight-spotlight-launches-at-advertiser-summit\"\u003eTikTok Insight Spotlight Launches at Advertiser Summit\u003c/h2\u003e\n\u003cp\u003eJun 3 - Excited to share that TikTok Insight Spotlight, a product I worked on during my time at TikTok, was officially unveiled at the company\u0026rsquo;s annual advertiser summit on June 3rd, 2025. \u003ca href=\"https://www.theverge.com/news/678255/tiktok-advertiser-summit-ai-targeting-data-seo\"\u003eThe Verge covered the launch extensively\u003c/a\u003e, highlighting the AI-driven capabilities I helped develop.\u003c/p\u003e","title":"AI-Powered TikTok Trend Analysis: Billion Parameter Insights for Advertising"},{"content":"Marrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\nMarket Analysis Problem Statement The current second-hand market in North America lacks efficiency, with transactions primarily occurring through fragmented social media group chats. As consumer purchasing habits become more rational, there is growing demand for second-hand transactions, but the current infrastructure does not support an effective market. The fragmentation of the market across multiple chat groups and platforms creates information asymmetry, where buyers cannot easily find available items and sellers struggle to reach interested buyers. Additionally, the cumbersome process of creating detailed listings discourages many potential sellers from participating in the market, limiting the overall volume of available goods. This inefficiency results in a significant number of usable items being discarded rather than resold, creating both economic waste and environmental impact.\nTarget Market The initial target market consists of Chinese international students in North American universities, beginning with Washington University in St.Louis, and then expanding to other colleges. This demographic was selected for several strategic reasons: they represent a cohesive cultural group with similar communication habits, they are geographically concentrated on campuses, and they can be easily verified through .edu email addresses to reduce platform abuse. This community also experiences regular high-volume second-hand transaction periods coinciding with academic calendars, particularly during move-out periods at semester ends. Following successful penetration of this initial market, expansion will target the broader university student population before eventually extending to the general American user base.\nKey Market Assumptions The business model is built on five critical assumptions that will be validated through initial market testing:\nThe potential supply of second-hand products significantly exceeds the current transaction volume, indicating an untapped market opportunity. The complexity of listing creation represents the primary barrier preventing potential sellers from participating in the market. A streamlined buying experience with easier product discovery will attract more buyers to the platform. The platform will experience network effects once it reaches a critical mass of listed items, drawing in additional buyers and sellers. The majority of second-hand products (excluding specialty categories like housing rentals and rideshares) are priced between $10-100, making them low-risk transactions. Product Vision and User Experience Marrket will innovate the second-hand marketplace experience by leveraging AI (image recognition, content generation) to dramatically simplify the listing process. The core value proposition centers on allowing sellers to create comprehensive, attractive listings with minimal effort - just a few photos and basic information. The AI system analyzes the images, generates detailed product descriptions, suggests appropriate categories, and recommends pricing based on market data. For buyers, the platform offers an intuitive, searchable interface organized by product categories, with advanced filtering capabilities to quickly find desired items. The unified marketplace creates transparency in pricing and availability that is absent in the current fragmented chat-based system.\nUser Flow Visualization ┌────────────────┐ ┌───────────────────┐ ┌─────────────────┐ ┌───────────────┐ │ │ │ │ │ │ │ │ │ Upload Photos │────▶│ AI Generates Draft│────▶│ Review \u0026amp; Publish│────▶│ Manage Listing│ │ │ │ Description │ │ │ │ │ └────────────────┘ └───────────────────┘ └─────────────────┘ └───────────────┘ │ │ │ │ │ ▼ │ ┌───────────────────┐ │ │ │ │ │ Transaction \u0026amp; Pay │ │ │ │ │ └───────────────────┘ │ ▲ ▼ │ ┌──────────────────┐ ┌───────────────────┐ ┌─────────────────┐ │ │ │ │ │ │ │ │ │ Browse Category │────▶│ View Listing │────▶│ Contact Seller │───────────┘ │ │ │ │ │ │ └──────────────────┘ └───────────────────┘ └─────────────────┘ Core Features AI-Powered Listing Generation The cornerstone of Marrrket\u0026rsquo;s platform is its innovative AI listing generation system. Sellers simply upload multiple photos of their item and provide minimal information such as the item name and general category. The AI system then analyzes the images to identify the product, its condition, key features, and appropriate categorization. It generates a comprehensive product description, suggests an appropriate price range based on market data for similar items, and creates a complete listing draft for the seller to review. This process transforms what is typically a 15-20 minute task into a 2-3 minute interaction, dramatically reducing the barrier to listing items for sale. The seller maintains full control to edit any aspect of the AI-generated content before publishing the listing.\nIntuitive Category System Marrrket organizes products into clearly defined categories that reflect the unique needs of the university student market. Primary categories include furniture, electronics, textbooks, household items, clothing, and special categories for housing rentals and rideshares. The categorization system is designed to balance simplicity with sufficient specificity to aid discovery. Each category features customized filters relevant to that product type - for example, electronics listings include filters for condition, brand, and age of the device, while textbook listings filter by course subject and edition. This tailored approach ensures buyers can quickly narrow their search to relevant items.\nStreamlined Payment and Verification To minimize transaction friction while ensuring security, Marrrket integrates with Zelle as its primary payment platform, leveraging its popularity among university students. For higher-value transactions, the platform offers an optional escrow service where the payment is held until the buyer confirms receipt of the item as described. User verification is handled through .edu email domain authentication, creating an additional layer of trust within the platform. The system also implements a reputation system where both buyers and sellers can rate their transaction experience, building credibility over time.\nTechnical Architecture Frontend Implementation The frontend architecture employs React with Ant Design components to create a responsive, mobile-friendly user interface. During the initial phase, the buyer interface may be partially implemented using Notion for rapid deployment, with plans to migrate fully to the custom React interface as the platform matures. The user interface prioritizes simplicity and visual browsing of items, with large product images and clear, concise information display. The design system maintains consistency across all interfaces while optimizing for both desktop and mobile browsing scenarios.\nBackend Systems The backend implementation uses Flask for API endpoints with initial data storage utilizing Notion\u0026rsquo;s built-in database capabilities for rapid development. As the platform scales, data will migrate to either SQL or MongoDB, with additional consideration for vector database implementation to enhance AI-powered search capabilities. The system architecture is designed with modularity in mind, allowing individual components to be upgraded or replaced as requirements evolve. Authentication, listing management, messaging, and transaction processing are implemented as separate services to maintain flexibility and scalability.\nAI Integration Architecture ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Image Analysis │────▶│ Context Generation │────▶│ Content Creation │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ │ │ │ ▼ ▼ ▼ ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Object Detection│ │ Market Analysis │ │Listing Template │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ The AI system integrates multiple technologies to transform basic image and text inputs into comprehensive product listings. The process begins with image analysis using object detection to identify the product type, condition, and key features. This information feeds into a context generation phase where market data for similar items is analyzed to determine appropriate pricing and categorization. Finally, the content creation phase generates a structured description following optimized templates for each product category. The system continuously improves through feedback loops, where user edits to the generated content are used to refine future suggestions.\nRevenue Model In the initial phase, Marrrket will prioritize user acquisition and platform growth over immediate monetization. The long-term revenue strategy consists of two primary streams:\nListing Package Fees: Users can list their first 5 items for free, encouraging initial platform adoption. Beyond that, tiered packages are available - a Basic Package for 5-20 listings and a Power Package for 20-50 listings. To build trust and mitigate risk for sellers, listing fees are automatically refunded if items don\u0026rsquo;t sell, demonstrating the platform\u0026rsquo;s confidence in its ability to connect buyers and sellers effectively.\nTransaction Fees: For items priced above $200, a 5% commission is applied to the transaction. This approach ensures that the platform primarily monetizes higher-value transactions where the commission represents a reasonable cost relative to the total value, while keeping lower-value transactions (which constitute the majority of student exchanges) commission-free to encourage platform adoption.\nAs the platform matures, additional revenue opportunities may include premium listing features, promoted listings for greater visibility, and value-added services such as professional photography or pickup/delivery coordination.\nGo-to-Market Strategy Phase 1: St. Louis Campus Launch The initial launch will focus on St. Louis area universities, strategically timed for late April 2025 to coincide with the end-of-semester period when students are moving out of housing and seeking to sell unwanted items. This timing capitalizes on a natural high-volume period in the second-hand market. Marketing efforts will employ a multi-channel approach with carefully prioritized tactics:\nTraditional campus advertising through strategically placed posters in high-traffic areas like student unions, dormitories, and international student centers. Partnerships with Chinese student organizations to promote the platform through their established communication channels, including WeChat groups and official accounts. Targeted email marketing to student email lists, emphasizing the platform\u0026rsquo;s benefits for both buying and selling. Implementation of a referral program where existing users receive incentives for bringing new users to the platform. Selective digital display advertising on platforms frequently used by the target demographic. Content marketing through popular platforms like Xiaohongshu and Instagram, featuring success stories and platform benefits. Phase 2: Expansion Strategy Following successful implementation in the initial market, expansion will proceed methodically to additional campuses with significant international student populations. The strategy leverages network effects by expanding to geographically connected areas where students may already have connections to the initial user base. Marketing messages will evolve to emphasize proven success metrics from the initial market, such as average time to sell items and average savings for buyers compared to new purchases. As the platform grows beyond the Chinese international student community, marketing will emphasize the platform\u0026rsquo;s ease of use and enhanced features compared to general marketplaces, while maintaining the focus on university communities to preserve the trust and verification benefits of the .edu email system.\nRisk Assessment and Mitigation Strategies Several key risks have been identified that could impact the platform\u0026rsquo;s success, along with corresponding mitigation strategies:\nInsufficient Listing Volume: The platform requires a critical mass of listings to attract buyers. To mitigate this risk, pre-launch partnerships with student organizations will be established to seed initial inventory, and incentives will be offered for early sellers. Additionally, the team will consider listing items themselves if necessary to ensure adequate initial inventory.\nAI Quality Issues: If the AI-generated descriptions fail to meet quality standards, sellers may abandon the platform. To address this risk, the system will initially be more conservative in its suggestions, offering simpler descriptions that sellers can enhance rather than attempting complex descriptions that might contain errors. A continuous improvement process based on user edits will be implemented to refine the AI over time.\nTrust and Safety Concerns: Second-hand marketplaces can face issues with fraudulent listings or unsafe transactions. The .edu email verification provides a first layer of protection, which will be supplemented by clear community guidelines, a reporting system for problematic listings or users, and an escrow option for higher-value transactions.\nPayment Verification Challenges: Ensuring payments are properly made and verified is critical to platform trust. Beyond integrating with Zelle, the platform will implement a confirmation system where both parties must acknowledge the transaction is complete before it is finalized in the system.\nCompetitive Response: Existing platforms may attempt to replicate the AI-powered listing feature. To maintain competitive advantage, Marrrket will focus on building deep expertise in the specific needs of the university market segment and continuously enhancing the AI capabilities based on the growing dataset of student transactions.\nConclusion Marrrket represents a significant innovation in the second-hand marketplace sector by directly addressing the primary friction point in the market: the complexity of creating product listings. By leveraging artificial intelligence to dramatically simplify this process, the platform has the potential to unlock substantial untapped inventory in the university second-hand market. The strategic focus on Chinese international students as an initial target market provides a cohesive, geographically concentrated user base with consistent needs, allowing for efficient marketing and network growth. With successful execution of this strategy, Marrrket can establish itself as the preferred second-hand marketplace for university students before expanding to broader markets\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu.\n","permalink":"https://chenterry.com/archived/marrrket/","summary":"\u003cp\u003eMarrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\u003c/p\u003e","title":"Marrrket: AI Listing Secondhand Marketplace"},{"content":"Exploring the evolution of technology and its impact on society Understanding the past is crucial for shaping the future. By studying the evolution of technology, from early computing to modern AI systems, we can better understand how technological innovations have transformed society and anticipate future developments. This theme explores key technological milestones, their societal impacts, and the lessons we can learn from them.\n","permalink":"https://chenterry.com/main-themes/tech-history/","summary":"\u003ch2 id=\"exploring-the-evolution-of-technology-and-its-impact-on-society\"\u003eExploring the evolution of technology and its impact on society\u003c/h2\u003e\n\u003cp\u003eUnderstanding the past is crucial for shaping the future. By studying the evolution of technology, from early computing to modern AI systems, we can better understand how technological innovations have transformed society and anticipate future developments. This theme explores key technological milestones, their societal impacts, and the lessons we can learn from them.\u003c/p\u003e","title":"Tech History"},{"content":"Multi-agent system for cross boarder e-commerce sales automation. Co-founder and head of product. https://cognogpt.com\nCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\nMarket Opportunity The target addressable market size is around 200K-250K, with the marketing being independent brands selling from China \u0026amp; Southeast Asia to the U.S. Our beachhead market is mid-sized Chinese brands selling consumer electronics and other slow-moving consumer goods to the U.S. with the annual recurring rate of more than 2 million USD, consisting around 10K potential end users. Some potential adjacent markets are China and South East Asia B2C E-commerce companies selling unbranded products to the US on their own platform that consolidates products from many factories.\nProducts \u0026amp; Services Cogno+ is an AI-driven interactive assistant designed to enhance e-commerce business growth. It has both personalized engagement and automated and optimized customer service. While the current chatbots and web tools still require extensive workflow setup and are limited in their problem-solving abilities, Cogno has this multi-agent real-time plug-and-play system that offers easy setup, 24-hour support, and high domain knowledge for a better personalization experience that makes us differ from other products in the competition.\nBusiness \u0026amp; Sales Strategy Cogno+ provides mid-sized international e-commerce brands with automated sales support to help them increase conversions and upselling. We aim to explore beyond the current boundaries of human-ai interaction, developing fully scalable sales automation systems for better engagement between brands and customers around the world.\nIn terms of our business strategy, we will employ a B2B SaaS model, charging a tier-based subscription fee based on website traffic (given the cost of using language model APIs). Our business model allows service for a range of e-commerce brands and ensures that brands only pay for the value they are getting via Cogno (increased sales conversion and product upselling).\nFor our product roadmap, building on our existing multi-agent system, we aim to continue research and development of dynamic prompting to improve the coordination capabilities of our central logic agent to allow us to leverage more custom domain agents simultaneously. We are also working to improve user interactions with graphics user interfaces and web interaction as input to language model systems. With these future improvements, we believe we are at the cusp of reinventing how online shopping interactions between brands and customers are made.\nWe will initiate customer acquisition with our beachhead market of midsized Chinese e-commerce brands selling consumer electronics and other slow-moving consumer goods to the US with an annual recurring revenue of more than 2 million, and eventually expand to the larger domestic and international e-commerce landscape. We will target customers through distribution channels such as Shopify, WooCommerce, and WordPress as well as direct sales and expo exhibitions with Chinese online retailers. By targeting industry leaders and influencers, we aim to gain a firm grounding in industries such as consumer electronics and other slow-moving consumer goods verticals. Finally, we will conduct SEO and social media marketing to expand our market beyond the Asia-Pacific to the global e-commerce market, eventually bringing Cogno+ to brands and customers in all of our major e-commerce markets.\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu, Madison Bratley, Ryan Philips, Eric Chen, Echo Zhou, Esther Zhou, Jingfan Yao, Steven Wang, Lu Zhou, Wendy Hu, Wendy Huang, Letty Lin, Jianpeng Su, Professor Eduardo Acuna, Professor Karen Gordon, Jeff Smith, Toni Milushev, The Farley Center, The Garage, as well as everyone who provided mentorship and advice along the way.\n","permalink":"https://chenterry.com/archived/cogno/","summary":"\u003cp\u003eMulti-agent system for cross boarder e-commerce sales automation. Co-founder and head of product.\n\u003ca href=\"https://cognogpt.com\"\u003ehttps://cognogpt.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\u003c/p\u003e","title":"Cogno: Multi-agent AI for Sales Automation"},{"content":"Developing capable multi-agent systems for complex reasoning and human-AI collaboration Single-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\n","permalink":"https://chenterry.com/main-themes/multi-agent-systems/","summary":"\u003ch2 id=\"developing-capable-multi-agent-systems-for-complex-reasoning-and-human-ai-collaboration\"\u003eDeveloping capable multi-agent systems for complex reasoning and human-AI collaboration\u003c/h2\u003e\n\u003cp\u003eSingle-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\u003c/p\u003e","title":"Multi-agent LLM Systems"},{"content":"Extracting meaningful insights from unstructured multi-modal content Most times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\n","permalink":"https://chenterry.com/main-themes/data-insights/","summary":"\u003ch2 id=\"extracting-meaningful-insights-from-unstructured-multi-modal-content\"\u003eExtracting meaningful insights from unstructured multi-modal content\u003c/h2\u003e\n\u003cp\u003eMost times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\u003c/p\u003e","title":"Data Insights"},{"content":"","permalink":"https://chenterry.com/search/","summary":"Search","title":"Search"}]