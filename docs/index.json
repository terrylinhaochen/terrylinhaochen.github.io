[{"content":"Extracting actionable insights from large-scale distributed online conversations. Engineered content optimization features that generate variations and simulate engagement through agentic crowds.\n","permalink":"https://chenterry.com/about/projects/crowdlistening/","summary":"\u003cp\u003eExtracting actionable insights from large-scale distributed online conversations. Engineered content optimization features that generate variations and simulate engagement through agentic crowds.\u003c/p\u003e","title":"CrowdListening | Evanston, Illinois"},{"content":"Agent AI for security.\n","permalink":"https://chenterry.com/about/experience/microsoft/","summary":"\u003cp\u003eAgent AI for security.\u003c/p\u003e","title":"Microsoft | Redmond, Washington"},{"content":"Built LLM system that performs real-time conversation processing and detects conceptual misconceptions. Facilitated Socratic learning through synchronous voice interactions.\n","permalink":"https://chenterry.com/about/research/c3lab/","summary":"\u003cp\u003eBuilt LLM system that performs real-time conversation processing and detects conceptual misconceptions. Facilitated Socratic learning through synchronous voice interactions.\u003c/p\u003e","title":"Northwestern C3 Lab | Evanston, Illinois"},{"content":"Developed LLM system that identifies and connects students with similar regulation gaps, achieving 88% precision and 89% recall in categorization.\n","permalink":"https://chenterry.com/about/research/deltalab/","summary":"\u003cp\u003eDeveloped LLM system that identifies and connects students with similar regulation gaps, achieving 88% precision and 89% recall in categorization.\u003c/p\u003e","title":"Northwestern Delta Lab | Evanston, Illinois"},{"content":"Making lifelong learning engaging and accessible. Research agent ai and generation workflows. Designed and implemented an agentic LLM learning companion, working on search, recommendation, etc.\n","permalink":"https://chenterry.com/about/experience/ouraca/","summary":"\u003cp\u003eMaking lifelong learning engaging and accessible. Research agent ai and generation workflows. Designed and implemented an agentic LLM learning companion, working on search, recommendation, etc.\u003c/p\u003e","title":"Ouraca | Palo Alto, California"},{"content":"Leveraging billion parameter data to provide actionable trends and insights for key agencies. Designed keyword clustering methods for insight extraction, worked on creative video ad creation.\nContent Intelligence and Trend Analysis Developed advanced analytics platforms for understanding content performance patterns, creator audience dynamics, and emerging trends across TikTok\u0026rsquo;s global creative ecosystem.\nAI-Powered Creative Tools Built intelligent content generation tools that help creators optimize their creative workflows, from ideation to production, using machine learning models trained on billions of content interactions.\nKey Achievements Scale: Processed billions of content interactions to extract actionable insights Innovation: Designed novel keyword clustering algorithms for trend identification Impact: Enabled creative teams to make data-driven decisions on content strategy Collaboration: Worked closely with engineering teams to productionize ML models The work focused on bridging the gap between raw platform data and actionable creative insights, helping both internal teams and external agencies understand what drives engagement and virality on the platform.\n","permalink":"https://chenterry.com/about/experience/tiktok/","summary":"\u003cp\u003eLeveraging billion parameter data to provide actionable trends and insights for key agencies. Designed keyword clustering methods for insight extraction, worked on creative video ad creation.\u003c/p\u003e\n\u003ch2 id=\"content-intelligence-and-trend-analysis\"\u003eContent Intelligence and Trend Analysis\u003c/h2\u003e\n\u003cdiv style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 20px 0;\"\u003e\n  \u003cimg src=\"/images/projects/tiktok/tiktok-video-9.png\" alt=\"Content Trend Analysis\" style=\"width: 100%; border-radius: 8px;\"\u003e\n  \u003cimg src=\"/images/projects/tiktok/tiktok-video-10.png\" alt=\"Creator Performance Dashboard\" style=\"width: 100%; border-radius: 8px;\"\u003e\n  \u003cimg src=\"/images/projects/tiktok/tiktok-video-11.png\" alt=\"Audience Insights Platform\" style=\"width: 100%; border-radius: 8px;\"\u003e\n\u003c/div\u003e\n\u003cp\u003eDeveloped advanced analytics platforms for understanding content performance patterns, creator audience dynamics, and emerging trends across TikTok\u0026rsquo;s global creative ecosystem.\u003c/p\u003e","title":"TikTok | San Jose, California"},{"content":"There\u0026rsquo;s a famous quote from Sam Altman: \u0026ldquo;In the near future, we will see one-person billion-dollar companies.\u0026rdquo; We\u0026rsquo;re not in the near future anymore. We\u0026rsquo;re there.\nThe question isn\u0026rsquo;t whether it\u0026rsquo;s possible—it\u0026rsquo;s about which tools make it possible. After building and running CrowdListen, I\u0026rsquo;ve developed a stack that handles everything from market research to code deployment to content creation. What follows isn\u0026rsquo;t a listicle of productivity apps. It\u0026rsquo;s a philosophy of work, implemented through four AI systems that each handle a distinct function of running a company.\nRunning a company solo with AI requires four capabilities: knowing what to build, building it, running it, and growing it. Each pillar has a dedicated AI system. None of them require you to context-switch or manage a team. They just work.\nCrowdListen: Knowing What to Build The hardest part of building products isn\u0026rsquo;t coding—it\u0026rsquo;s knowing what to code. Most founders waste months building features nobody wants because they\u0026rsquo;re operating on intuition instead of evidence. I\u0026rsquo;ve been guilty of this myself. You convince yourself that because you personally experience a pain point, others must too. Sometimes you\u0026rsquo;re right. Often you\u0026rsquo;re not.\nCrowdListen is an AI-powered audience insight platform that transforms social conversations into actionable product specs. It aggregates discussions from Reddit, TikTok, Twitter/X, and the broader web, then uses multi-modal AI pipelines to extract themes, cluster feature requests, and rank them by demand signals. The key insight is evidence-backed product decisions. Every insight links to real conversations. You\u0026rsquo;re not guessing—you\u0026rsquo;re synthesizing what thousands of people have already told you.\nBefore building anything significant, I run a deep analysis on the problem space. What are people actually complaining about? What solutions have they tried? What\u0026rsquo;s missing? For prioritization, every feature request in CrowdListen\u0026rsquo;s kanban board links back to source conversations with quotes and confidence scores. No more \u0026ldquo;I think users want X.\u0026rdquo; Understanding how different segments talk about problems also helps craft messaging that resonates—you learn not just what to build, but how to talk about it.\nThis might seem like an obvious approach, but most founders skip it. There\u0026rsquo;s something seductive about the blank page, about building from pure imagination. But imagination untethered from reality produces products that solve problems nobody has. The discipline of starting with evidence, even when you think you already know the answer, consistently surfaces insights that would have taken months to discover through iteration alone.\nClaude Code: Building Everything Once you know what to build, you need to build it. Claude Code is Anthropic\u0026rsquo;s CLI tool for software engineering, and it\u0026rsquo;s become my entire engineering team.\nThis isn\u0026rsquo;t GitHub Copilot-style autocomplete. This is an agent that plans and executes multi-step tasks autonomously, understands entire codebases through semantic analysis, writes tests and debugs production code, manages git workflows and creates PRs, and uses MCP servers for extended capabilities like database operations and browser testing. The magic is in the iteration. Claude Code doesn\u0026rsquo;t just write code—it thinks through implications, checks for edge cases, and maintains consistency with existing patterns.\nMy typical workflow looks like this: I describe the feature in plain English. Claude Code explores the codebase, understands the architecture. It writes the implementation across multiple files. Runs tests, fixes issues, creates a commit. I review, approve, merge. For CrowdListen, Claude Code handles everything—React frontend, Python backend, Supabase integrations, MCP server development, deployment configurations. It\u0026rsquo;s not a coding assistant. It\u0026rsquo;s a coding agent.\nWhat strikes me most about this shift is how it changes the nature of engineering work. The bottleneck is no longer implementation speed—it\u0026rsquo;s clarity of thought. If I can articulate precisely what I want, it gets built. If my thinking is muddled, the output reflects that. In some ways, Claude Code has made me a better product thinker because it forces me to be explicit about decisions I might have previously hand-waved through.\nOpenClaw: Operating the Business Coding is maybe 30% of running a company. The rest is operations: responding to users, writing documentation, managing tasks, coordinating between systems, handling the endless administrative work that scales with growth.\nOpenClaw is an AI agent that runs locally on my machine and treats my entire workspace as its operating environment. It communicates via natural language through Signal, Telegram, or web, and executes tasks across all my systems. The mental model is simple: OpenClaw is the AI that talks to me in natural language but acts on systems. It reads SOPs, follows workflows, and executes multi-step operations without me touching a keyboard.\nWhat does this look like in practice? I can ask \u0026ldquo;What clips do we have about AI agents?\u0026rdquo; and it searches the library, returning top matches with scores. I can say \u0026ldquo;Make a week of content\u0026rdquo; and it picks clips, queues renders, and monitors progress. \u0026ldquo;What\u0026rsquo;s ready to post?\u0026rdquo; triggers a check of published and review queues with a summary. \u0026ldquo;Sync the library\u0026rdquo; triggers API calls and manages state. These aren\u0026rsquo;t pre-programmed commands—they\u0026rsquo;re natural language requests that OpenClaw interprets and executes.\nThe key difference from Claude Code is that OpenClaw handles non-coding tasks. It\u0026rsquo;s for operations, not implementation. When I need to write code, I use Claude Code. When I need to run the business, I use OpenClaw. This division matters because the mental models are different. Coding requires precision and technical context. Operations require workflow awareness and system coordination. Having specialized agents for each prevents the cognitive overhead of context-switching.\nCrowdListen Video: Marketing at Scale You can build the best product in the world, but if nobody knows about it, you have nothing. Marketing for a solo founder used to mean either spending hours on content creation or spending money on agencies. Neither option scales well when you\u0026rsquo;re trying to move fast.\nCrowdListen Video, internally called CrowdListen Studio, is a two-interface system for creating short-form video content for TikTok and Instagram. It combines a visual Studio web app with OpenClaw\u0026rsquo;s conversational interface. The pipeline works like this: you drop a video into the system, Gemini AI watches it and returns timestamped \u0026ldquo;meme moments\u0026rdquo; ranked by energy score. This creates a clip library with 38+ pre-analyzed moments, each with scores, captions, and visual descriptions that you can browse by source or filter by score.\nContent creation becomes conversational. I can tell OpenClaw \u0026ldquo;Make a clip about scope creep\u0026rdquo; and it searches the library for the best match, picks a clip with the right energy, queues the render, and notifies me when it\u0026rsquo;s done. No video editing software. No context-switching. Just describe what you want and it appears in your review queue.\nWhat makes this approach different from hiring a content team or using a traditional video editor is the feedback loop speed. I can go from idea to published content in minutes rather than days. This matters because marketing effectiveness depends on iteration—testing different angles, seeing what resonates, doubling down on what works. When each iteration takes days, you might get a few dozen attempts per quarter. When each iteration takes minutes, you can run hundreds of experiments.\nThe Integration Layer These four tools don\u0026rsquo;t operate in isolation—they form an integrated system. Market intelligence from CrowdListen tells me what to build. Implementation via Claude Code turns insights into products. Operations via OpenClaw keep everything running. Growth via CrowdListen Video brings in more users. More users generate more feedback, which feeds back into CrowdListen\u0026rsquo;s analysis. This is a flywheel, and each cycle compounds.\nThe philosophy underlying this stack is simple: automate everything except the parts that require your unique judgment. Vision—what should this company become? Taste—what feels right versus what the data says? Relationships—the human connections that can\u0026rsquo;t be delegated. Risk—the bets only you can make. Everything else is infrastructure. And infrastructure should run itself.\nI catch myself sometimes marveling at the absurdity of it. A single person, running what amounts to a fully-staffed company, through conversations with AI systems. It sounds like science fiction. But it\u0026rsquo;s Tuesday. This is just how work happens now.\nWhat\u0026rsquo;s Next The current stack is version 1.0. What\u0026rsquo;s coming excites me more than what already exists. CrowdListen Video will soon auto-post directly to TikTok and Instagram via their APIs. OpenClaw will manage a content calendar, scheduling and queuing automatically based on optimal timing. An analytics loop will track performance and feed back into clip selection algorithms. Most importantly, cross-system memory will allow all agents to share context about what\u0026rsquo;s been built, what\u0026rsquo;s working, and what\u0026rsquo;s not.\nThe one-person billion-dollar company isn\u0026rsquo;t a distant future. It\u0026rsquo;s a toolkit problem. And the toolkit is here.\nTry the Stack If you\u0026rsquo;re curious to experiment with any of these tools, here\u0026rsquo;s where to start. CrowdListen is the audience insight platform—use it to understand what to build. Claude Code is available through Anthropic. CrowdListen Video is open source if you want to build your own content pipeline.\nThe future of work isn\u0026rsquo;t about hiring more people. It\u0026rsquo;s about deploying the right agents.\nRelated Reading Product Engineers and AI Multipliers - How AI is transforming product engineering teams into smaller, more efficient units The Best Practices Lie: On Dealing with Ambiguity - Why institutional training becomes a liability in genuinely uncertain situations From Raw Social Data to Real Research - The technical architecture behind CrowdListen\u0026rsquo;s insight engine ","permalink":"https://chenterry.com/posts/one-person-billion/","summary":"\u003cp\u003eThere\u0026rsquo;s a famous quote from Sam Altman: \u0026ldquo;In the near future, we will see one-person billion-dollar companies.\u0026rdquo; We\u0026rsquo;re not in the near future anymore. We\u0026rsquo;re there.\u003c/p\u003e\n\u003cp\u003eThe question isn\u0026rsquo;t whether it\u0026rsquo;s possible—it\u0026rsquo;s about which tools make it possible. After building and running \u003ca href=\"https://crowdlisten.com\"\u003eCrowdListen\u003c/a\u003e, I\u0026rsquo;ve developed a stack that handles everything from market research to code deployment to content creation. What follows isn\u0026rsquo;t a listicle of productivity apps. It\u0026rsquo;s a philosophy of work, implemented through four AI systems that each handle a distinct function of running a company.\u003c/p\u003e","title":"Running a 1-Person Billion Dollar Company: The AI Stack That Makes It Possible"},{"content":"The Open Web\u0026rsquo;s Second User The open web is a miracle. Anyone can publish, learn, and collaborate. It\u0026rsquo;s the closest thing to humanity\u0026rsquo;s living memory. This open ecosystem fueled today\u0026rsquo;s AI breakthroughs.\nRight now, as you read this, AIs are silently crawling through millions of web pages, processing more information in minutes than most humans consume in months. This isn\u0026rsquo;t the future—it\u0026rsquo;s happening today, and it\u0026rsquo;s about to accelerate exponentially.\nBut here\u0026rsquo;s the problem: the entire web was designed for humans clicking links at human speed. When AIs need to process thousands of sources simultaneously, analyze complex data relationships, and generate insights in real-time, our current infrastructure breaks down completely.\nThe companies that solve this infrastructure mismatch won\u0026rsquo;t just capture market share—they\u0026rsquo;ll control how intelligence itself accesses human knowledge. This is the battle that will determine whether the open web evolves or fractures.\nThe Market Opportunity: Infrastructure for Agents The transition from human-centric to agent-centric web consumption represents one of the largest infrastructure shifts since the birth of the internet. The opportunity extends beyond search improvement to creating the foundational layer that enables autonomous agents to reason, research, and act at scale.\nCurrent Market Players The competitive landscape reveals four distinct strategic approaches to AI-native search infrastructure. Perplexity AI has carved out the consumer market by pioneering conversational search with real-time web access, creating a bridge between traditional search and AI-native interfaces through human-readable answers with source attribution. Meanwhile, Exa AI is building the technical foundation for AI consumption, developing semantic retrieval systems that index content by meaning rather than keywords—a fundamental shift from optimizing for human clicks to serving AI reasoning tasks.\nThe most ambitious vision comes from Parallel Webs, which seeks to create the \u0026ldquo;Programmatic Web\u0026rdquo; where AIs declare their requirements and the system determines fulfillment automatically, extending beyond search to encompass computation, reasoning, and verifiable provenance in an open market system. OpenAI represents the integration play, weaving search capabilities directly into the AI reasoning loop through SearchGPT and web browsing, making web access feel like extended memory rather than a separate function.\nThese approaches reveal the fundamental strategic question facing the market: whether AI search will evolve existing paradigms or require completely new infrastructure designed from first principles for machine consumption.\nThe 10x Infrastructure Challenge Traditional search infrastructure optimized for human consumption creates fundamental mismatches for AI agents. Humans need fast results, while AIs can trade time for comprehensive analysis. HTML and CSS were designed for visual rendering, but agents require structured data for reasoning. The scale disparity grows stark: humans browse dozens of pages, while AIs might process thousands simultaneously. Additionally, AIs need verifiable provenance rather than the trust signals humans rely on.\nThe Content Understanding Problem Beyond infrastructure lies an even deeper challenge: teaching machines to understand content the way humans do, while serving entirely different information needs. The complexity becomes apparent when examining how human queries contain multiple layers of meaning that current systems struggle to decompose.\nConsider the deceptively simple search \u0026ldquo;best laptop for programming.\u0026rdquo; Humans understand this as a request for product recommendations filtered by use case, performance benchmarks specific to development tasks, price-to-value analysis within implicit budget constraints, compatibility with development environments, and reliability considerations for professional use. Current AI search systems typically return surface-level product lists rather than the comprehensive decision frameworks humans actually need.\nContext sensitivity compounds this challenge dramatically. The same query terms can indicate vastly different intents depending on context—\u0026ldquo;Python performance\u0026rdquo; might refer to programming language optimization, snake behavior analysis, or Monty Python comedy reviews. \u0026ldquo;Scaling issues\u0026rdquo; could mean business growth challenges, software architecture problems, or literal measurement difficulties. AI systems must develop sophisticated disambiguation capabilities that go far beyond keyword matching to understand semantic relationships and contextual clues.\nTemporal relevance adds another layer of complexity that traditional search largely ignores. AI agents need to understand not just what information is accurate, but when it\u0026rsquo;s applicable. Technology recommendations become obsolete within months, regulatory information varies by jurisdiction and changes frequently, and scientific research builds incrementally with newer studies potentially invalidating older conclusions. This temporal understanding requires AI systems to maintain dynamic knowledge graphs that track the evolution of information over time.\nPerhaps most challenging is the shift toward multimodal content consumption. Modern information increasingly combines text, images, videos, and interactive elements in ways that traditional text processing cannot handle. Product reviews now embed comparison charts and video demonstrations alongside written analysis, while technical documentation includes code samples, architectural diagrams, and interactive tutorials. AI search systems must extract meaning across these modalities while preserving the complex relationships between different content types—a challenge that requires fundamental advances in cross-modal understanding rather than incremental improvements to existing approaches.\nTechnical Foundations: Relevance Models and Intent Classification Understanding how AI search systems actually work requires examining the sophisticated machine learning architectures underlying modern search relevance. The transition from keyword-based retrieval to semantic understanding represents one of the most significant technical advances in information retrieval.\nEvolution of Relevance Models Classical Approaches: Traditional search relied heavily on term frequency-inverse document frequency (TF-IDF) and BM25, which measure relevance based on statistical word occurrence patterns. BM25 leverages term frequency and inverse document frequency to rank documents effectively, achieving strong performance through its intuitive probabilistic foundations. Despite being decades old, BM25 remains a cornerstone of traditional information retrieval systems due to its simplicity and inherent interpretability.\nDense Retrieval Revolution: Modern AI search systems increasingly employ dense retrieval models that represent text as compact numerical vectors (embeddings) capturing semantic meaning. Unlike keyword matching or sparse statistical methods, dense retrieval uses neural networks to encode sentences, paragraphs, or documents into high-dimensional vector spaces where semantic similarity translates to geometric proximity.\nDense Passage Retrieval (DPR) represents a foundational approach, using bi-encoders with separate neural networks for queries and documents. DPR summarizes entire documents in single token embeddings, enabling fast semantic matching but potentially losing granular detail.\nColBERT (Contextualized Late Interaction over BERT) advances this approach by maintaining token-level embeddings for both queries and documents. Rather than compressing everything into single vectors, ColBERT preserves fine-grained semantic information, then performs \u0026ldquo;late interaction\u0026rdquo; by comparing query tokens with document tokens. This architecture achieves up to 26% improvement in Mean Average Precision on MSMARCO passage ranking datasets while maintaining computational efficiency comparable to single-vector approaches.\nHybrid Integration: Recent research reveals that state-of-the-art neural models don\u0026rsquo;t replace classical approaches but enhance them. Cross-encoder variants of models like MiniLM employ semantic variants of BM25, using transformer attention heads to compute soft term frequency while controlling for term saturation and document length effects. This suggests neural models leverage the same fundamental mechanisms as BM25 while adding semantic understanding capabilities.\nLearning to Rank Architectures AI search systems employ sophisticated Learning to Rank (LTR) approaches that fall into three categories, each with distinct advantages for different use cases:\nPointwise Ranking treats relevance scoring as a regression or classification problem, predicting individual document relevance scores. While conceptually simple and compatible with standard machine learning algorithms, pointwise approaches optimize for score accuracy rather than relative ranking quality, potentially compromising ordering performance.\nPairwise Ranking focuses on relative document ordering by comparing document pairs and optimizing for correct pairwise preferences. Popular algorithms like RankNet, LambdaRank, and LambdaMART employ pairwise approaches because predicting relative order aligns more closely with ranking objectives than absolute score prediction. However, computational complexity scales quadratically with document count, and pairwise methods may produce globally inconsistent rankings.\nListwise Ranking directly optimizes entire document lists, enabling metric-driven loss functions that incorporate ranking evaluation measures. This approach can learn complex item relationships and dependencies within result lists, making it particularly effective for query-dependent ranking where document relevance varies significantly based on search context. The computational expense and large labeled data requirements limit listwise adoption to resource-rich environments.\nIntent Classification and Query Understanding Modern AI search systems must decompose natural language queries into structured intent representations—a process requiring sophisticated natural language understanding capabilities.\nIntent Complexity Decomposition: When users search \u0026ldquo;best laptop for programming,\u0026rdquo; they\u0026rsquo;re actually expressing multiple layered intents: product recommendations filtered by use case, performance benchmarks for development tasks, price-to-value analysis within budget constraints, compatibility requirements, and reliability considerations. Advanced intent classification systems use transformer-based models like BERT to achieve 89% combined test accuracy compared to 66% with traditional approaches.\nContextual Disambiguation: The same query terms can indicate vastly different intents depending on context. \u0026ldquo;Python performance\u0026rdquo; might refer to programming language optimization, snake behavior analysis, or comedy reviews. Intent classification systems must resolve these ambiguities using contextual embeddings that capture semantic relationships beyond surface-level keyword matching.\nQuery Enhancement Pipeline: Modern query understanding involves several sequential processing steps. Spell-checking corrects common typos (\u0026ldquo;Pythn\u0026rdquo; → \u0026ldquo;Python\u0026rdquo;), entity extraction identifies proper nouns and structured data (\u0026ldquo;iPhone 15,\u0026rdquo; \u0026ldquo;New York\u0026rdquo;), and query expansion adds semantically related terms using embedding models or knowledge graphs. This preprocessing transforms raw user input into structured representations suitable for semantic matching.\nMultilingual and Multimodal Extensions: Advanced systems like Google\u0026rsquo;s MUM (Multitask Unified Model) extend intent understanding beyond text to images, video, and audio content while supporting multilingual queries. These capabilities enable AI agents to understand and respond to complex, multimodal information requests that traditional keyword-based systems cannot process.\nImplications for AI-Native Search These technical foundations reveal why building effective AI search infrastructure presents such significant challenges. Companies must master not just individual techniques but their complex interactions: how dense retrieval models integrate with learning-to-rank systems, how intent classification informs query expansion, and how multimodal understanding scales across languages and content types.\nThe technical complexity explains the performance gaps visible in current benchmarks. Systems that successfully integrate these components—like Parallel AI\u0026rsquo;s superior performance on complex research tasks—demonstrate meaningful architectural advantages over approaches that excel in only single dimensions. However, the rapid pace of advancement in each technical area suggests these advantages may prove temporary as competitors master similar integration challenges.\nThe Vision: A Programmatic Web Tomorrow\u0026rsquo;s dominant architecture will abandon adaptation for complete infrastructure reimagining:\nUnified Data, Compute, Reasoning Instead of returning static documents, search results become executable environments. When an AI needs to analyze market trends, it doesn\u0026rsquo;t just get links to reports—it gets the data, analytical tools, and computation needed to run custom analyses. For instance, when researching \u0026ldquo;renewable energy ROI by region,\u0026rdquo; instead of receiving static PDFs, an AI would access live datasets, financial models, and analytical tools to generate custom investment analyses with verified calculations.\nDeclarative Interfaces Today\u0026rsquo;s AIs must reverse-engineer what they need from human-oriented search results. Tomorrow\u0026rsquo;s infrastructure will understand intent directly. Instead of searching \u0026ldquo;renewable energy adoption rates 2024\u0026rdquo; and parsing through marketing pages, AIs will declare: \u0026ldquo;I need verified renewable energy deployment data by country and technology type, with confidence intervals and methodology documentation.\u0026rdquo; The infrastructure translates intent to execution automatically.\nTransparent Attribution Built so every source and insight is tracked and credited. Contributions become measurable and transparent. This creates economic incentives for high-quality data contribution rather than the current attention-based economy.\nOpen, Value-Based Markets Incentivized so participants earn based on value they add. Staying open wins, not due to virtue, but because it\u0026rsquo;s economically superior. Data providers get compensated for quality and usage, not just eyeballs.\nThis infrastructure transformation is already creating a competitive battlefield, with distinct players pursuing fundamentally different strategies.\nMarket Competition Analysis The current market reveals significant funding competition, with both Parallel AI and Exa AI achieving similar valuations around $700-740M despite different strategic approaches. Parallel AI has raised $130M+ focused on comprehensive agent infrastructure, while Exa AI has secured $107M+ targeting AI-native search specialization.\nPerformance benchmarks demonstrate meaningful differentiation, with Parallel AI showing 4.1x better accuracy on OpenAI\u0026rsquo;s BrowseComp benchmark and superior cost efficiency across multiple testing scenarios. However, these technical advantages may prove temporary as the market evolves toward either platform dominance or commodity competition (detailed metrics available in Appendix A).\nCompetitive Dynamics Technical superiority alone won\u0026rsquo;t determine winners; network effects and market positioning drive the real competition:\nThe Infrastructure Play: Parallel Webs is building comprehensive agent infrastructure, achieving superior performance benchmarks while offering cost-effective solutions for enterprise workflows.\nThe Search Specialization Play: Exa AI focuses on perfecting AI-native search with semantic understanding, targeting developers building AI applications.\nThe Integration Play: Companies like OpenAI are betting that seamless integration with AI reasoning will win, making search invisible but essential.\nThe Experience Play: Perplexity is proving that AI-enhanced search can create superior user experiences, building brand loyalty in the transition period.\nThe competitive landscape reveals a deeper strategic question: will AI search infrastructure become the next dominant platform category, or will it follow the path of cloud computing toward commoditization?\nEarly indicators suggest both outcomes are possible. Current performance gaps show meaningful differentiation, but cloud infrastructure\u0026rsquo;s evolution warns us that technical advantages often prove temporary. Winners in commodity markets succeed through scale, integration, and operational excellence—not just superior algorithms.\nThis timeline pressure intensifies the current battle. Technical leaders like Parallel AI must build platform effects and ecosystem lock-in before their performance advantages erode. Meanwhile, integrated players like OpenAI may ultimately win through distribution, even if their pure search capabilities lag.\nGlobal Market Fragmentation: The China Factor But this battle isn\u0026rsquo;t happening in a vacuum. The vision of universal AI search infrastructure faces a harsh reality: the global web is fragmenting along geographical and platform lines, with China representing the most dramatic example of an alternative model.\nChina\u0026rsquo;s Alternative Web Architecture China\u0026rsquo;s information ecosystem operates on different principles than the open web:\nPlatform-Centric Content: Instead of distributed websites, content concentrates on super-platforms:\nWeChat Official Accounts: Professional and media content lives within WeChat\u0026rsquo;s ecosystem Xiaohongshu (Little Red Book): Product discovery and lifestyle content in a closed platform Douyin/TikTok: Short-form content dominates attention and discovery Toutiao (ByteDance): News and information aggregation within proprietary algorithms Case Study: TikTok and Xiaohongshu as Search Destinations The shift away from traditional web search is most visible in how younger consumers discover and research products. More than half of people now prefer to research products on video and social platforms over traditional browsers—a fundamental change that challenges the entire premise of web-based AI search infrastructure.\nTikTok: Video-First Search Monetization TikTok exemplifies how platform-centric search creates new economic models entirely separate from the open web:\nSearch Behavior Patterns:\n57% of TikTok users actively utilize the platform\u0026rsquo;s search functionality 23% search within 30 seconds of opening the app 58% discover new brands on the platform, 1.5x higher than other platforms Search as Revenue Surface: TikTok has transformed search from an information retrieval tool into a sophisticated advertising platform through:\nAutomatic Search Placement: Ads seamlessly integrated into search results using existing In-Feed creative Search Ads Campaign: Keyword-based targeting specifically for search results pages Predictive Search Monetization: Algorithm-driven keyword suggestions that can be monetized (as shown in targeting interfaces with monthly search volumes for \u0026ldquo;card games\u0026rdquo; reaching 9K searches) This represents a complete inversion of traditional search economics. Instead of serving neutral results with separate ad placements, the search experience itself becomes the primary monetization surface.\nXiaohongshu: AI-Enhanced Visual Discovery Xiaohongshu represents the most sophisticated example of AI-native search within a closed platform ecosystem, demonstrating how platforms are building their own AI search capabilities independent of web infrastructure:\nNative AI Search Integration: Xiaohongshu has deployed \u0026ldquo;点点\u0026rdquo; (DiànDiǎn), an AI search assistant that provides:\nLifestyle Service Integration: AI-powered travel, entertainment, and lifestyle recommendations Conversational Search: Natural language queries like \u0026ldquo;长沙一日游推荐\u0026rdquo; (Changsha one-day trip recommendations) generate comprehensive, contextual responses Multi-Modal Understanding: The AI processes text queries, images, and user context to deliver personalized recommendations Predictive Search Architecture: The platform\u0026rsquo;s search suggestions demonstrate sophisticated understanding of user intent:\nAutocompleted queries range from specific locations (\u0026ldquo;长沙网红打卡地方推荐\u0026rdquo;) to experiential searches (\u0026ldquo;长沙好玩的地方推荐一日游\u0026rdquo;) Search suggestions include temporal elements, showing the platform understands context like seasons, events, and trending topics Each query connects to both user-generated content and commercial opportunities seamlessly Visual-First Information Architecture: Unlike text-based search engines, Xiaohongshu\u0026rsquo;s search creates fundamentally different data relationships:\nImage-Centric Results: Search results prioritize visual content over text descriptions Creator-Commerce Integration: Search results blend user experiences, product recommendations, and purchase opportunities in unified visual layouts Cultural Context Embedding: AI understands cultural nuances, local preferences, and social signals that traditional web crawling cannot capture Platform-Specific AI Advantages: Xiaohongshu\u0026rsquo;s closed ecosystem enables AI capabilities that external search providers cannot replicate:\nBehavioral Data Integration: AI learns from user interactions, saves, purchases, and engagement patterns within the platform Real-Time Content Understanding: Fresh, culturally relevant content is immediately available to AI systems without crawling delays Economic Alignment: AI recommendations optimize for platform engagement and monetization rather than neutral information retrieval Implications for AI Search Infrastructure These platform-centric models create several challenges for companies building universal AI search infrastructure:\nContent Access Barriers: The most valuable and current information increasingly lives behind platform APIs with restrictive access policies. AI agents may struggle to access the content that humans actually consume for decision-making.\nMonetization Competition: Platforms like TikTok generate revenue directly from search interactions, creating economic incentives to keep users within their ecosystem rather than directing them to external information sources.\nUser Behavior Divergence: As search behavior shifts toward visual, social, and algorithm-curated discovery, traditional text-based search infrastructure becomes less relevant to actual human information consumption patterns.\nEconomic Model Mismatch: Platform-centric search monetizes attention and engagement within closed loops, while web-based AI search infrastructure depends on open content access and attribution—fundamentally incompatible economic models.\nImplications for Global AI Infrastructure Baidu\u0026rsquo;s Positioning: Baidu\u0026rsquo;s AI search efforts (Ernie Bot, integrated search) may dominate Chinese markets not through superior technology, but through platform partnerships and regulatory alignment. Western AI search providers face structural disadvantages in accessing Chinese content platforms.\nRegional Infrastructure Requirements: The fragmented global landscape suggests that AI search infrastructure may need to be regionally specialized rather than globally uniform. This could favor:\nLocal Champions: Companies like Baidu in China, or potential regional players in Europe/India Platform Partnerships: Direct integrations with dominant local platforms Regulatory Compliance: Infrastructure designed around local data sovereignty requirements The Western Assumption Risk: Current AI search development assumes the Western open web model is universal. But if major markets operate through closed platforms, purely web-based AI search may capture only a fraction of global information consumption.\nThe Stakes The goal transcends web preservation. The opportunity lies in unlocking what comes next: AIs solving complex problems, accelerating discovery, creating things we can\u0026rsquo;t yet imagine.\nThe choice is binary: we build the open web for its second user, or it fractures beyond repair. Victory here extends beyond market dominance to controlling how intelligence accesses and processes human knowledge.\nAs agents become our primary interface to information, the infrastructure serving them becomes the most critical layer in the knowledge economy. The question isn\u0026rsquo;t whether this transition will happen, but who will build the rails for humanity\u0026rsquo;s next chapter.\nAt stake is nothing less than the future of how intelligence—both human and artificial—discovers, processes, and acts on information. The battle for search is really the battle for the cognitive infrastructure of tomorrow.\nAppendix A: Market Data and Performance Analysis Pricing Comparison Feature Parallel AI Exa AI Free Tier 20,000 requests $10 credits Search API $0.004 - $0.009 per request $5 per 1K requests (1-25 results) Premium Search - $25 per 1K requests (26-100 results) Task API (Deep Research) $0.005 - $2.40 per request Not available Chat API $0.005 per request Not available Monthly Plans Pay-per-use only Core: $49/month (8K credits)\nPro: $449/month (100K credits) Enterprise Custom pricing + volume discounts Custom pricing Rate Limits Search: 600 req/min\nTask: 2,000 req/min Not specified Startup Credits Up to $5K for qualified startups Standard free credits only Funding and Valuation Company Total Funding Latest Round Valuation Key Investors Parallel AI $130M+ Series A: $100M (Nov 2024) $740M Kleiner Perkins, Index Ventures, Khosla Ventures, First Round Exa AI $107M+ Series B: $85M (Sep 2024) $700M Benchmark, Lightspeed, NVIDIA (NVentures), Y Combinator Performance Benchmarks Benchmark Parallel AI Exa AI Key Difference BrowseComp (OpenAI) 58% accuracy 14% accuracy 4.1x better accuracy WISER Benchmark 81% accuracy @ $42 CPM 48% accuracy @ $107 CPM 1.7x accuracy, 2.5x cost efficiency BrowseComp Subset 58% accuracy @ $156 CPM 29% accuracy @ $233 CPM 2x accuracy, 1.5x cost efficiency Enterprise Deep Research 48% accuracy 14% accuracy 3.4x better performance *CPM = Cost Per Million requests\n*Source: Company benchmark reports and public performance data, November 2024\nTechnology Positioning Aspect Parallel AI Exa AI Core Focus Programmatic web infrastructure for agents Semantic search engine for AI Search Approach Multi-modal: search + deep research + reasoning Neural link prediction and embeddings Target Market Enterprise AI agents and automation Researchers and AI developers Differentiation End-to-end agent workflow platform Purpose-built AI-native search Performance Superior on complex research tasks Strong on semantic understanding Cost Model Predictable per-request pricing Credit-based with monthly tiers ","permalink":"https://chenterry.com/posts/battle_for_search/","summary":"\u003ch2 id=\"the-open-webs-second-user\"\u003eThe Open Web\u0026rsquo;s Second User\u003c/h2\u003e\n\u003cp\u003eThe open web is a miracle. Anyone can publish, learn, and collaborate. It\u0026rsquo;s the closest thing to humanity\u0026rsquo;s living memory. This open ecosystem fueled today\u0026rsquo;s AI breakthroughs.\u003c/p\u003e\n\u003cp\u003eRight now, as you read this, AIs are silently crawling through millions of web pages, processing more information in minutes than most humans consume in months. This isn\u0026rsquo;t the future—it\u0026rsquo;s happening today, and it\u0026rsquo;s about to accelerate exponentially.\u003c/p\u003e","title":"The Battle for Search: Defining the 10x Infrastructure Layer for an Agentic Future"},{"content":"In the fall of 2022, I sat down with my co-founders to discuss what we wanted to achieve by building Cogno. Our goal was simple: gain the expertise to build and scale so we\u0026rsquo;d be ready when we faced an opportunity truly worth pursuing. We wanted to learn by doing, to understand the mechanics of creation before the next big wave hit.\nTwo months later, ChatGPT launched.\nAfter getting my hands on it during Thanksgiving break, I knew it was going to mark a new era of what\u0026rsquo;s possible. The interface was crude, the responses inconsistent, but the glimpse of capability was unmistakable. Building AI agents suddenly shifted from distant science fiction to immediate possibility—a combination of imagination and rapid prototyping that investors found genuinely interesting.\nWe were at the right timing, or so it seemed.\nThe Momentum Lesson A year later, we had built a functioning system with real progress. Yet marketing to SMBs for tech adoption proved to be more of an ordeal than building an intricate system. The technical challenges—the ones we loved solving—turned out to be the easy part. The hard part was convincing small businesses to change how they worked, to trust a new system, to invest time in learning something different.\nIt was during this exact period that Turbo.ai started to take off. I remember thinking our product had more technical depth, that we\u0026rsquo;d built something more defensible with better architecture. I was focused on the elegance of the solution rather than the urgency of the problem it solved.\nTurns out, I was more than wrong.\nIn this new age of AI, momentum is all that matters. Technical depth, while valuable, takes a backseat to speed, market positioning, and the ability to capitalize on windows of opportunity before they close. Turbo understood something we missed: in rapidly evolving markets, being first often trumps being best.\nThe Commercialization Reality My subsequent experience at ByteDance and TikTok reinforced another uncomfortable truth: commercialization of AI remains a far stretch for many consumer products. Working on Symphony products at TikTok, I witnessed the gap between AI capability and real commercial value. The technology was impressive, but translating that into sustainable business models—especially in consumer contexts—proved consistently challenging.\nThe enterprise world moves differently. B2B customers will pay for clear ROI and efficiency gains. But consumer applications require something deeper: they need to become habits, to solve problems people didn\u0026rsquo;t know they had, to create new behaviors rather than just optimize existing ones.\nThe Funding Signal Now, a year later, as funding rounds get bigger and bigger, I\u0026rsquo;m starting to wonder if we\u0026rsquo;re witnessing preparation for a funding winter. When capital becomes abundant for AI startups, it often signals that investors are placing their bets before a potential downturn. Companies are sealing their positions at the table, raising large rounds to survive the deeper waters ahead.\nThere\u0026rsquo;s a palpable sense of urgency in the ecosystem. Not the good kind of urgency that comes from clear market demand, but the anxious kind that comes from feeling like tickets for opportunities are selling out. Every week brings news of another massive AI round, another player securing their spot in what feels like an increasingly exclusive game.\nThe Window Frame This brings me to the concept I\u0026rsquo;ve been thinking about: the closing window frame of AI applications.\nIn the early days of any transformative technology, there\u0026rsquo;s a period of experimentation where many approaches seem viable. Multiple technical paths, business models, and market strategies coexist. This is the wide-open window phase—lots of room for different players to find their niche.\nBut as the technology matures and market dynamics clarify, the window frame starts to close. Successful patterns emerge. Distribution advantages compound. Network effects kick in. Capital requirements increase. What once felt like an open field becomes a defined arena with clear winners and everybody else.\nI believe we\u0026rsquo;re watching this transition happen in real-time with AI applications. The experimental phase is ending. The consolidation phase is beginning.\nThe Agentic Opportunity A generation of agentic startups is taking shape. Werdelin believes we\u0026rsquo;re still in the earliest days of figuring out what \u0026ldquo;agentic businesses\u0026rdquo;—companies built around AI agents—will actually look like. He compares it to the early 2000s when he was building an internet video startup before YouTube came around. Back then no one knew how to answer simple questions like: Should videos autoplay when you open a link, or require a click? Should the next one start automatically? As he builds Audos, Werdelin is realizing that many of these basic features simply don\u0026rsquo;t exist yet for generative AI.\nThis observation reinforces the urgency I feel about timing. We\u0026rsquo;re not just in the early days of AI adoption—we\u0026rsquo;re in the early days of understanding what AI-native experiences should even look like. The fundamental interaction patterns, the basic UX conventions, the core product assumptions are all still being established. This represents both massive opportunity and massive risk, depending on how quickly you can move and how well you can read the emerging patterns.\nThe Urgency I Feel There\u0026rsquo;s a specific urgency I feel watching this unfold. It\u0026rsquo;s not FOMO exactly—it\u0026rsquo;s more like watching the last few seats at a table fill up, knowing that the conversation happening at that table will determine the next decade of technological progress.\nThe question isn\u0026rsquo;t whether AI will transform everything—that\u0026rsquo;s already decided. The question is who will build the infrastructure, platforms, and applications that define how that transformation happens. And increasingly, it feels like those positions are being claimed right now.\nFor those of us who\u0026rsquo;ve been building in this space, who\u0026rsquo;ve learned the hard lessons about technical depth versus market momentum, who understand both the possibilities and the pitfalls—this feels like the moment where preparation meets opportunity. The window frame is closing, but it\u0026rsquo;s not closed yet.\nThe tickets aren\u0026rsquo;t sold out. But they\u0026rsquo;re selling fast.\n","permalink":"https://chenterry.com/posts/closing_window_ai/","summary":"\u003cp\u003eIn the fall of 2022, I sat down with my co-founders to discuss what we wanted to achieve by building Cogno. Our goal was simple: gain the expertise to build and scale so we\u0026rsquo;d be ready when we faced an opportunity truly worth pursuing. We wanted to learn by doing, to understand the mechanics of creation before the next big wave hit.\u003c/p\u003e\n\u003cp\u003eTwo months later, ChatGPT launched.\u003c/p\u003e\n\u003cp\u003eAfter getting my hands on it during Thanksgiving break, I knew it was going to mark a new era of what\u0026rsquo;s possible. The interface was crude, the responses inconsistent, but the glimpse of capability was unmistakable. Building AI agents suddenly shifted from distant science fiction to immediate possibility—a combination of imagination and rapid prototyping that investors found genuinely interesting.\u003c/p\u003e","title":"The Closing Window Frame of AI Applications"},{"content":"Google\u0026rsquo;s Learn About reframes both search and chat as an explore-first learning loop. It lowers the cost of asking good questions, manages cognitive load with structured, clickable cards, and continuously surfaces \u0026ldquo;questions you didn\u0026rsquo;t know to ask.\u0026rdquo; Users resolve a specific doubt while simultaneously zooming out to the surrounding question space, all without breaking their thinking flow.\nLearn About\u0026rsquo;s clean interface design centers around the core question \u0026ldquo;What would you like to learn about?\u0026rdquo; The PDF Reading Companion offers guided document analysis, while The Reading Nook provides curated book exploration with visual book covers that invite browsing and discovery.\nThe Problem: Friction in Learning Discovery Most learners struggle with two recurring frictions: picking what to study and understanding what they picked. Learn About addresses both by turning fragmented minutes into genuine learning and durable curiosity. It does this by minimizing the friction of asking, embedding follow-up modules directly inside answers, and maintaining interactivity so that exploration continues without re-prompting.\nThe experience privileges clarity, moderate information density, and explicit emphasis on what matters most, which together reduce overload while preserving momentum. Reflection is encouraged through timely prompts and lightweight feedback, and the system adapts to the learner by adjusting difficulty, sequencing, and recommendations as it observes preferences and progress.\nLearn About\u0026rsquo;s interface for exploring \u0026ldquo;Dystopian Literature and Nineteen Eighty-Four\u0026rdquo; demonstrates the card-based approach with suggested topics, contextual information, and interactive elements that maintain learning flow.\nWhat Makes \u0026ldquo;Learn About\u0026rdquo; Different The single biggest unlock is that Learn About lowers the cost of asking. Instead of waiting for users to craft perfect prompts, it embeds ready-to-tap follow-ups into each response so the next question is one click away. Equally important, it lets users zoom out without losing the thread by showing neighboring questions alongside the current answer. This arrangement promotes deeper dives and lateral pivots within the same cognitive flow.\nBecause it blends search-grade grounding and multi-modal sourcing with the flexibility of conversational chat, users get precise, cite-able synthesis that remains responsive and compact. Most distinctively, Learn About treats \u0026ldquo;unknown unknowns\u0026rdquo; as a first-class job to be done by regularly proposing high-leverage sub-questions and adjacent topics that expand the learner\u0026rsquo;s map.\nThe conversational AI system architecture shows how Learn About structures the learning experience through suggested topics, learning aids, exploration cards, and smart prefills that reduce prompting friction.\nInteractive Cards: The Core Design Pattern Cards are the core unit of meaning and motion in Learn About. The system provides multiple card types that give learners different ways to proceed without retyping:\nInteractive Lists present related content in browsable, clickable formats. For example, when exploring \u0026ldquo;The Scream,\u0026rdquo; users can browse different versions of the artwork with visual thumbnails and contextual information.\nInteractive List cards allow users to explore related content visually, as shown with different versions of \u0026ldquo;The Scream\u0026rdquo; painting, each with thumbnail images and descriptive details.\n\u0026ldquo;Why It Matters\u0026rdquo; Cards explain contemporary relevance and applications. These cards help learners understand why historical or abstract concepts remain significant today.\n\u0026ldquo;Why It Matters\u0026rdquo; cards provide contemporary context, explaining how \u0026ldquo;The Scream\u0026rdquo; became a universal cultural icon representing anxiety and inner turmoil across different media.\nKnowledge Checks convert passive reading into active recall. These lightweight quizzes help reinforce learning without breaking the exploration flow.\nKnowledge check cards engage users with interactive quizzes that test understanding while maintaining the conversational flow.\nVocabulary Builders introduce and define key terms with pronunciation guides and clear explanations.\nVocabulary cards provide pronunciation guides and comprehensive definitions, helping learners build domain-specific knowledge progressively.\nSimple controls like \u0026ldquo;Simplify,\u0026rdquo; \u0026ldquo;Go Deeper,\u0026rdquo; and \u0026ldquo;Get Quotes\u0026rdquo; let learners tune granularity on demand. Smart prefills provide a few contextually relevant starters so that continuing the thread always feels one tap away.\n\u0026ldquo;Learn About,\u0026rdquo; Adapted to Books A reading-first Learn About helps people quickly grasp and then deepen a book\u0026rsquo;s ideas through short, lively exchanges that unfold as modular replies. The system demonstrates sophisticated contextual awareness by dynamically suggesting related topics and themes as users explore literary works.\nThe contextual sidebar shows how Learn About surfaces related historical and cultural topics while reading The Great Gatsby, including \u0026ldquo;The Roaring Twenties,\u0026rdquo; \u0026ldquo;The Jazz Age,\u0026rdquo; and \u0026ldquo;Prohibition in the United States,\u0026rdquo; creating natural pathways for deeper exploration.\nKey components include Contextual Background that clarifies the time, author, and intellectual setting, Key Concepts that introduce terms and definitions with crisp explanations, Reflection Prompts that invite a pause to think before proceeding, Relevance Notes that explain why an idea matters now and where it applies, Intertextual Pointers that connect the current book to contrasting viewpoints elsewhere, and Quotations that anchor the discussion in the text with light annotation that preserves the author\u0026rsquo;s voice.\nThe analysis interface demonstrates Learn About\u0026rsquo;s depth by providing thematic breakdowns like \u0026ldquo;The Geography of Wealth and Class,\u0026rdquo; \u0026ldquo;The Illusion of the American Dream,\u0026rdquo; and \u0026ldquo;The Role of Women in the 1920s\u0026rdquo; alongside the primary text, enabling rich contextual learning.\nEvery element is designed to be tappable so that the next micro-question feels like continuing a conversation rather than composing a fresh prompt. This creates a seamless learning experience where curiosity drives discovery rather than predetermined lesson plans.\nCompetitive Positioning General-purpose LLMs offer flexible dialogue but depend heavily on the user\u0026rsquo;s prompting skill and do not consistently deliver structured learning surfaces. Traditional search excels at source grounding but leaves the curation, triage, and synthesis to the user. Community forums provide depth and texture at the cost of time and noise, since value is scattered across threads.\nA reading-focused Learn About occupies the middle ground: it retains search-level grounding and community-like richness while orchestrating everything into concise, interactive cards that reduce question friction and guide depth. In practice, this means less skimming, less context-switching, and more time spent understanding.\nCore Product Logic Two design commitments drive the experience:\nReduce Prompting Cost: The interface embeds good questions into the prior answer so the user advances with clicks rather than blank-box composition.\nMaximize Effective Information: Each turn delivers the smallest amount needed to unlock a next action, whether that means asking a sharper question, recognizing a pivotal idea, or applying something concrete.\nShort, modular replies keep attention intact and make it obvious where to go next.\nImplementation Strategy Scope and Roadmap for Books Domain Coverage should begin with a well-curated canon of classics that already have rich secondary literature, then expand toward the long tail through retrieval-augmented generation. Structured sources such as high-quality guides and glossaries can provide reliable scaffolds, while unstructured sources such as reviews and community discussions add nuance and counterpoints.\nMultiple entry points make the system feel native in different contexts:\nMain conversation for direct book Q\u0026amp;A Podcast page for audio-led overview that feeds context into chat Search page for natural-language book finding grounded in catalog corpus Book detail page for deep dives seeded by highlights or chapters Personalization and Memory A lightweight user model grows with every interaction by observing which cards the learner opens, where they came from, and what they bookmark. Over time, the system tunes difficulty, chooses modalities that fit a person\u0026rsquo;s habits, and proposes recommendations that align with their goals. The aim is not heavy-handed adaptation but gentle alignment that keeps the learning curve inside the user\u0026rsquo;s zone of proximal development.\nTechnical Implementation The PDF Reading Companion showcases Learn About\u0026rsquo;s document analysis capabilities, providing contextual information about \u0026ldquo;The Great Gatsby\u0026rdquo; with options to \u0026ldquo;Simplify,\u0026rdquo; \u0026ldquo;Go deeper,\u0026rdquo; or \u0026ldquo;Get images,\u0026rdquo; demonstrating the system\u0026rsquo;s adaptive response to user learning preferences.\nA simple Node.js and Firebase stack is sufficient for a working prototype. An ad-hoc but consistent JSON schema can power card templates, and a clear behavioral system prompt can keep outputs compact, structured, and render-ready. Resource types may include articles, diagrams, videos, photos, timelines, and tables, each with known fields that the renderer expects.\nThe PDF Reading Companion interface reveals sophisticated document processing capabilities that maintain context across different interaction modes. Users can seamlessly transition between reading the source material and exploring related concepts, with the system maintaining awareness of their current position and learning objectives throughout the session.\nStart with a few shot examples and conservative temperature, then graduate to lightweight finetunes for the related-question generator as traffic grows and patterns stabilize.\nGreat questions live in the long tail, so the system benefits from synthetic query generation to probe coverage gaps and from semantic clustering to compare what people ask with what they ought to ask to progress. Early on, a straightforward retrieval pattern—embeddings, top-k selection, and generation that honors a card schema—establishes reliability.\nMeasuring Success Interaction Quality shows up directly in average turns per session and card click-through rates, as well as explicit helpfulness signals such as upvotes and downvotes. Learning Outcomes can be proxied through completed knowledge checks, intertextual clicks that lead to saved or started books, and the frequency with which users choose to go deeper rather than simplify. Personalization should be evaluated by comparing engagement and conversion on individualized suggestions against generic baselines.\nExtensions: Listen-In Experiences Two sidecar experiences can broaden appeal without complicating the main loop:\nReader Panel synthesizes community viewpoints into a short, host-led exchange that captures live debate in a couple of minutes of text or audio. Expert Panel synthesizes book content and academic sources into a focused discussion on a theme. Both can be pre-cached and surfaced as contextual cards that invite a quick listen before jumping back into interactive exploration.\nWhy This Pattern Wins Learn About is learner-centric rather than content-centric. It starts from a live confusion and expands the neighborhood around it without demanding more effort than a tap. It preserves attention by keeping answers short, modular, and emphatic about what matters now. It compounds value as every interaction sharpens the system\u0026rsquo;s sense of what to show next.\nMost importantly, it is a portable blueprint: the same mechanics that make it excellent for books also apply to lectures, papers, podcasts, and any domain where exploration precedes mastery. By treating curiosity as a navigable space rather than a series of isolated questions, Learn About transforms how we think about AI-assisted learning—from reactive query-response to proactive knowledge exploration.\nThe future of learning isn\u0026rsquo;t about better answers to the questions we already know to ask. It\u0026rsquo;s about systems that help us discover the questions we never thought to ask, making the unknown knowns visible and actionable. Google\u0026rsquo;s Learn About shows us what that future looks like: not just smarter search, but curiosity as an interface.\n","permalink":"https://chenterry.com/posts/google_learn_about/","summary":"\u003cp\u003eGoogle\u0026rsquo;s Learn About reframes both search and chat as an explore-first learning loop. It lowers the cost of asking good questions, manages cognitive load with structured, clickable cards, and continuously surfaces \u0026ldquo;questions you didn\u0026rsquo;t know to ask.\u0026rdquo; Users resolve a specific doubt while simultaneously zooming out to the surrounding question space, all without breaking their thinking flow.\u003c/p\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/google-learn-about/learn-about-main-interface.png\" alt=\"Google Learn About Main Interface\" loading=\"lazy\"\u003e\n \n\u003cem\u003eLearn About\u0026rsquo;s clean interface design centers around the core question \u0026ldquo;What would you like to learn about?\u0026rdquo; The PDF Reading Companion offers guided document analysis, while The Reading Nook provides curated book exploration with visual book covers that invite browsing and discovery.\u003c/em\u003e\u003c/p\u003e","title":"Google Learn About - Interest Directed, AI Aided Learning"},{"content":" Google NotebookLM\u0026rsquo;s main interface showing featured notebooks and recent projects, demonstrating the platform\u0026rsquo;s approach to organizing and managing research across multiple sources and domains.\n","permalink":"https://chenterry.com/posts/google_notebooklm/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/google-notebooklm/notebooklm-main-interface.png\" alt=\"Google NotebookLM Main Interface\" loading=\"lazy\"\u003e\n \n\u003cem\u003eGoogle NotebookLM\u0026rsquo;s main interface showing featured notebooks and recent projects, demonstrating the platform\u0026rsquo;s approach to organizing and managing research across multiple sources and domains.\u003c/em\u003e\u003c/p\u003e","title":"Google NotebookLM - Long Context Windows and Multimodality"},{"content":"AI has attracted extraordinary investment, but the question of what consumers will actually pay for remains unsettled. Today’s monetization strategies—subscriptions like ChatGPT Plus or Kimi, API usage fees, and ad-driven AI applications—resemble the early internet era, where infrastructure outpaced clear consumer value. On the B2B side, AI often enhances existing workflows, such as creative generation in advertising, but rarely justifies direct pricing. Advertisers care less about model sophistication than about measurable outcomes—new creative material, better engagement, or lower cost per click. As a result, product value is abstracted into incremental revenue lift, not standalone payment. For consumers, subscription-based AI products work only when the perceived utility is personal, persistent, and irreplaceable—writing, coding, or tutoring assistants that feel indispensable.\nUltimately, AI monetization may hinge not on the intelligence itself but on its embodiment—how seamlessly it integrates into habits, tools, and ecosystems people already pay for. The next wave of value capture will likely come from domain-specific AI layers that own distribution (e.g., productivity, media, or commerce), rather than from general intelligence sold as a service.\n","permalink":"https://chenterry.com/posts/monetizing_ai/","summary":"\u003cp\u003eAI has attracted extraordinary investment, but the question of what consumers will actually pay for remains unsettled. Today’s monetization strategies—subscriptions like ChatGPT Plus or Kimi, API usage fees, and ad-driven AI applications—resemble the early internet era, where infrastructure outpaced clear consumer value. On the B2B side, AI often enhances existing workflows, such as creative generation in advertising, but rarely justifies direct pricing. Advertisers care less about model sophistication than about measurable outcomes—new creative material, better engagement, or lower cost per click. As a result, product value is abstracted into incremental revenue lift, not standalone payment. For consumers, subscription-based AI products work only when the perceived utility is personal, persistent, and irreplaceable—writing, coding, or tutoring assistants that feel indispensable.\u003c/p\u003e","title":"The Monetization Paradox of AI"},{"content":"Semantic Retrieval vs. Traditional Search Understanding the fundamental difference between semantic retrieval systems and traditional web crawlers: Exa operates as a semantic retrieval index over a pre-crawled corpus, focusing on high-quality, text-rich pages suitable for LLM context windows rather than real-time web crawling.\nModern Web Crawling Challenges The evolving landscape of web access restrictions: Modern platforms like Reddit, Xueqiu, and others implement sophisticated barriers to automated crawling, fundamentally changing how AI systems can access and index web content.\n","permalink":"https://chenterry.com/posts/search_topic/","summary":"\u003ch2 id=\"semantic-retrieval-vs-traditional-search\"\u003eSemantic Retrieval vs. Traditional Search\u003c/h2\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/ai-search-topic-modeling/semantic-retrieval-explanation.png\" alt=\"Semantic Retrieval Explanation\" loading=\"lazy\"\u003e\n \n\u003cem\u003eUnderstanding the fundamental difference between semantic retrieval systems and traditional web crawlers: Exa operates as a semantic retrieval index over a pre-crawled corpus, focusing on high-quality, text-rich pages suitable for LLM context windows rather than real-time web crawling.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"modern-web-crawling-challenges\"\u003eModern Web Crawling Challenges\u003c/h2\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/ai-search-topic-modeling/crawler-restrictions-challenges.png\" alt=\"Crawler Restrictions and Challenges\" loading=\"lazy\"\u003e\n \n\u003cem\u003eThe evolving landscape of web access restrictions: Modern platforms like Reddit, Xueqiu, and others implement sophisticated barriers to automated crawling, fundamentally changing how AI systems can access and index web content.\u003c/em\u003e\u003c/p\u003e","title":"Weighing Social Interactions: Reconstructing Meaning from Large Multi-modal Datasets"},{"content":"When Dan Shipper reveals in a recent podcast that Every\u0026rsquo;s engineers write virtually zero code, the claim sounds almost impossible. Then he breaks down their numbers: 15 people, daily AI newsletter, multiple shipped products, and a million-dollar consulting arm. No traditional coding. No massive engineering team. Just humans orchestrating AI agents like conductors leading a symphony.\nThis isn\u0026rsquo;t just efficiency optimization—it\u0026rsquo;s a complete reimagining of how companies operate when AI becomes your primary workforce multiplier. Every represents the most radical example of AI-first operations in practice, and the lessons extend far beyond media companies to anyone building in the AI era.\nThe AI-Native Operating Model Every\u0026rsquo;s success stems from what Shipper calls the \u0026ldquo;allocation economy\u0026rdquo;—a future where humans manage AI capabilities rather than execute tasks directly. Their team orchestrates an ensemble of specialized AI agents: Claude for complex analysis, Codex for implementation, \u0026ldquo;Friday\u0026rdquo; for automation, and \u0026ldquo;Charlie\u0026rdquo; for content generation. Each AI agent functions like a specialist team member with distinct strengths.\nThis approach treats AI agents as workforce multipliers rather than simple tools. When Every needs market research, they don\u0026rsquo;t assign someone to spend weeks gathering data. Instead, they deploy Claude for analysis, Friday for automation of data collection, and Charlie for synthesizing insights into readable formats. The human provides strategic direction and quality control while AI handles specialized execution.\nThe result? Every achieves enterprise-level output with startup-level resources. Their content creation process leverages AI research assistants that gather information from dozens of sources, AI editors that refine voice and tone, and AI distribution systems that optimize content across platforms—all coordinated by humans who focus on creative connections and strategic thinking.\nThe Generalist Advantage: Why Everyone Becomes a Manager Traditional career advice emphasizes specialization—become the best at one thing and defend that expertise. Shipper\u0026rsquo;s experience at Every flips this logic. When AI handles specialized execution with increasing sophistication, the valuable skill becomes orchestrating multiple capabilities.\nShipper calls this \u0026ldquo;compounding engineering\u0026rdquo;—the ability to combine existing AI capabilities in novel ways rather than building from scratch. At Every, team members function as generalists who can navigate across domains. A single person might use Claude for research, Friday for automation, Charlie for content creation, and Cursor for any necessary coding—all in service of one project.\nThis shift favors broad fluency over deep expertise in any single area. The engineer of the future looks more like a conductor than a craftsperson, coordinating specialized AI agents rather than writing every line of code. When AI can generate functional code from natural language descriptions, the bottleneck shifts from implementation speed to problem identification and solution design.\nEvery\u0026rsquo;s anti-code approach has enabled them to ship products at remarkable speed while maintaining small team size. Their consulting frameworks, educational products, and operational tools all leverage existing AI capabilities rather than requiring months of traditional development cycles.\nContent as the Core Engine Every\u0026rsquo;s business model centers on content, but not in the traditional sense. Their daily AI newsletter isn\u0026rsquo;t just marketing—it\u0026rsquo;s their primary product, research vehicle, and community-building tool. Each newsletter serves multiple functions: educating their audience, testing new ideas, building authority in the AI space, and generating leads for their consulting business.\nDan has written extensively about this approach, arguing that in an AI-driven economy, the ability to create high-quality, consistent content becomes a core business capability. Every\u0026rsquo;s content engine feeds their consulting practice, which in turn provides case studies and insights for their content, creating a virtuous cycle that compounds over time.\nDistributed Team, Concentrated Impact With team members scattered globally, Every operates as a fully distributed company. But their AI-first approach makes remote coordination more effective than traditional companies achieve with co-located teams. AI tools handle much of the routine coordination work—scheduling, project tracking, status updates—while the team focuses on high-level strategy and creative work.\nTheir consulting arm demonstrates this principle at scale. Rather than hiring consultants for each client, they\u0026rsquo;ve built AI-assisted consulting frameworks that allow a small team to serve enterprise clients effectively. The AI handles research, analysis, and initial recommendations, while human consultants focus on strategic interpretation and client relationships.\nThe Compound Effects of AI-First Every\u0026rsquo;s approach creates several compound effects that traditional startups struggle to achieve:\nSpeed of Learning: AI assistance accelerates every learning cycle. Market research that might take weeks happens in days. Content creation and testing cycles compress from months to weeks. This faster feedback loop allows Every to iterate and adapt more quickly than competitors.\nResource Efficiency: By leveraging AI for routine tasks, Every achieves enterprise-level output with startup-level resources. Their cost structure remains lean while their capabilities scale exponentially.\nKnowledge Leverage: Every\u0026rsquo;s content creation process captures and systemizes insights that become intellectual property. Their AI systems learn from past work, making future projects more efficient and higher quality.\nThe AI-First Playbook: Making Any Company AI-Native Shipper\u0026rsquo;s approach offers a concrete roadmap for organizations ready to embrace AI-first operations:\nCEO Sets the Example: AI adoption starts at the top. Shipper doesn\u0026rsquo;t just mandate AI use—he demonstrates it. He shares prompt libraries, shows how he uses different AI agents for various tasks, and makes his AI workflow transparent to the entire team. When leadership actively models AI-first behavior, it signals that this isn\u0026rsquo;t optional technology—it\u0026rsquo;s core to how the company operates.\nInternal Prompt-Sharing Sessions: Every hosts regular sessions where team members share effective prompts, useful AI workflows, and creative applications they\u0026rsquo;ve discovered. This creates a culture of continuous learning and prevents the siloed adoption that often limits AI impact.\nSystematic Upskilling on AI Tools: Rather than assuming people will figure out AI on their own, Every invests in structured training. Team members learn not just how to use individual tools, but how to think about AI capabilities, break down problems for AI assistance, and combine multiple AI agents effectively.\nThe AI Operations Lead Role: Perhaps most importantly, Every has created a new role: the AI operations lead. This person identifies opportunities for AI integration, evaluates new tools, and helps teams implement AI workflows. It\u0026rsquo;s part technical translator, part change management, part strategic advisor.\nSpeed as Competitive Advantage: AI\u0026rsquo;s primary advantage remains speed. Companies that learn to operate at AI speed—faster iteration, faster learning, faster adaptation—gain sustainable advantages over those operating at human speed. Every\u0026rsquo;s ability to compress research, content creation, and product development cycles from months to weeks exemplifies this principle in practice.\nThe Future of Work: Job Reshoring and the Allocation Economy Shipper\u0026rsquo;s most counterintuitive prediction challenges conventional wisdom about AI and employment. Rather than mass job displacement, he sees AI enabling job reshoring—bringing work back to higher-cost regions like the U.S. because AI removes the labor cost arbitrage that drove offshoring.\n\u0026ldquo;When a developer in San Francisco can be 10x more productive with AI than they were before, the cost difference between San Francisco and Bangalore becomes irrelevant,\u0026rdquo; Shipper argues in the interview. The economic logic that sent millions of jobs overseas reverses when AI amplifies human capabilities rather than replacing them.\nThis reshoring effect extends beyond tech work. AI-assisted professionals in expensive markets can compete with lower-cost regions by achieving dramatically higher output per person. The question shifts from \u0026ldquo;who can do this work cheapest?\u0026rdquo; to \u0026ldquo;who can orchestrate AI capabilities most effectively?\u0026rdquo;\nEvery\u0026rsquo;s success with 15 people suggests that AI-native companies achieve outcomes that previously required much larger teams. Traditional metrics like lines of code written or hours worked become less relevant than output quality and speed of learning. The companies that thrive will be those that most effectively orchestrate AI capabilities rather than those that build everything from scratch.\nWhy This Matters Now Every\u0026rsquo;s approach isn\u0026rsquo;t just a case study—it\u0026rsquo;s a preview of how business works when AI capabilities become ubiquitous. Their radical embrace of AI-first operations offers a template for what becomes possible when we stop thinking about AI as a tool and start thinking about it as a new foundation for how work gets done.\nThe question for every startup becomes: are you building an AI-enhanced version of an old model, or are you creating something genuinely AI-native? Every\u0026rsquo;s example suggests that the latter approach doesn\u0026rsquo;t just create efficiency gains—it unlocks entirely new possibilities for what small teams can accomplish when they learn to orchestrate intelligence rather than just apply it.\n","permalink":"https://chenterry.com/posts/ai_native/","summary":"\u003cp\u003eWhen Dan Shipper reveals in a recent podcast that Every\u0026rsquo;s engineers write virtually zero code, the claim sounds almost impossible. Then he breaks down their numbers: 15 people, daily AI newsletter, multiple shipped products, and a million-dollar consulting arm. No traditional coding. No massive engineering team. Just humans orchestrating AI agents like conductors leading a symphony.\u003c/p\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/every/every-homepage.png\" alt=\"Every Homepage\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t just efficiency optimization—it\u0026rsquo;s a complete reimagining of how companies operate when AI becomes your primary workforce multiplier. Every represents the most radical example of AI-first operations in practice, and the lessons extend far beyond media companies to anyone building in the AI era.\u003c/p\u003e","title":"Learnings from Every on Building an AI-Native Startup"},{"content":"Most APM advice focuses on how to sound like a product manager. But sounding right has never built a great product. What matters is developing the ability to be directionally right about the future — and proving it with metrics.\nThe best product thinkers I’ve learned from aren’t impressive because of their frameworks or polished answers. They stand out because they form falsifiable beliefs about how the world works: what users will care about, which constraints will dominate, where value will actually accumulate. Their predictions can be wrong, which means their thinking can be evaluated. That’s the only way judgment gets sharper.\nAPM interviews try to measure this, but their structure often selects for the opposite: well-rehearsed narratives, tidy templates, and answers optimized to look competent rather than expose real reasoning. It’s a performance loop. You can get very good at it without ever putting your thinking at risk or checking whether your instincts survive contact with user behavior.\nThe alternative is harder but more meaningful: build habits of thought that force you to take real positions, specify what would disprove them, and design metrics that reveal whether you’re actually right.\nUnderstanding the APM Role Associate Product Manager positions are designed for early-career professionals transitioning into product management. Unlike senior PM roles that require deep domain expertise, APM interviews focus on potential, learning agility, and foundational thinking skills.\nKey responsibilities typically include supporting product strategy development, where you\u0026rsquo;ll contribute to high-level product decisions and roadmap planning. You\u0026rsquo;ll spend significant time conducting user research and analysis to understand customer needs and behaviors. Writing product requirements and specifications forms a core part of the role, as does collaborating closely with engineering, design, and business teams to ensure successful product delivery. Additionally, you\u0026rsquo;ll be responsible for analyzing product metrics and user feedback to inform future product iterations and improvements.\nTypes of PM Interview Questions PM interviews vary from company to company, but regardless of where you interview you\u0026rsquo;re likely to run into questions that test your ability to design products, solve ambiguous problems, build effective strategies, and analyze and execute with data.\nYou\u0026rsquo;ll also likely face questions like \u0026ldquo;Tell me about yourself\u0026rdquo; or \u0026ldquo;What\u0026rsquo;s your favorite product?\u0026rdquo; With these, interviewers aim to get to know you as a product manager, and how you\u0026rsquo;d fit in with the company culture.\nCore Interview Categories 1. Product Design Questions Product design questions ask you to design or improve a product. Common question types include designing a product from scratch, such as \u0026ldquo;Design an app for job-seekers to match with employers.\u0026rdquo; You might also encounter improvement questions like \u0026ldquo;Improve headspace,\u0026rdquo; where you need to identify pain points and propose enhancements to existing products. Another category focuses on designing for specific audiences, such as \u0026ldquo;Design a vending machine for the blind,\u0026rdquo; which tests your ability to consider unique user constraints and accessibility needs. Finally, there are \u0026ldquo;moonshot\u0026rdquo; questions, particularly favored at Google, that ask you to \u0026ldquo;Design X, assuming you have infinite resources,\u0026rdquo; allowing you to think beyond typical constraints.\nOverall, your goal is to come up with a feasible product given the constraints of the question. Feasible, in this context, means that your product solves a problem in a way that makes sense for the company and that users will like.\nThese typically take 25 minutes excluding follow-ups. Interviewers like to ask product design questions early on to filter out candidates who don\u0026rsquo;t have essential product skills, but they\u0026rsquo;re a popular question type during on-site interviews as well. Expect to answer several in a standard PM interview loop.\nA Framework for Answering Product Design Questions A solid process is to first clarify the problem, gather relevant context until you understand users and their challenges, explore and select the best option, and finally, justify your answer. Follow this 7-step process:\nClarify and get context Define users Identify pain points and opportunity areas Brainstorm possible solutions Define a product vision Prioritize features Evaluate and recap Let\u0026rsquo;s work through an example to illustrate. Assume you\u0026rsquo;ve been asked the question \u0026ldquo;Design a better gym experience.\u0026rdquo;\nStep 1: Clarify and Get Context First, clarify any vagueness about the problem you\u0026rsquo;ve been given. After you\u0026rsquo;ve clarified ambiguous company names and keywords, gather context that will help you understand the problem space and define any strategic considerations that might influence your design.\nQuestions like \u0026ldquo;What\u0026rsquo;s the timeline?\u0026rdquo; or \u0026ldquo;Are there any constraints I should be aware of?\u0026rdquo; are fair game, but be prepared for an interviewer to give you a vague answer. In this case, make assumptions and state them to your interviewer so they have a chance to correct you if you\u0026rsquo;re veering off-track.\nExample: For our example question \u0026ldquo;Design a better gym experience\u0026rdquo; you\u0026rsquo;d want to ask a few questions to clarify the scope of the problem. Start by understanding what they mean by \u0026ldquo;gym experience\u0026rdquo; and what constitutes \u0026ldquo;better\u0026rdquo; in this context, including whether there are specific metrics you should focus on driving. Clarify the scope by asking whether you\u0026rsquo;re improving a home gym product or a commercial gym setting. Determine if you\u0026rsquo;re working on one particular gym location or designing improvements for an entire chain. Finally, establish whether you should focus on a specific gym brand like Planet Fitness or propose general improvements that could apply broadly.\nLet\u0026rsquo;s assume your interviewer doesn\u0026rsquo;t offer much detail. They say: \u0026ldquo;Make whatever assumptions you like; we just know that the gym experience works for some and not others and we want to make it better for more users.\u0026rdquo;\nYou state to your interviewer that, going forward, you\u0026rsquo;ll answer the question assuming you\u0026rsquo;re improving a generic commercial gym.\nStep 2: Define Users Next, define subsets of users that would effectively partition the total user base and pick an interesting subset for a deep dive. There are many ways to segment users. To get started, you could consider demographics like age or income level, which help you understand who your users are. You might also look at behavioral characteristics like frequency of use or users taking certain target actions, which reveal how people interact with your product. Geographic location can also be a meaningful segmentation approach, especially when regional preferences or constraints affect user behavior.\nAny of these is a good jumping-off point, but a great answer segments users in a way that is meaningful for the question at hand. One helpful tip is to segment users who might have very different needs. After choosing a segmentation scheme and listing a few basic user subsets, pick a segment that interests you and articulate why that segment is valuable to discuss. Consider user subsets that may be strategic, such as early adopters who can drive product adoption and provide valuable feedback. Think about the scale of impact or the depth of pain that users feel, as addressing high-pain segments can create significant value. Also evaluate how well you or your company can address these users in the context of your question, ensuring your solution is realistic and achievable.\nExample: Let\u0026rsquo;s say you decide to segment users according to their usage of the gym. Sample segments might include \u0026ldquo;pro\u0026rdquo; users who frequent the gym regularly and have established workout routines, novice users who are new to fitness and may feel intimidated or unsure about equipment, users who primarily attend group classes and rarely use individual equipment, and users who maintain memberships but rarely or never actually visit the gym.\nLet\u0026rsquo;s say you want to target absent gym members:\n\u0026ldquo;From this list, users who have memberships but don\u0026rsquo;t go to the gym stand out as a good segment for a deeper dive because they\u0026rsquo;re obviously experiencing an obstacle to achieving a goal. They were motivated enough to join the gym and are spending money on a membership, but something is preventing them from going. As an added benefit, I suspect we might be able to create value for other gym-goers and even attract new gym members if we solve this issue.\u0026rdquo;\nStep 3: Identify Pain Points and Opportunity Areas Next, consider any goals your users might have. Any explicit obstacles are pain points. If nothing\u0026rsquo;s blocking users from achieving their goals but some point of friction clearly exists, you\u0026rsquo;ve uncovered an opportunity area. Take an empathetic position and focus on how to best serve these users and you will uncover both.\nTake a moment to summarize the goals you think your user subset has, then brainstorm pain points and opportunity areas preventing them from achieving those goals.\nTip: Use the \u0026ldquo;broad then deep\u0026rdquo; mini-framework to quickly hone in on key pain points.\nConsider the circumstances users are in. Is the goal urgent? Are the decisions involved important or trivial? Asking yourself these questions and putting yourself in the user\u0026rsquo;s shoes demonstrates user empathy, a key PM skill, so be sure to articulate your thought process to your interviewer.\nExample: Returning to our gym example, it\u0026rsquo;s clear that absent gym members had certain goals in mind when they bought their gym memberships. Their goals likely include some mix of the following:\nGetting healthier Looking better Gaining confidence Making friends (or at least being social) What\u0026rsquo;s stopping these users from following through on these goals and using the gym? Your brainstorm could include emotional components like feeling uncomfortable at the gym. Discomfort might have many dimensions, including feeling intimidated or feeling bored.\nThinking about their circumstances, you realize that going to the gym is a decision they\u0026rsquo;re making from somewhere else - likely work or their homes. That creates logistical hurdles such as:\nBusy schedules Traffic A gym location that might not be convenient Let\u0026rsquo;s say you decide to address the emotional hurdles rather than logistical hurdles as logistics will be more specific to each user and harder to solve:\n\u0026ldquo;Users might not be going to the gym because they feel uncomfortable. Gyms can feel sterile and intimidating, which could prevent users who want to gain confidence or be social from using the gym. Gyms can also be boring, especially during peak times when users vie for limited equipment and might have to wait around awkwardly between sets.\nIt\u0026rsquo;s possible that users face a combination of emotional hurdles like feeling bored at the gym, and logistical issues, like a busy schedule, which would make it hard to decide to get to the gym after a busy or stressful day. If we can make the gym more compelling by solving the boredom or intimidation issue, the decision to go should be easier for my target users.\nGoing forward, I\u0026rsquo;d like to address the boredom issue at the gym. Specifically, the problem of waiting around for popular equipment to open up.\u0026rdquo;\nStep 4: Brainstorm Possible Solutions Now, brainstorm ideas that could solve the pain point you\u0026rsquo;ve identified.\nTip: One helpful tip for coming up with solutions is to think of products you like — especially anything solving a similar problem in another context. For example, Duolingo might be a good analogue for our gym example as it effectively removes barriers for people who want to improve themselves (by learning a language) but who have trouble with motivation, routine, or planning. The app is fun, easy to use, and the emphasis is on consistency rather than intensity. These are all characteristics you might incorporate into your design.\nAim to generate at least three solid ideas before moving on to the next step. It\u0026rsquo;s fine to brainstorm a bigger list and eliminate weak ideas. Check each idea against the pain point you identified and don\u0026rsquo;t be afraid to get creative. Interviewers want to see excitement and passion for product.\nExample: Continuing our gym example, recall that you\u0026rsquo;ve decided to address the pain point of users feeling bored at the gym, especially during peak hours as users wait for equipment. Your brainstorm of possible solutions could include making the gym experience more customized through a workout recommendation system that accounts for peak hours, suggesting alternative exercises when popular equipment is unavailable. Another approach might involve \u0026ldquo;gamifying\u0026rdquo; the gym experience so users feel more engaged at the gym overall, perhaps through challenges, achievements, or social features. You could also consider creating a specific activity to fill the time between sets, helping users feel more engaged while waiting rather than standing around awkwardly.\nStep 5: Come Up with a Forward-Looking Product Vision Pick the strongest solution from the previous section and take a moment or two to envision what that product will look like in five or ten years. Come up with a brief tagline that really emphasizes the point you\u0026rsquo;re making. You want to leave the interviewer with a soundbite they\u0026rsquo;ll remember when they\u0026rsquo;re scoring your interview. Write it on the whiteboard (if you\u0026rsquo;re using one) and refer back to it as you continue.\nExample: Continuing our gym example, a forward-looking vision might look like this:\n\u0026ldquo;A visit to the gym is made up of more than just the active time of working out, and that non-active time is often awkward or boring. In a futuristic gym, that time could be fun and exciting.\u0026rdquo;\nThat could be enough for a short-and-sweet product vision, but if you want to add a bit more detail, you could finish with:\n\u0026ldquo;Let\u0026rsquo;s build a gym that offers engaging activities available in between sets, helping users stay in the right mindset while getting a more effective workout.\u0026rdquo;\nStep 6: Prioritize Features Once you\u0026rsquo;ve crafted a compelling product vision, it\u0026rsquo;s time to prioritize features.\nWe recommend walking the interviewer through a quick user journey where a user interacts with your product. This will help you concretely identify how your product fits into the existing user flow which will help you prioritize features. It\u0026rsquo;ll also help you stay user-centric instead of over-defining an idea that appeals to you personally.\nThen, brainstorm a short list of features based on your use case(s) and prioritize according to which features best support your product vision. Be sure to tie the discussion back to user pain points. Here are a few helpful dimensions to consider when evaluating features: scale, which measures how many users this feature will help and the breadth of its impact; ease of expansion, which considers how easily this feature will expand to other user subsets and whether it will attract new users to the platform; and strategic impact, which evaluates how well this feature supports the company vision and broader business objectives.\nYou don\u0026rsquo;t need to describe every part of the feature in detail, but interviewers do expect you to describe what the user sees and interacts with and how that delivers on the product goals and vision you defined.\nExample: For our gym example, recall your product vision:\n\u0026ldquo;Let\u0026rsquo;s build a gym that has supporting activities available in between sets, helping users stay in the right mindset and have more effective workouts.\u0026rdquo;\nTo begin prioritizing features, you might describe different specific approaches to supporting user\u0026rsquo;s workouts. As you decided to focus on the problem of boredom between sets, it\u0026rsquo;s worth detailing that experience in a bit more detail. Ask yourself \u0026ldquo;What\u0026rsquo;s the process of waiting?\u0026rdquo;\n\u0026ldquo;If you (a user) are standing around waiting, you\u0026rsquo;re typically either waiting to use a machine or taking a break between sets. We only have a small window; maybe five to ten minutes where we want to keep you engaged and entertained, but not distracted from your workout. Therefore our product should be a short experience that improves the user\u0026rsquo;s workout. For instance, we could create an app to log workouts and provide users with stats tracking, workout journal prompts, or tips and recommendations for their next set.\u0026rdquo;\nStep 7: Evaluate and Recap As you wrap up, summarize your insights and your overall design for your interviewer, then spend a few minutes evaluating your design and discussing next steps. Consider:\nTradeoffs you made Alternate use cases and edge cases What you\u0026rsquo;d do differently given more time Common follow-up questions from interviewers include: \u0026ldquo;Can you see any risks with this design?\u0026rdquo; and \u0026ldquo;What challenges do you anticipate in implementing this product?\u0026rdquo; Discussing risks and challenges preemptively signals to interviewers that your ideas are grounded in reality.\nExample: To close the example question:\n\u0026ldquo;One risk we should consider is that gyms are busy places with heavy equipment, so it could be dangerous if people are too distracted. We\u0026rsquo;d also have to make sure this doesn\u0026rsquo;t slow down users\u0026rsquo; workouts or create more bottlenecks.\u0026rdquo;\nTips for Answering Product Design Questions Consider Company Strategy\nBringing strategic concerns into the mix can help you reach a broader and deeper understanding of the problem at hand. Here are some prompts to consider:\nCompany mission: What\u0026rsquo;s the company mission? Why does the company care about this space? How could this product support the mission? Company strategy: What are the relevant strategic goals the company might have? How could this tie into those or open up a new strategy? What products make sense strategically? Market understanding: What alternatives already exist in the market? Where are the gaps? What\u0026rsquo;s valuable in this market? Ace Moonshot Questions by Abstracting the Problem\nMoonshots are bold ideas that go beyond incremental changes. They\u0026rsquo;re frequently associated with Google interviews, but you can \u0026ldquo;moonshot\u0026rdquo; any design question by digging deep until you\u0026rsquo;ve uncovered the root of an important problem in a way that feels transformative.\nYou\u0026rsquo;re on the right track if you\u0026rsquo;ve found a way to solve the problem completely and simply for the user. If you\u0026rsquo;re able to eliminate the conditions that cause the problem in the first place, even better.\nWe\u0026rsquo;ve found two different strategies helpful for candidates practicing moonshots. One is to think of the product as a black box. The input is the user\u0026rsquo;s current state, and the output is the user as they want to be. By eliminating all assumptions about how to get users from where they are to where they want to be and exploring what could happen in that space, really transformative solutions can sometimes become clear.\nAnother strategy is to use something like the 5 Whys technique to dig deeper and deeper until you uncover the root of the problem.\nYou could also consider what the user\u0026rsquo;s actual goals are. For instance, if you\u0026rsquo;re asked to build an alarm clock for deaf users, consider that the user\u0026rsquo;s goal is to wake up at a certain time. Rather than limiting yourself to modifying how existing alarm clocks work, this framing allows for new options that rethink what an alarm clock is. Our best advice is to focus on the user and their needs — this approach will never steer you wrong.\nConsider Existing Users, Strategic Goals, and Market Context When Answering Product Improvement Questions\nSometimes, you\u0026rsquo;ll be asked to improve an existing product. Questions like \u0026ldquo;Improve Instagram\u0026rsquo;s home tab\u0026rdquo; are similar to \u0026ldquo;design X\u0026rdquo; questions, but you\u0026rsquo;ll have to consider established users, goals, and market dynamics. Keep these in mind, and consider addressing under-served users, supporting new use cases, or adapting a product to open new strategic opportunities - these are all common areas for improvement.\nCommon Product Design Pitfalls The first major pitfall is jumping straight to a solution without explaining your thought process. Make sure you walk the interviewer through how you think and why your idea is a good solution. Don\u0026rsquo;t decide on a solution immediately after hearing the question, and keep an open mind because you\u0026rsquo;ll usually find better ideas as you explore the problem space.\nAnother common mistake is forgetting to segment users, even when you think you\u0026rsquo;ve been given a specific user group. For instance, if you were asked \u0026ldquo;Design Gmail for kids,\u0026rdquo; you might assume kids are already a narrow segment. However, kids are still a broad group, and you could segment further by age range or school level to focus your design more effectively.\nA related pitfall is trying to design for everybody, which usually results in a product that nobody actually cares about. If you don\u0026rsquo;t narrow your scope appropriately, you\u0026rsquo;ll struggle to gain meaningful insights about your users and their specific pain points.\nFinally, avoid simply checking off the boxes of a framework. While it\u0026rsquo;s helpful to use a framework to stay on track, the steps are recommendations to help you find insights, not a rigid checklist you must complete. Make sure you consider your answer holistically at every stage rather than mechanically moving through steps.\nHow to Brainstorm Product Ideas Many PM product interview questions boil down to \u0026ldquo;What is the future of X?\u0026rdquo; X might be an industry, such as wearables, a location, such as airports, or even a non-tech product, like a wrench.\nThe purpose of this question is to test your ability to identify and communicate an inspiring-yet-actionable product vision.\n\u0026ldquo;Future of X\u0026rdquo; questions are some of the most fun interview questions, though they can pose challenges. Similar to behavioral questions, it\u0026rsquo;s easy to ramble. Applying a structured approach can help you bound your thinking effectively while still sharing your passion and vision.\nWe have two key recommendations to keep you on track:\nRemember to apply your product thinking skills to these questions. Whatever your bold vision for the future is, make sure it satisfies a real and important need. Articulate the details and continuously tie your answer back to that need.\nBefore the interview, curate a \u0026ldquo;big ideas\u0026rdquo; list of all the technologies, trends, and concepts that excite you. This list should be big enough in scope that no matter what \u0026ldquo;future of X\u0026rdquo; question you get, you can pull something from your list to speak intelligently about. This will prevent you from rambling or pigeonholing yourself into an idea that doesn\u0026rsquo;t feel exciting to you.\nHere\u0026rsquo;s a simple process for answering the \u0026ldquo;future of X\u0026rdquo; question using the big ideas list:\nStep 1: Build a \u0026ldquo;Big Ideas\u0026rdquo; List Before your interviews, put together a \u0026ldquo;big ideas\u0026rdquo; list — a roster of interesting ideas that you\u0026rsquo;re passionate about and familiar with. Start by brainstorming a list of all your favorite trends and technologies that might lead to a radically different future.\nSome interesting trend ideas might include decentralized finance, the circular economy, or cooperatively owned digital platforms. The technologies you list can be anything that inspires you — examples include blockchain, quantum computing, nanotech, etc. For inspiration, we recommend browsing the news in science or tech or watching/reading science fiction. Let your imagination wander.\nPick three to five of your favorite ideas and really study them.\nMake sure you know the arguments for and against any existing technologies in your chosen areas, as well as the go-to-market strategies for any major startups operating in these spaces. Familiarize yourself with popular industry figures who are driving innovation and discourse. Additionally, stay informed about ongoing political discussions that might impact your ideas, such as regulatory debates around privacy, antitrust, or emerging technology governance.\nHaving done the legwork beforehand, you won\u0026rsquo;t waste time second-guessing yourself during your interview.\nStep 2: Combine Product Thinking with an Appropriate \u0026ldquo;Big Idea\u0026rdquo; Once you\u0026rsquo;re given the question, begin deploying your usual product thinking approach to understand the product/company/industry goals and users of X. Questions to ask yourself include how you might segment users to better understand their diverse needs and behaviors, what their key pain points are and what challenges they face with current solutions, what characteristics define the product, company, or industry you\u0026rsquo;re analyzing, and what constraints exist today and how those limitations might evolve or be overcome in the future.\nAt the same time, map items in your big ideas list to the space, and choose the most appropriate to apply X. You might choose one big idea, or combine a few for a really original take — this is up to you.\nStep 3: Critically Answer the Question Take the pain points or new opportunities you\u0026rsquo;ve identified and think critically about how some of your big ideas could solve them. Take some time to quietly imagine how this product will fit into the future 10, 20, or 30 years down the line. When you\u0026rsquo;re ready, begin outlining your vision for your interviewer.\nA simple process for arriving at a compelling answer is:\nSummarize big idea(s) and briefly state why you think they will revolutionize X in terms of relevant product/company/industry goals and users. Walk the interviewer through the existing user journey, calling out any user pain points or opportunity areas. Paint a picture of the future user journey with your big idea(s) incorporated. Cite important risks and mitigation strategies as well as any tradeoffs. Remember to apply critical thinking skills throughout. It\u0026rsquo;s easy to get swept away in the romance of your vision, but there are always risks and tradeoffs to consider. This is where all the research you did in building your big ideas list will help. Being able to cite arguments against something you believe in shows maturity and the ability to handle complexity.\nExample: \u0026ldquo;What is the Future of Libraries?\u0026rdquo; Let\u0026rsquo;s say your interviewer asks a question that boils down to \u0026ldquo;What is the future of libraries?\u0026rdquo; They may actually ask you \u0026ldquo;How will we consume books in the future?\u0026rdquo; or \u0026ldquo;How would you improve the library system?\u0026rdquo; These are essentially the same questions, and you can use the same approach to answer all three.\nAssume your big ideas list includes both context-awareness and the sharing economy and you want to apply both to answer the future of libraries question.\nThe Future of Libraries Including Context Awareness\nFirst, you\u0026rsquo;d summarize your first big idea and walk through the existing user journey:\n\u0026ldquo;Context awareness, or the ability of technology to understand a user\u0026rsquo;s context and alter its services accordingly, will radically change the future, especially libraries. Currently, when a \u0026lsquo;user\u0026rsquo; walks into a library, they have to search the catalog for desired books, often failing to find the thing they need, or missing an opportunity to find a book they would have loved.\u0026rdquo;\nNext, you\u0026rsquo;d paint a vivid picture of an improved user journey, thanks to your big idea:\n\u0026ldquo;Imagine a future where, when users walk into a library, they\u0026rsquo;ll be able to scan a unique user-identification barcode at a terminal, which will allow the library to recommend exactly what they should read. This might be based on schooling, current interests, and possibly even browser history (if users opt-in.) The library can then personalize its information to match user interests, instead of surfacing, say, a bunch of advanced accounting books the user has absolutely no interest in.\u0026rdquo;\nFinally, you\u0026rsquo;d cite risks and tradeoffs:\n\u0026ldquo;Of course, libraries are also about exploration. The algorithms that suggest content should still suggest fresh, new information. There are some big risks here with user privacy and model training which we should make sure to address\u0026hellip;\u0026rdquo;\nThe Future of Libraries Including the Sharing Economy\nYour interviewer might press you for another idea, or you might want to cover another idea you\u0026rsquo;re passionate about, say, the sharing economy:\n\u0026ldquo;Another important way I believe libraries will evolve will include the principles of the sharing economy; a community-based \u0026lsquo;peer-to-peer\u0026rsquo; approach to distributing goods and services. Specifically where libraries are concerned, there is an incredible number of physical books sitting in people\u0026rsquo;s houses. There\u0026rsquo;s huge untapped potential here for increased personal interactions, increased access to books, especially niche books that a typical library might not carry, and more sustainable processes for book storage and transportation.\u0026rdquo;\nNext, you\u0026rsquo;d paint a picture of the new and improved future:\n\u0026ldquo;In the future, the sharing economy will help us share not just our homes or our cars, but also our personal possessions, like books. Imagine a future where, if you want a book that\u0026rsquo;s unavailable at your library, you can instead exchange books with your neighbor, who has a copy. This \u0026lsquo;decentralized library system\u0026rsquo; will create tons of new social connections across communities.\u0026rdquo;\nFinally, to cover tradeoffs:\n\u0026ldquo;One issue with this, however, is that cataloging all these books will be hard work. Advanced Optical Character Recognition (OCR) technology that can turn images into machine-readable text may be able to help with this. For example, if a user takes a picture of their bookshelf and all of their books get cataloged for them, that\u0026rsquo;s significantly less work. The technology isn\u0026rsquo;t quite there yet though\u0026hellip;\u0026rdquo;\n2. Product Strategy Questions Product strategy questions focus on your ability to develop a long-term product plan that aligns with the company strategy. There are many common variants of strategy questions that assess your ability to build go-to-market (GTM) or pricing strategies, assess options for market entry, or monetize an existing product.\nYour interviewer will evaluate your understanding of key product strategy concepts, such as:\nMarket identification and competitive analysis Product road mapping, product positioning, and product pricing strategy Measuring product success Examples of product strategy questions: You might be asked whether Amazon should enter the food delivery business, which would test your ability to analyze market opportunities and competitive positioning. Another common question asks what the biggest threat to YouTube is, requiring you to think about industry dynamics and competitive landscape. You could also face pricing questions like being asked to price a new line of high-end mineral water bottles for the US market as a PM at a beverage company.\nBasic Framework:\nMarket analysis - Size, growth, competitive landscape Company fit - Core competencies, strategic alignment, resources Barriers and risks - What could go wrong? How significant? Success requirements - What would need to be true for this to work? Recommendation - Clear yes/no with reasoning How to Answer Strategy Questions Strategy interviews typically range from 40 minutes to an hour. A good answer builds to a logically defensible strategy that you can summarize in a few sentences, but a great answer includes more than a solid strategy. Aim to come to an incisive understanding of the landscape, synthesizing your exploration into new insights. These insights will be the basis of your strategy, but they should also open up future possibilities.\nYou also want to save a few minutes to discuss what you\u0026rsquo;d do differently given more information, discuss risk mitigation strategies, or describe how your long-term vision would evolve.\nThe thought of doing all that in 40 minutes may be intimidating, but remember that interviewers care more about your process than your final answer.\n7-Step Framework for Product Strategy Questions Note that under this framework, you won\u0026rsquo;t start generating options until step 5. This is key. It\u0026rsquo;s impossible to make good decisions without understanding the task at hand (clarifying the problem), where you are (defining the landscape), and where you\u0026rsquo;re trying to go (product/company/industry goals.) We recommend you spend up to 50% of your time on steps 1 through 4.\nStep 1: Clarify the problem Step 2: Define the product, company, or industry goals Step 3: Define the landscape Step 4: Define guiding principles Step 5: Establish an option set Step 6: Make your decision and argue for it Step 7: Evaluate and recap\nLet\u0026rsquo;s work through an example. Assume you\u0026rsquo;ve been asked \u0026ldquo;What\u0026rsquo;s the biggest threat to YouTube over the next 5 years?\u0026rdquo;\nStep 1: Clarify the Problem First, take time to clarify the problem and check any assumptions. This ensures that you and the interviewer are on the same page and you\u0026rsquo;re addressing the proper scope. Asking good questions also shows your interviewer that you have a sense for important information that might be missing and that you know to ask for it, rather than making assumptions.\nStart by scoping out each component of the question. Look for company names, nouns, and verbs. For example, \u0026ldquo;Should Amazon start selling live plants?\u0026rdquo; breaks down to a few key components that need clarification. First, when you say Amazon, do you mean Amazon online, or can you consider Amazon-owned brick and mortar entities like Whole Foods? Second, by selling, do you mean Amazon would sell directly, or could you consider facilitating third-party sales? Finally, by live plants, what\u0026rsquo;s in scope—individual potted plants, trays of seedlings, or even whole trees?\nAnything that you find helpful to making sense of the problem is fair game. Specifically, you want to clarify any vagueness in the question, ask about constraints that might affect your analysis, and simply state important assumptions you\u0026rsquo;re making so the interviewer has a chance to correct you if you\u0026rsquo;re veering off track.\nExample: For our YouTube question, you might ask whether we mean YouTube as a company or just the core YouTube product, and whether we\u0026rsquo;re considering threats globally or just in the US.\nLet\u0026rsquo;s assume the interviewer confirms that we\u0026rsquo;re talking about the whole company when we say YouTube, not just the core product, and that we\u0026rsquo;re considering threats globally.\nStep 2: Define the Product, Company, or Industry Goals Once you\u0026rsquo;ve scoped the problem, you\u0026rsquo;re ready to begin defining important goals. The key is to land on a clear and concise sense of what matters most to the entity that you\u0026rsquo;re strategizing for, whether it be a product, a company, or an industry. You probably won\u0026rsquo;t know the real product/company/industry goals off-hand, so you\u0026rsquo;ll have to make reasonable assumptions based on the product portfolio, company mission, or other facts you might know. List those assumptions for your interviewer, as well as any relevant:\nKey business value-drivers Existing strategies Specific strengths and weaknesses Tip: If you\u0026rsquo;re new to this, we recommend familiarizing yourself with classic strategic analysis frameworks like value chain analysis. This will help you move through Step 2 faster.\nExample: For our YouTube question, consider how YouTube\u0026rsquo;s business works and what\u0026rsquo;s most important to it.\nStart by making some reasonable assumptions:\n\u0026ldquo;YouTube\u0026rsquo;s most important product is its core product, the YouTube video platform. It drives most of YouTube\u0026rsquo;s revenue. Though they have other initiatives like Music and TV, over the 5-year time frame, this will be the primary driver of success.\nYouTube\u0026rsquo;s primary metric is watch time. Their business model is primarily based on ad revenue, though there are some alternative monetization schemes. YouTube\u0026rsquo;s most important goal is to increase watch time.\nAt the heart of watch time is the relationship between viewers and creators. Creators post on the platform to reach an audience and make money. Viewers come to watch videos made by creators. Advertisers display ads to these viewers, creating revenue for YouTube and creators. This is a strong, positive cycle, and supporting it is critical for YouTube.\u0026rdquo;\nBased on the above analysis, your takeaway would be: YouTube\u0026rsquo;s most important goal is to drive up watch time by maintaining a strong relationship between creators and viewers.\nStep 3: Define the Landscape Next, consider outside influences that may affect your strategic decision. Two key questions to ask yourself include what aspects of the landscape will influence your options and ultimately your decision, and what opportunities or challenges you might reasonably expect to face.\nHere are a few of the most common factors to consider when defining the landscape. First, examine market and competition dynamics—is the market unusually competitive, are there entrenched players dominating the market, and what are key competitors doing? Second, assess public opinion and cultural climate—what does public sentiment look like, is the environment welcoming of your product, company, or industry, and how will users and the broader public react to change? Third, analyze technology factors—what technologies already exist, what new technologies are emerging, and what kind of technological barriers to entry exist? Finally, consider regulation—how might relevant governments react to change, what existing regulation should you watch out for, and how stable is the overall regulatory environment?\nUse your judgment as a PM to select the dimensions that are most relevant to your question. Be sure to leverage all the work you did defining company goals. It\u0026rsquo;s easy to get lost mapping the landscape–everything seems relevant at first, but anchoring on goals will help you quickly discern the context that matters.\nExample: For our YouTube question, given that the relationship between creators and viewers is critical, you might decide to focus on threats in terms of what could disrupt that relationship:\n\u0026ldquo;A key company goal for YouTube is nurturing the relationship between creators and viewers. If we consider big threats to be those that threaten this relationship, I see two main categories:\nThe most obvious are direct competitors who convince creators to post on their platforms instead of YouTube. They could do this by making it easier to create and reach an audience or by offering more money. TikTok, Instagram, and Snapchat could all fit the bill.\nFrom the viewer side, anything that pulls viewers away also weakens this relationship. The same competitors are relevant, but so are indirect competitors; platforms that pull away time from our users. For example, Netflix and other streaming platforms, Roblox, Minecraft, Twitch, and Twitter.\nOn the plus side, YouTube has a strong brand and is seen as a go-to platform for user-generated video content.\u0026rdquo;\nStep 4: Define Guiding Principles Your landscape analysis coupled with the goals you defined in step 2 will allow you to converge on a set of guiding principles that will help you build your option set and eventually make your strategic decision.\nGuiding principles, in this context, are a snapshot view of the situation that captures business rationale and allows for consistent decision-making without second-guessing priorities.\nExample: For our YouTube example, given the goals and threats identified, you might decide on these guiding principles:\nYouTube cares most about watch time The biggest concern, then, is what could disrupt viewers and/or creators - especially the relationship between the two Competitors that can effectively draw the attention of both creators and viewers are an outsized threat to YouTube\u0026rsquo;s business Step 5: Establish an Option Set Now that you have defined relevant context and established guiding principles, you can finally begin brainstorming strategic options and ranking how well they fit against your principles and goals.\nYou probably have a few options in mind already. Write them down first then return to the original question to come up with more. Your brainstorm will depend on the specifics of the question.\nThen, based on your guiding principles and the context you\u0026rsquo;ve built so far, filter out any weak options so you can focus on the most important. Be sure to explain why you remove each option so your interviewer can track your logic.\nExample: For our YouTube question, your option set would probably include direct competitors which host user-generated video content that lures both creators and viewers away, such as:\nTikTok Instagram Snapchat Twitch You might also include indirect competitors that pull viewers away from YouTube without specifically targeting creators, such as:\nNetflix Twitter Based on our guiding principles, which emphasize the relationship between creators and viewers, you decide to eliminate indirect competitors from the option set, as they don\u0026rsquo;t explicitly target creators and thus don\u0026rsquo;t pose as big a threat to the creator-viewer relationship.\nStep 6: Make Your Decision and Argue for It Similar to filtering out weak options, making your final decision means referring back to your guiding principles and goals to pick the strongest option. You\u0026rsquo;ll likely have to decide between multiple good options, so you\u0026rsquo;ll need to clearly justify your choice. There are many factors to consider when making final strategic decisions. Depending on the question, you might consider:\nScale Impact Cost Risk(s) Likelihood of success Additional benefits (like partnerships or product synergies) Example: For our YouTube question, let\u0026rsquo;s say you conclude that TikTok is the biggest threat:\n\u0026ldquo;Let\u0026rsquo;s consider TikTok to be the biggest threat to YouTube. Though all three of my final options (TikTok, Instagram, and Snapchat) could pull users and creators away from YouTube, TikTok has the clearest success so far and is growing rapidly, particularly in a popular form of video (short-form) that YouTube hasn\u0026rsquo;t quite cracked yet.\nTikTok has made it easy for creators to post and build an audience, making their platform an attractive place to start as a new creator or move to as an existing one. Short-form video is extremely popular with viewers and the experience is very sticky, pulling viewers away from spending time on YouTube. TikTok also does well with an important demographic of younger users, who are critical for the short- and long-term success of YouTube.\u0026rdquo;\nStep 7: Evaluate and Recap As a last step, evaluate and recap the work you\u0026rsquo;ve done above. To preempt follow-up questions, it\u0026rsquo;s a good idea to:\nDiscuss tradeoffs Share what you\u0026rsquo;d do differently if you had more time Summarize how you\u0026rsquo;d mitigate potential risks Emphasize why your choice sets up the company for success Example: As we close our example, you could include something like this in your evaluation:\n\u0026ldquo;If I had more time, I\u0026rsquo;d like to have explored the implications of acting on TikTok as the main threat. For instance, TikTok users skew younger than the average YouTube user. If we\u0026rsquo;re not careful, we might make strategic choices that prioritize YouTube\u0026rsquo;s younger audience to the detriment of its older core users.\nOverall, YouTube has a solid hold on core users and their use case, so as long as we don\u0026rsquo;t harm their experience with any competitive move we\u0026rsquo;d make to address TikTok, we\u0026rsquo;ll likely be able to maintain our competitive edge.\nI\u0026rsquo;d also like to do more research. This has all been based on intuition and assumption, so while I\u0026rsquo;d want to confirm all of my assumptions, the key assumption I\u0026rsquo;d need to look into is how creators really feel, both those on YouTube, and those who have chosen other platforms, before I could give a definitive answer as to whether the TikTok threat is as big as I\u0026rsquo;m assuming.\u0026rdquo;\nCommon Strategy Question Pitfalls Set context. Don\u0026rsquo;t just jump to an answer. Your interviewer wants to see your ability to make sense of the situation and justify your decisions. Structure your answer. Whether you use this framework or create your own, there\u0026rsquo;s a lot to discuss. Having a clear structure can keep you on track and make your answer easier to follow. Make sure to answer the question. It\u0026rsquo;s easy to get caught up laying out a bunch of strategic insights. Don\u0026rsquo;t forget to drive towards an answer and turn these into a specific decision. Make clear choices, give details about your approach, and make it concrete. Consider trade-offs. Most strategies mean prioritizing one aspect to the detriment of another. Don\u0026rsquo;t forget to explain what the trade-offs are and why you think they\u0026rsquo;re reasonable (or when they wouldn\u0026rsquo;t be). Overall, remember that we recommend light use of frameworks. You don\u0026rsquo;t want to sound robotic or force yourself down a single path too early. Use this framework as guidance as you start answering strategy questions, but work on developing your own intuition about how to structure your answers that builds on your unique strengths.\nHow to Answer Go-to-Market Strategy Questions Go-to-Market (GTM) strategy questions assess your ability to launch a product successfully. Example questions include:\n\u0026ldquo;How would you launch a new product recommendation carousel for Amazon?\u0026rdquo; \u0026ldquo;How would you launch a standalone Amazon restaurant food delivery app?\u0026rdquo; \u0026ldquo;Come up with a launch strategy for Google Glass\u0026rdquo; In addition to standard strategy concerns such as company mission, product strategy, and landscape, examine your user base more carefully.\nAsk yourself:\nDoes the launch aim to increase engagement among existing users, or is the company targeting new users? How will I reach my target audience? How will I convince them the product is valuable? What additional resources and support will I need to execute the launch plan successfully? All of these are valid paths to explore as you answer a GTM question.\nStep 1: Define the Landscape At first, your approach will mirror the basic strategy question framework. You\u0026rsquo;ll work to uncover the reason for the launch and in doing so, gain an understanding of target users, define a set of product goals, and consider the competitive landscape. Then, you\u0026rsquo;ll define a reasonable GTM strategy to support those goals.\nWhen asking clarifying questions, be sure to ask your interviewer for clarification on:\nWhat the product does How it sits in the company\u0026rsquo;s product portfolio Why the product is being launched Ask yourself \u0026ldquo;What is this company seeking to do as a business, and with this product specifically?\u0026rdquo; Any feature, product, and company strategies you discuss should align. For example, if you\u0026rsquo;ve been asked to launch a new feature, consider how the feature serves to accomplish product goals. If you\u0026rsquo;ve been asked to launch a new product, consider how the product serves to accomplish company goals.\nBefore jumping into GTM considerations, consider the competitive landscape at hand. As in general product strategy questions, ask yourself:\nWhat aspects of the landscape will influence the company\u0026rsquo;s options, and ultimately its decision? What opportunities or challenges might I reasonably expect to face? Step 2: Identify a GTM Strategy Go-to-market strategies vary widely from company to company. Your answer will depend on the specifics of the scenario you\u0026rsquo;re given, but it can be helpful to read about other companies\u0026rsquo; GTM strategies to get a sense of what\u0026rsquo;s possible. For example:\nInbound GTM strategy: A company creates helpful content to attract the attention of its target audience Sales-driven GTM strategy: Empowers highly trained and often technical salespeople to sell complicated products with long sales cycles Demand generation GTM strategy: Aims to create audience demand by generating buzz around the launch As you begin the GTM portion of the question, define the subset of users who will need to see, trust, and ultimately buy into the new product or feature. Follow this two-phased approach:\nPhase 1: Identify a user segment you want to acquire, and find ways to reach them. Phase 2: Brainstorm how to convince those users to first try the product, and ultimately stick with it.\nFinally, prioritize the approach that makes the most sense and briefly discuss how you\u0026rsquo;d execute. You can return to the standard framework for this piece.\nGTM Example: Spotify Podcast Launch Let\u0026rsquo;s work through an example: \u0026ldquo;As the PM launching Spotify\u0026rsquo;s Podcast product, how would you acquire users?\u0026rdquo;\nClarifying Questions First, ask clarifying questions such as:\n\u0026ldquo;Do we already have a catalog of podcasts?\u0026rdquo; \u0026ldquo;Will this product exist inside the Spotify app, or will it be a standalone offering?\u0026rdquo; Assume your interviewer gives you the following details:\nWe do have a catalog of podcasts. We will offer the most widely distributed podcasts, and we have a few exclusive deals with big names. The podcast product will exist inside the Spotify app. There will be a podcast tab and podcast content will be listed on the homepage. Define Business and Product Goals A GTM question essentially asks how you\u0026rsquo;d acquire (the right) users for a new feature or product, so you\u0026rsquo;d want to consider why Spotify wants to acquire users before proceeding. Explore the business and product goals:\n\u0026ldquo;Let\u0026rsquo;s talk about Spotify as a business and what podcasts offer. Spotify\u0026rsquo;s business is based on a tiered model, where premium users pay a subscription and free users listen to ads. Spotify wants both types of users to spend time listening so they choose to pay for a subscription or generate ad revenue.\nPodcasts are a popular content type. They can cover a range of topics and they appeal to many different people. Many podcasts have avid followers, and most release new episodes at a regular cadence. Also, podcasts are a different type of content than music, allowing Spotify to diversify.\u0026rdquo;\nFrom there, connect the dots. Summarize why launching a successful podcast product would be a win for Spotify:\n\u0026ldquo;If Spotify offers a compelling podcast experience, it could convince existing free users to spend more time listening, possibly even sign up for Premium features, increasing the Lifetime value (LTV) of our customers. Podcasts could also make Spotify attractive to new types of users, for example, podcast fans who weren\u0026rsquo;t interested in the music catalog.\u0026rdquo;\nAnalyze Competitive Landscape Before jumping into GTM options, consider the competitive landscape:\n\u0026ldquo;Podcast services are a crowded space and it\u0026rsquo;s easy to enter with the widely available catalog of content. While users may be loyal to a particular podcast, they probably don\u0026rsquo;t care what app they use to listen to as long as they like the UI. This could make it easier to acquire a user, but also easier to lose them. There may also be some inertia in getting a user to change their listening habit. The most notable differentiation is that some services have exclusive podcasts, and it sounds like Spotify will also have some.\u0026rdquo;\nDefine Target User Segment Use your judgment as a PM to choose a product goal and user segment that are compelling. For this example:\n\u0026ldquo;I\u0026rsquo;m going to assume that the primary goal is to convert existing Spotify users, especially free users, into podcast listeners. This strategy makes it easier to get traction within the crowded podcast landscape, and there\u0026rsquo;s a significant long-term benefit to retaining users for longer periods of time.\u0026rdquo;\nPhase 1: Reach Target Users Now that you\u0026rsquo;ve laid out a product goal and a target user segment, and discussed the competitive landscape, it\u0026rsquo;s time to identify GTM strategies.\n\u0026ldquo;As I think about GTM strategies, initially reaching target users is going to be easy because our goal is to convert existing Spotify users. The most obvious way to reach them is through the homepage when they open the app. We have their user information and activity data stored in our database; we may even have product usage signals to tell us who\u0026rsquo;d be interested in podcasts. In a more general sense, we can identify places our users are likely to be – e.g. reaching out through targeted ads or partnerships with other products that our users also tend to use.\u0026rdquo;\nPhase 2: Convince Users to Try and Stick \u0026ldquo;How we convince users to try Spotify Podcasts depends on the method we use to reach them. Convenience and exclusive content are likely to drive usage. If they\u0026rsquo;re in the app, we might present a personalized recommendation for a podcast so they can see the value right away — and they\u0026rsquo;re likely already intending to listen to something. If they\u0026rsquo;re not already in the app, we could reach them through email, getting them excited about the content, or giving them a limited-time offer to try podcasts. We could also make exclusive deals with podcasters.\u0026rdquo;\nPrioritize GTM Strategy As you wrap up the GTM discussion, prioritize a GTM strategy that makes sense given all your work up to this point:\n\u0026ldquo;In this case, given the immediacy and the scale of users to reach, in-app promotion is the most effective. Next steps include adding podcast recommendations within the cards on the homepage, directing users to the Podcast tab, and perhaps making personalized playlists for users to check out.\u0026rdquo;\nEvaluate Strategy Finally, finish with an evaluation of your strategy. What are the risks, challenges, and tradeoffs that could come with this strategy? What might you consider as the long-term approach?\n\u0026ldquo;In the relatively near term, Spotify wants to acquire new users. The existing user base is helpful for that: Attract more exclusive deals on better terms because you can guarantee a big audience to bring in these podcast audiences. A notable challenge is that podcast users might not be inclined to subscribe to Premium because podcasts have ads burned into them already. You would need to invest in valuable Premium-only features, such as auto-download for offline listening (assuming these aren\u0026rsquo;t widely available elsewhere).\u0026rdquo;\nGTM Strategy Framework Summary Define the landscape - Understand product goals, competitive context, and strategic alignment Identify user segments - Determine who you\u0026rsquo;re trying to reach and why Phase 1: Reach users - Find ways to get in front of your target audience Phase 2: Convert users - Convince them to try and stick with the product Prioritize approach - Choose the most effective strategy given your constraints Evaluate strategy - Discuss risks, tradeoffs, and long-term considerations How to Answer Pricing Strategy Questions Pricing strategy questions assess your ability to set a product price that makes sense in a given business context. Pricing is much more complex than just setting a number, and answering pricing questions well requires a firm understanding of how the product serves users and the core business. Example questions include:\n\u0026ldquo;How would you go about pricing a self-driving car?\u0026rdquo; \u0026ldquo;How would you price buying a season of a show?\u0026rdquo; \u0026ldquo;As a Google PM, what\u0026rsquo;s the next piece of hardware you would launch, and how would you price it?\u0026rdquo; To answer these, work through the general framework first, then consider the many ways product pricing can impact the success of a product and the larger company. For starters:\nPrice affects revenue and gross margin. Generally, higher-priced products sell fewer units, but the gross margin on each unit is higher. Different prices attract different user groups. For example, luxury goods with sky-high prices attract users with certain characteristics in common. Pricing affects how people perceive the product. For example, consumers expect luxury brands to sell expensive products and they expect a quality product in return, whereas a mass-market product priced similarly might seem like a ripoff. Keep these considerations in mind as you work to understand the problem context, the product and company goals, and the competitive landscape. Once you\u0026rsquo;ve covered those, you can hone in on a pricing strategy.\nStep 1: Define the Landscape When asking clarifying questions, be sure to ask how the product offers value beyond the company\u0026rsquo;s existing portfolio. Understanding new features or content offerings will help you discern who the target users are, which will help you set a reasonable pricing strategy.\nYou\u0026rsquo;ll also want to ask questions to help you understand how the product will be launched - is it a brand-new offering, in which case you\u0026rsquo;ll set the initial price? Or has there been a change that requires new pricing? If this is the case, you\u0026rsquo;ll have to consider the existing pricing strategy and how it might make sense to diverge from it.\nAs you define product goals and company strategy, consider where the company is right now, and where it would like to go. For example:\nIs the company entering a new market? Seeking market dominance? Is the company seeking short-term profits or investing in a strategy for the long term? What about this product is novel, and how will offering this novelty affect the company\u0026rsquo;s brand? What risks exist in moving ahead with this product? Finally, consider landscape factors:\nCompetition: Is the market highly competitive? Are there entrenched players? If so, is the company one of them? Public opinion: How will the public react to the product? Are there any regional factors to consider? Internationalization: How is the current pricing strategy applied in international markets (if any)? How relevant is the scalability of the pricing strategy to other geographies? Consumer Price Sensitivity: Is inflation affecting the market right now? What is current consumer price sensitivity? All of these factors can potentially affect your pricing strategy, but the list is not exhaustive. Figure out what makes sense given the specifics of your question.\nStep 2: Identify a Pricing Strategy After you\u0026rsquo;ve defined goals and landscape considerations, move on to pricing. Here are some common pricing strategies to be aware of:\nValue-based pricing: Set product price according to the value of the product as perceived by its users. Cost-plus pricing: One of the simplest pricing strategies. Simply apply a markup to production cost. Dynamic pricing: Instead of setting a price for an extended period of time, in some markets it makes sense to price products dynamically according to fluctuating demand. Surge pricing from rideshare companies like Uber and Lyft is an example. Consider whether there is an existing strategy in place, in which case you\u0026rsquo;d likely adapt that to suit your product, or whether there is another benchmark price you can make use of. If you\u0026rsquo;re pricing an entirely new product, you might use the cost-plus strategy taking into consideration the company\u0026rsquo;s brand and the expected value of the product to users.\nThen, consider the company\u0026rsquo;s macro and micro goals - overall, what is the company mission? What is it trying to do in the space? And what is it trying to do with this product? Don\u0026rsquo;t forget to consider how the product sits in the company\u0026rsquo;s product portfolio. Sometimes a new offering will cannibalize existing products. Use your judgment as a PM to make a call.\nTip: You may have other insights personal to you. Studying behavioral economics might have given you insights into how to frame relative prices. Previous work experiences where you launched a substitute for an existing product may have given you a reference for answering pricing questions. Strategy questions are a great place to showcase your particular interests as a PM, so don\u0026rsquo;t hold these back.\nPricing Example: Uber Autonomous Vehicles Let\u0026rsquo;s work through an example: \u0026ldquo;Assume Uber is rolling out autonomous vehicles (AVs) in San Francisco. How would you price a ride for these vehicles?\u0026rdquo;\nClarifying Questions First, ask clarifying questions. For the Uber question, you\u0026rsquo;d want to ask:\n\u0026ldquo;What\u0026rsquo;s the performance (and regulatory status) of these vehicles?\u0026rdquo; \u0026ldquo;How many vehicles will Uber have? Will they all be deployed at once?\u0026rdquo; \u0026ldquo;Is this a separate service from UberX?\u0026rdquo; You could ask your interviewer what Uber\u0026rsquo;s goals are for launching these autonomous vehicles, but don\u0026rsquo;t be surprised if the interviewer asks you to define that yourself. Let\u0026rsquo;s assume your interviewer tells you that:\nThe technology is approved by regulators Performance-wise, the AVs operate more safely than human drivers AVs will make up roughly 10% of the total fleet size at peak hours Define Product Goals and Strategy Next, define the product goals for the AV fleet, and Uber\u0026rsquo;s overall strategy. It can be helpful to consider what\u0026rsquo;s novel about the product:\n\u0026ldquo;Releasing AVs will change Uber\u0026rsquo;s operations significantly. AVs don\u0026rsquo;t need drivers, so they can operate more frequently and don\u0026rsquo;t need to be paid a wage — though there could be a rental fee if Uber doesn\u0026rsquo;t own the vehicles. The lack-of-driver factor obviously lowers the cost of operating the vehicles. Also, AVs are a new technology, which is exciting and could strengthen Uber\u0026rsquo;s brand as cutting-edge but could cause concern about drivers being replaced or about safety.\nOverall, AVs are likely a significant investment. If deployed well, this would make Uber a leader in the space. Uber doesn\u0026rsquo;t seem to be aiming for short-term profits with this launch. I\u0026rsquo;m going to assume, going forward, that Uber is seeking market dominance. In terms of product goals, AVs can support Uber\u0026rsquo;s market domination goal by increasing vehicle availability while possibly reducing cost and by strengthening Uber\u0026rsquo;s positioning as a tech leader.\u0026rdquo;\nAnalyze Strategic Landscape The last factor to consider before jumping into pricing is the strategic landscape.\nRecall that your interviewer already confirmed that the AVs drive safer than humans and that the technology is approved by regulators, so those two important factors don\u0026rsquo;t pose risks. The most important thing to note about the space Uber operates in is that it\u0026rsquo;s highly competitive:\n\u0026ldquo;Most rideshare users care about convenience and price; there\u0026rsquo;s not much brand loyalty to speak of. Users often compare Uber with competitors like Lyft when they\u0026rsquo;re looking for a ride. This means that Uber\u0026rsquo;s margin is squeezed; it must keep ride prices low to avoid losing customers, yet it must keep rates high enough to acquire drivers especially during peak hours.\nAnother thing to consider is that San Francisco has public transit options, especially in its most populous areas. These options are usually lower cost at the expense of longer travel times. Users could also just walk or drive themselves.\nOverall, it\u0026rsquo;s a tough environment. Uber faces significant competition with thin margins. AV technology is not widespread though, so it\u0026rsquo;s a potential differentiator.\u0026rdquo;\nBrainstorm Pricing Options Next, brainstorm pricing options. Uber already has a pricing structure in place, so you\u0026rsquo;d want to start there. Then, adjust the pricing model to accommodate the AV fleet, including any strategic factors you previously identified:\n\u0026ldquo;Uber has a marketplace model that prices rides dynamically, based on current supply and demand. I see no reason not to use this pricing model as a baseline, and adjust it to the AV product. For example, at peak hours, the AVs will create some increase in supply, but the increase will be more notable during off-hours where they can provide a significant, predictable baseline; for example, for users trying to get a ride at 5:00 a.m.\nOn the strategic side, we\u0026rsquo;re assuming that Uber is trying to grow market share. To do that, it needs to get users excited enough about this technology, or, the AVs need to be better priced and more convenient than the competition. While Uber could explore putting a premium on the price as a signal of quality, it seems the best way to encourage usage is to offer these rides at a lower price than the competition. Uber can afford to do this because AVs have lower per-mile costs. Lower prices might encourage users who are hesitant about AVs to give them a chance.\nOne final consideration is that, from an Uber-wide perspective, AVs will compete with existing services. Uber won\u0026rsquo;t want to lose drivers, so AVs can\u0026rsquo;t be priced so low that users will be hesitant to take standard rides. There will have to be a balance between these factors.\u0026rdquo;\nMake Pricing Recommendation Given the above, you\u0026rsquo;re ready to give a more specific answer to the question at hand:\n\u0026ldquo;Overall, I would propose that Uber use a market-based pricing model with a slight discount on AV rides to place prices below the competition. Uber\u0026rsquo;s margins are likely thin, so I\u0026rsquo;ll estimate that Uber can tolerate a price reduction of about 20%. Ideally, I\u0026rsquo;d want to better understand exactly what Uber\u0026rsquo;s tolerable loss-to-profit-margin is with the goal of outpacing competition in the long term. Without further information, I\u0026rsquo;d recommend aiming for rides to cost about 20% less than UberX.\u0026rdquo;\nEvaluate and Consider Long-term As you wrap up, evaluate your answer. Consider strategic pitfalls that might challenge your pricing model. To dive a little deeper, you might consider how this strategy plays out long term or how you\u0026rsquo;d account for risks you identified but didn\u0026rsquo;t cover in detail:\nLonger-term vision: You could talk about how the initial AV rollout helps Uber get to a fully autonomous fleet and plan pricing around that goal. Price evolution: You could propose that Uber offer a promotion (at first) to get riders interested, then raise prices once riders are used to the service. Competitive response: You could consider how competitors might react to this and what you could advise to watch out for with this strategy. For example, competitors might temporarily raise driver pay to drive discontent with Uber, causing operational and public relations issues. Pricing Strategy Framework Summary Define the landscape - Understand value proposition, launch context, company goals, competitive factors Identify pricing strategies - Consider value-based, cost-plus, dynamic pricing approaches Adapt existing models - Leverage current pricing structure if available Consider strategic goals - Align pricing with company mission and market position Make specific recommendation - Provide concrete pricing with clear rationale Evaluate long-term implications - Discuss evolution, risks, and competitive responses How to Answer Growth Strategy Questions Growth strategy questions assess your ability to find the right levers to pull to maximize growth. Example questions include:\n\u0026ldquo;How would you increase the number YouTube users?\u0026rdquo; \u0026ldquo;How would you 10X Google Cloud IoT usage?\u0026rdquo; \u0026ldquo;How would you increase Gmail ads revenue by 20%?\u0026rdquo; There are many paths to growth. Depending on the question you might decide to focus on attracting new users, driving engagement with existing users, or even reducing churn. Your choice will be highly context-dependent. The key may lie in untapped user behavior, alterations to the product or the business model, or a strategic partnership.\nGrowth questions can be answered using the standard strategy framework with a slightly different lens. You\u0026rsquo;ll spend more time making sense of where the product/company is now and diving deeper into key factors like user behavior to figure out how to drive growth.\nStep 1: Define the Landscape Begin by clarifying the problem and gathering enough context to allow you to model the goal as a simple equation.\nFor example, say you were asked to reduce churn by 10%. Churn rate = (lost customers / Total customers at the beginning of the period) x 100.\nFrom this equation, you know that you need to either decrease the number of lost customers or keep the number of lost customers steady while increasing total customers - or both. Check-in with your interviewer and confirm that your model reflects the situation at hand. If there is any ambiguity in the question, ask your interviewer.\nAsk any questions that will help you understand the product, company, and landscape considerations impacting the status quo and how they might play into growth goals. Be sure you understand:\nThe key value drivers of the product or business Any existing strategies; understand why they were chosen, what metrics are being tracked, and how those metrics have changed over time Any specific strengths and weaknesses in the product or business Step 2: Identify Options for Growth Next, you\u0026rsquo;ll define an overall strategy that makes sense for increasing growth given your goal and the context you outlined. From there, you\u0026rsquo;ll brainstorm options, choose one, justify, and evaluate your answer.\nHere are some common growth strategies to be aware of:\nMarket Penetration: Growth opportunities abound in existing markets. To penetrate deeper into a current market, companies typically work to increase market share by attracting new users or altering their pricing strategy. This can also involve increasing marketing efforts, enhancing product features, or considering mergers and acquisitions within the same market to consolidate and expand their presence.\nMarket Expansion: Instead of focusing on the existing market, companies will often work to expand their reach by entering new markets with their existing products. This could involve geographic expansion, targeting new customer segments, or finding new uses for an existing product.\nProduct Development: Instead of looking for new users for an existing product, companies can expand their product offerings to continue selling in the same market.\nDiversification: With a diversification strategy, a company might try to enter a new market with a new product. Diversification is generally considered the riskiest growth strategy.\nAs you work on your overall strategy, consider where the company is in the marketplace, what its overall business strategy is, and what the company\u0026rsquo;s mission and culture are. A small startup might get away with a bolder move than a large entrenched player, for example. You don\u0026rsquo;t want to give an answer that\u0026rsquo;s out of touch with the ethos of the company.\nAs you identify opportunities, don\u0026rsquo;t be afraid to dig deep into user behavior. Generally, growing a company means reaching more users or getting \u0026ldquo;better\u0026rdquo; usage. Consider the user subsets who aren\u0026rsquo;t using your product - are there large groups that have characteristics or even pain points in common? The larger the subset, the easier it will be to achieve significant growth if you reach them, though you should always be cautious when making assumptions about large groups. Ask your interviewer more clarifying questions if there are specific insights you\u0026rsquo;re looking for.\nFinally, prioritize the next steps of your plan to increase growth and summarize and evaluate your answer. To prioritize, be sure to refer back to the product and company strengths you identified and your understanding of the market. Consider:\nLikelihood of success Expected impact Any short (and especially) long-term strategic benefits Once you\u0026rsquo;ve made your choice, recap your process and evaluate your answer. You might:\nTransition into product thinking to come up with a feature or a new product that fills a gap identified Briefly explore an alternate idea that you didn\u0026rsquo;t go in-depth on, say, a strategic partnership or acquisition that might offer synergies Offer risk mitigation strategies for any risks that came up Discuss the details of executing your plan, including any metrics you\u0026rsquo;d track, and counter metrics you\u0026rsquo;d want to keep an eye on Growth Example: YouTube Watch Time Let\u0026rsquo;s work through an example: \u0026ldquo;How would you increase YouTube\u0026rsquo;s average watch time by 2x in the next 3 years?\u0026rdquo;\nClarify the Problem First, clarify any ambiguity in the question:\n\u0026ldquo;Do we mean the watch time for all YouTube products or for the core YouTube product?\u0026rdquo;\nLet\u0026rsquo;s assume your interviewer confirms that you\u0026rsquo;re increasing watch time for the core product only. Now, create a high-level equation to capture your goal:\n\u0026ldquo;I think of average watch time as number of users x (average watch time / user). To drive average watch time up, we\u0026rsquo;d need to increase either the number of users, or the average watch time per user (while holding the other constant, at least). The best case, of course, is that we raise both. Does that capture the situation?\u0026rdquo;\nUnderstand Current Context Assume your interviewer gives the go-ahead. The goal is relatively straightforward, but you should still aim to arrive at a better understanding of YouTube\u0026rsquo;s strategy, its place in the competitive landscape, and its current approach:\n\u0026ldquo;YouTube represents the largest video platform in the marketplace. It\u0026rsquo;s dominant in many (but not all) of the markets they\u0026rsquo;re in. It has billions of users and the watch time is likely in the hundreds of billions of hours per year. YouTube does really well with educational content, longer video logs (vlogs), and is basically the default for embedding video on the web.\u0026rdquo;\n\u0026ldquo;YouTube does have some notable competitors across different types of video content, from TikTok (at the short end) to Netflix (at the premium end). YouTube has tried to compete with both to some extent, with Shorts and Originals, as short-form video is a notably fast-growing space.\u0026rdquo;\nGenerate Growth Opportunities Next, it\u0026rsquo;s time to generate opportunities for growth. Given the context defined and your model, what opportunities are available? Recalling your high-level equation where you identified that to grow YouTube, you\u0026rsquo;d need to increase the number of users or watch time (or both):\n\u0026ldquo;To acquire new users, we\u0026rsquo;d need to grant access to users who currently can\u0026rsquo;t use YouTube or acquire users who don\u0026rsquo;t use YouTube by choice. Given the scale of YouTube, the second group is probably small and fragmented. To increase watch time, we could consider why people watch YouTube and what keeps them from watching more. I\u0026rsquo;d guess that a few of the key reasons are:\nLimited time to spend watching video Data limitations Preference for other kinds of content (e.g. short-form video) For the purposes of this discussion, I\u0026rsquo;ll proceed with the goal of increasing watch time rather than acquiring new users.\u0026rdquo;\nDig Into User Behavior You may find more specific and actionable angles to the problem if you decide to dig deeper into specific use cases. Niches won\u0026rsquo;t get you to 2x growth, but there may be important use cases worth considering. Some options to explore might include influencer content or potential upside in educational content:\n\u0026ldquo;Influencer content is a huge use case, and one problem affecting YouTube is that many influencers favor other platforms, reducing the time viewers spend on YouTube. Likewise, educational content is already a big vertical, but many people don\u0026rsquo;t consider YouTube their first choice for learning, so there\u0026rsquo;s likely still more upside for growth.\u0026rdquo;\nPrioritize Growth Options There are many options to consider. Use your judgment, but remember, getting more specific about user behaviors, types of content, and how the market interferes with watch time could all yield valuable insights. Given the challenges above, assume you\u0026rsquo;ve narrowed your brainstorm down to the following ideas to expand watch time:\n\u0026ldquo;My top ideas are:\nExpand in international markets Claim a larger share of short videos Get back into premium video Expand in a new or underserved use case, like education or influencer content\u0026rdquo; Make Strategic Recommendation With the shortlist above, it\u0026rsquo;s time to prioritize a strategy, building on YouTube\u0026rsquo;s strengths and your understanding of the market. Remember to consider factors like the likelihood of success, expected impact, and any longer-term strategic benefits:\n\u0026ldquo;In this case, I believe expanding our footprint in short-form video could be a good strategy. Though we haven\u0026rsquo;t had great success yet and there\u0026rsquo;s tough competition for viewers and creators, it\u0026rsquo;s an important and large space. We have significant strengths as a brand and platform, especially because many TikTok creators do also maintain a YouTube presence. Succeeding here also helps us expand in verticals like education, so this helps us continue to grow.\u0026rdquo;\nAddress Previous Failures In this example, you noted that YouTube had failed to seriously compete in short-form video in the past. If you want to recommend that strategy going forward, you\u0026rsquo;d want to call out why YouTube hasn\u0026rsquo;t found success and account for that in your strategy:\n\u0026ldquo;Notably, we\u0026rsquo;ve tried, but we haven\u0026rsquo;t succeeded yet in this space. To move forward, we\u0026rsquo;d need a more concrete plan to show that success is feasible. Leaning into the strengths of YouTube, it doesn\u0026rsquo;t make sense to just copy competitors like TikTok or Reels. We should use our position to our advantage, that is, our existing content, brand, relationships with creators, connection to Google, and more.\u0026rdquo;\nFrom here, you could go a number of ways:\nYou could dive into a bit of product thinking here to come up with a valuable product You could think about partnerships or programs to drive more creators to YouTube You could think about acquisition targets to give YouTube a boost in audience or product functionality Let\u0026rsquo;s assume for the sake of the example that you suggest building an app for Shorts — leveraging YouTube\u0026rsquo;s strengths in a form factor commonly used with short video.\nEvaluate Risks and Tradeoffs As you close your answer, you\u0026rsquo;d consider risks, challenges, and tradeoffs:\n\u0026ldquo;The largest risk, as we acknowledged before, is that YouTube faces strong competition here and we haven\u0026rsquo;t yet succeeded in the space. This comes with a risk of wasting resources and attention but continuing not to make gains. I suggested building an app for Shorts. One important tradeoff here is that while an app may make it easier to acquire some users, it will be more difficult to bring them into the larger YouTube ecosystem. This might not be important though; it\u0026rsquo;s a factor worth digging into more deeply given more time.\u0026rdquo;\nGrowth Strategy Framework Summary Define the landscape - Create equation model, understand current strategy and competitive position Identify growth options - Consider market penetration, expansion, product development, diversification Dig into user behavior - Analyze user segments and barriers to increased usage Generate opportunities - Brainstorm specific growth initiatives based on insights Prioritize strategy - Choose approach based on likelihood, impact, and strategic benefits Evaluate risks and tradeoffs - Address challenges, consider alternatives, discuss execution details 3. Execution Questions Execution questions assess a candidate\u0026rsquo;s experience in problem-solving, team management, and data-driven decision-making. The format of these questions varies, but the most common types are root cause analysis and decision-making questions.\nHow to Answer Root Cause Analysis Questions The most common type of execution question is root cause analysis.\nRoot cause analysis (RCA) questions present a scenario. Something unexpected has happened with a product, and as a PM, it\u0026rsquo;s your responsibility to figure out what\u0026rsquo;s going on. For example, an interviewer might ask:\n\u0026ldquo;Imagine you are a PM at Lyft and there\u0026rsquo;s been a 20% increase in ride cancellations. What would you do about this?\u0026rdquo;\nThese interviews are unlike most others in the PM loop in that they are conducted via a \u0026ldquo;role-play\u0026rdquo; interviewing style. Your interviewer expects you to ask questions about the context of the problem, the product, and any relevant metrics. Using the information you gather, you\u0026rsquo;ll iterate through increasingly precise hypotheses until you identify a root cause. Often, your interviewer will play the role of an uninformed data analyst, requiring you to ask concrete questions about reasonably available data.\nFor example, one of your first clarifying questions might be \u0026ldquo;Is there anything unusual going on with drivers?\u0026rdquo; In other PM interviews your interviewer would answer directly, but in execution interviews, they\u0026rsquo;ll likely respond with something like \u0026ldquo;As a data analyst, I can help you if you clarify what you want to know in terms of data we might have about drivers?\u0026rdquo;\nThe interviewer will push you to be more specific and concrete about your requests for data throughout the interview.\nWhat Interviewers Are Looking For\nYour ultimate goal in working through RCA questions is to identify the root cause of the problem, but your interviewer will care most about your approach.\nRCA questions are different from product design, strategy, and analytical questions in that they do have a correct answer, but a logical and thoughtful approach to problem-solving can land you a \u0026ldquo;strong hire\u0026rdquo; recommendation even if you don\u0026rsquo;t arrive at the exact right solution. Be sure to demonstrate your ability to make sense of a situation given limited data, generate and continually refine reasonable hypotheses as you learn more information, gather information iteratively through targeted questions, and ultimately identify a root cause that explains the observed problem.\nA Framework for Answering RCA Questions\nRCA questions at most companies are designed to be solved in 25 minutes so a framework to keep you on track is critical. Follow this 6-step process:\nClarify and gather context Form high-level hypotheses Gather data Refine hypotheses and repeat Identify the root cause Evaluate Let\u0026rsquo;s continue the Lyft RCA question as an example. Assume you\u0026rsquo;ve been asked:\n\u0026ldquo;Imagine you are a PM at Lyft and there\u0026rsquo;s been a 20% increase in ride cancellations. What would you do about this?\u0026rdquo;\nStep 1: Clarify and Gather Context\nFirst, ask clarifying questions about anything that will help you make sense of the issue, scope the problem, and align with your interviewer.\nDon\u0026rsquo;t hesitate to ask broad questions like \u0026ldquo;How does this affect our core business?\u0026rdquo; but be aware that your interviewer may not give you direct answers. Instead, they\u0026rsquo;re likely to prompt you to ask for specific data that you\u0026rsquo;d reasonably have access to as PM.\nA helpful tip for getting into the right mindset is to envision what it\u0026rsquo;d really be like to be in the situation you\u0026rsquo;ve been given. What would your priorities be as PM? What kind of data would you need to begin to diagnose the issue?\nExample: For our example question \u0026ldquo;Imagine you are a PM at Lyft and there\u0026rsquo;s been a 20% increase in ride cancellations. What would you do about this?\u0026rdquo; you\u0026rsquo;d want to clarify what \u0026ldquo;ride cancellations\u0026rdquo; means since this could be interpreted in multiple ways. You\u0026rsquo;d also want to know what the baseline is for comparison when considering a \u0026ldquo;20% increase.\u0026rdquo; You should ask what counts as a ride cancellation, whether you\u0026rsquo;re seeing cancellations both before matching with a driver and after, whether you\u0026rsquo;re only seeing riders canceling or if this number includes drivers too, what this 20% increase is compared to (such as week over week), and whether cancellations have been steadily increasing or if you\u0026rsquo;re seeing a sudden spike.\nNote that this question is very specific. Completed rides are central to Lyft\u0026rsquo;s business, so it\u0026rsquo;s obvious that a 20% increase in ride cancellations is bad. You may get a more ambiguous question, like \u0026ldquo;10% of Netflix users are inactive. What would you do?\u0026rdquo; In this case, ask clarifying questions about how this affects business. You may find that what\u0026rsquo;s been framed as a problem may not be the core issue.\nStep 2: Form High-Level Hypotheses\nOnce you\u0026rsquo;ve gathered some context, you can start forming high-level hypotheses about what may have happened. Keep in mind that you have limited time to narrow your scope, so a good strategy is to first eliminate broad areas from consideration.\nAt this stage, you\u0026rsquo;re simply breaking up the universe of possible root causes into meaningful chunks that you can dive into later and eventually prioritize or discard. Common sources of problems include unintended technical issues or product bugs, product changes that were possibly intended but had unintended consequences, operational changes within the company, and external events beyond the company\u0026rsquo;s direct control.\nRemember that communicating your process is critical, so share your full list of hypotheses with your interviewer before you begin prioritizing in the next step.\nExample: For our example question regarding Lyft cancellations, your broad hypotheses might include a product bug or technical issue such as rides being automatically canceled or an issue with the app not showing users that a driver is on the way. It could also be a product change in the Lyft app such as making the \u0026ldquo;cancel\u0026rdquo; button more prominent. Another possibility is a change to rider/driver operations, which is a key component of the service—for example, a change in driver pay that leads drivers to cancel less profitable rides. Finally, it could be an external factor like a Taylor Swift concert that has created massive traffic, causing users to cancel when they see long wait times.\nStep 3: Gather Data\nAt this point, you\u0026rsquo;ve got a solid set of high-level hypotheses. The next step is to determine which of these is worth expanding on. You\u0026rsquo;ll do that by gathering more data.\nOne helpful strategy for identifying what\u0026rsquo;s driving the problem is to isolate key variables. For the sake of an execution interview, a key variable is a variable that correlates highly with the issue at hand.\nFor example, say you\u0026rsquo;re diagnosing a problem with a smartphone app and the problem is evenly distributed between users across age groups and geographic locations. If, when you look at the distribution of errors experienced by device, you see that 90% of users experiencing the issue use a Google Pixel, you might consider device to be a key variable.\nYou\u0026rsquo;d then gather more data to figure out what Google Pixel usage correlates with the issue. It could be a certain version of the app installed on Pixels, something about the device itself, or even a factor that users who buy Pixels have in common. To isolate key variables, ask yourself which variables could reasonably be correlated with the variation you\u0026rsquo;re seeing and how you might test whether these variables are having an impact without confounding variables affecting the outcome.\nWhen you\u0026rsquo;re ready, ask your interviewer whatever questions feel meaningful to help you gather the right data. Be sure to explain why you\u0026rsquo;re asking each question and how the answer changes your understanding of the problem.\nExample: Recall that you brainstormed the following list of high-level hypotheses that might explain the increase in cancellations:\nA product bug or technical issue such as rides being canceled or an issue with the app not showing users that a driver is on the way. A product change in the Lyft app such as making the \u0026ldquo;cancel\u0026rdquo; button more prominent. A change to our rider/driver operations such as a change in driver pay that leads drivers to cancel less profitable rides. An external factor like a Taylor Swift concert has created massive traffic causing users to cancel when they see long wait times. Continuing the example:\n\u0026ldquo;If a product or technical issue caused this, we should be able to attribute it to a particular launch or update. Were there any releases that happened around the time of the spike? Was there a particular version of the app where this started? If so, app version would be a variable we\u0026rsquo;ve isolated as affecting this metric, which gives us a clear direction to explore.\nIf a change in our operations is causing the cancellations, it\u0026rsquo;s likely that other metrics have been affected. Have we seen a change in the number of active drivers? Have we seen a change in the number of ride requests?\u0026rdquo;\nAssume the interviewer tells you:\nThere\u0026rsquo;s no clear release or app version that caused the issue. We have seen a drop in the number of drivers who complete a ride, but no drop in ride requests. Step 4: Refine Hypotheses and Repeat\nThe goal here is to gather evidence to either support or deprioritize each hypothesis as you drive toward the root cause of the problem.\nWith more data, refine your hypotheses, generate new questions to ask, and refine further. You may find yourself repeating this step a few times. This is normal. Every additional piece of data helps to make your hypotheses more specific.\nBe sure to communicate every step of your thought process to your interviewer and check in frequently. Doing so can preempt tough follow-up questions after you give your final answer.\nExample: Continuing our Lyft example, recall that your interviewer confirmed that:\nThere has been no clear release or app version that caused the increase in ride cancellations. There has been a drop in number of drivers who complete a ride, but no drop in ride requests. Refine your hypothesis as follows:\n\u0026ldquo;Based on these insights, I\u0026rsquo;d like to deprioritize my hypotheses that the increase in cancellations is due to either a technical issue or a product change, as there\u0026rsquo;s no evidence that a release or app version is causing the increase in cancellations, and we haven\u0026rsquo;t seen a decrease in ride requests.\nThere is more evidence that the issue is related to our operations or an external event. Specifically, it seems like something that\u0026rsquo;s particularly affecting drivers. Perhaps riders are seeing longer wait times (because there are fewer drivers) so they\u0026rsquo;re canceling.\u0026rdquo;\nLet\u0026rsquo;s assume that your interviewer confirms that your analysis is correct, and that drivers are being affected.\nStep 5: Identify the Root Cause\nOnce you have enough data, explain what you think the root cause of the issue is. If you\u0026rsquo;ve communicated your thought process throughout, you\u0026rsquo;ll have made a logical case for your choice, so you\u0026rsquo;ll only need to give a quick summary here before moving on to evaluation.\nExample: Continuing our Lyft example, recall that you have confirmation from your interviewer that the problem is:\nLikely related to Lyft\u0026rsquo;s operations, an external event, or both Affecting drivers specifically Given this insight, it would be helpful to consider what factors would affect drivers\u0026rsquo; willingness and ability to drive. Here are a few specific hypotheses that fit the information you have:\nAn external event is causing traffic difficulties Drivers are unhappy with Lyft and leaving Drivers are being paid or treated better on another platform and so are being pulled away Assume you\u0026rsquo;ve found no evidence that an external event is causing traffic difficulties over the time period in question, and there is no strong evidence to support drivers are unhappier with Lyft than they were prior to the increase in cancellations. You\u0026rsquo;d be left with the hypothesis that drivers are being paid or treated better elsewhere, and are leaving Lyft for another platform.\nStep 6: Evaluate\nIf you followed the framework and communicated throughout, your answer will be backed up by data. Quickly recap your major findings, and then spend a few minutes discussing what to do about the root cause you\u0026rsquo;ve identified. Consider:\nWhether the root cause should be fixed. Sometimes, a change in a metric is caused by a product change that the company is otherwise happy with. The next steps may just be to keep an eye on things and to act only when it\u0026rsquo;s clear user experience is being degraded. If a fix is needed, consider what mitigation makes sense. Many candidates reach for a product change immediately, but that\u0026rsquo;s not always the best option. Use your judgment as a PM. Example: Wrapping up our Lyft example, let\u0026rsquo;s say that your interviewer confirms that your hypothesis is correct and that Uber recently increased driver pay substantially, drawing drivers away from Lyft. Close with:\n\u0026ldquo;In the short term, Lyft is losing out — but given margins, I can\u0026rsquo;t imagine Uber would be able to sustain the economics of such an increase for the long term. We could choose to do nothing for now and wait for Uber to eventually reduce driver pay.\nIf we did want to respond, I would consider small changes to the app such as making the \u0026lsquo;cancel\u0026rsquo; button slightly less prominent. We might also consider more drastic measures such as charging a fee for cancellation, but this could drastically harm the user experience.\nAnother option could be to raise driver pay to match Uber\u0026rsquo;s, but we would need to understand the business implications in much more depth before choosing to go that route.\u0026rdquo;\n\u0026ldquo;Okay\u0026rdquo; vs. \u0026ldquo;Good\u0026rdquo; vs. \u0026ldquo;Great\u0026rdquo; Answers\nIn an okay answer, the candidate asks questions that are relevant and offers a light interpretation of the situation but doesn\u0026rsquo;t explain their overall thinking on the problem. It feels like they have an intuitive sense of what could be going on, but it\u0026rsquo;s not clear if their approach is comprehensive or deeply thought out. Alternately, they may quickly jump to plausible but specific explanations and focus on proving or disproving them, slowing down their ability to make sense of the situation.\nA good answer is one where the candidate shares what possibilities they\u0026rsquo;re considering and it is clear why they\u0026rsquo;re asking the questions they do. The candidate interprets responses and explains how they change their understanding. The candidate confidently navigates the possibility space and it feels like they make steady progress toward an answer.\nA great answer does the above but also displays deep insight and ability to contextualize the problem and uses that to guide their approach. For instance, the candidate might draw on product insights, explaining why and when users or drivers cancel, then apply that to quickly narrow in on the most important metrics to check. While regular answers to these questions usually feel like an exploration, great answers often feel like they quickly and intelligently cut through the possibilities and lock on to the heart of the problem.\nCommon RCA Pitfalls\nImmediately guessing at the root cause: If you are trying to prove or disprove a narrow hypothesis, it will take a long time to find the answer (or the interviewer will give up and guide you toward the answer to keep moving.) Not explaining what you\u0026rsquo;re considering or why you\u0026rsquo;re asking particular questions: If you don\u0026rsquo;t explain your approach, you\u0026rsquo;re more likely to get lost. To the interviewer, it may feel like there\u0026rsquo;s no rhyme or reason to your process. It also makes it harder for the interviewer to guide or help you. How to Answer Decision-Making Questions Decision-making questions present a scenario where you\u0026rsquo;re asked to make a product decision. Often, your choices are similar and there\u0026rsquo;s no obvious winner. For example, you may be asked:\n\u0026ldquo;You\u0026rsquo;re a PM on Facebook Watch. Your team is working on a redesign. This redesign improved the watch time, but it also caused a drop in likes and comments. Should you ship this redesign?\u0026rdquo;\nThe key to delivering a solid answer is to spend time clarifying product and company goals and defining decision criteria upfront.\nWhat Interviewers Are Looking For\nInterviewers are looking the same skill set as in RCA questions, but there is added emphasis on:\nProduct thinking Strategic thinking and business acumen Ability to evaluate options Note that decision-making questions are an excellent opportunity for you to showcase your domain expertise, so don\u0026rsquo;t shy away from referencing personal experience.\nA Framework for Answering Decision-Making Questions\nDecision-making questions are designed to take roughly 25 minutes. Follow this 4-step process to arrive at a decision efficiently and confidently:\nClarify and gather information Set decision criteria Evaluate options Decide and recap Let\u0026rsquo;s continue using the above question as an example. Assume you\u0026rsquo;ve been asked:\n\u0026ldquo;You\u0026rsquo;re a PM on Facebook Watch. Your team is working on a redesign. This redesign improved watch time, but it also caused a drop in likes and comments. Should you ship this?\u0026rdquo;\nStep 1: Clarify and Gather Information\nDecision-making questions require you to make a big decision without much context, so always begin by asking clarifying questions. In the next step, you\u0026rsquo;ll begin to explain your criteria for evaluating your options, so your goal here should be to gather enough information to understand how your decision will ultimately affect business and define your priorities as a PM.\nClarify any unclear terms including company names, important nouns, and adjectives. If a metric was referenced, be sure to gather more information about that metric. What is the baseline value you\u0026rsquo;re comparing against? Is the change positive or negative (or is this unclear?)\nDon\u0026rsquo;t hesitate to ask questions about how the scenario you\u0026rsquo;ve been given affects the business, but be aware that your interviewer might not offer that information. In that case, it\u0026rsquo;s fair to make assumptions based on what you know of the product or company and its goals, but be sure to communicate any assumptions to your interviewer.\nExample: For our example question \u0026ldquo;You\u0026rsquo;re a PM on Facebook Watch. Your team is working on a redesign. This redesign improved watch time, but it also caused a drop in likes and comments. Should you ship this?\u0026rdquo; There is a lot of missing information. You\u0026rsquo;d want to ask specifically about product and company goals, clarify the vague metrics referenced, and try to gather details about the redesign. For example:\nProduct and company goals\n\u0026ldquo;How does Facebook Watch fit into Meta\u0026rsquo;s mission and strategic goals?\u0026rdquo; Metrics\n\u0026ldquo;How much did watch time increase, and how much did likes and comments decrease?\u0026rdquo; \u0026ldquo;Should I assume total watch time increased, or is this watch time per session?\u0026rdquo; \u0026ldquo;Are these likes and comment metrics specific to video, or are we seeing this effect across other types of content?\u0026rdquo; Redesign\n\u0026ldquo;Why did we decide to redesign Facebook Watch?\u0026rdquo; \u0026ldquo;What was changed?\u0026rdquo; Let\u0026rsquo;s assume your interviewer told you that total watch time increased with the redesign, and that likes and comments are down for video only. Otherwise, they told you not to focus on the specifics of the redesign, and that you should assume whatever makes sense to you.\nStep 2: Set Decision Criteria\nNext, set decision criteria based on the information you gleaned in the last step.\nSetting decision criteria is similar to goal setting in other interviews. Aim to sum up the most important factor(s) to consider based on the context defined above. Clear decision criteria will help you stay focused and objective while building support for your decision later, so we recommend spending significant time on this step.\nYour decision criteria will hinge on what\u0026rsquo;s most important to your product and to the company. Use your judgment to figure out what\u0026rsquo;s most relevant to your question. Common areas to consider include:\nStrategic product or company goals The current market including key competitors, trends, pending regulation, etc. Clear user needs or pain points The company\u0026rsquo;s mission Explain your thought process to your interviewer and repeat your key takeaways before moving on. If you have access to a whiteboard, record your decision criteria so you can refer back later.\nExample: Continuing our example question \u0026ldquo;You\u0026rsquo;re a PM on Facebook Watch. Your team is working on a redesign. This redesign improved watch time, but it also caused a drop in likes and comments. Should you ship this?\u0026rdquo; There are many contexts you might explore. For instance:\n\u0026ldquo;Meta\u0026rsquo;s mission is to give people the power to build community and bring the world closer together, and video is a rich medium for communicating.\nVideo allows for better monetization through ads, both as a platform for display and as a way to keep users engaged.\nStrategically, Meta has invested in video content across several platforms and is in close competition with other platforms that are video-forward, like TikTok and YouTube.\nIn particular, short-form video is a fast-growing and important sector in video content. Meta released Instagram Reels to compete in the space, and Reels also surface on Facebook.\nGiven all this, it\u0026rsquo;s critical that Facebook is seen as a go-to destination for video, both for sharing and watching. As the market for video changes, Facebook has an opportunity to become a leader in the space.\nThis will be the basis of my decision criteria moving forward. I will evaluate my options based on how well either shipping or not shipping the redesign supports the goal of making Facebook a go-to destination for video, particularly short-form video.\u0026rdquo;\nStep 3: Generate and Weigh Options\nNow it\u0026rsquo;s time to come up with options. If you haven\u0026rsquo;t already been given options to choose from, brainstorm a few solutions. Variation is good; coming up with three entirely different solutions is better than coming up with five variations on the same idea.\nTip: Keep your mind open, and try not to evaluate options against your criteria just yet. If you do, you\u0026rsquo;re prone to making a decision right away and sandbagging the other option(s).\nBriefly discuss:\nThe potential effects of each option, both positive and negative Any important dependencies that might affect the overall outcome The tradeoffs associated with each Be sure to make the pros and cons of each option very explicit. This will ensure that your decision rests on logic rather than gut instinct. Only after you\u0026rsquo;ve fully fleshed out options, dependencies, and trade-offs should you begin to weigh your options against your decision criteria.\nExample: For our Facebook Watch example question, your options are already clear. You\u0026rsquo;ve been asked whether you should or should not ship the redesign — no brainstorming is required. Instead, weigh your options:\n\u0026ldquo;Shipping the redesign improves watch time. This means users are watching more and/or longer videos, learning from them, or otherwise enjoying them. Increased watch time also increases ad revenue. Creators will be happy that they\u0026rsquo;ll get more views and revenue. However, a reduction in likes and comments might reduce the sense of community around video — a key component of Meta\u0026rsquo;s company mission. For short-form content, keeping users engaged with the videos and building a behavioral loop around watching is particularly important.\nOn the other hand, not shipping leads to more comments and likes. Comments and likes are how users communicate about video content; their voice. Viewers and creators might form stronger bonds because of that two-way communication. However, many creators won\u0026rsquo;t care about comments more than views and watch time.\u0026rdquo;\nStep 4: Decide and Recap\nFinally, decide which option offers the most benefits and fewest risks based on your decision criteria. If you followed the previous steps, your decision should be fairly to easy to justify.\nQuickly recap the logic behind your decision criteria and your choice given your options. Tradeoffs will have been covered in the previous step, so your evaluation of your answer will focus on what to do next. Consider mitigation strategies or compromises you might make, and offer ideas for solving potential issues that might come up.\nExample: As we close our example answer, recall that you\u0026rsquo;ve come to the following conclusions:\nIt\u0026rsquo;s critical that Facebook is seen as a go-to destination for video, both for sharing and watching Shipping the redesign improves total watch time Not shipping leads to more comments and likes Close with something like:\n\u0026ldquo;Evaluating against my decision criteria, which is that it\u0026rsquo;s critical that Facebook is a go-to destination for video, I\u0026rsquo;ll choose to ship. While comments and likes are valuable, watching video is the core behavior that viewers come to do and it supports creators more directly. A decrease in likes and comments might degrade the sense of community initially, but many creators won\u0026rsquo;t care about likes and comments more than watch time because watch time affects their revenue. If we\u0026rsquo;re not at risk of losing creators with the redesign, I\u0026rsquo;m confident we\u0026rsquo;ll continue to draw viewers, particularly in short-form content.\nGiven more time, I would like to investigate why the redesign is reducing likes and comments, as both are important for cultivating the sense of community Meta strives for. There may be an easy fix for bringing likes and comments back up without scrapping the redesign. For example, if the redesign made the video player bigger and likes and comments got cut off on the page, I\u0026rsquo;d consider proposing a new feature where likes and comments shift to the top of the screen.\u0026rdquo;\n\u0026ldquo;Okay\u0026rdquo; vs. \u0026ldquo;Good\u0026rdquo; vs. \u0026ldquo;Great\u0026rdquo; Decision-Making Answers\nAn okay answer explains the trade-offs between options. The candidate does a reasonable job evaluating these trade-offs and makes a coherent argument for their choice, drawing on product or strategic insights.\nA good answer places the situation within a larger product context. This context informs the criteria used to evaluate the situation. The candidate paints a picture of how the trade-offs affect the product as a whole and their decision is well-supported.\nA great answer demonstrates a deep understanding of the product or strategy, allowing the candidate to identify the key factors at play. The trade-offs and their impact on the product are deeply thought through and go beyond obvious, immediate effects. Additionally, the candidate might even suggest how to execute their decision, say by finding a middle-ground approach that optimizes the benefits or by devising a release strategy that minimizes risk.\nCommon Decision-Making Pitfalls\nDeciding too early: Don\u0026rsquo;t make a snap decision when you hear the options. Keep your mind open throughout, and you may be surprised to find another option that makes more sense. Choosing too early often leads to downplaying your evaluation of the other options to try to \u0026ldquo;convince\u0026rdquo; your interviewer. They\u0026rsquo;ll wonder why you missed potential benefits.\nNot setting decision criteria early: These questions are about trade-offs, so you have to be able to evaluate what trade-offs make sense. If you don\u0026rsquo;t have decision criteria, you\u0026rsquo;ll end up making a decision after the fact, leaving you with a weaker argument.\n4. Analytical Questions Analytical questions focus on your ability to collect and analyze relevant data and make decisions accordingly. Developing a logical process for choosing metrics and designing experiments is key, as is the ability to justify your decision-making throughout.\nThe major subcategories of analytical questions deal with metrics and experimentation, especially A/B testing. There is often overlap with estimation and execution questions which also assess your ability to break down problems and take action in ambiguous situations.\nTo answer these:\nAim to establish your ability to use data and metrics to inform product decisions and drive development Explain what different metrics can tell you about product performance, and where they fall short Explain what experiments you\u0026rsquo;d run to gain insight into open questions, and how the results would validate your assumptions Discuss how you identify and track key performance indicators (KPIs) to guide product development Examples of analytical questions:\n\u0026ldquo;You\u0026rsquo;re the PM of YouTube\u0026rsquo;s Analytics. What would you pick as the three key metrics, and why?\u0026rdquo; \u0026ldquo;How would you determine success for Instagram Reels?\u0026rdquo; \u0026ldquo;Devise an A/B test to improve user frustration with Google Maps\u0026rdquo; How to Answer A/B Testing Questions A/B tests are one of the core tools a product manager can employ for understanding user behavior. In fact, at many large tech companies, product managers are heavily involved with experimentation as a means to validate their product decisions.\nThis is why A/B testing-related questions are often asked in a product management interview. For example:\n\u0026ldquo;What experiments would you run on Google\u0026rsquo;s homepage to increase search queries?\u0026rdquo; \u0026ldquo;What are the top 3 types of A/B Experiments you would run on Facebook ads to increase revenue?\u0026rdquo; An excellent experimentation interview answer will cover these five critical basics:\n1. Hypothesis First, for any A/B experiment you propose, tell your interviewer what your hypothesis is. What are you actually even testing here? For example, perhaps you believe that by increasing the size of a button, it will increase the clickthrough rate (CTR).\n2. Methodology Now that we have a hypothesis, what exactly are we going to engineer differently to test this hypothesis? For instance, let\u0026rsquo;s run two cohorts of users. In the first cohort, the users will be our control, and will see exactly the same experience as present. In the second cohort, the users will see a button that is increased in area by 50%.\nIt\u0026rsquo;s important to be precise here. The interviewer needs to understand what exactly is being proposed, and what the experimental setup will look like. A big component of precision in methodology is defining who exactly the experiment is being run on. Are we targeting all users on the platform? Or should we pick a proper segment of users for whom we feel this test will be particularly well suited.\nDon\u0026rsquo;t forget to add a control to your experiments — without a control, it\u0026rsquo;s impossible to actually gain useful insights from the data.\n3. Metrics Great, now we have an experimental setup. Next, you need to tell your interviewer what metrics you\u0026rsquo;ll be actually concerned with. What metrics will actually convey useful insights to your product and engineering teams?\nIn the button example, you\u0026rsquo;ll obviously want to be tracking CTR, but there are a few other metrics that might be relevant and are worth listing:\nImpression count CTR on other buttons on the page Button hover time Time spent on page Bounce rate on the button\u0026rsquo;s clickthrough link (assuming the button leads to a new webpage) To answer these questions, you\u0026rsquo;ll need to understand the goals of the experiment and anticipate potential pitfalls from launching the proposed redesign.\n4. Impact At the end of a day, running an experiment just tells us information. Tell your interviewer how this information will actually be useful. What metrics will you use to make an informed decision about whether or not to launch the proposed feature?\nPerhaps you want to ensure that CTR increases with the redesign. Or perhaps the increased clicks on the button shouldn\u0026rsquo;t decrease the overall clicks of buttons on the page.\nUltimately, the answer to this question depends on the specific feature and the goal of the redesign. Here is a great opportunity to relate your answer to the overall vision and goals of the company.\n5. Tradeoffs Lastly, every proposed redesign or feature has some sort of tradeoff. What are some potential pitfalls to launching your proposed feature that might not be evident via a purely data-based analysis?\nFor example, perhaps, by making the button larger, you\u0026rsquo;ll increase the CTR on that button in question at the expense of other elements on the page. Some other great examples of points to make in typical A/B interview questions are qualities like \u0026ldquo;meaningful interactions\u0026rdquo; and user delight. These are not easily captured via metrics, and therefore are often missed in an overly quantitative mindset.\nHow to Answer Metrics Questions Expect to face metrics questions that assess your ability to set feature/product/company goals, brainstorm and select relevant metrics, and apply product and strategic reasoning. Common questions include:\n\u0026ldquo;Define a north star metric for Airbnb\u0026rdquo; \u0026ldquo;What metrics would you measure as a PM launching a new feature on WhatsApp?\u0026rdquo; \u0026ldquo;How do you define success metrics?\u0026rdquo; Common follow-up questions include:\n\u0026ldquo;What additional metrics besides your north star would be helpful to track?\u0026rdquo; \u0026ldquo;Are there any blind spots or challenges associated with your chosen metrics?\u0026rdquo; \u0026ldquo;What strategies can you think of for driving your metrics?\u0026rdquo; The GAME Framework for Metrics Questions We recommend the GAME framework as an effective method for moving through metrics questions:\nStep 1: Clarify Step 2: Goals Step 3: Actions Step 4: Metrics Step 5: Evaluate\nLet\u0026rsquo;s work through an example: \u0026ldquo;How would you measure the success of Instagram\u0026rsquo;s Discover feed?\u0026rdquo;\nStep 1: Clarify Clarify any ambiguity in the question and state your assumptions first. This way, you\u0026rsquo;ll ensure you\u0026rsquo;re on track as you begin to build out your answer.\nExample clarifying questions:\n\u0026ldquo;Should I be considering Meta\u0026rsquo;s goals as the parent company of Instagram?\u0026rdquo; \u0026ldquo;Should I include the search bar in the Discover feed in this analysis?\u0026rdquo; Let\u0026rsquo;s assume the interviewer says that you should consider Meta\u0026rsquo;s goals and that the search bar is out-of-scope for now.\nStep 2: Goals Next, begin defining the main goals for the company, product, or feature. You\u0026rsquo;ll refer back to this goals analysis throughout your answer. Consider:\nWhat is the product vision? What is this product\u0026rsquo;s point of view? How does this support the company mission? Example: \u0026ldquo;I am assuming that Meta\u0026rsquo;s mission is to give people the power to build community. The Discover feed is an interest-based feed that shows users content they wouldn\u0026rsquo;t find otherwise, which supports Meta\u0026rsquo;s mission by exposing users to new parts of the world.\nFor now, I\u0026rsquo;ll claim that the goal of the Discover feed is \u0026lsquo;Help users develop and expand their interests.\u0026rsquo;\u0026rdquo;\nStep 3: Actions Once you\u0026rsquo;ve settled on a clear goal, it\u0026rsquo;s time to build a list of actions you want to see users take in support of that goal. A common technique for building this list is to run through a user journey map.\nExample user journey for the Discover feed:\nUser opens Instagram and is shown the Home feed User clicks on the Discover tab and sees the grid of posts and Reels User scrolls through the content User clicks on a post or Reel, which expands it User may like, comment, visit the poster\u0026rsquo;s profile, etc. User may return to the feed or exit the app Key actions that indicate users are developing and expanding their interests:\nOpen the Discover feed regularly Open the posts we show them Like, comment on, or share content we show them there Follow creators they find there Scroll for more content Search for related topics Step 4: Metrics Once you\u0026rsquo;ve identified key actions users should take, it\u0026rsquo;s time to build out a list of relevant metrics to track these key actions. Consider these types of metrics:\nProduct metrics: Daily Active Users (DAU), Click-Through Rate (CTR), Time Spent Objectives and key results (OKRs): Acquired users in specific markets, DAU growth for a product, Churn reduction Business metrics: Revenue, Net profit margin, Customer lifetime value Quality metrics: Bug backlog size, Mean time to resolution (MTTR), Page load time Leading indicators: User signups, Button clicks Counter (guardrail) metrics: Churn rate, Customer satisfaction scores\nExample metrics brainstorm: \u0026ldquo;In order for users to develop and expand their interests through the Discover feed, they need to open the feed regularly and interact with the content in some way. I believe opening posts users have been shown on the Discover feed is critical.\nThere are multiple ways to measure how users are opening posts from the Discover feed:\nTotal number of posts opened by user Time spent looking at content by user per session Scroll depth per session per user Percentage of Discover sessions with a post click Each option tells me something slightly different. Returning to my stated goal - if I want to know that users are developing their interests, I would choose time spent by user per session as my metric, as it also captures depth of interest.\u0026rdquo;\nStep 5: Evaluate All metrics are strong in some areas and weak in others. Describe the tradeoffs of different metrics you considered. Ask yourself \u0026ldquo;Where does my metric fall short?\u0026rdquo;\nExample evaluation: \u0026ldquo;I chose time spent as my key metric, but I can see a case where users spend a lot of time on the Discover feed because they\u0026rsquo;re bored and are looking for something interesting but they haven\u0026rsquo;t found it yet.\nIdeally, I would like to couple time spent with a counter metric that measures a user\u0026rsquo;s engagement in the content such as likes or creators followed from the Discover feed.\u0026rdquo;\nCommon Metrics Question Pitfalls Answer the question asked. If they asked for one north star metric, don\u0026rsquo;t end up with three options due to indecision. Make sure your answer is an actual metric. Don\u0026rsquo;t use \u0026ldquo;engagement\u0026rdquo; as your north star metric. Engagement is not a metric; it\u0026rsquo;s a class of metrics. Specify what you actually want to measure (e.g., DAU, time spent, purchases). Clarify how your metric is measured. If your north star is DAU, what makes a user \u0026ldquo;active\u0026rdquo;? If the metric is time spent, is that per session? Per user per month? Ensure your metric gives you useful information. A small feature\u0026rsquo;s effect on the whole product\u0026rsquo;s DAU will likely be drowned out by noise, so overall DAU may not be useful for that feature. \u0026ldquo;Okay\u0026rdquo; vs. \u0026ldquo;Good\u0026rdquo; vs. \u0026ldquo;Great\u0026rdquo; Metrics Answers Okay answer: Gives a broad explanation of what the goals of the product are. Lists feasible metrics with reasonable justification for choosing one as north star.\nGood answer: Gives a clear sense of the product\u0026rsquo;s goals based on reasonable insights. Explains what actions would show goals are being met, then finds metrics to measure those actions. Understands where metrics might be misleading and suggests guardrails.\nGreat answer: Finds deep understanding of the product and why it exists. Articulates specific goals that clearly communicate what\u0026rsquo;s important. Has good sense of what they need to understand and can translate that into actions and metrics. North star metric justification feels deeply tied to the goal and larger organizational success.\n5. Estimation Questions Estimation questions are some of the most unusual questions you\u0026rsquo;re likely to face. For example, an interviewer could ask you to estimate how many windows are in New York City. Not all companies ask estimation questions, but it\u0026rsquo;s still a useful skill to have in your pocket.\nThe good news is that hiring managers aren\u0026rsquo;t looking for perfect answers to these questions. What interests them is your approach to problem-solving.\nExamples of estimation questions:\n\u0026ldquo;Estimate the number of Uber drivers needed in the San Francisco Bay Area\u0026rdquo; \u0026ldquo;How much does the Empire State Building weigh?\u0026rdquo; \u0026ldquo;How much does the Google Play store make in a year?\u0026rdquo; Framework:\nBreak down the problem - What are the key components? State assumptions - Be explicit about your reasoning Use benchmarks - Upper/lower bounds, personal anecdotes for educated estimates Sanity check - Does your answer make sense? Quick Reference for Estimation Questions For PM estimation questions, it\u0026rsquo;s useful to develop intuition about numbers and memorize a few key ones. This fact sheet is NOT meant to be memorized completely. Use this guide to get a general sense of numbers and sizes. Figures are updated as of Q2 2025.\nDemographics \u0026amp; Population\nUS population: 341M California population: 40M NYC population: 8.5M World population: 8.2B US households: 130M Average people per household in the US: 3 Life expectancy: 72 Median household income: $81,000 Geographic \u0026amp; Physical\nSize of continental US: 3M square miles Weight of an average car: 4,000 lbs Technical\nAmazon S3 Standard cost: $0.023 / GB / month Average file size for a 90-min 720p movie: roughly 1.5GB Average file size for smartphone camera picture: roughly 3-9 MB Average CTR for Google search ad: 5% Average landing page conversion rate: 4% Average WiFi bandwidth: 280 Mbps Cost of iPhone 17 Pro: $1,099 Cost of Google Pixel 9: $799 Company Revenues (2024)\nAmazon: $867B Apple: $538B Google: $350B Meta: $164B Netflix: $53B Airbnb: $15B Dropbox: $2.5B Google (Alphabet) net income: $100B Apple R\u0026amp;D expenditure: $31B User Populations (2025)\nNetflix subscribers: 300M Google G Suite monthly active users: 2.5B Uber drivers worldwide: ~8.8M Twitter daily active users: 250M Americans that own a smart speaker: 35% of adults Amazon products (excluding marketplace): 12M+ (total with marketplace: 400M) 6. Technical Questions Technical interview questions are no longer common in PM interviews. Still, they may pop up, depending on the company. Your recruiter will tell you whether you can expect one, so if you\u0026rsquo;re unsure, ask your recruiter.\nThese questions focus on:\nThe software development process Any specific technologies you\u0026rsquo;ll work with at your target company The technical aspects of product development, including technical communication skills Ultimately, your interviewer will evaluate your ability to communicate around and with engineering concepts. You don\u0026rsquo;t have to \u0026ldquo;become an engineer\u0026rdquo; for technical PM questions — it\u0026rsquo;s important to maintain your focus on the user experience.\nExamples of technical questions:\n\u0026ldquo;What happens when you type a URL in the browser?\u0026rdquo; \u0026ldquo;How do autonomous vehicles work?\u0026rdquo; 7. Behavioral Questions Framework: STAR Method (Situation, Task, Action, Result)\nCommon questions and what they\u0026rsquo;re really testing:\n\u0026ldquo;Tell me about a time you had to influence someone without authority\u0026rdquo;\nTesting: Leadership potential, communication skills Good answer: Focus on understanding their perspective first, finding mutual benefits, and persistent but respectful follow-up \u0026ldquo;Describe a project where you had to work with ambiguous requirements\u0026rdquo;\nTesting: Comfort with uncertainty, problem-solving approach Good answer: Show how you clarified requirements through questions, research, and iteration \u0026ldquo;Give an example of when you had to prioritize competing demands\u0026rdquo;\nTesting: Decision-making framework, trade-off analysis Good answer: Demonstrate clear criteria for prioritization (impact, effort, alignment with goals) Preparation tip: Have 5-7 stories ready that can be adapted to different behavioral questions. Include examples from work, school projects, side projects, and leadership roles.\nCommon Interview Questions \u0026ldquo;What\u0026rsquo;s your favorite product?\u0026rdquo; The favorite product question is one of the most common you\u0026rsquo;ll face during PM interviews. Hiring managers aim to understand your thought process, product thinking, and your values.\nWhen answering this question, keep these \u0026ldquo;Three P\u0026rsquo;s\u0026rdquo; in mind:\nPassion: Choose a product you\u0026rsquo;re passionate about and actually use Perspective: Share a nuanced perspective on the product Personality: Help reveal aspects of your personality through your choice First explain why you chose your favorite product. What features or aspects did you find innovative or effective? How does it solve a specific problem, need, or pain point? Show product knowledge by explaining its position in its market and what makes it stand out from competitors.\n\u0026ldquo;Why do you want to work at this company?\u0026rdquo; The hiring processes at many tech companies today are very competitive. With hundreds of qualified candidates, it\u0026rsquo;s important to differentiate yourself by showing genuine interest.\nBe sure to communicate that you\u0026rsquo;ve done your research. Explain how you align with company values and goals and how you can add value. Explain what about the company interests you - their culture, mission, values - and how your skills and experience align with their current needs.\n\u0026ldquo;Tell me about yourself\u0026rdquo; This is a standard opening question in most interviews to break the ice. Summarize your background, skills, and qualifications concisely and relevant to the position. Highlight your most important experiences and emphasize strengths that align with job requirements. This question also allows you to showcase your personality and motivation for pursuing product management.\nInterview Technique Framework Regardless of the question type, follow this systematic approach to deliver strong, structured answers:\n1. Listen \u0026amp; Take Notes This may seem obvious, but this is the most important part of the interview. Don\u0026rsquo;t just listen, actively listen. Jot down notes as you hear the question for keywords or important points. Make frequent eye contact with the interviewer. Show interest and excitement to tackle the question.\n2. Ask Clarifying Questions Before answering the question, ask a question! No matter how straightforward the question seems, ask clarifying questions to understand the important details and your interviewer\u0026rsquo;s expectations.\nSample clarifying questions:\n\u0026ldquo;Is this product scoped to a particular set of users?\u0026rdquo; \u0026ldquo;What platforms are our users on?\u0026rdquo; \u0026ldquo;Are we launching internationally or domestically?\u0026rdquo; \u0026ldquo;Ok, you\u0026rsquo;re asking me to _____. Is that correct?\u0026rdquo; Fundamentally, this step ensures you and your interviewer are on the same page.\n3. Pause \u0026amp; Think I know it\u0026rsquo;s tempting to jump into an answer, and to be honest, most PM interview candidates are great talkers.\nBut stop. Don\u0026rsquo;t speak yet.\nAs an interview coach, I notice huge increases in quality of product management interview answers when the interviewee pauses for even just 10–15 seconds to consider their approach. It may feel like your interviewer expects an immediate answer, but trust me — they\u0026rsquo;ll appreciate it much more when you have a coherent and cogent response.\n4. Structure Your Answer After thinking, you\u0026rsquo;ll need to deliver a structured answer to the question. Walk through your structure before answering.\nPro tip: This is a great opportunity to write the structure on the whiteboard as you speak.\nThe simplest structure is a three-point answer. Your intro sentence could be: \u0026ldquo;Ok, I\u0026rsquo;m going to cover three potential products that answer your question, and explain the tradeoffs of each. The three products are _____, _____, and _____.\u0026rdquo;\nNow, your interviewer has a sense of your answer\u0026rsquo;s structure, and can redirect you as needed.\n5. Explain Here\u0026rsquo;s the meat of the interview question. Dive into the details of your answer and explain your thought process. Depending on the interview question, you\u0026rsquo;ll want to structure and present your answer differently.\nGenerally, make sure to sit tall, confidently explain your answer, and make eye contact. Use the whiteboard liberally — it\u0026rsquo;s a great visual aid for your answer.\n6. Pivot and Check-in You may be mid-way through your answer when something goes wrong. Here are some possible pitfalls and how to pivot:\nYour interviewer gives concerned body language: Based on your interviewer\u0026rsquo;s posture or gesturing, it seems like perhaps you\u0026rsquo;re not on the right track. Pause where appropriate and check in. \u0026ldquo;Ok, I\u0026rsquo;ve just answered the first part of my question. I\u0026rsquo;ll now move on to the next segment of my answer. Is that alright?\u0026rdquo;\nYou realize your answer is wrong: This happens frequently. Midway through talking, you realize your answer is flawed in a fundamental way. Don\u0026rsquo;t fret — this is really common during PM interviews! Depending on your personal style, figure out a smooth way to pivot mid-answer. For instance, \u0026ldquo;As I discuss this point, I realize that there are several flaws with my answer. I\u0026rsquo;d actually like to shift my answer a bit toward….\u0026rdquo;\nYou forget your point: Ideally, this wouldn\u0026rsquo;t happen because you already defined a clear structure. However, if you blank out, don\u0026rsquo;t worry. Nine times out of ten, you just need a bit more time to think about the answer, so ask for it. \u0026ldquo;Ok, I\u0026rsquo;ll need a bit more time to think through the second part of my question. Please give me a few moments.\u0026rdquo;\nClassic mistake: Interviewees set up an awesome structure, but then somehow go completely off track and the interviewer is lost. Stick to your structure, or communicate your changes clearly if a change is necessary.\n7. Summarize Your Answer You\u0026rsquo;re almost there! But there\u0026rsquo;s one critical step left. In no longer than 30 seconds, reiterate what you\u0026rsquo;ve told your interviewer, using the same structure that you defined in step #4. \u0026ldquo;In summary, we\u0026rsquo;ve discussed ____, ____, and ____.\u0026rdquo;\nPreparation tip: Have 5-7 stories ready that can be adapted to different behavioral questions. Include examples from work, school projects, side projects, and leadership roles.\n2. Product Sense Questions These test your ability to think like a user and identify product opportunities.\n\u0026ldquo;How would you improve [existing product]?\u0026rdquo;\nFramework:\nClarify the scope - What specific aspect? Which user segment? Identify the user and their goals - Who uses this and why? Analyze current pain points - Where does the experience break down? Propose solutions - Prioritize by impact and feasibility Define success metrics - How would you measure improvement? \u0026ldquo;Design a product for [target user group]\u0026rdquo;\nFramework:\nUnderstand the user - Demographics, behaviors, needs, pain points Define the problem - What job are they hiring your product to do? Ideate solutions - Multiple approaches, then converge Prioritize features - MVP vs. future iterations Success metrics - Both quantitative and qualitative Example walkthrough: \u0026ldquo;Design a fitness app for busy professionals\u0026rdquo;\nUser research: 25-40 year olds, work 50+ hours/week, want to stay healthy but struggle with time/consistency Core problem: Limited time and unpredictable schedules make traditional fitness routines difficult Solution: Micro-workout app with 5-15 minute sessions, adaptive to schedule/location Key features: Quick workout library, calendar integration, progress tracking, social accountability Metrics: Weekly active users, session completion rate, user retention 3. Analytical/Technical Questions Estimation questions:\n\u0026ldquo;How many smartphones are sold globally each year?\u0026rdquo; \u0026ldquo;Estimate the market size for food delivery apps\u0026rdquo; Framework:\nBreak down the problem - What are the key components? State assumptions - Be explicit about your reasoning Sanity check - Does your answer make sense? Data interpretation:\n\u0026ldquo;Daily active users dropped 15% last month. How would you investigate?\u0026rdquo; Framework:\nClarify the metric - How is DAU defined? What\u0026rsquo;s the historical trend? Hypothesize causes - External factors, product changes, technical issues, seasonality Prioritize investigation - What would you check first and why? Define next steps - How would you test your hypotheses? 4. Strategic Thinking \u0026ldquo;Should [Company X] enter [Market Y]?\u0026rdquo;\nFramework:\nMarket analysis - Size, growth, competitive landscape Company fit - Core competencies, strategic alignment, resources Barriers and risks - What could go wrong? How significant? Success requirements - What would need to be true for this to work? Recommendation - Clear yes/no with reasoning Interview Evaluation Rubrics Understanding how interviewers evaluate your performance can help you focus your preparation. Here are the key rubrics used across top tech companies:\nEstimation Rubric Criteria Very Weak Weak Neutral Strong Very Strong Problem Solving Failed to ask questions; rushed into an answer Serious errors or significant guidance needed in scoping the problem Scoped the problem, but missed key element(s) Asked good questions and used answers effectively to scope a solvable problem Asked insightful questions; set up an easy calculation and an accurate answer Critical Thinking Failed to calculate on the fly or show competence with numerical relationships Attempts to quantify answer were sloppy or needed lots of guidance Arrived at a fair answer with a nudge or two Made accurate and quick calculations; gave helpful and appropriate approximations Above-average \u0026ldquo;ease\u0026rdquo; in using numbers to an advantage; creative use of approximation, \u0026ldquo;rules of thumb\u0026rdquo;, etc. Possible Errors Failed to discuss possible errors Failed to reasonably justify decisions when pressed and/or made bad judgment calls Correctly identified some possible mistakes, but missed others Correctly identified possible errors and suggested ways to increase accuracy with more information Covered possible errors thoroughly, offered alternatives, neatly summarized pros and cons Communication Failed to communicate clearly despite repeated prompts Poor communication throughout; interviewer had trouble following despite prompts Communication varied—clear in some areas, but vague/incomplete in others Good communication skills; articulated thought process clearly and consistently Clear communication; anticipated questions, articulated reasons for decision, and checked in throughout Collaboration Failed to take the lead; didn\u0026rsquo;t respond to guidance Struggled to stay on track without guidance Took the lead and performed well, but needed redirects or hints Effectively led the discussion and involved the interviewer throughout Took the lead and made exceptional use of the interviewer; discussion was more collaboration than interview Creativity Failed to show enthusiasm or creative thinking Solutions were bland; didn\u0026rsquo;t show interest in the problem Showed reasonable insight, but nothing exceptional Demonstrated logical and creative thought processes Displayed exceptional creativity and outside-the-box thinking; maintained logic and found novel solutions Analytical Rubric Criteria Very Weak Weak Neutral Strong Very Strong Data Literacy Failed to use data to answer the question Failed to identify obvious patterns in data Reasonable ability to reason with data, but missed key points Used data effectively; good questions, solid assumptions, logical conclusions Valuable data insights and logical arguments; identified key patterns and reports to run Comfort with Metrics Failed to show a basic understanding of relevant metrics Struggled to define metrics and/or compare different metrics Reasonable knowledge, but missed \u0026ldquo;best fit\u0026rdquo; metrics for a given problem Clearly discussed pros and cons of various metrics; made a solid argument for choice(s) Thoroughly discussed metrics; proposed ways to streamline analytical processes Diagnosis Failed to probe the origin of the problem; rushed into an answer Attempted to diagnose, but made serious errors or needed significant guidance Fair diagnosis, but missed nuances Asked good questions and used answers effectively to scope a solvable problem Asked insightful question; set up a clear, easy-to-use test plan Prioritization Failed to prioritize efforts when solving a problem Prioritization was sloppy or had obvious errors Fair prioritization after given a nudge or two Prioritized effectively; accounted for urgency, cost/benefit, testability, etc. Prioritized admirably; proposed plan provided maximum benefits with minimal cost Execution Failed to define a clear plan to solve the problem Struggled to communicate action plan in an executable way Well-defined plan but missed key points, or good intuition but plan wasn\u0026rsquo;t fully executable Correctly identified key elements of the problem; provided KPIs and logical solutions Comprehensive plan; best-fit KPIs, bottlenecks addressed, other solutions given Collaboration Failed to take the lead; didn\u0026rsquo;t respond to guidance Struggled to stay on track without guidance Took the lead and performed well, but many have needed redirects or hints Effectively led the discussion and involved the interviewer throughout Took the lead and made exceptional use of the interviewer; discussion was collaborative Curiosity Failed to show interest and/or ask good questions Attempted to collect information, but failed to intuit the right areas to dig into Asked good questions and/or had good ideas, but missed key points Asked great questions, and worked to understand the entire problem context Gathered information effectively; asked insightful questions that got to root of issue Execution Rubric Criteria Very Weak Weak Neutral Strong Very Strong Variable Isolation Failed to isolate variables when problem-solving Variable isolation was sloppy or had obvious errors Isolated variables with reasonable skill; a nudge or two Isolated variables effectively; accounted for likelihood, urgency, cost/benefit, testability, etc. Isolated variables admirably; resulting action plan was elegant and robust Data Literacy Failed to use data to answer the question Failed to identify obvious patterns in data Reasonable ability to reason with data, but missed key points Problem was effectively broken down and worked through in a logical way Extracted valuable data insights; identified key patterns, suggested reports/tests, made logical arguments Logical Thinking Failed to probe the origin of the problem; rushed into an answer Displayed some logical thought processes but made serious errors or needed significant guidance Thought processes were somewhat logical despite a few errors Strong sense of likely problem sources; able to test and confirm this intuition effectively Flawless logic; no mistakes or oversights; clearly articulated logical thinking Problem Solving Failed to consider context; showed no intuition for where problems are likely to originate Intuition was incorrect and/or not justifiable when pressed Reasonable knowledge and intuition for where problems are likely to originate Good communication skills; articulated thought process clearly and consistently Deep intuition for how the \u0026ldquo;system\u0026rdquo; works; plan and solution reflected this Communication Failed to communicate clearly despite repeated prompts Poor communication throughout; interviewer had trouble following despite prompts Communication varied—clear in some areas, but vague/incomplete in others Effectively led the discussion, involved the interviewer throughout Clear communication; anticipated questions, articulated reasons for decision, and checked in throughout Collaboration Failed to take the lead; didn\u0026rsquo;t respond to guidance Struggled to stay on track without guidance Took the lead and performed well, but needed redirects or hints Communicated requirements/requests with technical accuracy; could partake in \u0026ldquo;live\u0026rdquo; technical discussion Took the lead and made exceptional use of the interviewer; discussion was more collaboration than interview Product Strategy Rubric Criteria Very Weak Weak Neutral Strong Very Strong Competitive Analysis Failed to consider competition Poor understanding of competitive landscape Fair understanding of key competitors and trends Extensive insight into relevant competitors and their positioning Deeply aware of landscape; discussed competitive strategies logically and creatively Market Analysis Failed to consider the market Poor understanding of the market Fair understanding of key market forces and competitive dynamics Significant knowledge of relevant market dynamics Deep market insight; discussed opportunities logically and creatively Business Strategy Failed to consider business goals and basic strategy Strategy unclear or full of errors Basic business sense; gave a fair strategy with some substance Delivered a logical strategy that fit greater business context Strategic and nuanced understanding of landscape; compelling arguments Roadmapping Failed to articulate a product roadmap Roadmap unclear or full of errors Built a product roadmap with some guidance Roadmap fits goals and landscape; logical assumptions and alternatives Roadmap clear, compelling, and adaptive to change Clarifying Questions Failed to ask questions and/or interact with the interviewer Struggled to ask good questions and/or made faulty assumptions Asked good questions but missed key points Asked insightful questions and adapted strategy to fit Scoped problem well; gave high-quality, creative strategy Tradeoffs/Errors Failed to mention tradeoffs or possible errors Mentioned but didn\u0026rsquo;t justify tradeoffs; made faulty judgment calls Covered possible errors and tradeoffs; could have made better choices Discussed tradeoffs logically and identified possible errors Deep knowledge of tradeoffs; alternatives and pros/cons offered Passion/Creativity Failed to show enthusiasm or creative thinking Solutions were bland and/or didn\u0026rsquo;t show interest Some interest and reasonable insight, but nothing exceptional Showed impressive knowledge, enthusiasm, and creativity Gave inspired answers; showed clear passion Communication Failed to communicate clearly despite repeated prompts Poor communication; trouble following despite prompts Communication varied—clear in some areas, but vague in others Good communication; explained logic clearly and consistently Clear communication; anticipated questions, justified decision, checked in often Collaboration Failed to take the lead; didn\u0026rsquo;t respond to guidance Struggled to stay on track without guidance Led/performed well, but needed redirection/hints Effectively led the discussion; involved the interviewer throughout Took the lead and made exceptional use of the interviewer; discussion was collaborative Product Design Rubric Criteria Very Weak Weak Neutral Strong Very Strong Business Acumen Failed to show an understanding of the context of the business Struggled to tie back to business goals or company mission Discussion around business goals was unclear or flawed Clearly discussed business goals, positioning, and industry trends Nuanced understanding of the landscape, insightful arguments, logical assumptions User-Centricity Failed to consider the end user Struggled to anchor answer on end users despite guidance Attempted user-centric design, but missed key points Discussed pain points and opportunities; prioritized appropriately Analyzed users accurately, prioritized effectively, and recalled users throughout Product Vision Failed to discuss the future of the product Struggled to articulate a vision for the future Laid out a possible future with some minor errors Showed thoughtfulness and intuition when articulating the product vision Exemplary product intuition; gave data-backed arguments tied to UX Clarifying Questions Failed to ask questions and/or interact with the interviewer Struggled to ask the right questions and/or made assumptions without clarifying Asked good clarifying questions, but missed key points Asked insightful questions and adapted design to fit Asked surprising and insightful questions; came up with high-quality, novel design(s) Tradeoffs and Errors Failed to mention tradeoffs and possible errors Mentioned but didn\u0026rsquo;t justify tradeoffs when pressed and/or made faulty judgment calls Covered possible errors and tradeoffs, but could have made better choices Logical tradeoff discussion; correctly identified possible errors Deep knowledge of tradeoffs; offered alternatives; neatly summarized pros/cons Passion and Creativity Failed to show enthusiasm or creative thinking Gave bland solutions and/or made incorrect judgment calls Displayed interest and reasonable insight, but nothing exceptional Showed extensive knowledge, enthusiasm, and creativity Gave inspired answers; showed clear passion Communication Failed to communicate clearly despite repeated prompts Poor communication throughout; interviewer had trouble following despite prompts Communication varied—clear in some areas, but vague/incomplete in others Good communication skills; articulated thought process clearly and consistently Clear communication; anticipated questions, justified decision, checked in often Collaboration Failed to take the lead and didn\u0026rsquo;t respond to guidance Struggled to stay on track without guidance Took the lead and performed well, but needed redirects or hints Effectively led the discussion; involved the interviewer throughout Took the lead and made exceptional use of the interviewer; discussion was collaborative Technical Rubric Criteria Very Weak Weak Neutral Strong Very Strong Data Literacy Failed to use data to answer the question Failed to identify obvious patterns in data Showed a reasonable ability to reason with data, but missed key points Used data effectively; asked good questions, made solid assumptions, formed logical conclusions Extracted valuable data insights; identified key patterns; suggested reports/tests to run; made logical arguments Comfort with Metrics Failed to show a basic understanding of relevant metrics Struggled to define metrics for given problems and/or compare different metrics Showed reasonable knowledge, but missed the \u0026ldquo;best fit\u0026rdquo; metrics for a given problem Clearly discussed pros and cons of various metrics, and made solid argument for choice(s) Thoroughly and accurately discussed metrics; recommended ways to streamline analytical processes Clarifying Questions Failed to ask questions and/or interact with the interviewer Struggled to ask the right questions and/or made assumptions without clarifying Asked good questions but missed key points Asked insightful questions and adapted strategy to fit Scoped problem effectively; delivered high-quality, creative strategy Tradeoffs and Errors Failed to mention tradeoffs and possible errors Mentioned tradeoffs, but failed to justify decisions when pressed and/or made incorrect judgment calls Covered possible errors and tradeoffs, but could have made better choices Discussed tradeoffs logically and correctly identified possible errors Deep knowledge of tradeoffs; offered alternatives; neatly summarized pros/cons Software Implementation Failed to show basic knowledge of how software is implemented Unable to dive deeply into implementation or process Understands basic software implementation Solid understanding of technical processes underlying software products Deep, engineering-level understanding of software Technical Communication Failed to communicate with technical team members Struggled to translate \u0026ldquo;PM-speak\u0026rdquo; into technical terms; needed significant clarification Effective at translating requirements into technical terms Communicated requirements/requests with technical accuracy; could partake in \u0026ldquo;live\u0026rdquo; technical discussion Matches technical team in ability to communicate technically ","permalink":"https://chenterry.com/posts/apm-interview-preparation-guide/","summary":"\u003cp\u003eMost APM advice focuses on how to sound like a product manager. But sounding right has never built a great product. What matters is developing the ability to be directionally right about the future — and proving it with metrics.\u003c/p\u003e\n\u003cp\u003eThe best product thinkers I’ve learned from aren’t impressive because of their frameworks or polished answers. They stand out because they form falsifiable beliefs about how the world works: what users will care about, which constraints will dominate, where value will actually accumulate. Their predictions can be wrong, which means their thinking can be evaluated. That’s the only way judgment gets sharper.\u003c/p\u003e","title":"Preparing for Associate Product Manager Interviews: A Comprehensive Guide"},{"content":"Every methodology has its breaking point.\nThe problem-solving frameworks that get you into a top school, the pattern recognition that earns you a job at a big tech company, the \u0026ldquo;best practices\u0026rdquo; that make you a star student—all of these become systematic liabilities the moment you encounter genuinely ill-defined problems. Which, unfortunately for ambitious people, is exactly where the most important work happens.\nAs I explore what I want to do with my life, and reflect past decision, I realize: we\u0026rsquo;ve been trained to excel at the wrong things.\nAs much as I take pride in my ability to \u0026ldquo;ask the right questions,\u0026rdquo; identify effective methods, and \u0026ldquo;navigate ambiguity\u0026rdquo; by finding answers others overlook, I\u0026rsquo;m starting to realize how much I\u0026rsquo;ve been playing in a safe zone. Having a job lined up while wanting to pursue startups part-time in college isn\u0026rsquo;t exactly what I\u0026rsquo;d call \u0026ldquo;navigating ambiguity.\u0026rdquo; When it comes to genuinely not knowing what I\u0026rsquo;ll be doing or where I\u0026rsquo;ll end up six months from now, I panic at the idea.\nThis reveals something uncomfortable about how institutional training actually works. In school and corporate environments, you can often be rewarded for appearing to work hard—for having the right frameworks, asking sophisticated questions, and demonstrating systematic thinking. The performance of competence gets rewarded alongside actual competence.\nBut in startups, that entire system breaks down. People won\u0026rsquo;t listen to you or think differently of you just because you have \u0026ldquo;Founder\u0026rdquo; or \u0026ldquo;Head of Product\u0026rdquo; stamped on your LinkedIn profile or email signature. The quality of your product and depth of your customer empathy is all that matters. The institutional markers of success—the titles, the frameworks, the sophisticated analysis—become irrelevant when you\u0026rsquo;re face-to-face with users who simply don\u0026rsquo;t care about your credentials.\nPaul Graham\u0026rsquo;s insight about startups being counterintuitive reveals something deeper about this disconnect. As he notes in \u0026ldquo;Before the Startup\u0026rdquo;, there are many ski instructors but few running instructors—not because running coaching is difficult, but because running feels intuitive while skiing requires overriding your instincts. Similarly, startups are counterintuitive, which is why startup advice exists at all. But there\u0026rsquo;s a related phenomenon: institutional training teaches us to excel at activities that seem like entrepreneurship but aren\u0026rsquo;t.\nResearch on complex decision-making reveals something uncomfortable: without strong self-regulation skills, people default to limiting patterns when facing uncertainty. They avoid difficulty, fall into perfectionist paralysis, or break problems down mechanically without going deep enough.\nThe ability to step back, see problems clearly, and consciously adapt behavior is rare. It doesn\u0026rsquo;t naturally develop with age or experience—many people carry the same behavioral patterns into middle age.\nThis shows up everywhere, but it\u0026rsquo;s particularly visible in how we approach complex problems.\nThe Problem Decomposition Trap Institutions, as a product, have done a great job of creating the impression that skill is associated with completing coursework. While this might have been true years back—you\u0026rsquo;d have to do a CS degree to learn how to code—it\u0026rsquo;s becoming less relevant as an indicator. You become good at something by doing that thing, not necessarily learning how to become good at that thing. And to actually do something well, you have to develop an understanding, rather than just follow patterns.\nAt leading institutions, students are trained to find the quickest path to a solution—to identify \u0026ldquo;hacks\u0026rdquo; and implement \u0026ldquo;best practices.\u0026rdquo; This system works brilliantly for well-defined problems with known solution spaces. Get the grade, pass the test, optimize the metric. But it creates a dangerous mental model when applied to genuinely ambiguous situations.\nThe institutional approach teaches us to pattern-match rapidly: This looks like that problem I solved before, so I\u0026rsquo;ll apply that framework. In startups, this intuition becomes a liability. The most valuable insights often come from staying with uncertainty longer than feels comfortable, from digging deeper into assumptions everyone else accepts as given.\nThis pattern-matching problem gets worse when you consider how different environments reward different approaches.\nSafety Nets and Startup Reality While I have limited exposure to working in big tech, I\u0026rsquo;ve developed somewhat of hints at why at large companies, the standard for success is different. If you can clearly and persuasively articulate an idea and convince your superiors, the system backs you up. In existing markets, you\u0026rsquo;ll capture some share. In growth markets, if your approach doesn\u0026rsquo;t work, you can pivot quickly. The infrastructure absorbs your experimental failures.\nStartups operate under entirely different constraints. Competition edges in from multiple directions, resources are brutally limited, and the cold start problem is real. The only real differentiation isn\u0026rsquo;t execution efficiency—it\u0026rsquo;s unique understanding of the problem itself. Whether that\u0026rsquo;s insight into user pain points everyone else misses, or redefining how to reach customers in ways that weren\u0026rsquo;t obvious.\nThis difference reveals why methodologies optimized for big company environments often fail in startup contexts. Big companies can afford to be \u0026ldquo;approximately right\u0026rdquo; because they have scale, distribution, and capital to make imperfect solutions work. Startups need to be \u0026ldquo;precisely right\u0026rdquo; about something others are wrong about.\nWhen Uncertainty Becomes Unbearable International students experience this pattern more intensely because the stakes genuinely feel higher when navigating unfamiliar systems.\nThe human brain isn\u0026rsquo;t well-equipped to handle uncertainty. We anchor to whatever evidence seems solid. When you\u0026rsquo;re navigating an unfamiliar system (US college admissions) with limited information, hiring a college consultant provides reassurance. It\u0026rsquo;s not about whether it\u0026rsquo;s objectively helpful—it\u0026rsquo;s about having something concrete to hold onto when everything else feels uncertain.\nThis intensifies in college. Visa status, a competitive job market, policy uncertainties—these aren\u0026rsquo;t abstract concerns but existential realities. So when companies promise \u0026ldquo;insider guides\u0026rdquo; to breaking into finance (organizations that offer an impression to having these guides are able to charge high premiums), even students with genuine ambitions to start companies or do research end up signing contracts for tracks of \u0026ldquo;breaking into\u0026rdquo; Wall Street or Silicon Valley. The path feels safer because it\u0026rsquo;s more defined.\nThis reveals something deeper about how uncertainty gets commodified. When stakes feel existential, the promise of \u0026ldquo;insider knowledge\u0026rdquo; becomes irresistible—regardless of whether that knowledge actually helps. These services often reinforce the exact pattern-matching mentality that becomes counterproductive in genuinely ambiguous situations.\nThe almost counterintuitive observation is that many of these students are precisely the people who should be exploring unknown territories. They have unique perspectives, cross-cultural insights, and often genuine intellectual curiosity. But the very conditions that make ambiguity feel threatening—uncertainty about belonging, about future prospects—are what make engaging with it most valuable. It\u0026rsquo;s a cruel catch-22: those who would benefit most from embracing uncertainty are often those who can least afford to.\nBreaking Free From Institutional Patterns The solution isn\u0026rsquo;t to abandon systematic thinking entirely, but to develop meta-awareness about when these approaches help and when they hinder. This requires recognizing the fundamental difference between well-defined problems (where institutional training excels) and ill-defined problems (where it often misleads).\nEffective navigation of ambiguous situations involves capabilities that institutional training rarely develops: comfort with not knowing while still making progress, iterative planning rather than linear execution, and the ability to hold assumptions lightly while testing them thoroughly.\nMost importantly, it requires learning to recognize when you\u0026rsquo;re applying familiar frameworks to genuinely novel situations—when you\u0026rsquo;re \u0026ldquo;playing house\u0026rdquo; rather than engaging with real complexity. The very thoroughness of elite institutional training can create blind spots, making it harder to step back and question whether your cognitive tools are adequate for the problem at hand.\nWhich brings me back to my own struggle with this.\nMaybe It\u0026rsquo;s Time to Loosen Up a Little Paul Graham notes that knowledge grows fractally—from a distance its edges look smooth, but when you get close enough, you notice gaps that seem obvious. The feeling of \u0026ldquo;surely someone has figured this out already\u0026rdquo; might be wrong.\nThis hits at something I\u0026rsquo;ve been wrestling with personally. We spend so much energy learning to navigate the known world efficiently that we forget how to be genuinely curious about the unknown parts. But those gaps—the things that seem obviously missing once you get close enough to see them—that\u0026rsquo;s where the interesting work lives.\nI catch myself falling into these same patterns constantly. Defaulting to frameworks when I should be sitting with confusion. Seeking pattern matches when I should be noticing what doesn\u0026rsquo;t fit any pattern I know. It\u0026rsquo;s uncomfortable to admit how often I\u0026rsquo;ve been \u0026ldquo;playing house\u0026rdquo; with ambiguity rather than actually engaging with it.\nBut here\u0026rsquo;s what I\u0026rsquo;m starting to understand: the discomfort of not knowing isn\u0026rsquo;t a bug to be fixed—it\u0026rsquo;s a signal. When your institutional training doesn\u0026rsquo;t immediately apply, when familiar frameworks feel inadequate, when you can\u0026rsquo;t quickly decompose the problem into manageable pieces, you might be looking at something genuinely important.\nThe real skill isn\u0026rsquo;t learning to eliminate that discomfort, but learning to sit with it productively. To stay curious about gaps that seem obvious but somehow remain unexplored. To trust that the feeling of \u0026ldquo;surely someone has figured this out already\u0026rdquo; might be wrong—and that if you get close enough to the fractal edge of knowledge, you\u0026rsquo;ll find whole territories waiting to be explored.\nMaybe the goal isn\u0026rsquo;t to get better at navigating ambiguity, but to get more comfortable being genuinely confused by things that matter.\nIf you\u0026rsquo;ve managed to read till here, it\u0026rsquo;s probably worth checking out Design Technology Research (DTR). DTR is a research and learning community where students design and study technologies that support how people learn, collaborate, and create. If you go to Northwestern, I\u0026rsquo;d recommend you apply.\n","permalink":"https://chenterry.com/posts/ambiguity/","summary":"\u003cp\u003eEvery methodology has its breaking point.\u003c/p\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/dealing-with-ambiguity/ambiguity.png\" alt=\"Mountain landscape\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eThe problem-solving frameworks that get you into a top school, the pattern recognition that earns you a job at a big tech company, the \u0026ldquo;best practices\u0026rdquo; that make you a star student—all of these become systematic liabilities the moment you encounter genuinely ill-defined problems. Which, unfortunately for ambitious people, is exactly where the most important work happens.\u003c/p\u003e\n\u003cp\u003eAs I explore what I want to do with my life, and reflect past decision, I realize: we\u0026rsquo;ve been trained to excel at the wrong things.\u003c/p\u003e","title":"The Best Practices Lie: On Dealing with Ambiguity"},{"content":"Work in Progress This post is currently under development. Content to be completed soon.\nAppendix This post is a work in progress. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/posts/xiaomi-primer/","summary":"\u003ch2 id=\"work-in-progress\"\u003eWork in Progress\u003c/h2\u003e\n\u003cp\u003eThis post is currently under development.\n\u003cem\u003eContent to be completed soon.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"appendix\"\u003eAppendix\u003c/h3\u003e\n\u003cp\u003eThis post is a work in progress. For further clarification or discussion, please reach out to \u003ca href=\"mailto:terrychen2026@u.northwestern.edu\"\u003eterrychen2026@u.northwestern.edu\u003c/a\u003e.\u003c/p\u003e","title":"Understanding Xiaomi: IP Risk \u0026 Starting Again"},{"content":"Summary OpenAI\u0026rsquo;s rapid product cadence isn\u0026rsquo;t just releasing tools—it\u0026rsquo;s consolidating power. By integrating models, infrastructure, and interfaces into a single AI operating system, the company is reshaping where startups can compete and how value accrues across the AI stack.\nOpenAI isn\u0026rsquo;t just launching new products; it\u0026rsquo;s redefining where startups can safely operate. In less than three years, it has evolved from a research lab into a full-stack AI platform whose reach now spans infrastructure, models, applications, and compliance. Each release expands its gravitational field, redrawing the boundaries of opportunity for founders and investors. Understanding OpenAI\u0026rsquo;s strategic expansion is therefore not about tracking a single company; it\u0026rsquo;s about mapping the blast radius of a platform that is systematically consolidating multiple layers of the AI value chain.\nAnalysis of OpenAI\u0026rsquo;s recent product releases reveals an ambitious three-pillar strategy that goes far beyond language models. The company is positioning itself as the operating system for AI-powered work, combining a unified assistant surface with heavy multimodal research and development, all supported by hyperscale infrastructure. This isn\u0026rsquo;t just about building better models—it\u0026rsquo;s about creating an integrated platform that captures value across the entire AI stack.\nThe Operating System for Work and Life Based on their product release patterns and hiring focus, OpenAI appears to be executing a clear progression from ChatGPT as a chat interface toward a comprehensive AI operating system. This evolution involves three critical components: positioning ChatGPT as the primary interface for AI interactions, developing advanced reasoning capabilities with safety guardrails, and building massive infrastructure scale to support global deployment. Their enterprise strategy emphasizes data sovereignty and compliance, particularly in regulated industries where local data residency becomes a competitive necessity.\nTogether, these moves reveal that OpenAI is no longer competing at the level of models, but at the level of workflows. ChatGPT, Atlas, and related products form a single interface through which users think, search, and act. The company’s differentiation now lies less in model quality than in coordination—how seamlessly its products orchestrate tasks across text, voice, and visual contexts. For startups, that means the competitive frontier has shifted: value now accrues not to who builds the smartest model, but to who controls the user’s entry point into intelligent work.\nThe Architecture of Dominance OpenAI\u0026rsquo;s approach mirrors the logic of an operating system rather than an application suite. The assistant surface is the user shell; multimodal models are the compute kernel; and enterprise infrastructure provides the permissions, policies, and data flows that make the system safe and scalable. This architectural cohesion is what gives OpenAI its durability. Each new feature—Agents, Memory, or Company Knowledge—plugs into the same orchestration layer, reinforcing a feedback loop between capability and distribution that becomes increasingly difficult for smaller players to break.\nThe Strategic Timeline: From Foundation to Platform OpenAI\u0026rsquo;s roadmap reveals a methodical approach to market expansion across five distinct layers: Product, Research, Infrastructure, Enterprise, and Human. Each layer follows a deliberate progression from foundational capabilities to advanced platform features.\nIn the immediate term, OpenAI is consolidating its product offering around Atlas browser integration and search capabilities, while simultaneously advancing deliberative alignment and o-series reasoning models. The infrastructure focus remains on online storage and data movement, supporting enterprise residency requirements across Japan, India, Singapore, and South Korea. For human-centered features, Study Mode and parental controls establish OpenAI\u0026rsquo;s presence in regulated environments.\nThe next phase introduces agentic workflows and memory-aware user experiences, supported by robustness research against attacks and grounded reasoning capabilities. Infrastructure scaling continues with real-time streaming and multimodal training capabilities, while enterprise features expand to include pricing platforms and administrative controls. Well-being guardrails and expert councils for youth and health contexts demonstrate OpenAI\u0026rsquo;s commitment to responsible deployment.\nThe longer-term vision encompasses OS-level assistant integration and vertical solution playbooks, supported by generalizable agent capabilities and world-modeling research. Stargate build-outs will provide exascale orchestration capabilities, while industry-specific playbooks and partner ecosystems will address specialized market needs. Personalized pedagogy and trust benchmarks will complete the human-centered AI platform.\nCompetitive Positioning: Understanding the Battlefield If the timeline shows how OpenAI expands, the competitive landscape shows what resistance it meets along the way. OpenAI\u0026rsquo;s footprint now spans nearly every tier of the AI stack—from the chips that power training to the browsers where users interact. What\u0026rsquo;s notable is not just the breadth of competition, but the pace at which OpenAI enters new domains once they become strategically adjacent to its assistant experience.\nThe multimodal battleground is particularly intense, with OpenAI\u0026rsquo;s video capabilities through Sora competing against established players like Google (Veo), Luma, Kling, Hailuo, ElevenLabs, and Cartesia. Voice and text-to-speech represent another competitive front against Google, Meta, and Bytedance. At the application layer, ChatGPT, Atlas, Agents, and Study Mode compete against Anthropic, Google, Perplexity, Cohere, and various other companies with AI product offerings.\nThe Supply Chain Reality: Dependencies and Leverage Points Understanding OpenAI\u0026rsquo;s industry position requires examining the supply chain dynamics that constrain and enable its growth. At the supply level, OpenAI depends heavily on semiconductor providers including NVIDIA, AMD, Intel, TSMC, and Samsung Foundry for the computational infrastructure that powers its models. While primarily NVIDIA-dependent, potential alternative accelerator systems from Google TPU, AWS Trainium, Microsoft, and Cerebras represent possible diversification options, though OpenAI\u0026rsquo;s actual usage of these alternatives remains limited. Cloud and datacenter infrastructure from Azure, AWS, Google Cloud,etc support their deployment requirements.\nThe channel relationships reveal OpenAI\u0026rsquo;s distribution strategy. Enterprise platforms including Salesforce, ServiceNow, and Workday provide pathways to business customers, while collaboration and productivity tools like Slack, Teams, Google Workspace, Microsoft 365, Notion, and Figma represent potential integration opportunities. Browser surface integration through Atlas creates a direct consumer touchpoint, competing with traditional web search and productivity workflows.\nComplement relationships highlight critical dependencies for OpenAI\u0026rsquo;s platform strategy. Data and licensing partnerships with AP, Financial Times, Le Monde, Reddit, and Stack Overflow provide the content foundation for training and response generation. Safety and evaluation frameworks from Scale AI, ARC, and Metaprompt help ensure responsible deployment. Identity and compliance solutions from Okta, Auth0, Microsoft Entra, and Stripe Identity handle the enterprise security requirements that make large-scale deployment possible.\nThese dependencies also signal where OpenAI directs capital and hiring—particularly in online storage, data movement, and distributed systems that underpin its enterprise ambitions. OpenAI\u0026rsquo;s hiring patterns and organizational focus suggest deliberate prioritization of infrastructure scaling and platform consolidation. The company appears to be emphasizing online storage and data movement capabilities, distributed systems for enterprise deployment, and agent platforms that enable browser-native workflows. These focus areas, evidenced through job postings and team expansions, align directly with the platform strategy of becoming the coordination layer for AI-powered work.\nCritical Milestones: When the Platform Consolidates Based on their announced roadmap and product release patterns, several key inflection points will likely determine OpenAI\u0026rsquo;s platform success. The 2023 launch of ChatGPT Plus established consumer monetization and early plugin ecosystem momentum, creating the foundation for platform expansion. The 2024 releases of GPT-4o, Search, and Canvas created a unified assistant surface with real-time multimodal capabilities, positioning OpenAI to capture more complex user workflows.\nLooking ahead, the Summer 2025 rollout of Study Mode, Atlas, and Agent Mode appears designed to move OpenAI beyond conversation into browser-native agentic workflows, changing how users interact with AI systems.\nThese milestones matter because they represent platform lock-in moments. Once enterprises commit to data residency infrastructure and users adopt agentic workflows, switching costs increase dramatically. The browser-native experiences create new interaction patterns that become increasingly difficult for competitors to displace.\nGlobal Scaling: The Infrastructure Imperative OpenAI\u0026rsquo;s global hiring footprint reveals the scale of its platform ambitions. With 86.33% of hiring concentrated in the United States across San Francisco, Remote-US, NYC, Seattle, and Washington DC, OpenAI maintains strong coordination around its core product and research development. However, the international distribution across Japan (3.49%), Ireland (2.68%), Singapore (2.14%), India (1.61%), Australia (1.61%), South Korea (1.34%), Germany (0.54%), and France (0.27%) indicates strategic positioning for regional expansion and compliance requirements.\nThis geographic distribution suggests a strategic approach to international expansion, with local presence in key regulatory jurisdictions potentially supporting enterprise adoption in international markets. The concentration in specific cities—Tokyo, Dublin, Singapore, Delhi, Sydney, Seoul, Munich, and Paris—indicates a hub-based approach to regional scaling rather than distributed expansion.\nOpenAI’s geographic distribution underscores how infrastructure strategy and regulatory positioning converge. Concentration in U.S. hubs allows for tight coordination, while targeted expansion into Asia and Europe aligns with data residency requirements and enterprise trust. The result is a hub-and-spoke model of compliance—regional enough to meet local regulation, centralized enough to maintain product velocity. For startups, this creates both clarity and constraint: regions where OpenAI lacks presence may offer short-term white space, but the window narrows quickly once compliance infrastructure lands.\nStrategic Implications for AI Startups OpenAI\u0026rsquo;s trajectory signals a decisive end to the era of thin AI wrappers. The company\u0026rsquo;s integration of model, interface, and infrastructure has collapsed what used to be a multi-layer market into a vertically unified platform. For founders, defensibility now depends on depth—specialization, proprietary data, or domain-specific regulation—rather than breadth. Some will thrive as complements, building tools that extend the platform\u0026rsquo;s reach into verticals OpenAI can\u0026rsquo;t or won\u0026rsquo;t prioritize. Others will seek independence through novel architectures or community-owned ecosystems.\nOpenAI\u0026rsquo;s expansion marks the normalization of AI as infrastructure. By controlling how people write, search, and learn, it\u0026rsquo;s setting behavioral defaults that future builders will either align with or challenge. The blast radius isn\u0026rsquo;t destruction—it\u0026rsquo;s redefinition. The companies that survive will be those that understand where OpenAI\u0026rsquo;s platform stops and differentiated value begins.\nAppendix: Analytical Methodology and Limitations This analysis synthesizes over one hundred OpenAI product releases and four hundred job postings using automated web crawling and strategic content synthesis. It favors breadth over depth—identifying cross-layer patterns in OpenAI\u0026rsquo;s expansion rather than case-level detail. While this reveals the company\u0026rsquo;s overarching trajectory, it should be complemented with vertical-specific research for actionable insight.\nThe Missing Layer of Collective Semantics Current assistants compress vast human discourse into atomic answers, overlooking the higher-order structures of collective meaning—clusters of opinion, degrees of conviction, and the evolution of public narratives. The Reddit partnership brings social data into OpenAI\u0026rsquo;s responses but stops short of indexing group intelligence. This gap defines a white space: translating collective semantics into measurable indicators for decision-making in investing, branding, or governance. Whoever builds that interface between crowd cognition and AI reasoning will occupy the next layer beyond OpenAI\u0026rsquo;s platform.\nCredits Thanks to Livia and Richard for keeping me company amid rambling, ChatGPT for helping with the visualizations, and Gemini for the synthesis.\n","permalink":"https://chenterry.com/archived/openai-blast-radius/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI\u0026rsquo;s rapid product cadence isn\u0026rsquo;t just releasing tools—it\u0026rsquo;s consolidating power.\u003c/strong\u003e By integrating models, infrastructure, and interfaces into a single AI operating system, the company is reshaping where startups can compete and how value accrues across the AI stack.\u003c/p\u003e\n\u003cp\u003eOpenAI isn\u0026rsquo;t just launching new products; it\u0026rsquo;s redefining where startups can safely operate. In less than three years, it has evolved from a research lab into a full-stack AI platform whose reach now spans infrastructure, models, applications, and compliance. Each release expands its gravitational field, redrawing the boundaries of opportunity for founders and investors. Understanding OpenAI\u0026rsquo;s strategic expansion is therefore not about tracking a single company; it\u0026rsquo;s about mapping the blast radius of a platform that is systematically consolidating multiple layers of the AI value chain.\u003c/p\u003e","title":"OpenAI's Blast Radius"},{"content":"From Indexing to Understanding Intent in Discovery Systems Discovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoning—delivering fast, contextually relevant answers while offering deeper AI-powered synthesis when users explore unfamiliar or complex topics.\nWhy Traditional Search Falls Short Traditional keyword search works best when the user knows exactly what to ask. However, most discovery today begins with uncertainty: a half-remembered quote, a general theme, or a desire for inspiration. Users often don\u0026rsquo;t know what they\u0026rsquo;re looking for until they see it. The problem is both cognitive and technical—users\u0026rsquo; mental models evolve as they explore. Modern discovery tools must anticipate that evolution, interpret vague intent, and surface meaningfully related ideas rather than exact word matches.\nFrom Indexing to Intent-Based Query Completion The transition from indexing to intent-driven discovery mirrors a broader movement from databases that store information to systems that reason about context. In practice, this takes two complementary forms: fast responses and deep responses.\nFast responses are built for immediate clarity. They combine multiple recall routes—lexical, semantic, behavioral, and social—to provide accurate answers quickly. This approach excels in known-item retrieval and factual questions. For instance, Fable\u0026rsquo;s new search experience goes beyond simple keyword matching to understand emotional and stylistic dimensions. When a reader searches \u0026ldquo;books that feel like autumn evenings,\u0026rdquo; the system infers ambience and tone rather than relying solely on metadata tags. This design transforms search from literal matching to contextual association.\nFable\u0026rsquo;s enhanced search showing predictive completion for incomplete queries like \u0026ldquo;heaven and earth g\u0026rdquo;\nThe system demonstrates superior intent understanding by providing relevant suggestions before users complete their queries. The \u0026ldquo;After\u0026rdquo; version shows how modern search anticipates user needs, surfacing \u0026ldquo;The Heaven \u0026amp; Earth Grocery Store\u0026rdquo; and related titles when users type partial queries.\nFable\u0026rsquo;s improved relevance matching for author searches like \u0026ldquo;james\u0026rdquo;\nSimilarly, when searching for \u0026ldquo;james,\u0026rdquo; the enhanced system prioritizes contextually relevant results like \u0026ldquo;James (Pulitzer Prize Winner)\u0026rdquo; rather than generic matches, demonstrating how intent-aware systems understand query context.\nDeep responses, in contrast, serve users exploring open-ended questions. They rely on large language models to synthesize information, drawing connections across content sources and explaining the reasoning behind conclusions. Red (Xiaohongshu) offers a compelling example through its AskNow feature, where AI-generated insights are grounded on verified community posts, maintaining both authenticity and transparency. Similarly, Reddit Answers (currently in beta) uses real user discussions as evidence for its responses, ensuring that generated insights remain rooted in human context rather than abstract data patterns.\nRed\u0026rsquo;s AskNow feature providing AI-generated company analysis with structured insights on market competition, privacy policies, and legal disputes\nThe AskNow interface demonstrates sophisticated content synthesis, taking user queries about companies like AppLovin and generating comprehensive analyses that include market positioning (competition with Meta, Google), regulatory challenges (privacy policies like IDFA), and risk factors (legal disputes affecting stock prices)—all grounded in community discussions rather than abstract data.\nThis dual architecture—fast for confidence, deep for curiosity—captures how systems can dynamically adapt to user intent during a single session.\nThe Two-Loop Discovery Engine Intent-aware systems operate through a continuous feedback structure combining retrieval and reasoning. The retrieval loop aggregates relevant results using hybrid signals such as keyword relevance, vector similarity, and recency. The reasoning loop interprets ongoing behavior—clicks, refinements, and skips—to update an Intent State that guides subsequent outputs. Each iteration yields one of three outcomes: a direct answer, a curated content set, or an AI-generated synthesis.\nMapping Intent and Context Different user goals demand different discovery experiences. The table below illustrates how systems can tailor responses according to intent and specificity.\nUser Goal Specificity Ideal Response Interface Type Example Retrieve Exact Concise factual snippet with citation Inline summary \u0026ldquo;Release date of Dune Part Two.\u0026rdquo; Learn Fuzzy Conceptual overview with examples and follow-ups Accordion-style cards \u0026ldquo;How do neural embeddings improve search?\u0026rdquo; Decide Mid Structured comparison with trade-offs Comparison grid with rationale notes \u0026ldquo;Which AI search tools balance transparency and cost?\u0026rdquo; Explore Open Serendipitous content spanning adjacent ideas Visual knowledge map or gallery \u0026ldquo;Books that inspire design thinking.\u0026rdquo; Design Principles for Intent-Aware Search An effective discovery system prioritizes clarity, adaptability, and transparency. Each result should show why it was retrieved—through cues such as \u0026ldquo;popular in similar sessions\u0026rdquo; or \u0026ldquo;matches your theme and tone.\u0026rdquo; Systems should learn within each session, refining their understanding as the user interacts. They must balance novelty with relevance, maintaining an exploratory rhythm without overwhelming the user. Transparency matters most: showing data sources, confidence levels, and offering manual control over personalization builds long-term trust. Lastly, discovery should feel seamless across formats—books, posts, videos, and conversations—as part of a single cognitive journey.\nWhen the Corpus Falls Short An intent-aware system recognizes when no direct answer exists. Instead of ending in failure, it synthesizes a grounded response that discloses its sources and confidence level. These generated insights transform information gaps into moments of learning—highlighting missing viewpoints, summarizing scattered data, or surfacing counterexamples that broaden understanding.\nMeasuring Success Through Comprehension Success in intent-based discovery extends beyond engagement metrics. Systems should measure Time to Insight (TTI) to understand how quickly users reach clarity, Exploration Depth to gauge how broadly users traverse concepts, and Trust Indicators reflecting how often users expand citations or rely on AI explanations. Another key metric, Intent Alignment, measures how closely the system\u0026rsquo;s inferred goal matches the user\u0026rsquo;s evolving intent. These metrics move focus from clicks to comprehension, rewarding designs that help people think more efficiently.\nWhy Intent Awareness Matters Search and recommendation are converging into a unified discipline focused on intent understanding. Platforms like Fable, Red, and Reddit show how discovery is shifting from indexing content to interpreting context. The future of search will belong to systems that bridge speed and synthesis—those that recognize a user\u0026rsquo;s goal, respond in real time, and expand understanding with each interaction.\n","permalink":"https://chenterry.com/posts/intent_driven_discovery/","summary":"\u003ch1 id=\"from-indexing-to-understanding-intent-in-discovery-systems\"\u003eFrom Indexing to Understanding Intent in Discovery Systems\u003c/h1\u003e\n\u003cp\u003eDiscovery is moving from static data retrieval toward systems that understand why a user is searching, not just what they type. The next generation of search experiences must merge precise recall with adaptive reasoning—delivering fast, contextually relevant answers while offering deeper AI-powered synthesis when users explore unfamiliar or complex topics.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-traditional-search-falls-short\"\u003eWhy Traditional Search Falls Short\u003c/h2\u003e\n\u003cp\u003eTraditional keyword search works best when the user knows exactly what to ask. However, most discovery today begins with uncertainty: a half-remembered quote, a general theme, or a desire for inspiration. Users often don\u0026rsquo;t know what they\u0026rsquo;re looking for until they see it. The problem is both cognitive and technical—users\u0026rsquo; mental models evolve as they explore. Modern discovery tools must anticipate that evolution, interpret vague intent, and surface meaningfully related ideas rather than exact word matches.\u003c/p\u003e","title":"From Indexing to Understanding Intent in Discovery Systems"},{"content":"When Knowledge Becomes Fluid For most of history, knowledge has been bound — fixed in books, trapped in formats, and constrained by how it could be consumed. You could read a page, listen to a lecture, or watch a documentary, but each existed in isolation. With generative models, that boundary begins to dissolve. Knowledge itself becomes fluid — able to reshape, reframe, and re-express itself across contexts and mediums.\nAs Dan Shipper illuminates in his exploration of language models, we now have what amounts to \u0026ldquo;free energy for text\u0026rdquo; — the ability to transform any piece of knowledge through compression, expansion, and translation operations. What once required manual summarization and editing can now be synthesized in real time. A single idea can compress into focused insights or expand into comprehensive explorations, adapting not just in length but across dimensions of style, tone, and perspective. Instead of treating knowledge as static, we can begin to design it as something alive — capable of adapting to how, when, and where we learn.\nScaling Knowledge Most modern learning apps do a good job of compressing information, but they still rely on manual curation. Platforms like Blinkist summarize a few thousand books, turning complex ideas into short, digestible snippets. Useful, but limited. Language models transcend this constraint by operating as cultural technologies — giving us access to the best of what humanity knows about any topic, compressed into the right form for any given context.\nBut compression itself has evolved beyond simple summarization. Knowledge can now be compressed across multiple dimensions simultaneously: comprehensive for breadth, engaging for attention, stylistic for voice, or contextual for specific audiences. The same complex text can become a technical analysis, a conversational explanation, or an irreverent commentary — each compression preserving different aspects of the original while serving different cognitive needs.\nSource: Dan Shipper, Every\nSource: Dan Shipper, Every\nThis multi-dimensional transformation means that accessibility is no longer just about reading level or length. Knowledge can adapt its entire presentation — its tone, complexity, cultural references, and emotional register — to match not just what you need to know, but how you need to encounter it. The scale of knowledge expands not by adding editors, but by giving knowledge itself the ability to self-express across infinite dimensions of human understanding.\nFluid Learning Learning is deeply contextual. Reading a dense essay might work at a desk, but not while commuting or cooking. The same knowledge can take different forms depending on where we are and what we\u0026rsquo;re doing. Language models enable this flexibility through three types of expansion: comprehensive expansions that provide broad overviews, contextual expansions that fit information to your specific background and circumstances, and creative expansions that explore possibility spaces and generate new connections.\nThis transformation respects the fundamental constraint of human attention while creating new pathways for engagement. When you ask a question, it can expand into an answer tailored not just to what you want to know, but to who you are, what you already understand, and how much cognitive energy you have available. A complex concept might become a Wikipedia-style explanation for comprehensive understanding, a personalized tutorial fitted to your experience level, or a creative exploration that connects the idea to your existing interests.\nThe result is a more continuous, natural learning flow — where every question contains its answer, waiting to be expanded in the precise form you need. Instead of forcing you to adapt to the medium, the medium adapts to you, creating conditions for understanding and stepping back to let learning emerge.\nConnecting Ideas The next step is not just summarizing knowledge, but connecting it. A truly generative library doesn\u0026rsquo;t only compress information — it discovers relationships. It can find the echoes that cut across books, fields, and centuries. It might reveal that The Art of War and Measure What Matters both explore alignment under uncertainty — one in ancient warfare, the other in modern management.\nThis kind of synthesis transforms learning from retrieval to insight. Instead of static archives, we begin to build creative constellations of ideas. Knowledge becomes something that grows through its connections, not just its content. We move from consuming isolated summaries to experiencing patterns of thought that evolve as we explore them.\nInteractive Understanding Summaries can tell you what to think, but they rarely teach you how to think. A new generation of learning systems makes this process interactive by leveraging the fundamental difference between compression and expansion operations. Compression is predictable — like squeezing a lemon, you get concentrated essence of what was already there. Expansion is creative and unpredictable — like an acorn growing into a tree, the final form depends on the conditions and interactions along the way.\nThis distinction transforms how we design learning experiences. Rather than passively reading compressed knowledge, you engage with systems that can expand your questions into explorations. You ask about Stoicism, and the system doesn\u0026rsquo;t just compress existing texts — it expands the inquiry into new territories, generating connections you hadn\u0026rsquo;t considered, posing questions that emerge from the intersection of your curiosity and the knowledge space.\nOver time, these systems learn from your curiosity patterns. The experience becomes conversational — a collaboration between human intuition and machine reasoning. Learning feels less like consumption and more like cultivation, where understanding grows through the unpredictable but guided expansion of ideas, shaped by your attention and questions.\nA Living Medium Generative AI changes what a book even is. It turns knowledge into a living medium — fluid, adaptive, and co-creative. Instead of locking ideas into fixed containers, we can let them flow. A book becomes a conversation, a lecture becomes an experience, and learning becomes something that moves with us.\nThis transformation builds on something uniquely human: our capacity to ask questions. Question-asking creates room for answers, and answers create room for more questions — the fundamental engine of learning and creativity. Language models extend this capacity by creating a world where every question already contains its answer, waiting to be expanded in whatever form serves your understanding best.\nWhen knowledge becomes fluid, understanding no longer depends on how much we can read or memorize. It depends on how well we can collaborate with systems that transform our curiosity into insight — compressing vast knowledge into focused understanding, expanding simple questions into rich explorations, and translating ideas across the boundaries that once separated disciplines, formats, and minds. Knowledge moves, and we move with it.\n","permalink":"https://chenterry.com/posts/fluid-knowledge-ai-transforms-learning/","summary":"\u003ch1 id=\"when-knowledge-becomes-fluid\"\u003eWhen Knowledge Becomes Fluid\u003c/h1\u003e\n\u003cp\u003eFor most of history, knowledge has been bound — fixed in books, trapped in formats, and constrained by how it could be consumed. You could read a page, listen to a lecture, or watch a documentary, but each existed in isolation. With generative models, that boundary begins to dissolve. Knowledge itself becomes fluid — able to reshape, reframe, and re-express itself across contexts and mediums.\u003c/p\u003e\n\u003cp\u003eAs \u003ca href=\"https://every.to/@danshipper\"\u003eDan Shipper\u003c/a\u003e illuminates in his exploration of language models, we now have what amounts to \u0026ldquo;free energy for text\u0026rdquo; — the ability to transform any piece of knowledge through compression, expansion, and translation operations. What once required manual summarization and editing can now be synthesized in real time. A single idea can compress into focused insights or expand into comprehensive explorations, adapting not just in length but across dimensions of style, tone, and perspective. Instead of treating knowledge as static, we can begin to design it as something alive — capable of adapting to how, when, and where we learn.\u003c/p\u003e","title":"When Knowledge Becomes Fluid: How AI Transforms Learning"},{"content":"Understanding Google\u0026rsquo;s Product Ecosystem Google has evolved from a simple search engine into a comprehensive ecosystem of interconnected products and services that power much of the modern internet experience. This analysis examines Google\u0026rsquo;s core products and their strategic evolution into AI-powered services that define contemporary technology investment opportunities. From traditional consumer applications to cutting-edge AI experiments, Google\u0026rsquo;s product portfolio demonstrates a coherent strategy of data collection, user engagement, and technological advancement.\nCore Product Portfolio Google\u0026rsquo;s product ecosystem spans multiple categories, each serving different user needs while contributing to the company\u0026rsquo;s overall data and advertising strategy. The core products include consumer staples like Search, Gmail, Chrome, and YouTube, productivity tools such as Google Docs, Google Calendar, and Google Drive, platform services like Android and Google Play, and emerging technologies through Pixel devices and Gemini AI. This diversified portfolio creates multiple touchpoints with users throughout their digital lives, generating valuable data that powers Google\u0026rsquo;s advertising business and AI development.\nProduct Portfolio and Market Position Segment Flagship products Market position / scale (latest reliable figures) Monthly Active Users / User Base Search \u0026amp; Ads (Google Services) Google Search, YouTube Ads, Google Ads/Ad Manager, Shopping Search: ~ninety percent worldwide share (Statcounter, Sept 2025). (StatCounter Global Stats) 4.97 billion global users; 8.5 billion daily searches YouTube YouTube, Shorts, Premium/Music MAUs: ~two-and-a-half to two-point-seven billion; Shorts: ~two hundred billion daily views; Premium: one hundred twenty-five million subscribers. (DemandSage) 2.54 billion MAU; 125 million Premium subscribers Cloud Google Cloud Platform (GCP), Workspace for enterprise Cloud IaaS/PaaS share: ~thirteen percent (Q2 2025), behind AWS (~thirty percent) and Azure (~twenty percent). Revenue: $13.6B in Q2 2025, +32% YoY; operating income ~$2.8B. (Statista) $50+ billion annual run rate; 28% QoQ customer growth Platforms \u0026amp; OS Android, Chrome/ChromeOS, Play Android: ~seventy-four percent global mobile OS share. Chrome: ~seventy-two percent global browser share (Sept 2025). (DemandSage) Android: 3-4.2 billion devices; Chrome: 3.45 billion users; Play: 2.5 billion users Productivity (consumer \u0026amp; edu) Gmail, Drive, Docs/Sheets/Slides, Meet, Classroom Email client share (opens): Gmail ~twenty-four to twenty-six percent, second to Apple Mail (Litmus/industry panels, 2025). Workspace scale: billions of users; paying customers in the single-digit millions (public figures are older). (Litmus) Gmail: 1.8-2.5 billion users; Drive: 3 billion MAU; Workspace: 6+ million paying customers Maps \u0026amp; Local Google Maps, Maps Platform APIs Usage: widely cited at one-plus billion MAUs; third-party estimates range higher; Google continues deep integration (AI route summaries, business info). (Center AI) 2+ billion MAU (Q3 2024); projected 2.2 billion by Q1 2025 Hardware Pixel phones/tablets, Nest (home), Chromecast Complements services; revenue included in \u0026ldquo;Subscriptions, Platforms \u0026amp; Devices\u0026rdquo; inside Google Services. (Breakouts not separately disclosed.) (Q4 Inc.) Integration with ecosystem; exact user counts not disclosed Google Search leads with nearly 5 billion global users conducting 8.5 billion searches daily, while Chrome browser reaches 3.45 billion users worldwide. The productivity suite, anchored by Gmail\u0026rsquo;s 1.8-2.5 billion users and Drive\u0026rsquo;s 3 billion monthly active users, demonstrates Google\u0026rsquo;s success in transitioning from search to comprehensive digital services. YouTube\u0026rsquo;s 2.54 billion monthly active users and 125 million Premium subscribers showcase the platform\u0026rsquo;s dominance in video content and subscription services.\nAI-Powered Search Evolution Google Search has transformed into a multimodal AI platform. Circle to Search enables gesture-based queries on Android devices, while AI Mode provides conversational search with follow-up suggestions. Google Lens extends visual search beyond object recognition to complex tasks like solving handwritten math problems and real-time translation, demonstrating Google\u0026rsquo;s push toward intuitive, context-aware interfaces.\nGemini: Google\u0026rsquo;s AI Assistant Platform Gemini serves as Google\u0026rsquo;s flagship AI assistant and comprehensive thinking partner for complex reasoning, creative projects, and analytical work. Unlike standalone AI platforms, Gemini\u0026rsquo;s integration with Google\u0026rsquo;s ecosystem provides unique advantages: real-time Search access, Google Workspace integration, and personalized responses based on user data. This ecosystem approach positions Gemini as a direct competitor to ChatGPT while leveraging Google\u0026rsquo;s existing platform advantages.\nNotebookLM: Research and Analysis Platform NotebookLM represents Google\u0026rsquo;s approach to AI-powered research and knowledge management, positioning itself as a research and thinking partner grounded in trusted information sources. The platform is built on the latest Gemini models and designed to work with user-provided documents, creating a personalized knowledge base that can be queried and analyzed through natural language interactions.\nThe \u0026ldquo;Understand Anything\u0026rdquo; tagline reflects NotebookLM\u0026rsquo;s capability to process and synthesize information from multiple sources, making it particularly valuable for academic research, business analysis, and content creation. Unlike general-purpose AI assistants that draw from broad internet knowledge, NotebookLM focuses on understanding and analyzing specific documents uploaded by users, ensuring that responses are grounded in trusted, user-selected sources. This approach addresses concerns about AI hallucination and provides users with more reliable research assistance.\nGoogle Labs: Experimental AI Features Google Labs serves as Google\u0026rsquo;s experimental platform for testing cutting-edge AI features before mainstream deployment. The platform enables rapid iteration and user feedback collection for emerging technologies.\nFlow represents a breakthrough in AI filmmaking, using Veo for video generation to create cinematic clips with visual consistency. This tool democratizes professional video production through intelligent automation.\nDaily Listen showcases another experimental direction: AI-generated personalized audio content that curates topics from across the web, demonstrating Google\u0026rsquo;s exploration of audio-first AI experiences.\nGoogle\u0026rsquo;s Position in the AI Search Era The transition to AI-powered search represents perhaps the most significant shift in Google\u0026rsquo;s business model since its founding. Recent changes to Google\u0026rsquo;s search infrastructure reveal a strategic repositioning that has profound implications for both the company\u0026rsquo;s competitive moat and the broader internet ecosystem. Last month, Google quietly removed the num=100 search parameter — the small flag that let users view up to 100 results at once. The maximum is now ten. It sounds trivial, but it\u0026rsquo;s a massive shift in how the web works. By collapsing access to the \u0026ldquo;long tail\u0026rdquo; of search, Google just reduced the visible internet by 90 percent.\nThis strategic partnership with Reddit exemplifies Google\u0026rsquo;s approach to expanding content access while maintaining search dominance.\nThat long tail has always mattered. It\u0026rsquo;s where niche knowledge lives — community posts, independent blogs, GitHub issues, Reddit threads. It\u0026rsquo;s also the layer most large language models rely on, directly or indirectly, through Google\u0026rsquo;s indexed ranking of relevance. Even when OpenAI, Perplexity, or Anthropic crawl the web themselves, Google\u0026rsquo;s structure guides what they find and prioritize. Removing access to deep results means those models — and the startups that depend on them — now see a much smaller portion of the web.\nThe impact has been immediate and measurable. According to Search Engine Land, 88 percent of websites reported a drop in impressions after the change. Reddit, which often ranks in positions 11–100, saw its visibility collapse; its mentions in LLM outputs plunged, and its stock fell roughly 15 percent, wiping out around $5 billion in market value. What looked like a minor search tweak turned out to be a profound re-wiring of online discovery. This example illustrates the interconnected nature of Google\u0026rsquo;s influence — changes to search parameters don\u0026rsquo;t just affect Google, they reshape the entire information ecosystem that other AI companies depend upon.\nFor startups, the implications are brutal. Visibility just got harder. The open-web assumption — that a good product will eventually be found — no longer holds. If your site doesn\u0026rsquo;t rank in the top 10, it may as well not exist. In an AI-driven ecosystem, discoverability is no longer distributed; it\u0026rsquo;s gated by a few dominant indexes and interfaces. This represents a fundamental shift from the democratized web of the early 2000s to a curated, AI-mediated information environment where Google\u0026rsquo;s algorithmic decisions determine what knowledge exists in practical terms.\nThe deeper story is about power and distribution. Google\u0026rsquo;s decision protects its data moat and limits how easily AI competitors can piggyback on its index. But it also accelerates a larger shift: from an open web to a closed network of curated answers. As search turns into synthesis, Google becomes not just the map of the internet — but its gatekeeper. This transformation positions Google uniquely in the AI era, where access to high-quality training data becomes increasingly valuable and scarce.\nFor builders, the takeaway is simple but sobering. Great products don\u0026rsquo;t guarantee reach anymore; distribution does. If no model or platform can see you, users can\u0026rsquo;t either. The future of the internet isn\u0026rsquo;t about publishing to be found — it\u0026rsquo;s about integrating to be surfaced. This shift fundamentally alters the startup landscape, making Google\u0026rsquo;s ecosystem integration not just advantageous but essential for visibility in an AI-mediated world.\nStrategic Implications and Investment Thesis Google\u0026rsquo;s product ecosystem reveals a coherent strategy of building an AI-powered platform that touches every aspect of digital life. The integration of AI capabilities across traditional products like Search and new experimental platforms like NotebookLM demonstrates Google\u0026rsquo;s commitment to maintaining technological leadership in the AI era. This comprehensive approach creates multiple competitive advantages: extensive data collection for model training, diverse distribution channels for AI capabilities, and integrated user experiences that increase platform stickiness.\nFrom an investment perspective, Google\u0026rsquo;s product evolution suggests several key trends. First, the company is successfully transitioning from advertising-dependent revenue models to AI-powered service offerings that could command premium pricing. Second, the integration of AI across the product portfolio creates new monetization opportunities and strengthens competitive moats. Third, the experimental approach through Google Labs enables rapid innovation cycles and risk mitigation for emerging technologies.\nThe breadth of Google\u0026rsquo;s product portfolio also provides resilience against competitive threats. While individual products may face direct competition, the interconnected nature of the ecosystem creates switching costs and network effects that protect Google\u0026rsquo;s market position. As AI capabilities become more central to user interactions, Google\u0026rsquo;s head start in both AI research and product integration positions the company well for sustained growth in the evolving technology landscape.\nReferences StatCounter Global Stats. \u0026ldquo;Search Engine Market Share Worldwide.\u0026rdquo; StatCounter. Accessed October 2025. https://gs.statcounter.com/search-engine-market-share\nDemandSage. \u0026ldquo;How Many People Use YouTube? (2025 Active Users Stats).\u0026rdquo; DemandSage. 2025. https://www.demandsage.com/youtube-stats/\nStatista. \u0026ldquo;The Big Three Stay Ahead in Ever-Growing Cloud Market.\u0026rdquo; Statista. 2025. https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/\nDemandSage. \u0026ldquo;Android Usage Statistics (2025) - Global Market Share.\u0026rdquo; DemandSage. 2025. https://www.demandsage.com/android-statistics/\nLitmus. \u0026ldquo;Email Client Market Share and Popularity.\u0026rdquo; Litmus. 2025. https://www.litmus.com/email-client-market-share/\nCenter AI. \u0026ldquo;37 Google Maps Statistics and Interesting Facts.\u0026rdquo; Center AI. 2025. https://center.ai/blog/google-maps-statistics-and-interesting-facts/\nAlphabet Inc. \u0026ldquo;Alphabet Announces Second Quarter 2025 Results.\u0026rdquo; SEC Filing. Q4 Inc. 2025. https://s206.q4cdn.com/479360582/files/doc_financials/2025/q2/2025q2-alphabet-earnings-release.pdf\nGoogle Labs. \u0026ldquo;The home for AI experiments at Google.\u0026rdquo; Google. Accessed October 2025. https://labs.google.com\nAlphabet Inc. \u0026ldquo;Alphabet Inc. Form 10-Q for the quarterly period ended June 30, 2025.\u0026rdquo; Securities and Exchange Commission. 2025. https://www.sec.gov/Archives/edgar/data/1652044/000165204425000062/goog-20250630.htm\nSundar Pichai. \u0026ldquo;Q3 2024 Alphabet Earnings Call Transcript.\u0026rdquo; Alphabet Inc. October 2024. https://seekingalpha.com/article/4730692-alphabet-inc-goog-q3-2024-earnings-call-transcript\nTechCrunch. \u0026ldquo;Google\u0026rsquo;s AI and Machine Learning Advances in 2025.\u0026rdquo; TechCrunch. 2025. https://techcrunch.com/2025/09/24/it-isnt-your-imagination-google-cloud-is-flooding-the-zone/\nGoogle Blog. \u0026ldquo;Google\u0026rsquo;s Product Strategy and AI Integration.\u0026rdquo; Google. 2024. https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/\nBloomberg Technology. \u0026ldquo;Google Cloud Platform Growth and Market Position.\u0026rdquo; Bloomberg. 2024. https://www.bloomberg.com/news/videos/2024-10-30/bloomberg-technology-10-30-2024-video\nGartner. \u0026ldquo;Magic Quadrant for Strategic Cloud Platform Services 2024.\u0026rdquo; Gartner Research. 2024. https://www.gartner.com/en/documents/5851847\nIDC. \u0026ldquo;Worldwide Public Cloud Services Spending Guide.\u0026rdquo; International Data Corporation. 2025. https://my.idc.com/getdoc.jsp?containerId=prUS52460024\nSearch Engine Land. \u0026ldquo;Google Search Parameter Changes and Website Visibility Impact.\u0026rdquo; Search Engine Land. 2025.\nAppendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/posts/google_primer/","summary":"\u003ch2 id=\"understanding-googles-product-ecosystem\"\u003eUnderstanding Google\u0026rsquo;s Product Ecosystem\u003c/h2\u003e\n\u003cp\u003eGoogle has evolved from a simple search engine into a comprehensive ecosystem of interconnected products and services that power much of the modern internet experience. This analysis examines Google\u0026rsquo;s core products and their strategic evolution into AI-powered services that define contemporary technology investment opportunities. From traditional consumer applications to cutting-edge AI experiments, Google\u0026rsquo;s product portfolio demonstrates a coherent strategy of data collection, user engagement, and technological advancement.\u003c/p\u003e","title":"Understanding Google: A Primer"},{"content":"OpenAI\u0026rsquo;s announcement that developers can build apps and tools directly inside ChatGPT isn\u0026rsquo;t just another feature drop; it\u0026rsquo;s a distribution shift. When AI becomes a canvas, the winners are the coordination layers that turn ideas into shipped product. The market\u0026rsquo;s immediate reaction—Figma jumping nearly fifteen percent—signals that investors increasingly view design collaboration platforms as the natural aggregation points for AI-generated work.\nFigma and Lovable illustrate two paths to that future. Lovable compresses ideation into working UI quickly; Figma converts individual creativity into team progress at enterprise scale. The question isn\u0026rsquo;t which tool \u0026ldquo;has more AI,\u0026rdquo; but who best translates AI\u0026rsquo;s raw generation into reliable, multi-stakeholder workflows.\nWedge vs. Workflow: The Lovable Challenge Lovable is a terrific wedge: it transforms ambiguous PRDs into working UI and code with startling speed. But wedges must graduate into workflows to hold value in teams. High-fidelity nuance still benefits from direct manipulation; complex data flows still require versioned review, access control, and code governance. Until AI-first generators own those moments of accountability, they amplify Figma\u0026rsquo;s role as the coordination substrate rather than displace it.\nSequencing Loops → Platform Gravity Figma\u0026rsquo;s early loop—real-time, browser-first collaboration—pulled in designers. The second loop—shared libraries, specs, and comments—pulled in PMs and engineers. The next loop is AI actions embedded in those same surfaces: generate variants, auto-redline, bind to live data, and export code with guardrails. Each loop recruits a new cohort, increases retention for the previous cohort, and raises switching costs. AI doesn\u0026rsquo;t replace these loops; it accelerates them.\nAs others have argued about Figma\u0026rsquo;s \u0026lsquo;browser-first\u0026rsquo; bet and cross-side effects, the platform advantage becomes clear when considering the full design lifecycle. Figma doesn\u0026rsquo;t just enable individual creativity; it orchestrates the entire collaborative process that turns ideas into shipped products. This includes design system maintenance, component libraries, developer handoff specifications, and stakeholder review processes.\nToday, roughly one-third of Figma\u0026rsquo;s users are professional designers—a group the platform has almost fully captured. But as AI continues to lower the barriers to design and creation, the remaining two-thirds—non-designers—represent the next wave of growth. The very market Lovable is nurturing today could eventually flow toward Figma, since Figma already integrates seamlessly into existing product and design ecosystems.\nWhat to Watch Next Three signals will reveal whether Figma turns AI into durable advantage. First, the mix shift toward non-designer actives, measured by viewer-to-editor conversion and time-to-first-comment on files created with AI features. Second, design-to-deployment cycle time, captured by reduction in handoff defects and PR-to-ship latency on files sourced from Figma Make. Third, ecosystem velocity, reflected in monthly active plugins, enterprise-grade plugin adoption, and AI actions invoked per file. If these curves bend up together, Figma\u0026rsquo;s collaboration moat is compounding.\nThe Coordination Layer Thesis Critics argue that if AI collapses the distance between prompt and production code, the design surface could be bypassed entirely. But in practice, accountability moves toward shared surfaces when stakes rise. Compliance, accessibility, localization, and performance budgets require artifacts that non-designers can review and approve. The more AI generates, the more organizations need a legible, collaborative spine—which is Figma\u0026rsquo;s native terrain.\nWhere Value Accrues As foundation models commoditize, differentiation shifts to integration quality, governance, and cross-functional velocity. Platforms that already mediate conversations among designers, PMs, and engineers are positioned to convert generic model output into organization-specific, reviewable change. That is where budgets live.\nAI increases the volume of drafts, variants, and micro-changes. Without a shared system, that creates chaos; within Figma, it creates momentum. The same surface that shortened idea → design now shortens design → implementation—and the delta is monetizable.\nThe lesson of today\u0026rsquo;s announcement isn\u0026rsquo;t that AI will crown a new category king. It\u0026rsquo;s that AI amplifies whichever layer already coordinates work. Lovable shows how quickly AI can turn intent into interface. Figma shows how teams turn interface into impact. If the next decade of design looks more like engineering—faster, more legible, and more automated—the platform that standardizes those feedback loops will capture the bulk of the value. Right now, that center of gravity is Figma.\n","permalink":"https://chenterry.com/posts/figma_ai/","summary":"\u003cp\u003eOpenAI\u0026rsquo;s announcement that developers can build apps and tools directly inside ChatGPT isn\u0026rsquo;t just another feature drop; it\u0026rsquo;s a distribution shift. When AI becomes a canvas, the winners are the coordination layers that turn ideas into shipped product. The market\u0026rsquo;s immediate reaction—Figma jumping nearly fifteen percent—signals that investors increasingly view design collaboration platforms as the natural aggregation points for AI-generated work.\u003c/p\u003e\n\u003cp\u003e\n\n  \u003cimg src=\"/images/investing/figma-ai-era/chatgpt-figma-integration.png\" alt=\"ChatGPT-Figma Integration\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eFigma and Lovable illustrate two paths to that future. Lovable compresses ideation into working UI quickly; Figma converts individual creativity into team progress at enterprise scale. The question isn\u0026rsquo;t which tool \u0026ldquo;has more AI,\u0026rdquo; but who best translates AI\u0026rsquo;s raw generation into reliable, multi-stakeholder workflows.\u003c/p\u003e","title":"Why Figma Wins (In the AI Era Too)"},{"content":" From Content Aggregation to Original Research Crowdlisten transforms large-scale social conversations into actionable insight by integrating LLM reasoning with extensive model context protocol (MCP) capabilities. While being able to quantitatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\nDeep research features provide professional-looking research reports, yet the contents are far from original, as they\u0026rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet\u0026rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they\u0026rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.\nI\u0026rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlisten, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn\u0026rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows.\nI paused Crowdlisten in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I\u0026rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).\nWhat I\u0026rsquo;m particularly interested in exploring and building with Crowdlisten is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.\nProduct Suite Overview CrowdListen has evolved into a comprehensive suite of AI-powered products designed to address different aspects of social intelligence and content strategy. The Analyze product serves as our core offering, enabling users to discover what people really think about any topic through sophisticated AI-powered sentiment analysis and opinion mining capabilities. This goes beyond simple positive/negative categorization to understand nuanced perspectives, emotional context, and the underlying reasons behind audience reactions.\nOur Research product delivers an agentic research experience that systematically analyzes large volumes of social media content at scale. This comprehensive approach takes longer to run but provides significantly more thorough coverage across platforms, enabling researchers to uncover deeper insights and emerging patterns that automated dashboards typically miss.\nThe Predict product represents our foray into predictive analytics, allowing users to test content variations and predict audience engagement before publishing. Using AI simulation technology, teams can experiment with different messaging approaches and understand likely audience reactions without the risk and cost of live testing.\nFinally, our Insights+ product caters to enterprise users and power analysts who need advanced analytics capabilities and custom reporting features. This tier provides the depth and customization necessary for organizations making strategic decisions based on social intelligence data.\nThe Insight Paradox Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can\u0026rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlisten bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in \u0026ldquo;Essence of Creativity\u0026rdquo; - helping users understand massive amounts of information and generate meaningful insights when they \u0026ldquo;don\u0026rsquo;t know what output they want.\u0026rdquo;\nTechnical Architecture: Multi-Modal by Design The rationale behind Crowdlisten\u0026rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video\u0026rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.\nCrowdlisten\u0026rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM\u0026rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities.\nThis dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlisten can generate insights that reflect not just what is being said, but how it\u0026rsquo;s being said, the surrounding context, and the audience\u0026rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the \u0026ldquo;insight paradox\u0026rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlisten: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.\nDetailed Analysis Capabilities The platform provides granular breakdowns of content performance and audience reactions. Users can explore specific themes, track sentiment over time, and identify the most engaging content types across different categories and industries. This helps brands understand not just what is being said, but why certain content resonates with their audience.\nThe opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.\nAdvanced Research Infrastructure CrowdListen\u0026rsquo;s research infrastructure is built around a sophisticated orchestration system that coordinates multiple specialized AI engines. The Research Command Center provides users with a unified interface to launch complex analysis workflows while monitoring the progress of different analytical engines in real-time.\nOur system utilizes the Research Engine, which orchestrates various AI engines including the Insight Engine for sentiment analysis, Media Engine for multimodal content processing, Query Engine for information retrieval, and Report Engine for generating executive-ready reports. This modular architecture allows for scalable analysis that can adapt to different research requirements.\nThe research interface enables users to input complex queries and optionally upload analysis templates to guide the investigation. The system then automatically determines which analytical capabilities to deploy, processing everything from web search and specialized platform data collection to multi-layered content analysis and synthesis.\nThis integrated approach represents a significant advancement over traditional social media monitoring tools, enabling researchers to conduct comprehensive investigations that would typically require weeks of manual work in a matter of minutes while maintaining the depth and rigor of human-led research.\nCase Study: Google NotebookLM Analysis To demonstrate Crowdlisten\u0026rsquo;s capabilities in product intelligence, we conducted a comprehensive analysis of user sentiment regarding Google\u0026rsquo;s NotebookLM tool. This case study showcases our platform\u0026rsquo;s ability to extract nuanced insights about emerging AI tools and understand user adoption patterns.\nWhen analyzing user sentiment around NotebookLM, our system provided a comprehensive overview showing that customer feedback indicates NotebookLM is effective for information synthesis and content generation, particularly in educational settings. However, users express concerns about the lack of persistent chat history, word count limits, and potential biases in the auto-generated podcast feature. Approximately 56% of users have a positive sentiment, praising its summarization capabilities and educational applications, while 34% express negative sentiment due to usability issues and accuracy concerns.\nOur thematic analysis reveals that Information Synthesis and Summarization is the most discussed topic, with 100 mentions representing 33.39% of all conversations. The sentiment breakdown shows overwhelmingly positive feedback for this core functionality, with users particularly appreciating the tool\u0026rsquo;s ability to synthesize information from uploaded documents and aid in quick comprehension and analysis.\nThe detailed sentiment analysis shows specific user opinions, including praise for NotebookLM\u0026rsquo;s effectiveness in summarizing and synthesizing information from uploaded documents, its utility for creating study guides and educational materials, and its ability to provide citations for generated information to help users verify accuracy and build trust in the tool\u0026rsquo;s output.\nOur analysis draws from 31 sources across 25 unique domains, indicating a moderate level of source diversity at 81%. The sources encompass various types including blogs, news outlets, and other platforms, offering a mix of perspectives. This comprehensive source analysis helps validate the reliability and breadth of our insights.\nThe platform also identifies related research opportunities, suggesting additional analysis areas such as specific research or writing challenges that NotebookLM helps users overcome, how effectively it addresses information overload, the biggest frustrations users encounter, and whether it has improved research workflows. This demonstrates our system\u0026rsquo;s ability to not only analyze current sentiment but also identify strategic research directions.\nContent Predictor: AI-Powered Engagement Forecasting One of our most innovative features is the Content Predictor, which allows users to test content variations and predict audience engagement before publishing. This tool represents a significant advancement in social media strategy, enabling teams to experiment with different messaging approaches without the traditional risks and costs associated with live testing.\nThe Content Predictor uses a sophisticated three-step workflow. Users begin by generating multiple versions of their content, allowing our AI to create variations optimized for specific platforms like Twitter, Instagram, or LinkedIn. Next, the system runs engagement simulations using AI-powered user reactions that model realistic audience behavior patterns. Finally, users can view detailed simulation results and select the most promising content variations based on predicted performance metrics.\nThis capability is particularly valuable for brands and content creators who need to maximize the impact of their social media presence. Rather than relying on intuition or conducting expensive A/B tests with real audiences, teams can now validate their content strategies in a controlled environment before committing to publication. The system considers factors such as platform-specific audience behaviors, trending topics, and historical engagement patterns to provide accurate predictions.\nThe Content Predictor exemplifies our broader mission of transforming social media from a reactive medium to a strategic tool where decisions are informed by data and predictive intelligence rather than guesswork.\nValidation and Impact Our solution has been validated through interviews with major brands like L\u0026rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlisten enables:\nRapid response to emerging trends Deep understanding of consumer sentiment across demographics Identification of microtrends before they become mainstream Competitive intelligence at unprecedented scale The Future of MCP-Driven Research We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.\nThis approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude\u0026rsquo;s interface) emerge as this paradigm gains traction.\nFor social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they\u0026rsquo;re covered in traditional reporting.\nCredits This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Roy, Ruiwan, Qiping, Tongming and other members on the Creative team at TikTok, who I\u0026rsquo;ve discussed early conceptions of this idea with.\nOn Social Intelligence Crowdlisten represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we\u0026rsquo;re helping brands make more informed decisions faster than ever before.\nAs noted in \u0026ldquo;Essence of Creativity\u0026rdquo;, the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we\u0026rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.\n📋 Version History v1.1 • Oct 25, 2025 • View changes • Updated Title\n💡 Click \u0026ldquo;View changes\u0026rdquo; to see exactly what changed between versions\n","permalink":"https://chenterry.com/posts/crowdlisten/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/projects/crowdlistening/homepage-new.png\" alt=\"CrowdListen Homepage\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003ch2 id=\"from-content-aggregation-to-original-research\"\u003eFrom Content Aggregation to Original Research\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://crowdlisten.com\"\u003eCrowdlisten\u003c/a\u003e transforms large-scale social conversations into actionable insight by integrating LLM reasoning with extensive model context protocol (MCP) capabilities. While being able to quantitatively analyze large volumes of data is already an interesting task, our focus is not just on content analysis at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\u003c/p\u003e","title":"From Raw Social Data to Real Research"},{"content":"There are two credible paths to building agentic experiences. The first is platform-first: stand up a unified agent framework with the core capabilities—multi-turn conversation, a knowledge base, and memory—and then layer in signifiers and affordances that fit your environment. The second is scenario-first: begin with the thinnest viable surface and add only the features that demonstrably create value beyond what ChatGPT or Copilot already provide, bringing in memory and other \u0026ldquo;platform\u0026rdquo; features only once they have earned their keep. The platform-first approach yields a consistent engineering experience and lets teams reuse prior agent work, but it risks poor agent–scenario fit. The scenario-first approach can feel messier and demands more from product managers, yet it validates real-world use cases faster. I don\u0026rsquo;t claim one approach is universally better—startups and large companies face different constraints—but I do believe there is only one way to prototype: ship quickly, test explicit hypotheses, and iterate without delay.\nA clarifying question keeps this cadence honest: what is the minimum version of the product that lets us learn whether the solution can find product–market fit? Counterintuitively, you often do not need a working prototype to answer that. Walking through end-to-end customer scenarios frequently reveals whether a proposed feature fits existing workflows and where it will break. That said, some questions hinge on new engineering—experiences that are hard to reason about in the abstract. In those cases, the objective is not to \u0026ldquo;build the demo,\u0026rdquo; but to surface and test the assumptions that matter. Each design choice should map to the outcome it seeks and to the user challenge it addresses. The simpler the stack, the more learning cycles you can run with less effort, which is the real engine of progress.\nThe AI-Powered Development Advantage Survey data from a16z Enterprise reveals why AI-powered tools are gaining traction over traditional low-code solutions, with natural language interaction and rapid prototyping leading the advantages.\nModern AI coding tools make this possible. Cursor, GitHub Copilot, and Claude Code compress build time by generating boilerplate, suggesting common patterns, and helping troubleshoot. A single engineer can now produce a functional MVP in a fraction of the time that used to require a small team. Much like Figma tightened the collaboration loop in design, these tools narrow the gap between product intent and implementation. The result is not merely faster engineering; it is broader participation. Product managers, designers, even sales and customer success teams can test ideas more directly, while engineers concentrate on production-grade systems and reliability concerns that truly benefit from their specialization.\nInvolving Cross-Functional Stakeholders An agentic experience is only as good as our understanding of the underlying problem. This is especially true for expert workflows—consumption-based cost estimation or SOC investigation, for example—where product and engineering teams are rarely the domain experts. Involving architects, sales engineers, and analysts only at the prompt-iteration stage is not enough. To build agent behaviors that actually fit, we have to internalize existing workflows and best practices, then design signifiers and affordances that match practitioner expectations. Language, steps, intermediate outputs, and handoffs should mirror how experts already think and work. When the agent speaks their dialect and respects their process, adoption follows because the experience feels native rather than novel for novelty\u0026rsquo;s sake.\nThis is exactly where the Figma analogy—Kevin Kwok\u0026rsquo;s point about non-linear returns from tighter collaboration loops—becomes operational. Figma did not just make drawing easier; it made critique, alignment, and decision-making happen in the same place, by the right people, at the right time. AI coding assistants catalyze a similar shift for agentic products: they collapse the distance between a domain expert\u0026rsquo;s intent and a working prototype, making assumptions explicit, turning tacit heuristics into checkable rules, and surfacing disagreements while they are still cheap to resolve. When prototypes function as shared canvases—co-edited by PMs, engineers, and subject-matter experts—the loop tightens further: experts shape the signifiers and workflows, product sharpens the hypotheses, and engineering focuses on robustness and safety. The compounding return comes not from adding more features, but from aligning agent behavior with the realities of the domain.\nLearnings from the Cost Estimator Agent To ground these principles, let\u0026rsquo;s look at an agentic implementation of a cost estimation scenario\nProject Context Customers need accurate cost estimates for budget planning and solution comparison, yet consumption-based pricing is notoriously hard to predict. We heard repeatedly from the field that this uncertainty stalls decisions and, in competitive deals, can tilt outcomes against us. Existing tools do not help enough. Web calculators feel like black boxes with coarse, inflexible inputs and little transparency. Spreadsheet models are opaque and fragile, with assumptions scattered across cells. Both often ask for inputs customers do not understand or cannot provide without heavy translation.\nIn other words, this is not a known unknowns problem where a general-purpose copilot can retrieve an answer upon request. Nor is it an unknown knowns problem where the customer already has a tried-and-true estimation method and we simply need to automate it. It is often an unknown unknowns problem: customers do not know what to ask, and they do not have the raw data in the needed form. The result is planning paralysis and, ultimately, stalled or lost deals.\nDesign Rationale Designing for \u0026ldquo;unknown unknowns\u0026rdquo; required optimizing along three intertwined dimensions. First, we focused on transparency and control so that users could see the reasoning behind estimates—the assumptions, intermediate calculations, and trade-offs—and adjust inputs with confidence. Numbers without narrative do not build trust, and trust is the currency of estimation. Second, we embedded domain expertise directly in the experience. Instead of pushing the knowledge gap back to the user, the system translated familiar facts—industry patterns, ingestion profiles, retention policies—into the metrics the pricing model requires, pre-populating where possible and teaching as it went. Third, we treated estimation as a process rather than a form, and we designed for iterative refinement. The goal was not a one-shot answer but a guided conversation that converges on confidence.\nAt a basic level, we began with an agent side-panel, similar to a Copilot, to unify product documentation, pricing schemas, and frequently asked questions. This supported conversational guidance throughout the estimation process, but it also exposed three frictions we had to solve in order to achieve fit. First, use-case discovery was weak: without strong signifiers, users did not know what to ask and often ventured beyond the agent\u0026rsquo;s scope. Second, chat lacked context: humans are economical with effort, so expecting users to restate all the fields they had filled and the stage they were in created unnecessary friction. Third, people don\u0026rsquo;t know what they don\u0026rsquo;t know: there is a structural gap between what customers know about their business (for example, number of users, typical event patterns) and what we require to estimate costs (for example, daily gigabytes ingested). Simply asking, \u0026ldquo;How many gigabytes per day?\u0026rdquo; does not bridge that gap.\nThese insights shaped a prototype with two synchronized surfaces: a pricing panel and an agent panel kept in bidirectional sync. Edits in the graphical interface updated the conversation\u0026rsquo;s context, and the agent\u0026rsquo;s reasoning flowed back as explanation cards anchored beside the fields they affected.\nIn brownfield scenarios, the agent could pull relevant account signals to prefill inputs and explain each value\u0026rsquo;s provenance. In greenfield scenarios, the experience offered size recommendations—small, medium, large, enterprise—that users could apply with one click, each accompanied by clear rationales and editable assumptions.\nWhen hard numbers were missing—say, daily ingestion in gigabytes—the agent asked questions users could answer about environment size, event rates, and retention needs, then converted those responses into derived estimates, showing the math and inviting adjustments. Under the hood, a focused knowledge base provided product and pricing facts, while three structured workflows—volume estimation, pricing estimation, and design recommendations—gave the conversation shape and kept it oriented toward decisions rather than dialogue for its own sake.\nEvaluation and Benchmarking Agent platforms encourage generality, but effectiveness must be demonstrated on concrete tasks. We evaluate the experience by asking whether it completes representative estimation scenarios end to end, how its outputs compare to human-expert baselines, and how quickly it converges to a result stakeholders trust. Accuracy matters, but so do user effort and confidence. When building agentic experiences, we should track time to an acceptable estimate, the number of clarifying turns, and whether users report understanding and accepting the assumptions they carry forward. Scenario coverage also matters: behavior needs to hold not only in the \u0026ldquo;happy path,\u0026rdquo; but across brownfield and greenfield cases, high-volume and bursty workloads, and strict-retention and cost-optimized policies. When behavior degrades, it should degrade gracefully with clear explanations, ranges, or a handoff to a human expert.\nIn larger organizations, evaluation pairs with safeguards. Data validation and drift monitoring ensure that quotes reflect current pricing and product information, with alerts when underlying references change. Guardrails protect embedded expert logic—estimation methods and pricing strategies—against prompt injection and leakage of system instructions, and they constrain access to sensitive APIs. Finally, bad-case handling is a first-class requirement: the system detects ambiguous inputs, surfaces low-confidence steps, and offers conservative defaults or escalation paths rather than silently producing spurious precision. Specifications and engineering plans that omit scenario walkthroughs, benchmarks, and safeguards drift toward imagined use cases and weak agent–scenario fit; those that include them turn agentic ambition into reliable impact.\nClosing Thoughts Choose a build path that fits your context, but always prototype to learn, not to impress. Use AI tools to shorten the distance between ideas and feedback. Bring domain experts into the design of signifiers and workflows so the agent respects reality. Make reasoning visible, embed expertise at the point of need, and shape the experience for iterative refinement. Then prove it with scenario-based evaluation and strong guardrails. This, I believe, is how you truly iterate at the pace of AI.\n","permalink":"https://chenterry.com/posts/agent_prototyping/","summary":"\u003cp\u003eThere are two credible paths to building agentic experiences. The first is platform-first: stand up a unified agent framework with the core capabilities—multi-turn conversation, a knowledge base, and memory—and then layer in signifiers and affordances that fit your environment. The second is scenario-first: begin with the thinnest viable surface and add only the features that demonstrably create value beyond what ChatGPT or Copilot already provide, bringing in memory and other \u0026ldquo;platform\u0026rdquo; features only once they have earned their keep. The platform-first approach yields a consistent engineering experience and lets teams reuse prior agent work, but it risks poor agent–scenario fit. The scenario-first approach can feel messier and demands more from product managers, yet it validates real-world use cases faster. I don\u0026rsquo;t claim one approach is universally better—startups and large companies face different constraints—but I do believe there is only one way to prototype: ship quickly, test explicit hypotheses, and iterate without delay.\u003c/p\u003e","title":"Iterating at the Pace of AI"},{"content":"Why We Need More Ways to Hear Customers Your last customer call was three weeks ago. Your PM dashboard shows green metrics. Yet your latest feature has a 12% adoption rate, and the support tickets keep growing. Sound familiar?\nHere\u0026rsquo;s the trap every growing product team falls into: you start with great customer connections, but success creates distance. Suddenly you\u0026rsquo;re building for your five power users while 95% of your market stays silent. The result? Products that feel over-engineered to newcomers and underwhelming to everyone else. We end up in what I call the \u0026ldquo;vocal minority trap\u0026rdquo;—building for the loudest voices while missing authentic needs from the broader market.\nWith AI-powered social listening tools and tighter feedback loops, we can flip from product → market to market → product development. Instead of building first and hoping for adoption, we can listen broadly to authentic customer discourse, form grounded hypotheses from real needs, then decide what to build. This approach lets us partner closely with key customers while staying connected to the broader market conversation.\nThe AI Way to Do It Traditional social listening tells you what people think about your brand. But what if you could use the same approach to discover what products people actually need—before you build them?\nBeyond internal testers and formal user research channels, the internet already hosts rich, authentic signals about how products—yours and competitors\u0026rsquo;—land in the real world. Community forums, Q\u0026amp;A sites, GitHub issues, Reddit threads, and support communities contain unfiltered customer discourse about pain points, expectations, and the exact language users employ when describing their needs. If we capture, filter, and analyze that discourse responsibly, we get a truer view of market needs than traditional research methods often provide.\nPractical workflow The system operates through two complementary approaches based on your research needs. Targeted discovery works best when you have specific product questions—like \u0026ldquo;why do users abandon our checkout flow?\u0026rdquo; Here, we compile a focused corpus from relevant public discussions (support forums, Reddit threads, GitHub issues), extract and normalize the text, then use large-context LLMs to identify and synthesize recurring themes with direct citations back to source material.\nOpen-ended exploration suits broader market research where you\u0026rsquo;re not sure what patterns might emerge. This approach generates embeddings at both sentence and discussion levels, applies semantic clustering to group similar concerns naturally, and labels each cluster with evidence-linked summaries. The goal isn\u0026rsquo;t to confirm existing hypotheses but to discover what customers actually discuss when they think no one from your company is listening.\nEvery discovered theme flows into a structured decision scaffold: theme → testable hypothesis → bet → telemetry → refinement. This prevents what I call \u0026ldquo;insight theater\u0026rdquo;—beautiful dashboards that don\u0026rsquo;t change what gets built. For example, discovering that users frequently mention \u0026ldquo;billing confusion\u0026rdquo; becomes the hypothesis \u0026ldquo;clearer usage breakdowns will reduce support tickets,\u0026rdquo; which becomes the bet \u0026ldquo;redesign invoice layout,\u0026rdquo; measured through \u0026ldquo;support ticket volume\u0026rdquo; and refined based on actual outcomes.\nLonger-term, the system runs as an always-on Customer Insight Radar that continuously ingests external communities alongside internal customer notes (with appropriate privacy filters). It tracks theme velocity—which concerns are growing versus shrinking—and maintains representative quotes so product managers and engineers can feel the human reality behind abstract metrics.\nPrimary vs. secondary sources When building your corpus, it\u0026rsquo;s crucial to distinguish between primary and secondary sources to maximize insight quality. Treat raw online discourse—posts, threads, issue comments, and direct user communications—as primary research. These represent unfiltered customer voices expressing genuine needs and frustrations in their own language. In contrast, polished content like vendor blog posts, marketing materials, and SEO-optimized pages should be considered secondary sources that often reflect corporate messaging rather than authentic user experience.\nMany \u0026ldquo;deep research\u0026rdquo; tools inadvertently amplify secondary sources because they index what\u0026rsquo;s easily crawlable and well-structured, leading to summaries of summaries rather than original insights. Our approach prioritizes going directly to the source, weighting firsthand accounts more heavily, and using secondary material only for background context or to triangulate findings from primary sources.\nA Bit More on Embeddings (and Why They Help) Think of embeddings as a way to teach computers what words and sentences actually mean. Instead of just matching exact keywords, AI can understand that \u0026ldquo;billing is confusing\u0026rdquo; and \u0026ldquo;invoices are unclear\u0026rdquo; are talking about the same problem, even though they use different words.\nEarly embedding methods learned one meaning per word—fast and useful for basic clustering, but they missed context. The word \u0026ldquo;bill\u0026rdquo; could mean an invoice or a proposed law, and the system couldn\u0026rsquo;t tell the difference. Modern contextual encoders solve this by creating different representations for the same word depending on how it\u0026rsquo;s used, then combining these into sentence and document-level meanings.\nFor social listening, sentence and document embeddings are the workhorses that enable semantic search (finding \u0026ldquo;unreconcilable line items\u0026rdquo; when someone searches for \u0026ldquo;can\u0026rsquo;t map charges to usage\u0026rdquo;), organic theme discovery without predefined categories, and tracking how customer language evolves over time. In practice, we create embeddings at both sentence and full discussion levels, use ANN (Approximate Nearest Neighbor) indexing for fast retrieval, and employ clustering methods that can handle the uneven, natural shape of real community discussions—always linking themes back to actual customer quotes for transparency.\nReasoning Models and Visualizations With smaller datasets, we can directly provide the extracted raw text to a large-context, strong-reasoning model. This goes beyond coarse sentiment categories or word clouds, enabling richer context and actionable insights. Curated prompts let an LLM (or agent) read the semantic neighborhoods, surface key themes, note counter-signals, and propose testable hypotheses.\nHarvesting Customer Testimonials at Scale One particularly powerful application involves extracting authentic customer impact stories from large collections of interview transcripts and recorded conversations. Rather than manually combing through dozens of hours of customer calls, modern AI tools can systematically identify and structure testimonial content in minutes rather than days.\nThe workflow is surprisingly straightforward: First, transcribe customer interviews, podcast episodes, or recorded calls using automated transcription services. Next, upload these transcripts to large-context reasoning models like NotebookLM or Claude. Then ask the AI to identify every quote describing specific product impact, organizing results into structured tables with columns for quote, customer name, company, and use case. Finally, use AI-powered copywriting tools to convert raw testimonials into polished, punchy customer quotes suitable for marketing materials.\nThis approach transforms 1-2 days of manual testimonial extraction into under an hour of guided AI work. More importantly, it ensures comprehensive coverage—human reviewers naturally miss valuable quotes buried in lengthy conversations, while AI can systematically process every statement for relevant content. The result is a richer, more complete picture of how customers actually describe your product\u0026rsquo;s value in their own language.\nDetailed thematic analysis reveals customer pain points through AI-powered clustering and sentiment analysis, enabling product teams to identify high-impact opportunities from authentic user discourse.\nThe insights dashboard translates raw social listening data into actionable product decisions, showing priority themes, opportunity sizing, and specific customer quotes that ground each recommendation in real user needs.\nWhat the output is—and isn\u0026rsquo;t The analysis output serves as a decision input rather than a presentation deck. Each synthesized theme should directly map to three actionable components: a testable product hypothesis that can be validated or refuted, an opportunity-size signal that helps prioritize resources, and a specific instrumentation plan that will measure success. This maintains the same decision scaffold mentioned earlier, ensuring that insights translate into concrete product decisions rather than remaining as interesting but unused research findings.\nSeeing It in Action Example 1: The Cost Experience From ~700 candidate threads, we curated ~100 high-signal discussions (Reddit, HN, Quora, vendor/community forums), normalized text, embedded at sentence/thread level, clustered themes, and linked each to example quotes. One dominant signal emerged: billing complexity and transparency drive most cost-related UX pain. Users struggle to reconcile invoices to usage, discover overruns after month-end, and use calculators that ignore dynamic workloads—leading to surprise spikes. Strategically, users prefer predictable costs over merely lower costs. The advantage is cost-experience design (real-time transparency, proactive controls, behavior-aware forecasting) rather than discounts alone. Platform “flavors” vary (e.g., BigQuery pricing confusion, Snowflake credit visibility, Databricks cluster trade-offs, Splunk ingestion spikes, Redshift monitoring blind spots), but the design response is consistent: plain-English cost impact at point of action, pre-threshold alerts, safe throttles, and workload-aware forecasting.\nExample 2: Tier-2 SOC Analyst Friction When we analyzed discussions from cybersecurity forums and support communities, five universal barriers emerged across different security tools:\nQuery language complexity: Analysts struggle with proprietary search syntaxes that vary across platforms False-positive overload: Too many alerts that turn out to be benign, creating alert fatigue Integration hurdles: Data silos prevent effective correlation between security tools Workflow friction: Constant context switching and manual processes break investigation flow Platform limitations: Analysts spend time troubleshooting tools rather than investigating threats The design mandate became clear: lower technical barriers, suppress noise, preserve investigative context, and streamline common workflows so analysts can focus on actual threats rather than tool mechanics. The most impactful solutions involved agentic experiences that automate routine tasks and stronger default configurations that require less customization.\nCrowdlistening\u0026rsquo;s main interface provides a clean entry point for product teams to analyze community discussions, with options for targeted discovery, open-ended exploration, and always-on insight monitoring.\nThe analysis workspace combines semantic search, clustering algorithms, and LLM reasoning to surface patterns in customer discourse while maintaining traceability back to original sources.\nCrowdlistening As shown in the cost-estimation agent presentation, Crowdlistening is a tool I built to extract patterns from collective discourse without flattening individual voices. It pairs LLM reasoning with a larger-context pipeline to ingest public discussions, structure them, and tie findings back to evidence. At its core, Crowdlistening treats raw discourse as primary data, emphasizes traceability (“show your work”), and optimizes for original insight over derivative summaries.\nCrowdlistening\u0026rsquo;s goal isn\u0026rsquo;t forced consensus; it\u0026rsquo;s to surface authentic needs, native customer language, and edge cases that formal channels miss. At enterprise scale—where the user base is large and diverse—listening broadly helps us prioritize what\u0026rsquo;s real over what\u0026rsquo;s merely loud. (More background at Crowdlistening.com.)\nSince the launch of MCPs, I\u0026rsquo;ve experimented with exposing Crowdlistening capabilities as MCP servers—directly accessible in clients like Copilot or Claude. Features remain similar (with some visualization limits), but inputs become more nuanced and multi-turn, making the experience far more intuitive for non-technical users.\nBuilding This at Large Organizations A Feature Proposal To enable the market → product workflow at enterprise scale, we could develop a Copilot MCP integration that serves as a conversational guide for early specification writing. This tool would fundamentally change how product requirements are generated by starting with customer evidence rather than internal assumptions.\nThe system would ingest customer meeting transcripts alongside curated online discussions, applying the same semantic analysis techniques to identify patterns across both formal and informal feedback channels. It would run evidence-linked synthesis with clear citations to primary sources, ensuring that every product decision can trace back to specific customer voices. Most importantly, it would automatically generate the theme → hypothesis → bet → telemetry → refinement scaffolds that slot directly into product specifications, making the customer-driven development process systematic rather than ad-hoc.\nGovernance and Guardrails Social data presents unique challenges around privacy, bias, and accuracy that require systematic safeguards. Our approach emphasizes ethical data collection through strict sourcing practices that respect user consent, aggressive deduplication to prevent over-weighting vocal users, and zero tolerance for capturing personally identifiable information. The goal is to understand collective patterns without compromising individual privacy.\nTo combat sampling bias—a critical risk when community discussions may not represent your full user base—we actively measure representativeness across demographics, use cases, and engagement levels. Every synthesized claim maintains auditable links back to source material, implementing a \u0026ldquo;show your work\u0026rdquo; principle that allows stakeholders to verify the evidence behind recommendations. Finally, we use MCP-based connectors to ensure data processing pipelines remain inspectable and secure, with clear governance over what data flows where and how it\u0026rsquo;s transformed.\nClosing Thought AI-enabled social listening doesn’t replace customer calls, design research, or telemetry; it enriches them—especially at the fuzzy front end. Used well, it helps us choose better problems, write crisper specs, and ship experiences that feel obvious in hindsight.\n","permalink":"https://chenterry.com/posts/need_validation/","summary":"\u003ch2 id=\"why-we-need-more-ways-to-hear-customers\"\u003eWhy We Need More Ways to Hear Customers\u003c/h2\u003e\n\u003cp\u003eYour last customer call was three weeks ago. Your PM dashboard shows green metrics. Yet your latest feature has a 12% adoption rate, and the support tickets keep growing. Sound familiar?\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s the trap every growing product team falls into: you start with great customer connections, but success creates distance. Suddenly you\u0026rsquo;re building for your five power users while 95% of your market stays silent. The result? Products that feel over-engineered to newcomers and underwhelming to everyone else. We end up in what I call the \u0026ldquo;vocal minority trap\u0026rdquo;—building for the loudest voices while missing authentic needs from the broader market.\u003c/p\u003e","title":"Social Listening for Product Insight"},{"content":"Understanding the Security Data Lake and SIEM Business Work in progress for understanding the security data lake and SIEM business.\nDefining the Business The Security Information and Event Management (SIEM) and data lake business centers on platforms that collect, store, analyze, and correlate security telemetry to detect threats, ensure compliance, and facilitate response. SIEMs focus on real-time alerting and investigation, while data lakes provide scalable, cost-effective storage for raw data, enabling advanced analytics and long-term retention. This solves escalating problems: exploding data volumes from cloud/IoT/tools (e.g., 90% of orgs use 40+ security tools), unsustainable SIEM costs (ingestion-based pricing), format inconsistencies impeding correlation, and regulatory needs for auditable logs (e.g., SEC/GDPR). Efficiency gains come via preprocessing (filtering 40-65% noise, normalizing to OCSF), enrichment (threat intel), and tiered routing, cutting MTTD/MTTR and costs. The market evolves from monolithic SIEMs to modular architectures with Security Data Pipeline Platforms (SDPPs) as intelligent layers, projected at $10.78B in 2025, growing to $19.13B by 2030 (12.16% CAGR), fueled by AI adoption and cloud shifts.\nKey Players \u0026amp; Competitive Landscape The landscape pits legacy SIEM vendors against innovative SDPPs and data lake specialists, with convergence blurring lines. Microsoft (Sentinel) leads in cloud-native growth, Datadog bridges observability-security, Databricks powers analytics-heavy lakes, Cribl dominates pipelines ($200M+ ARR), and Wiz (post-Google $32B acquisition) bolsters cloud security integrations. AI adoption accelerates, with 43% of orgs centralizing data strategies for ML-driven insights.\nPlayer Product Offerings Differentiation Market Position \u0026amp; Evolution Microsoft (Sentinel) Cloud SIEM; data connectors, ML analytics, Copilot for Security; integrates with Azure lakes. AI-powered threat hunting, multi-tenant management; updates in 2025 include enhanced visibility, AI insights for intel. Cloud leader; evolving to AI-SOC hub, 60%+ Fortune 500 adoption; partnerships boost education/training. Datadog Cloud SIEM, Observability Pipelines; log management, threat detection. Unified sec/ops; AI parsing/quota mgmt; 2025 updates: Code Security, data protection enhancements. Observability-security convergence; SIEM migration aid; strong in DevSecOps. Databricks Lakehouse Platform; Unity Catalog for governance, Delta Lake for storage. AI-driven analytics; 2025: serverless multicloud security, cybersecurity lakehouse for threats (e.g., State Street use). Data intelligence leader; evolving for sec lakes, 100+ use cases including AI risk mitigation. Cribl Stream/Edge/Search/Lake; data routing, reduction, lakehouse. Vendor-agnostic; AI copilot; 2025: tiered storage, SIEM integration (e.g., CrowdStrike Falcon). SDPP pioneer; $200M+ ARR; enables migrations, next-gen SIEM evolution. Wiz (Google) CNAPP; cloud security scanning, risk prioritization. Post-$32B acquisition: Enhances Google Cloud sec; integrates with lakes/SIEMs for vuln mgmt. Cloud sec disruptor; bolsters Google\u0026rsquo;s CNAPP, impacts multicloud strategies. Splunk (Cisco) Enterprise Security; federated search, data mgmt. Hybrid support; deep analytics. Legacy leader; evolving with pipelines for cost control. Elastic ELK Stack; data tiering, search. Open-source scalability. Versatile; lakehouse convergence. Abstract Security Streaming analytics; AI enrichment. Real-time detection; no-code UI. Emerging; SOC efficiency focus. Anomali Cloud SIEM + pipeline; threat intel. Converged TIP/SIEM; AI copilot. Migration ease; intel-driven. Stellar Cyber Open XDR + SDPP; multi-layer AI. Unified SecOps; mid-market. Integrated platform; agentic AI. The Technology \u0026amp; Strategy Tech includes log aggregation, ML anomaly detection, and scalable lakes (e.g., S3/Snowflake with Athena queries). Strategies shift to modular SIEMs (decoupling storage/analytics), SDPP preprocessing (filtering 80%+, OCSF normalization), and AI adoption (43% centralized data for ML; copilots like Sentinel\u0026rsquo;s for queries). Serves efficiently by enabling real-time streaming, cutting costs 50%+, speeding MTTR to minutes via agentic AI. Future: AI data engineers automating parsing/enrichment, data fabrics unifying layers, observability-sec convergence.\nFinding the Edge Differentiation: Microsoft excels in ecosystem integration/AI (Copilot boosts hunting); Datadog unifies sec/ops with pipelines (50%+ savings, AI parsing); Databricks leverages lakehouses for AI analytics (serverless sec, threat products); Cribl leads SDPP with tiered storage/SIEM evo (migrations in weeks); Wiz enhances CNAPP post-acquisition (Google Cloud sec, multicloud risk). Edges from AI copilots (natural queries), agentic systems (auto-optimization), hybrid support. Field heads to AI-SOCs (MTTR minutes), fabrics, convergence.\nReferences: Software Analyst: Market Guide 2025: The Rise of Security Data Pipelines - Market Guide 2025: The Rise of Security Data Pipelines \u0026amp; How SIEMs Must Evolve Mordor Intelligence: SIEM Market Analysis - Security Information and Event Management Market Size \u0026amp; Share Analysis IDC: Worldwide SIEM Forecast - Worldwide Security Information and Event Management Forecast, 2025–2029 Expert Insights: SIEM Market Overview 2025 - SIEM Market Overview: Key Stats And Insights For 2025 Detection at Scale: Transition from Monolithic SIEMs - The Transition from Monolithic SIEMs to Data Lakes for Security Analytics Omdia: Cybersecurity Data Fabrics 2025 - Market Landscape: Cybersecurity Data Fabrics 2025 SentinelOne: Data Lake Solutions - Singularity™ Data Lake overview Cribl: RSAC 2025 Insights - Five Non-Obvious Insights Shaping IT and Security from RSAC 2025 Contrary Research: Cribl Business Breakdown - Cribl Business Breakdown \u0026amp; Founding Story Microsoft: Sentinel Updates - What\u0026rsquo;s new in Microsoft Sentinel Datadog: DASH 2025 Features - DASH 2025: Guide to Datadog\u0026rsquo;s newest announcements Databricks: Security and Compliance Updates - What\u0026rsquo;s new in security and compliance at Data + AI Summit 2025 Wiz: Google Acquisition Analysis - Analysis: No Matter How Google Deal Turns Out, Wiz Wins Appendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/posts/ai-powered-security-data-pipelines/","summary":"\u003ch2 id=\"understanding-the-security-data-lake-and-siem-business\"\u003eUnderstanding the Security Data Lake and SIEM Business\u003c/h2\u003e\n\u003cp\u003eWork in progress for understanding the security data lake and SIEM business.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eThe Security Information and Event Management (SIEM) and data lake business centers on platforms that collect, store, analyze, and correlate security telemetry to detect threats, ensure compliance, and facilitate response. SIEMs focus on real-time alerting and investigation, while data lakes provide scalable, cost-effective storage for raw data, enabling advanced analytics and long-term retention. This solves escalating problems: exploding data volumes from cloud/IoT/tools (e.g., 90% of orgs use 40+ security tools), unsustainable SIEM costs (ingestion-based pricing), format inconsistencies impeding correlation, and regulatory needs for auditable logs (e.g., SEC/GDPR). Efficiency gains come via preprocessing (filtering 40-65% noise, normalizing to OCSF), enrichment (threat intel), and tiered routing, cutting MTTD/MTTR and costs. The market evolves from monolithic SIEMs to modular architectures with Security Data Pipeline Platforms (SDPPs) as intelligent layers, projected at $10.78B in 2025, growing to $19.13B by 2030 (12.16% CAGR), fueled by AI adoption and cloud shifts.\u003c/p\u003e","title":"AI-Powered Security Data Pipelines: The Future of Enterprise Cybersecurity"},{"content":"Agentic Workforce Our current rate of adoption for agentic workforces has significant room for improvement. AI coding is mainly for developers, but the true value unlock is when everyday people can integrate entire workflows (think assembly lines for repetitive work). All the work that one can conceive of how to do but needs to sit through should be delegated.\nDefining the Business An agentic workforce involves autonomous AI agents—systems that reason, plan, act, learn, and adapt—to handle complex tasks and workflows, augmenting or replacing human labor in repetitive or decision-heavy roles. This business solves inefficiencies in traditional work structures, such as high labor costs, error-prone manual processes, and scalability limits, by deploying AI agents that operate as \u0026ldquo;digital teammates\u0026rdquo; for tasks like data analysis, customer service, and automation. Efficiency is achieved through hyperautomation (e.g., 30% productivity gains), personalized experiences, and reduced MTTR in operations, with adoption projected to jump 327% by 2027. The market, part of broader AI, sees agentic AI driving $4.4T in value, but faces challenges like 40% project cancellations by 2027 due to costs and risks.\nKey Players \u0026amp; Competitive Landscape The landscape features AI leaders building agentic tools, with $33.9B in GenAI investments (2024-2025) and acquisitions like Capgemini-WNS ($3.4B) for agentic ops. Startups like Gradient Labs ($13M) target regulated sectors. Competition focuses on enterprise vs. consumer, with stocks like UiPath, NVIDIA rising 20-50% on agentic bets.\nPlayer Key Offerings Differentiation Investments/Acquisitions OpenAI GPT agents; o1 model for reasoning. Advanced reasoning; agentic frameworks for workflows. $157B valuation; io Products acquisition for hardware. Microsoft Copilot agents in Dynamics/365; Azure AI Studio. Enterprise integration; hybrid human-AI decisions. $1.3B AI; OpenAI partnership. Google Gemini agents; Project Astra. Decision intelligence; Android ecosystem. $75B data centers; AI acquisitions. Anthropic Claude for agentic tasks; constitutional AI. Ethical alignment; safe automation. $61.5B valuation; Amazon investments. UiPath RPA with agentic AI for processes. Hyperautomation; workflow orchestration. Stock focus; partnerships. Gradient Labs Agentic AI for customer support in regulated industries. Compliance-focused; reskilling integration. $13M raised (Monzo alums). The Technology \u0026amp; Strategy Tech: Agentic AI uses LLMs with tools/memory for autonomous actions (e.g., reasoning/planning in o1 models); multi-agent systems coordinate tasks. Strategies: Hybrid workforces (AI-human collaboration), governance frameworks; 2025 trends: Reasoning models, MoE, synthetic data. AI adoption: 70% orgs operationalize by 2025; productivity +30%, but 40% cancellations.\nFinding the Edge Edges: Ethical AI (Anthropic), enterprise scale (Microsoft), reasoning (OpenAI). Field heads to cognitive enterprises, hybrid workforces; investments like RSM\u0026rsquo;s $1B signal maturity. Differentiation via data governance, multi-agent orchestration.\nPrototyping \u0026amp; Explorations Prototypes: Multi-agent systems (Chain-of-Agents); explorations: AI data engineers, agentic L\u0026amp;D for upskilling. VC memos: Focus on agentic for ROI, but caution costs.\nRemaining Questions How will agentic AI reshape traditional job roles and responsibilities? Can organizations effectively manage the transition to hybrid human-AI workforces? What regulatory frameworks are needed for autonomous AI agents in the workplace? References: A2A Catalog: Agentic AI Tools Directory - Comprehensive directory of agentic AI tools and platforms McKinsey: AI in the Workplace - AI\u0026rsquo;s impact on workplace productivity and agentic systems Gartner: Agentic AI Trends - Agentic AI market trends and adoption forecasts Forrester: Workforce Automation - Automation platforms and agentic workforce solutions IDC: AI Workforce Market - AI workforce market analysis and projections Appendix This post has been pre-processed to remove potentially sensitive information concerning specific companies. For further clarification or discussion, please reach out to terrychen2026@u.northwestern.edu.\n","permalink":"https://chenterry.com/archived/agentic_workforce/","summary":"\u003ch2 id=\"agentic-workforce\"\u003eAgentic Workforce\u003c/h2\u003e\n\u003cp\u003eOur current rate of adoption for agentic workforces has significant room for improvement. AI coding is mainly for developers, but the true value unlock is when everyday people can integrate entire workflows (think assembly lines for repetitive work). All the work that one can conceive of how to do but needs to sit through should be delegated.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eAn agentic workforce involves autonomous AI agents—systems that reason, plan, act, learn, and adapt—to handle complex tasks and workflows, augmenting or replacing human labor in repetitive or decision-heavy roles. This business solves inefficiencies in traditional work structures, such as high labor costs, error-prone manual processes, and scalability limits, by deploying AI agents that operate as \u0026ldquo;digital teammates\u0026rdquo; for tasks like data analysis, customer service, and automation. Efficiency is achieved through hyperautomation (e.g., 30% productivity gains), personalized experiences, and reduced MTTR in operations, with adoption projected to jump 327% by 2027. The market, part of broader AI, sees agentic AI driving $4.4T in value, but faces challenges like 40% project cancellations by 2027 due to costs and risks.\u003c/p\u003e","title":"Human-Mediated Agentic Workflows"},{"content":"Understanding the Business of Search Ads Work in progress for understanding the search ads business.\nDefining the Business Search ads appear on Search Engine Results Pages (SERPs) for keyword queries, part of Pay-Per-Click (PPC) marketing. Ads display based on bids, at top/bottom of results or alongside organics. This auction model uses bid, quality, and relevance for placement. It connects advertisers to high-intent users, charging per click for engagement-based efficiency.\nAdvertisements Text ads include headlines, descriptions, site links; extensions add calls or locations. Formats like shopping ads feature images/prices. Revenue efficiency uses Click-Through Rate (CTR) (impressions to clicks), Cost Per Click (CPC) (cost per click), Return on Ad Spend (ROAS) (revenue/ad spend). High CTR (e.g., 6% in dating) shows relevance; ROAS \u0026gt;4:1 signals e-commerce success. Metrics guide optimization for lower costs, higher conversions.\n","permalink":"https://chenterry.com/archived/search_advertising/","summary":"\u003ch2 id=\"understanding-the-business-of-search-ads\"\u003eUnderstanding the Business of Search Ads\u003c/h2\u003e\n\u003cp\u003eWork in progress for understanding the search ads business.\u003c/p\u003e\n\u003ch3 id=\"defining-the-business\"\u003eDefining the Business\u003c/h3\u003e\n\u003cp\u003eSearch ads appear on Search Engine Results Pages (SERPs) for keyword queries, part of Pay-Per-Click (PPC) marketing. Ads display based on bids, at top/bottom of results or alongside organics. This auction model uses bid, quality, and relevance for placement. It connects advertisers to high-intent users, charging per click for engagement-based efficiency.\u003c/p\u003e\n\u003ch4 id=\"advertisements\"\u003eAdvertisements\u003c/h4\u003e\n\u003cp\u003eText ads include headlines, descriptions, site links; extensions add calls or locations. Formats like shopping ads feature images/prices. Revenue efficiency uses Click-Through Rate (CTR) (impressions to clicks), Cost Per Click (CPC) (cost per click), Return on Ad Spend (ROAS) (revenue/ad spend). High CTR (e.g., 6% in dating) shows relevance; ROAS \u0026gt;4:1 signals e-commerce success. Metrics guide optimization for lower costs, higher conversions.\u003c/p\u003e","title":"Search Advertisement"},{"content":"Opportunity Costs It\u0026rsquo;s never easy to discover that a product you\u0026rsquo;ve poured your heart, sweat, and tears into isn\u0026rsquo;t working out. Startups operate in constant ambiguity, and sometimes you can\u0026rsquo;t see light at the end of the tunnel after toiling away for what feels like an eternity. Sometimes there simply is no light.\nI\u0026rsquo;ve heard the phrase \u0026ldquo;Take more market risk when you are young, and more execution risk when you are older.\u0026rdquo; As I understand it, this suggests that people early in their careers should bet on markets and opportunities, even contrarian ones. I\u0026rsquo;m reflecting on this because I\u0026rsquo;ve been thinking deeply about how to best allocate my time and energy on the most promising projects. This isn\u0026rsquo;t about diversifying—I recognize my limited attention span, and pursuing everything simultaneously leads to burnout and mediocre results. Hence this post: an attempt to provide clarity.\nThe first sunsetted products - Cogno (Multi-agent Sales Assistant) and Marrrket (AI Enabled Secondhand Marketplace)\nAs AI tools mature, I\u0026rsquo;ve accelerated my shipping velocity dramatically. However, speed doesn\u0026rsquo;t guarantee success. Over the past two years, I\u0026rsquo;ve launched five products: Cogno (Cognogpt.com), Marrrket (Marrrket.com), Crowdlistening (Crowdlistening.com), and A2A Catalog (a2acatalog.com). I also helped ship three 0-1 products: Symphony Assistant (https://ads.tiktok.com/business/copilot/standalone), Insights Spotlight (https://ads.tiktok.com/business/en-US/blog/insights-spotlight-trends-tool), and Aibrary (aibrary.ai). Among these products, some worked, while some didn\u0026rsquo;t. Some worked, others didn\u0026rsquo;t. For transparency, I\u0026rsquo;ll focus only on projects I directly led.\nMarketing matters enormously, but converting traffic into revenue remains the critical challenge. Amid AI hype, one differentiation strategy involves controversy and viral marketing, as demonstrated by Cluely. While we await that outcome, Crowdtest.ai offers a retrospective case study. Founded by a freshman with significant Twitter following, Crowdtest.ai claimed $30k revenue within 24 hours of launch (with generous refund policies). Despite initial buzz, it maintained only ~1,500 MAU two months post-launch, with declining metrics since. This wasn\u0026rsquo;t success—the value proposition remained unclear. Who pays $100 to optimize Twitter posts? Does AI genuinely outperform human intuition here?\nCrowdtest.ai - A case study in viral marketing\nI replicated this approach for Crowdlistening\u0026rsquo;s predict feature, inspired by a16z\u0026rsquo;s social simulation investment thesis. The concept of agent-based testing grounds for ideas seemed compelling, but too many hypotheses remained unvalidated. Agents likely represent generalizations of language (as do human crowds), and the willingness-to-pay question persisted. While not my proudest feature launch, it provided valuable learning. Key lessons: build genuine Twitter following and scrutinize marketing claims—data reveals truth.\nEarly Catalog Products The \u0026ldquo;app store for everything\u0026rdquo; phenomenon continues proliferating. FlowGPT exemplifies 2022 success: an early Discord community that evolved into a digital prompt catalog, now perhaps hosting agent experiences. They maintain over two million monthly active users—an impressive achievement. Timing proved crucial: right product, early market entry, sufficient success factors (though monetization remains unclear). Building similar products today faces significantly higher competition given ChatGPT\u0026rsquo;s November 2022 launch impact.\nWebsite traffic of FlowGPT (June 2025)\nThe A2A Agents catalog concept originated from domain name exploration. I sought high-value MCP-related domains, assuming their long-term value, but discovered all premium options were taken. A2A Agents seemed the logical alternative. My reasoning: (1) .com domains retain maximum value, (2) I could compile the web\u0026rsquo;s most comprehensive A2A Agents directory, and (3) adding MCP Servers would prove easier than adding A2A Agents, given the agent-tool hierarchy. These hypotheses largely hold true, though I underestimated competitive scale and pace. I\u0026rsquo;ve observed at least five competing catalog websites that, despite lacking premium domain names, have compiled impressive directories.\nThis remains an active project, but I\u0026rsquo;ll maintain complete transparency about competitive dynamics. Revisiting my three assumptions: (1) While a2acatalog.com offers excellent branding, domain alone cannot capture and retain users—aggressive SEO optimization and content marketing remain essential (areas where I\u0026rsquo;m still developing expertise). Alternative channels like TikTok, Reddit, and Twitter seem suboptimal for non-viral content. (2) I can efficiently compile agent and server lists, but so can competitors. If sufficient profit exists, competitors will inevitably crawl catalogs and replicate content. Differentiation must focus on user experience—either simplified deployment for non-technical users or hyper-personalization for technical ones. (3) While I can integrate MCP servers, it\u0026rsquo;s premature to establish lasting moats, connecting to point two.\nMCP Server integration example\nI\u0026rsquo;m curious about catalog product differentiation strategies. I\u0026rsquo;ve observed websites with 150k+ traffic, though revenue models appear suboptimal. For a2acatalog.com and future products, my goal is either maximum reach or niche vertical dominance enabling consistent revenue streams. The latter requires entering verticals with marketing potential—not general-purpose platforms, but solutions addressing concrete user needs. For maximum reach, significant progress remains necessary.\nGeographic distribution of catalog users\nWork in progress, thoughts in process. (6.25) Analyzing sunsetted products reveals a pattern: nearly all failed due to customer acquisition challenges. This insight proves illuminating. Successful product launches demonstrate that the best product doesn\u0026rsquo;t always win—distribution channels do. Perhaps I should prioritize developing distribution channels, using products as traffic capture mechanisms.\nEarly Products in General Recently surveying agent products reveals persistent patterns—multi-agent systems for various applications, agentic experiences across domains. As markets have evolved from viewing agents as chatbots (2022) to potential employees, I question whether I should have persisted with certain projects and how to rapidly test ideas for advancing domain understanding.\n[ Reusing a Previous Blog Post] Reflecting on 2024 (and 2023 for the context of Cogno), among my list of failed projects, very few failed due to lack of innovation. Since I began working with LLMs in fall 2022, there has been an abundance of interesting GenAI technologies to experiment with. It started with “domain specific prompting/finetuning” and data flywheels (thou not even now does anyone know what this looks like in action). By spring 2023, the focus shifted to LLMs as agents, exemplified by the Generative Agents paper, Microsoft AutoGen, and a few opensource projects like MetaGPT. At Cogno, we also built multi-agent systems, integrating various function calling features and agent collaboration for complex task reasoning. Everyone built, few created value (Glean focused on enterprise search, while Moveworks created value through api actions, neither of which I believe agents to have mattered). Founders encouraged each other’s enthusiasm, while investors rushed to learn the latest buzzwords in LLM technology (‘prompt engineering’ and ‘function calls’ sounded less sexy compared to’agents’).\nBeing first to market rarely matters - people won’t remember you. What matters is creating defensible moats or developing critical elements that lead to unfair advantages. While Google’s technology investment in Android can be considered ’not just building a moat, but scorching the earth for 250 miles around the castle,’ most companies’ self-described technological differentiation is merely self-flattery and a feeble attempt to impress tech-enthusiast investors. Technology truly matters only when it can solve seemingly insurmountable challenges or optimize costs and operations. In every other situation, the focus should be on building sustainable advantages that ensure long-term survival. [Thoughts WIP]\nThoughts Going Forward I\u0026rsquo;m intrigued by results-based monetization models. We began with tokens (chat products)—measuring LLM word output—then shifted to conversations (Copilot) as intent interpretation and generation quality improved. Next evolution: charging based on results? Exa and Sierra are already proposing this, though significant improvements remain necessary. What happens when heavily filtered queries return no results? Extensive compute goes uncompensated—is this sustainable? Similarly, for MCPs and A2As, when users engage with agents through third-party clients, who receives payment? An interesting market system awaits development here.\n","permalink":"https://chenterry.com/posts/when-to-sunset-a-product/","summary":"\u003ch2 id=\"opportunity-costs\"\u003eOpportunity Costs\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s never easy to discover that a product you\u0026rsquo;ve poured your heart, sweat, and tears into isn\u0026rsquo;t working out. Startups operate in constant ambiguity, and sometimes you can\u0026rsquo;t see light at the end of the tunnel after toiling away for what feels like an eternity. Sometimes there simply is no light.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve heard the phrase \u0026ldquo;Take more market risk when you are young, and more execution risk when you are older.\u0026rdquo; As I understand it, this suggests that people early in their careers should bet on markets and opportunities, even contrarian ones. I\u0026rsquo;m reflecting on this because I\u0026rsquo;ve been thinking deeply about how to best allocate my time and energy on the most promising projects. This isn\u0026rsquo;t about diversifying—I recognize my limited attention span, and pursuing everything simultaneously leads to burnout and mediocre results. Hence this post: an attempt to provide clarity.\u003c/p\u003e","title":"When to Sunset a Product: A Decision Framework for Entrepreneurs"},{"content":"Every now and then I come across some article or discussion that just feels plain and mundane. All the words seem to make sense, yet at the same time, they feel almost predictable. Despite how well articulated these ideas were - be it in carefully formatted slide decks or confidently delivered proses - they fail to amaze. Ever since November of 2022, the ability to articulate words cohesively (I\u0026rsquo;m purposefully not using the word coherently) has become table stakes. In a society where frankly most work is evaluated on completion and length, LLMs have led to a rapid advancement of productivity. Yet I think we should make certain clarifications here - productivity gain is in automating repetitive and redundant tasks, this does not apply to all tasks, in fact, using GPT for sophisticated reasoning is almost guarenteed to produce mediocore results.\nLet\u0026rsquo;s admit it, a big chunck of the work we do everyday some one else can do. The tedius, repetitive, and standard operating procedure tasks don\u0026rsquo;t require drastic innovation, they just need a criteria to be evaluated on and human hours, lots of it. This is work that AI could automate. However, an issue I\u0026rsquo;ve been seeing recently is people using AI as a catch all for tasks that should involve a level of reasoning and for a lack of better word, taste. Product managers go asking LLMs for user pain points, product features, and even feedback for products. However, the thing to note here is that a LLM is probablistic - it\u0026rsquo;s trained on generalization - when you build for all, you build for none. This is why I caution my self and take a step back each time an LLM produces a lengthy blob of text that I don\u0026rsquo;t see obvious issues with through the first run - do I have enough knowledge in the field to have good taste?\nThis to me is a fascinating topic. Although I have some ideas of how to best use LLMs. I now ask my self to read more before formulating a response. At worse, this would be developing enough text corpus to develop probablistic predictions. At best, I\u0026rsquo;d even be able to reason and build on some good ideas and push the field a bit further. Here are some of the papers / books I plan to be reading in the coming weeks:\nMagic Link - Information Software and the Graphical Interface by Bret Victor Man-Computer Symbiosis by J.C.R Licklider Augmenting Human Intellect: A Conceptual Framework by D.C. Engelbart. 6.25 Something I\u0026rsquo;ve just recently started to acknowledge (I\u0026rsquo;ve heard this repeated many times, but it sort of just sunk in today) is that we are actually fast approaching a period where ai moves beyond the traditional \u0026ldquo;copilot\u0026rdquo; and human-in-the-loop dynmaic, moving to fully autonomous teams capable to aligning goals and executing multi-step tasks over a long horizon. This would mean a dramatic shift in our workforce, where we would actually have a significant portion of workers be non-human entities. The industrial revolution and rise of the internet has led to specialization in the work we do: for internet products, we\u0026rsquo;d have software engineers (Research, QA, ML) , PMs, Designers, Marketers, but I see the line to become increasingly blurred as we progress.\n","permalink":"https://chenterry.com/posts/poor_thinking/","summary":"\u003cp\u003eEvery now and then I come across some article or discussion that just feels plain and mundane. All the words seem to make sense, yet at the same time, they feel almost predictable. Despite how well articulated these ideas were - be it in carefully formatted slide decks or confidently delivered proses - they fail to amaze. Ever since November of 2022, the ability to articulate words cohesively (I\u0026rsquo;m purposefully not using the word coherently) has become table stakes. In a society where frankly most work is evaluated on completion and length, LLMs have led to a rapid advancement of productivity. Yet I think we should make certain clarifications here - productivity gain is in automating repetitive and redundant tasks, this does not apply to all tasks, in fact, using GPT for sophisticated reasoning is almost guarenteed to produce mediocore results.\u003c/p\u003e","title":"Using LLMs as a Cover up for Poor Thinking"},{"content":"Introduction We live in an era of unprecedented access to information. The web contains almost all the knowledge needed to complete virtually any task, yet many of us still struggle to learn effectively. Our ability to ask the right questions has become the limiting factor in unlocking knowledge acquisition. This fundamental shift is transforming how we learn, build expertise, and might revolutionize education itself.\nThe Traditional Knowledge Landscape Historically, human conversations have been the default method of acquiring knowledge. We seek out doctors for medical advice, mechanics for car problems, and teachers for academic subjects. These experts are valuable not just for their knowledge, but for their ability to understand questions we may not be able to formulate ourselves.\nWhat makes human-to-human teaching so effective is an expert\u0026rsquo;s ability to address our \u0026ldquo;unknown unknowns.\u0026rdquo; A good teacher has seen countless students facing similar challenges and can explain concepts at the appropriate level of understanding. They can conceptualize and respond to gaps in knowledge that students themselves might not recognize.\nThe Changing Nature of Expertise The traditional path to expertise has been structured and comprehensive. School builds foundational knowledge across multiple disciplines—calculus, linear algebra, and other fundamentals that eventually lead to specialized topics like machine learning or language models.\nHowever, a new paradigm is emerging. With language models (and the theoretical ability to ask perfect questions), one could develop targeted slices of knowledge directly related to specific tasks—whether building an electric boat, a rocket, or any other complex project. This approach enables a more direct path to practical knowledge.\nSchool still offers invaluable benefits beyond the curriculum itself. It creates structure, provides community, and compels us to explore topics we might otherwise avoid. It exposes us to applications we wouldn\u0026rsquo;t have discovered independently. But the rigid structure has limitations in an age of personalized learning.\nHistorical Barriers to Self-Directed Learning What has prevented us from simply learning everything we need from the web? Several key barriers have existed:\nInstitutional confinement: Knowledge was traditionally locked within institutions, requiring physical presence to access resources and expertise.\nContent variability: Even with open courseware, YouTube, and similar platforms, not every lecture or video guarantees the information you specifically need.\nDifficulty calibration: Content creators don\u0026rsquo;t know your expertise level. When material underestimates your knowledge, it becomes boring. When it overestimates your background, you risk getting lost and losing motivation.\nThe fundamental question becomes: Can we design systems that meet learners where they are?\nThe Potential of Question-Driven Learning Language models have compiled vast amounts of knowledge (excluding proprietary company data). However, this knowledge isn\u0026rsquo;t truly at our fingertips because:\nWe don\u0026rsquo;t always know what to ask Systems aren\u0026rsquo;t familiar enough with us as individuals to present information in easily digestible ways Theoretically, driven by precisely calibrated questions with adequate depth and breadth, we could achieve the quickest mastery of the minimal knowledge needed to complete any task. The ability to integrate this capability into our learning process will differentiate those who can ride this wave of knowledge transformation.\nConclusion This transformation creates tremendous opportunities for both individuals and companies. For learners, developing the skill to ask excellent questions becomes as valuable as the knowledge itself. For companies, there\u0026rsquo;s an opportunity to build products that facilitate this question-driven, personalized learning approach.\nAs we move forward, the limiting factor in knowledge acquisition will increasingly be our ability to ask the right questions rather than access to information itself. Those who master the art of asking will unlock potential beyond what traditional educational systems could provide.\n","permalink":"https://chenterry.com/posts/questions-new-bottleneck-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe live in an era of unprecedented access to information. The web contains almost all the knowledge needed to complete virtually any task, yet many of us still struggle to learn effectively. Our ability to ask the right questions has become the limiting factor in unlocking knowledge acquisition. This fundamental shift is transforming how we learn, build expertise, and might revolutionize education itself.\u003c/p\u003e\n\u003ch2 id=\"the-traditional-knowledge-landscape\"\u003eThe Traditional Knowledge Landscape\u003c/h2\u003e\n\u003cp\u003eHistorically, human conversations have been the default method of acquiring knowledge. We seek out doctors for medical advice, mechanics for car problems, and teachers for academic subjects. These experts are valuable not just for their knowledge, but for their ability to understand questions we may not be able to formulate ourselves.\u003c/p\u003e","title":"Questions as the New Bottleneck in Learning"},{"content":"Every builder I know checks the comments.\nNot because they trust them, but because they can’t help it. Beneath the noise lives a signal we all chase: proof that what we’re making actually matters to other people. In a world optimized for clicks and curation, the raw pulse of collective opinion has become one of the last places where human meaning leaks through.\nWe like to imagine ourselves as rational, independent thinkers. Yet an invisible social field shapes most of our decisions—what to buy, what to build, what to believe. Even when we resist consensus, we define ourselves in relation to it. Understanding what people think, and why they think it, isn\u0026rsquo;t optional for builders—it\u0026rsquo;s the difference between creating something people actually want and building expensive lessons in humility.\nThe Comfort and Context of the Crowd There’s wisdom in crowds, but not because they always converge on truth. Often, they don’t. Truth, especially early truth, tends to start as a contrarian position. But crowds provide something just as vital: context. They tell us what people are noticing, fearing, celebrating, misunderstanding. They give us the social coordinates to make sense of our own place in the world.\nPsychologists Baumeister and Leary once argued that belongingness is not optional—it’s as fundamental as food and shelter. That drive manifests everywhere: the need to check reviews before buying, the urge to see how others reacted to a film we just watched, the compulsion to refresh the feed after posting something we care about.\nBeing part of a group—even digitally—creates a sense of safety and meaning that\u0026rsquo;s hard to replicate in isolation. This is not mere conformity; it\u0026rsquo;s contextual navigation. Understanding the group helps us understand ourselves.\nThe visible pulse of crowd engagement: Like counts, shares, and comments become the social proof that drives the fundamental human need for belonging. These metrics transform abstract social validation into tangible, quantified feedback that fuels our participation in digital crowds.\nThe Two Broken Mirrors of Modern Information Today’s information ecosystem distorts this instinct in two opposite ways.\nOn one side, curated expertise filters the world into neat conclusions—clarity without nuance. On the other, personalized algorithms mirror our past clicks back to us, promising relevance while quietly narrowing perspective. Scroll through any trending page and you’ll see the split: polished certainty on one side, algorithmic déjà vu on the other. Both claim to know what matters. Neither truly listens.\nWhat\u0026rsquo;s missing is the raw human layer—the messy, unfiltered dialogue that happens in comment sections, forums, Discord servers, and threads. These spaces often turn chaotic, even hostile, but they contain the kind of pattern recognition that emerges when thousands of people react to the same thing—something curated feeds can\u0026rsquo;t replicate.\nZhihu\u0026rsquo;s \u0026ldquo;How to evaluate\u0026rdquo; format demonstrates how structured questioning shapes collective opinion formation. Note how the format itself becomes a lens that influences what aspects of a topic the crowd focuses on.\nAs Palantir\u0026rsquo;s Alex Karp has said, \u0026ldquo;Understanding the collective judgment of imperfect people in uncertain domains is the real frontier of intelligence.\u0026rdquo; He was speaking about intelligence analysis, but the principle applies perfectly to social media—real intelligence, human or artificial, emerges not from isolation but from aggregation with awareness: knowing when to listen, when to weigh, and when to dissent.\nListening at Scale This is the paradox every product builder faces: we need crowd insight, but most tools give us crowd metrics. We get volume without meaning, sentiment without story. What if we could change that?\nThis is where Crowdlistening enters the picture—a platform designed to systematically listen to the internet the way great founders listen to their customers, not by counting mentions, but by mapping meaning.\nTraditional social analytics measure volume, sentiment, reach. But these metrics flatten emotion into numbers. They strip away what makes human expression meaningful: tone, humor, frustration, context. Crowdlisten instead seeks to surface meaning clusters—patterns of how people express emotion, share experience, and assign value.\nChinese social media reveals how platform mechanics shape crowd behavior. This Zhihu discussion explores how social networks are inherently about human-to-human interaction, not just Q\u0026amp;A formats—highlighting the social substrate beneath all crowd intelligence.\nImagine thousands of YouTube comments on a new AI tool. A typical dashboard might tell you it’s “70% positive.” Crowdlistening reveals something deeper:\nthat creators praise its creative control but distrust its data usage,\nthat early adopters joke about “prompt fatigue,”\nthat a sub-community is already remixing it into new workflows.\nThese are not metrics. They’re narratives. And for product builders, narratives are where insight lives.\nFrom Data to Dialogue Listening to the crowd doesn’t mean surrendering to it. It means learning from its distributed intelligence. The crowd may not always be right, but it’s always revealing.\nCrowds show us contradictions between what people say they want and what they actually do. They highlight emotional undercurrents that surveys miss—like frustration masked as humor, or excitement mixed with fear. When interpreted thoughtfully, these signals help teams design with empathy rather than ego.\nThe humility of crowd discourse: \u0026ldquo;Because our knowledge is limited, we can\u0026rsquo;t make accurate judgments.\u0026rdquo; This admission of epistemic humility creates space for collective learning rather than individual certainty.\nPlatform design influences discourse quality. When questions allow diverse perspectives, the hope is that through viewing answers, people find intellectual peace rather than taking sides—a design philosophy for constructive crowd dynamics.\nIn this sense, crowd intelligence is a mirror, not a manual. It reflects the messy, contradictory human condition that every good product ultimately serves. The best builders don’t seek consensus; they seek comprehension. They use the crowd’s collective reflection as raw input for intuition and design judgment.\nThe Moral Dimension of Listening Karp often argues that the purpose of technology isn’t efficiency but preservation of human agency. That idea reframes listening as a moral act. When we listen to the crowd, we’re not just analyzing data; we’re acknowledging dignity. We’re saying that every voice—no matter how chaotic or conflicted—contains a fragment of the human experience worth understanding.\nWhat we call \u0026ldquo;crowd insight\u0026rdquo; is ultimately a question of empathy at scale. It challenges us to believe that what happens when imperfect people judge uncertain situations together can, when interpreted with care, bring us closer to truth than the confidence of a few perfect algorithms.\nThe practical value of crowd wisdom: For unknown domains, crowds help find entry points. For familiar domains, they reveal different problem-solving approaches. The layered relationship between domains mirrors how crowd intelligence operates at different levels of expertise.\nGoogle\u0026rsquo;s \u0026ldquo;knowledge bottles\u0026rdquo; concept: The vision of packaging expert knowledge for on-demand access reflects the tension between curated expertise and crowd-sourced intelligence—both approaches to democratizing knowledge at scale.\nListening as Design The point isn’t to glorify crowds or discredit experts. It’s to remember that listening—to many, not just a few—is an act of humility.\nThe next generation of intelligent systems won’t just analyze the world; they’ll learn to listen to it. And maybe, if we design them well, so will we. Because when we learn to hear the collective heartbeat of humanity—its fears, its hopes, its humor—we don\u0026rsquo;t just build better products. We build better mirrors of ourselves.\nThe meta-discussion of crowd platforms: A user proposes that Zhihu could monetize by charging for \u0026ldquo;How to evaluate\u0026rdquo; questions, recognizing the inherent value in structured crowd inquiry—demonstrating how the crowd can optimize its own intelligence-gathering mechanisms.\nThe anatomy of crowd discourse: This meta-analysis of Zhihu\u0026rsquo;s question patterns shows how format shapes thinking. The comparison to Quora reveals cultural differences in how platforms structure collective intelligence—each format creates different kinds of crowd wisdom.\nCrowd Intelligence in Practice The examples from Chinese social media platforms like Zhihu demonstrate how different cultural contexts shape crowd behavior and intelligence. The structured format of \u0026ldquo;How to evaluate X\u0026rdquo; questions creates a framework for systematic collective analysis, while the meta-discussions about platform design show crowds reflecting on their own intelligence-gathering processes.\nUnderstanding platform incentives: This detailed analysis of user-generated content dynamics shows how platform economics influence the quality and nature of crowd contributions—a critical factor in designing systems that harness collective intelligence effectively.\nThe essence of crowd evaluation distilled: \u0026ldquo;Actually, crowd evaluation.\u0026rdquo; Sometimes the most profound insights come in the simplest forms—this minimalist comment captures the fundamental nature of what we\u0026rsquo;re exploring.\nFrom concept to implementation: The journey from Josh Woodward\u0026rsquo;s conversations about \u0026ldquo;intelligence as a service\u0026rdquo; to practical applications demonstrates how visionary ideas about crowd and artificial intelligence converge into real-world systems that can \u0026ldquo;bottle up the knowledge of experts.\u0026rdquo;\nReferences Alex Karp, Palantir CEO Letters (2023–2024): on collective judgment and moral responsibility in machine intelligence.\nBaumeister, R. F., \u0026amp; Leary, M. R. (1995). “The need to belong: Desire for interpersonal attachments as a fundamental human motivation.” Psychological Bulletin, 117(3), 497–529.\nSurowiecki, J. (2004). The Wisdom of Crowds. Anchor Books.\n","permalink":"https://chenterry.com/posts/crowd_thesis/","summary":"\u003cp\u003eEvery builder I know checks the comments.\u003c/p\u003e\n\u003cp\u003eNot because they trust them, but because they can’t help it. Beneath the noise lives a signal we all chase: proof that what we’re making actually matters to other people. In a world optimized for clicks and curation, the raw pulse of collective opinion has become one of the last places where human meaning leaks through.\u003c/p\u003e\n\u003cp\u003eWe like to imagine ourselves as rational, independent thinkers. Yet an invisible social field shapes most of our decisions—what to buy, what to build, what to believe. Even when we resist consensus, we define ourselves in relation to it. Understanding what people think, and why they think it, isn\u0026rsquo;t optional for builders—it\u0026rsquo;s the difference between creating something people actually want and building expensive lessons in humility.\u003c/p\u003e","title":"Why People Care What Others Think"},{"content":"The Evolution of AI Value The first wave of generative AI focused primarily on content creation - ChatGPT writing articles, Midjourney generating images, essentially replacing traditional production roles. However, as these technologies mature, their greatest value might well shift towards distribution and personalization rather than raw production.\nFrom RSS to Recommender Systems The evolution of content distribution reveals how technology repeatedly transforms information access. RSS (Really Simple Syndication) represented an early attempt to solve content discovery, providing a pull-based system where users subscribed to feeds they cared about.\nPersonal Note: I interned at China Impact Investing Network (CINN), down the road from Huangzhuang, earning 100 RMB daily for translation and RSS-related tasks - work that would now be largely automated by GPT.\nAs content volume exploded, the focus shifted to algorithmic distribution through recommender systems. These attempted to match existing content to user preferences through increasingly sophisticated methods, but still fundamentally operated on a \u0026ldquo;create once, distribute many times\u0026rdquo; model.\nContent Creation vs. Distribution Costs The economics of content have always been defined by the balance between creation and distribution costs, as illustrated in our visualization. Traditional models rely on two fundamentally different approaches:\nProfessional vs. User-Generated Content Economics Traditional PGC platforms invest heavily in upfront content creation ($200-500 per article) while optimizing distribution costs ($0.001 per user). This creates an economic model where high-quality, centrally produced content must reach massive scale to be sustainable. Users remain passive consumers with minimal influence on content direction.\nUGC platforms invert this model by outsourcing creation costs to users while investing primarily in discovery infrastructure. This creates greater diversity but inconsistent quality. Both approaches increasingly allocate resources to distribution rather than creation as competition for attention intensifies.\nThe discovery paradox emerges: as content volume increases, the marginal value of new content approaches zero without effective discovery mechanisms. Users face decision fatigue from too many choices, and the market naturally shifts investment toward discovery rather than production.\nGeneration as Distribution: Personalized Content at Scale AI fundamentally changes this paradigm by collapsing the distinction between production and distribution. When content can be generated at the moment of consumption, personalized for each user, the model shifts from:\nTraditional: Create once → Distribute to millions\nGenerative: Create parameters → Generate millions of variations\nThe dual-axis economics chart reveals this transformation. Traditional content scales efficiently after high initial investment but delivers mediocre value. Generative approaches provide dramatically higher user value but face linearly increasing costs that become prohibitive at scale.\nThe optimal approach emerges through content modularity - recognizing that not everything needs regeneration. By identifying which components provide the most personalization value, hybrid approaches can maintain 80% of personalization benefits at 30% of the cost.\nThe 60/20/20 Rule: Strategic Content Modularity Most knowledge domains contain substantial core material that remains consistent across users. The 60/20/20 rule maximizes efficiency by segmenting content into:\n60% static core content (foundational principles, established facts) 20% cohort-level content (industry examples, cultural contexts) 20% individual personalization (connections to personal experience, learning pace) This approach creates a fundamentally different economic curve that scales more efficiently than traditional content while maintaining most personalization benefits. For a business book distributed to 50,000 professionals, this approach can deliver twice the relevance at the same cost as traditional publishing.\nMulti-modal Personalization: Beyond Text The personalization paradigm extends beyond text to encompass multiple modalities. Content can dynamically transform between formats - text summaries becoming virtual presenter videos, complex topics converting to interactive explanations, news transforming into personalized audio briefings. This multi-modal capability increases generation costs but dramatically enhances engagement and information retention.\nThese cross-modal transformations add approximately 30-40% to generation costs but can increase engagement by 200-300%, creating compelling economics despite the higher production expense. Each user\u0026rsquo;s preferred learning style becomes another dimension for personalization.\nThe User Advocate: Beyond Algorithmic Recommendation The most powerful personalization emerges not from content formatting but from deeper user understanding. The User Advocate concept represents an AI persona that truly comprehends the user\u0026rsquo;s interests, knowledge level, and perspective, then guides content creation accordingly.\nUnlike recommendation systems that rely on sparse signals, the Advocate builds a comprehensive user model through conversation and observation. This enables exploration of \u0026ldquo;unknown unknowns\u0026rdquo; - valuable topics users didn\u0026rsquo;t know to search for. The approach fundamentally changes platform economics by aligning incentives with actual user satisfaction rather than engagement metrics.\nFluid Knowledge and the Future The most significant impact of AI lies not in replacing content creators but in transforming how knowledge flows to individuals. As the internet solved information scarcity, generative AI now solves the problem of relevance through \u0026ldquo;fluid knowledge\u0026rdquo; (知识液化) that adapts perfectly to each person\u0026rsquo;s context.\nIn this emerging paradigm, content becomes transformable across formats, users experience the feeling of being truly understood, and exploration replaces search as the primary discovery model. The User Advocate becomes a critical interface between vast information spaces and human understanding, fundamentally changing our relationship with knowledge acquisition.\n","permalink":"https://chenterry.com/posts/generation_distribution/","summary":"\u003ch2 id=\"the-evolution-of-ai-value\"\u003eThe Evolution of AI Value\u003c/h2\u003e\n\u003cp\u003eThe first wave of generative AI focused primarily on content creation - ChatGPT writing articles, Midjourney generating images, essentially replacing traditional production roles. However, as these technologies mature, their greatest value might well shift towards distribution and personalization rather than raw production.\u003c/p\u003e\n\u003ch2 id=\"from-rss-to-recommender-systems\"\u003eFrom RSS to Recommender Systems\u003c/h2\u003e\n\u003cp\u003eThe evolution of content distribution reveals how technology repeatedly transforms information access. RSS (Really Simple Syndication) represented an early attempt to solve content discovery, providing a pull-based system where users subscribed to feeds they cared about.\u003c/p\u003e","title":"Value Add of AI: Generation as Distribution"},{"content":"SEO Guide: Implementation \u0026amp; Best Practices Table of Contents Search Engine Basics Technical Implementation On-Page Optimization Off-Page Strategies Modern Approaches Analytics \u0026amp; Tools What is SEO? Search Engine Optimization improves website visibility in organic search results. Three core processes determine rankings:\nCrawling Process and Timeframes Search engines discover pages by following links. New websites typically take 4-6 weeks for complete indexing. Factors affecting speed include site structure, server response time, and internal linking. The more efficiently your site is structured, the faster search engines can discover and index your content.\nWebsite builders handle this process differently. Wix automatically submits sitemaps to Google with pages typically indexed within 2-4 weeks. Squarespace generates and submits sitemaps automatically as well, usually getting indexed within 1-3 weeks. WordPress requires manual submission through Search Console or plugins like Yoast SEO. Shopify automatically submits sitemaps but benefits from manual Search Console verification for faster indexing.\nTo accelerate the crawling process, ensure your robots.txt file allows crawlers access to important pages, implement thorough internal linking connecting your important pages, and use Google Search Console to request indexing for priority pages. These techniques create clearer paths for search engine bots to discover and process your content efficiently.\nIndexing Process After crawling, search engines store page information in their index. They primarily index text and HTML elements, plus structured data, image alt text, and metadata. JavaScript content requires special handling for proper indexing, often needing client-side rendering to be properly processed. Search engines analyze both the content itself and its context within your site architecture.\nFor optimal indexing, use keyword-rich title tags that accurately describe page content, create compelling meta descriptions that encourage clicks, structure content with semantic HTML that signals content hierarchy, and provide descriptive image alt text for visual elements. Keeping important content in HTML rather than embedded in JavaScript improves indexing reliability and completeness.\nRanking Factors That Matter Content relevance and quality serve as fundamental ranking signals, with comprehensive content addressing user search intent receiving preference. Search engines analyze how thoroughly content answers likely user questions and provides value beyond basic information. The depth and originality of content directly influence ranking potential across competitive keywords.\nBacklinks from authoritative sources act as trust signals, with quality links from relevant, authoritative sites carrying significant ranking weight. The overall link profile, including diversity of sources and naturalness of acquisition, influences domain authority and ranking capability. Strategic link building focusing on relevance over quantity produces sustainable ranking improvements.\nUser experience metrics, including page speed, mobile-friendliness, and intuitive navigation, increasingly impact rankings through Core Web Vitals and other engagement signals. Search engines monitor how users interact with search results, factoring metrics like bounce rate and time-on-site into ranking decisions. Sites providing seamless, helpful experiences across devices generally outperform those with technical or usability issues.\nOn-page optimization through strategic keyword usage in titles, headings, and content helps search engines understand relevance and topic focus. The intelligent use of schema markup, HTTPS security, and proper crawlability provides additional signals that influence ranking decisions across competitive search landscapes.\nTechnical SEO Implementation Implementing a Proper HTML Structure \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;!-- Critical SEO elements --\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Primary Keyword - Secondary Keyword | Brand Name\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;A compelling 150-160 character description that includes keywords and encourages clicks.\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;https://www.example.com/page-url/\u0026#34;\u0026gt; \u0026lt;!-- Structured Data --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Article\u0026#34;, \u0026#34;headline\u0026#34;: \u0026#34;Article Title\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Author Name\u0026#34; }, \u0026#34;datePublished\u0026#34;: \u0026#34;2025-05-01T08:00:00+08:00\u0026#34; } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Semantic HTML Structure --\u0026gt; \u0026lt;header\u0026gt; \u0026lt;nav\u0026gt; \u0026lt;a href=\u0026#34;/about/\u0026#34;\u0026gt;About Us\u0026lt;/a\u0026gt; \u0026lt;!-- Additional navigation links --\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;main\u0026gt; \u0026lt;!-- Proper heading hierarchy --\u0026gt; \u0026lt;h1\u0026gt;Main Page Title with Primary Keyword\u0026lt;/h1\u0026gt; \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;Subheading with Related Keywords\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Content with natural keyword usage. Avoid keyword stuffing and focus on providing value to users.\u0026lt;/p\u0026gt; \u0026lt;!-- Internal linking --\u0026gt; \u0026lt;h3\u0026gt;Another Sub-section\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;More content with internal links to \u0026lt;a href=\u0026#34;/related-page/\u0026#34;\u0026gt;related pages\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;!-- Image optimization --\u0026gt; \u0026lt;img src=\u0026#34;image.jpg\u0026#34; alt=\u0026#34;Descriptive alt text with keywords if relevant\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;600\u0026#34;\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;!-- Footer links and content --\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; XML Sitemaps: Creation and Submission What Are Sitemaps?\nXML files that list all important URLs on your website Help search engines discover and crawl your content Include metadata about pages (last updated, change frequency, priority) Creating a Sitemap:\nAutomated Tools:\nWordPress plugins (Yoast SEO, Rank Math) Online generators (XML-Sitemaps.com) CMS features (Shopify, Wix automatically generate sitemaps) Manual Creation for Small Sites:\n\u0026lt;!-- Basic sitemap structure (simplified) --\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34;\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://example.com/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2025-05-01\u0026lt;/lastmod\u0026gt; \u0026lt;priority\u0026gt;1.0\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;!-- Add more URLs as needed --\u0026gt; \u0026lt;/urlset\u0026gt; Submitting Your Sitemap to Google:\nVia Google Search Console:\nCreate/verify your property in Search Console Navigate to \u0026ldquo;Sitemaps\u0026rdquo; section Enter your sitemap URL (typically \u0026ldquo;sitemap.xml\u0026rdquo;) Click \u0026ldquo;Submit\u0026rdquo; Via robots.txt:\n# Add to robots.txt file Sitemap: https://example.com/sitemap.xml Direct Indexing Request: Enter your sitemap URL directly in a browser Use Search Console\u0026rsquo;s URL Inspection for important pages Sitemap Best Practices:\nKeep under 50,000 URLs and 50MB per sitemap file Use sitemap index files for larger sites Update when adding significant content Only include canonical, indexable URLs Submitting Your Site to Google The process of submitting your site to Google when building a website yourself:\nCreate and Verify Google Search Console Property:\nGo to https://search.google.com/search-console Add your property (domain or URL prefix) Verify ownership through: HTML tag (add to section) DNS record (add to domain settings) HTML file upload (to root directory) Google Analytics linking (if already set up) Initial Indexing Methods:\nSubmit your sitemap via Search Console Create and share high-quality backlinks Interlink your content properly Share new content on social media (creates crawl paths) Monitor Initial Indexing:\nCheck Coverage reports in Search Console Use URL Inspection tool for specific pages Set up Google Alerts for your domain Implementing Structured Data Schema.org markup helps search engines understand your content better:\n\u0026lt;!-- Product schema example --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Example Product Name\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;https://example.com/photos/product.jpg\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is an example product description.\u0026#34; } \u0026lt;/script\u0026gt; \u0026lt;!-- Additional product details --\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;https://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;brand\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Brand\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Example Brand\u0026#34; }, \u0026#34;offers\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;Offer\u0026#34;, \u0026#34;price\u0026#34;: \u0026#34;99.99\u0026#34;, \u0026#34;priceCurrency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;availability\u0026#34;: \u0026#34;https://schema.org/InStock\u0026#34; } } \u0026lt;/script\u0026gt; Implementing Technical SEO with JavaScript // Example 1: Lazy loading for images document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { const lazyImages = document.querySelectorAll(\u0026#34;img.lazy\u0026#34;); if (\u0026#34;IntersectionObserver\u0026#34; in window) { const imageObserver = new IntersectionObserver(function(entries, observer) { entries.forEach(function(entry) { if (entry.isIntersecting) { const image = entry.target; image.src = image.dataset.src; image.classList.remove(\u0026#34;lazy\u0026#34;); imageObserver.unobserve(image); } }); }); lazyImages.forEach(function(image) { imageObserver.observe(image); }); } }); // Example 2: Performance optimization for external resources const head = document.getElementsByTagName(\u0026#39;head\u0026#39;)[0]; const preconnectLinks = [ \u0026#39;https://fonts.googleapis.com\u0026#39;, \u0026#39;https://cdn.example.com\u0026#39; ]; preconnectLinks.forEach(url =\u0026gt; { const linkElement = document.createElement(\u0026#39;link\u0026#39;); linkElement.rel = \u0026#39;preconnect\u0026#39;; linkElement.href = url; head.appendChild(linkElement); }); On-Page SEO Factors Content \u0026amp; Keyword Strategy Finding target keywords requires using specialized tools like Google Keyword Planner, Ahrefs, or SEMrush to identify terms with appropriate search volume and competition levels for your site\u0026rsquo;s current authority. The research process should balance opportunity with realistic ranking potential, particularly for newer domains that may struggle against established competitors for high-volume terms.\nStrategic keyword placement remains essential for signaling relevance to search engines. Your primary keyword should appear in the title tag, H1 heading, URL structure, and within the first 100 words of content to establish topic focus early. Secondary keywords should be distributed throughout H2 and H3 subheadings and naturally incorporated in body content. The focus should remain on creating valuable, readable content rather than keyword stuffing, which can trigger penalties from search algorithms designed to detect manipulation.\nKeyword competition varies significantly by type, creating different opportunities based on your site\u0026rsquo;s current authority. Head terms consisting of single words typically offer high search volume but face extreme competition from established domains with substantial backlink profiles. Body keywords containing 2-3 word phrases present moderate volume with manageable competition levels for sites with some established authority. Long-tail keywords comprised of 4+ words show lower individual volume but much less competition, making them ideal starting points for newer websites.\nNew websites should strategically target long-tail keywords first to build topical authority and demonstrate relevance before expanding to more competitive terms as domain authority grows. Assessing competition requires analyzing current top-ranking pages for your target keywords. When search results are dominated by major brands with thousands of quality backlinks and extensive content depth, newer sites should choose less competitive alternatives that offer realistic ranking potential while building authority in their niche.\nPage Speed Optimization Image optimization plays a critical role in both page speed and layout stability metrics. Implementing responsive image solutions with modern formats like WebP (while providing appropriate fallbacks for browser compatibility) significantly reduces load times. Always specifying width and height attributes prevents layout shifts during loading that negatively impact user experience and Core Web Vitals scores. The loading=\u0026ldquo;lazy\u0026rdquo; attribute further improves performance by deferring off-screen images until they\u0026rsquo;re needed.\nCritical CSS implementation separates essential styling needed for above-the-fold content from non-critical styles that can load after initial rendering. This technique delivers dramatically faster perceived loading experiences by prioritizing visible content styling. Loading non-critical CSS asynchronously through the preload pattern prevents render-blocking while ensuring all styles eventually apply. Including a noscript fallback maintains functionality for users with JavaScript disabled, ensuring universal accessibility.\nJavaScript optimization through strategic deferral prevents render-blocking issues and improves time to interactive metrics. Creating a systematic approach to loading non-essential scripts only after core page content becomes usable significantly improves user experience. Implementing script loading prioritization ensures critical functionality appears first while deferring analytics, advertising, and enhancement scripts until after the initial page load completes. This balanced approach maintains functionality while optimizing loading sequence for performance.\nAds and SEO Balance Google\u0026rsquo;s ranking algorithms penalize sites with excessive advertisements, particularly those creating poor user experiences or pushing important content below the fold. The Page Experience update specifically targets intrusive interstitials and aggressive ad implementations that interfere with content consumption. Finding the appropriate balance between monetization and search performance requires implementing ads in ways that complement rather than dominate the user experience.\nImplementation best practices include lazy loading advertisements using the Intersection Observer API to defer ad loading until users scroll near their position. This approach dramatically improves initial page load metrics while still effectively monetizing content. Positioning ads strategically between content sections rather than interrupting paragraphs mid-thought preserves reading flow and engagement. Maintaining a reasonable ad-to-content ratio under 30% signals to both users and search engines that content value remains the primary focus.\nOngoing monitoring of Core Web Vitals metrics following ad implementation provides critical feedback on performance impact. The Search Console Core Web Vitals report offers insights into real-world user experience metrics across your site, highlighting any pages where ad implementation negatively affects performance thresholds. Conducting regular A/B testing with different ad placements and loading strategies helps identify optimal approaches that balance revenue needs with search performance requirements in your specific content niche.\nOff-Page SEO Factors Backlink Quality vs. Quantity Backlinks serve as \u0026ldquo;votes of confidence\u0026rdquo; in Google\u0026rsquo;s ranking algorithm. A single high-quality link typically outweighs numerous low-quality links. The key factors determining backlink quality include domain authority (the overall trustworthiness of the linking site), topical relevance (how closely related the linking site\u0026rsquo;s content is to yours), anchor text (descriptive text containing target keywords), and link placement (with in-content links carrying more value than those in footers or sidebars).\nGoogle identifies low-quality sites through several signals including excessive ads with minimal content, poor engagement metrics like high bounce rates, thin or duplicated content, technical issues and poor mobile experience, manipulative linking practices, and limited expertise signals. Understanding these factors helps explain why competitors often block backlinks to other sites in their industry through nofollow attributes—they\u0026rsquo;re preventing the passing of \u0026ldquo;link equity\u0026rdquo; that would help your rankings for competitive keywords.\nOutbound Linking Strategy Outbound links can benefit SEO when implemented properly by establishing topical relevance for search engines, building content credibility by citing authoritative sources, improving user experience with supplementary information, and creating opportunities for relationship building with other sites in your industry. The implementation should follow best practices for both standard reference links and sponsored content.\nThe most beneficial linking targets include academic institutions with .edu domains, government resources with .gov domains, established industry publications and research papers, Wikipedia for general information that doesn\u0026rsquo;t require specialized expertise, and complementary businesses that offer services related to but not directly competing with yours. This strategic approach to outbound links signals to search engines that your content is well-researched and connected to authoritative sources in your field.\nLink Building Tactics Content-driven link acquisition focuses on creating assets that naturally attract links from other websites. This includes developing original research studies, comprehensive guides, or unique tools that serve as reference points for others in your industry. Visual content like infographics or diagrams with embed codes can also generate links, as can publishing expert interviews with recognized industry figures who may share the content with their audiences.\nGuest posting remains effective when done properly by targeting relevant, quality publications in your field. The key is providing genuinely valuable content rather than thin promotional pieces, including natural contextual links where appropriate, and focusing on establishing expertise rather than maximizing link quantity. This approach builds sustainable link profiles that withstand algorithm updates.\nThe broken link building process involves finding resource pages in your industry, checking for broken links using tools like Ahrefs or Screaming Frog, creating replacement content for the broken resource, and contacting the site owner with a helpful offer to provide your content as a solution. This technique provides immediate value to the site owner while securing relevant backlinks for your domain.\nModern SEO Approaches AI Content Creation Pipeline Effective AI-to-SEO process begins with thorough topic research, including identifying target keywords and search intent, analyzing competitors\u0026rsquo; content structure and coverage, and mapping content to appropriate sales funnel stages. This research foundation guides the AI-assisted drafting phase, where tools like Claude, ChatGPT, or Coze generate initial content drafts incorporating proper heading structure and addressing common queries through FAQ sections.\nHuman enhancement represents the critical differentiating step, where experts add unique insights and examples, incorporate proprietary data or exclusive information, and ensure the content matches brand voice and depth requirements. This human expertise layer transforms basic AI output into valuable content that demonstrates genuine domain knowledge. The SEO optimization stage then involves formatting with proper HTML structure, implementing metadata and schema markup, and applying Core Web Vitals best practices to ensure technical excellence.\nThe final publication and indexing process includes submitting to Google Search Console, building strategic internal and external links, and establishing ongoing monitoring to refine content based on performance data. Search engines evaluate AI content based on demonstrated expertise and original insights rather than generic information, the value provided to readers through specific problem-solving, and clear E-E-A-T signals that establish credibility and authority within the subject matter.\nFor user-generated content (UGC) indexing, implementing proper HTML structure is essential. This includes using appropriate schema markup to signal content type and authorship, providing clear content structure with proper heading hierarchy, and establishing content transparency through source attribution and timestamps. These technical elements help search engines properly categorize and index content generated through AI assistants or user contributions.\nCore Web Vitals Optimization Core Web Vitals optimization focuses on three critical metrics that directly impact search rankings. Largest Contentful Paint (LCP) measures loading performance with a target under 2.5 seconds, improved by optimizing server response times, preloading essential resources, properly compressing and formatting images, and utilizing content delivery networks for static assets. This metric directly affects user perception of site speed and initial engagement.\nFirst Input Delay (FID) measures interactivity with a target under 100ms, enhanced by minimizing JavaScript execution time, breaking up long tasks into smaller processes, optimizing event handlers for efficiency, and employing web workers for complex background operations. This metric ensures users can interact with your page quickly after it visually appears loaded, reducing frustration and improving engagement metrics.\nCumulative Layout Shift (CLS) measures visual stability with a target under 0.1, improved by setting explicit dimensions for all media elements, reserving appropriate space for dynamic content before it loads, avoiding insertion of new content above existing elements, and using transform animations instead of layout-triggering ones. This metric prevents frustrating experiences where page elements move unexpectedly as the user attempts to interact with them.\nE-E-A-T Implementation Experience, Expertise, Authoritativeness, and Trustworthiness (E-E-A-T) signals have become central to quality content evaluation. Effective author credentials display includes professional photos, relevant educational qualifications and certifications, specific industry experience metrics, and professional organization memberships. These credentials establish the content creator\u0026rsquo;s qualification to address the topic comprehensively and accurately.\nContent transparency markers reinforce trustworthiness through clear publication and update timestamps, fact-checking attribution when appropriate, and sources for statistics or claims. Additional E-E-A-T enhancements include linking to authoritative external sources, incorporating original research and proprietary data, providing comprehensive rather than surface-level topic coverage, maintaining regular content updates, prominently displaying relevant credentials and certifications, and implementing appropriate schema markup for author and content type.\nThese E-E-A-T signals collectively build credibility with both users and search algorithms, particularly important for YMYL (Your Money, Your Life) topics where accuracy and trustworthiness carry even greater weight in ranking decisions. Implementing these elements systematically across your content creates a strong foundation for both current rankings and resilience against future algorithm updates.\nSEO Best Practices Technical SEO Checklist Implement HTTPS across your site Ensure mobile responsiveness Fix crawl errors and broken links Optimize page speed Create and submit XML sitemaps Implement proper redirects (301 for permanent, 302 for temporary) Fix duplicate content issues with canonical tags Implement proper hreflang tags for international sites Content SEO Checklist Conduct thorough keyword research Create comprehensive, high-quality content Optimize title tags and meta descriptions Use descriptive alt text for images Include internal links to relevant pages Update content regularly Implement schema markup where appropriate Monitoring and Analytics // Example of setting up custom SEO tracking in Google Analytics 4 // This could be implemented in your main.js or analytics.js file // Track outbound links document.addEventListener(\u0026#39;click\u0026#39;, function(e) { const link = e.target.closest(\u0026#39;a\u0026#39;); if (link \u0026amp;\u0026amp; link.hostname !== window.location.hostname) { gtag(\u0026#39;event\u0026#39;, \u0026#39;outbound_link_click\u0026#39;, { \u0026#39;destination\u0026#39;: link.href, \u0026#39;link_text\u0026#39;: link.innerText || link.textContent, \u0026#39;page_location\u0026#39;: window.location.href }); } }); // Track scroll depth let scrollDepthTracked = { \u0026#39;25\u0026#39;: false, \u0026#39;50\u0026#39;: false, \u0026#39;75\u0026#39;: false, \u0026#39;100\u0026#39;: false }; window.addEventListener(\u0026#39;scroll\u0026#39;, function() { const scrollPercent = Math.round((window.scrollY / (document.body.offsetHeight - window.innerHeight)) * 100); if (scrollPercent \u0026gt;= 25 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;25\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;25%\u0026#39;}); scrollDepthTracked[\u0026#39;25\u0026#39;] = true; } if (scrollPercent \u0026gt;= 50 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;50\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;50%\u0026#39;}); scrollDepthTracked[\u0026#39;50\u0026#39;] = true; } if (scrollPercent \u0026gt;= 75 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;75\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;75%\u0026#39;}); scrollDepthTracked[\u0026#39;75\u0026#39;] = true; } if (scrollPercent \u0026gt;= 90 \u0026amp;\u0026amp; !scrollDepthTracked[\u0026#39;100\u0026#39;]) { gtag(\u0026#39;event\u0026#39;, \u0026#39;scroll_depth\u0026#39;, {\u0026#39;depth\u0026#39;: \u0026#39;100%\u0026#39;}); scrollDepthTracked[\u0026#39;100\u0026#39;] = true; } }); Tools and Resources Google Analytics \u0026amp; Search Console Google Search Console essentials:\nPerformance Report: Shows clicks, impressions, CTR, and rankings Coverage Report: Identifies indexing issues Mobile Usability: Flags responsiveness problems Core Web Vitals: Monitors user experience metrics Setup process:\n\u0026lt;!-- Verification meta tag for Search Console --\u0026gt; \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;YOUR_CODE\u0026#34; /\u0026gt; Optimization opportunities:\nFind queries driving traffic to create similar content Identify pages with high impressions but low CTR for title/description improvements Fix crawl errors promptly Monitor mobile experience issues Track Core Web Vitals compliance Google Analytics implementation:\n\u0026lt;!-- GA4 base installation --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;G-XXXXXXXX\u0026#39;); \u0026lt;/script\u0026gt; Key metrics to monitor:\nOrganic traffic sources and growth Landing page performance Conversion rates from organic visitors Engagement metrics (time on page, bounce rate) Exit pages and drop-off points Connect Analytics with Search Console to analyze which keywords drive not just traffic but meaningful engagement and conversions.\n","permalink":"https://chenterry.com/archived/guide_seo/","summary":"\u003ch1 id=\"seo-guide-implementation--best-practices\"\u003eSEO Guide: Implementation \u0026amp; Best Practices\u003c/h1\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#what-is-seo\"\u003eSearch Engine Basics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#implementing-a-proper-html-structure\"\u003eTechnical Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#on-page-seo-factors\"\u003eOn-Page Optimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#off-page-seo-factors\"\u003eOff-Page Strategies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#modern-seo-approaches\"\u003eModern Approaches\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tools-and-resources\"\u003eAnalytics \u0026amp; Tools\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"what-is-seo\"\u003eWhat is SEO?\u003c/h2\u003e\n\u003cp\u003eSearch Engine Optimization improves website visibility in organic search results. Three core processes determine rankings:\u003c/p\u003e\n\u003ch3 id=\"crawling-process-and-timeframes\"\u003eCrawling Process and Timeframes\u003c/h3\u003e\n\u003cp\u003eSearch engines discover pages by following links. New websites typically take 4-6 weeks for complete indexing. Factors affecting speed include site structure, server response time, and internal linking. The more efficiently your site is structured, the faster search engines can discover and index your content.\u003c/p\u003e","title":"A Practical Guide to SEO with Claude"},{"content":" Tobi Lutke\u0026rsquo;s Shopify internal memo\nSmaller and More Efficient Teams When should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\nCompared to late 2022, I\u0026rsquo;ve shifted more energy toward building and testing ideas directly, rather than hiring large teams to delegate to. Early-stage startups now need fewer product managers, especially when founders can build and validate ideas over a weekend. The marginal productivity increase associated with an extra headcount might now be lower than that of employing AI agent(s). In practice, this means smaller, more agile teams with outstanding capabilities from ICs. I believe the future belongs to ultra-small teams, for startups, this means any where from 2-5 people, with each member handling a job function while employing multiple AI agents to execute tasks. The benefits are clear: less time spent in meetings, quicker iterations, and the positive ripple effects of tight-knit teams. Rather than building large departments with specialized roles, companies can now assemble small, versatile teams augmented by AI capabilities. These teams can move faster and with greater autonomy than traditional corporate structures allow.\nAI Tools Enabling Rapid Prototyping Recent AI coding tools like Cursor have expedited the prototyping process, allowing developers (and non-technical people too) to quickly build functional MVPs in record time. These tools excel at generating boilerplate code, implementing common patterns, and even troubleshooting errors. A single engineer with Cursor can accomplish what previously required multiple developers working in tandem, or achieve in around one fifth the time they\u0026rsquo;d spend working on it alone.\nCollaboration workflows enabled by Figma, Credit to Kevin Kwok\nThis acceleration in prototyping parallels what we saw with Figma in design. As Kevin Kwok noted, \u0026ldquo;Tightening the feedback loop of collaboration allows for non-linear returns on the process\u0026rdquo; (Kwok, 2020). Just as Figma collapsed the barriers between designers and their collaborators, AI coding tools are now breaking down the technical barriers that previously separated product visionaries from implementation.\nWhile much of the attention has been on AI helping engineers be more efficient, I\u0026rsquo;m more interested in how its ripple effect on tangential roles - such as product managers and ui/ux designers. Just like how Figma allowed faster iteration cycles between designers and engineers/product managers, would this reduction in prototype testing allow for more innovative workflows where product people (with knowledge of engineering possibilities) could quickly test ideas and engineers focus more on the implementation of production scale systems?\nThis dynamic raises an interesting question for founders and engineering leaders: when does it make sense to hire additional team members versus investing in improving the AI capabilities of existing team members? The calculus now involves comparing the marginal cost of onboarding and training a new hire against the potential productivity gains from enhancing your current team\u0026rsquo;s AI workflow mastery.\nThe Rise of Product Engineers Software Engineers should become \u0026ldquo;Product Engineers\u0026rdquo; as LLMs have commoditized routine coding tasks. What\u0026rsquo;s truly valuable now are generalists who can code but also have an eye for UI/UX design, good product taste, and deep market understanding. Startups should increasingly seek engineers who talk directly with users, make decisions on what to build, and then build with AI assistance. This approach works because engineers inherently understand both technical constraints and opportunities better than anyone else. The traditional roles of \u0026ldquo;product manager\u0026rdquo; and \u0026ldquo;engineer\u0026rdquo; are merging into a hybrid that combines technical expertise with product sensibility.\n\u0026ldquo;Design is all of the conversations between designers and PMs about what to build\u0026rdquo; (Kwok, 2020). Similarly, product development is now larger than just engineers or product managers—it encompasses the entire collaborative process of identifying, designing, and implementing solutions. Too often, people view product management as just a career path rather than a mindset. This limited perspective overlooks the deep curiosity and problem-solving drive that true product managers embody—a drive that goes beyond mere job titles and organizational structures. The best product engineers embody this product mindset, focusing on solving real problems rather than just building features. Super ICs don\u0026rsquo;t wait for dev support—they build what they need when they need it, combining technical skills with product thinking to deliver complete solutions.\nShifting Emphasis: From Implementation to Idea Generation As AI handles more of the routine implementation work, the traits we look for in product engineers should also evolve. The most valuable skills now center around workflow generation and new idea creation rather than implementation details. The ability to conceptualize solutions, design effective workflows, and identify the right problems to solve becomes paramount.\nProduct engineers who excel in this new environment should demonstrate:\nStrong systems thinking across the entire product lifecycle The ability to rapidly iterate and test hypotheses Comfort with ambiguity and exploration over rigid planning Skill in designing human-AI collaboration workflows An understanding of when to leverage AI and when to apply human judgment This shift means that strong ICs with AI fluency can now accomplish what previously required teams of specialists. For early-stage startups, this dramatically changes the calculus around hiring, especially for traditional product management roles.\nShopify\u0026rsquo;s AI-first Approach Shopify\u0026rsquo;s CEO Tobi Lütke\u0026rsquo;s internal memo illustrates this shift perfectly. He writes:\n\u0026ldquo;We are entering a time where more merchants and entrepreneurs could be created than any other in history\u0026hellip; Having AI alongside the journey and increasingly doing not just the consultation, but also doing the work for our merchants is a mindblowing step function change here.\u0026rdquo;\nLütke emphasizes that \u0026ldquo;reflexive AI usage is now a baseline expectation at Shopify.\u0026rdquo; He notes that using AI well is a skill that needs to be learned through frequent use, and that AI acts as a multiplier for already high-performing individuals. He compares Shopify to a \u0026ldquo;red queen race\u0026rdquo; from Alice in Wonderland—you must keep running just to stay still. In a company growing 20-40% year over year, everyone must improve at that rate just to re-qualify for their position. With AI tools, this previously daunting expectation now seems achievable.\nThe Critical Role of Taste in Product Development As AI handles more of the execution, the differentiating factor for product engineers becomes \u0026ldquo;taste\u0026rdquo; - the ability to discern what makes for excellent product design, user experience, and strategic direction. Product taste is what separates adequate solutions from exceptional ones. In a world where AI can generate competent designs and functional code, the human with superior taste becomes indispensable. \u0026ldquo;Just like how the constraints on design at companies is often not a problem of pixels, but of people\u0026rdquo; (Kwok, 2020). As technical constraints dissolve through AI assistance, the human factors around judgment, taste, and strategic direction become the primary differentiators.\nTaste involves the ability to:\nIdentify which problems are worth solving Determine the appropriate level of complexity for solutions Recognize when simplicity serves users better than feature-richness Balance aesthetic appeal with functional needs Anticipate user needs before they\u0026rsquo;re explicitly requested These skills can\u0026rsquo;t be easily replicated by AI systems. While AI can execute with increasing competence, it still lacks the intuitive understanding of human needs and experiences that informs good taste.\nWhat This Means for Organizations Shopify\u0026rsquo;s approach includes several key principles organizations should consider:\nAI as a fundamental expectation - Not an optional tool but a core competency AI integration from the prototype phase - Using AI throughout the development process Headcount requests must demonstrate why AI can\u0026rsquo;t do the job - Teams must explore AI solutions first Universal application - This applies to all levels, including executives Organizations following these principles will likely outperform those treating AI as merely an optional tool. The principles represent a fundamental rethinking of how work gets done, not just a technological upgrade.\nWhile some companies are hiring for AI product managers nowadays, this might be a transitionary position, just as prompting is an intermediary step in human-LLM interaction—similar to the arrival of GUIs before the personal computer age became mainstream. Product managers now need to learn when to dive into detailed work and when to step back for strategic oversight, a balance that will continue to evolve as AI capabilities expand.\nThe Path Forward The future belongs to smaller, more nimble teams of highly capable individuals who leverage AI to achieve what previously required entire departments. This might be the new normal of work. For ICs, learning to effectively collaborate with AI tools is no longer optional—it\u0026rsquo;s essential for remaining competitive. For organizations, the challenge is creating environments where these super ICs can thrive, with processes and cultures that maximize the human+AI partnership rather than treating them as separate domains. Most importantly, this shift will enable more creative exploration and novel problem-solving, as routine tasks become increasingly automated. The most exciting products and services of the coming years will likely emerge from these small, AI-augmented teams combining human taste with AI-powered execution.\nRelated Reading A2A Catalog: A Human-Mediated Agentic Workforce - Deep dive into autonomous AI agents and workforce automation User Needs \u0026amp; Opportunities - Identifying product opportunities in the AI-first world When do I Sunset a Product? - Making strategic decisions in fast-moving AI product development Works Cited\nGoodspeed, Elizabeth. \u0026ldquo;Design Taste vs. Technical Skills in the Era of AI.\u0026rdquo; Nielsen Norman Group, 10 May 2024.\nKwok, K. (2020, June 19). Why Figma Wins. https://kwokchain.com/2020/06/19/why-figma-wins/\nLütke, Tobi. \u0026ldquo;Internal Company Memo on AI Usage.\u0026rdquo; Shopify, 2025.\n","permalink":"https://chenterry.com/posts/product_engineers/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/product_engineer/shopify_ai.png\" alt=\"Product Engineers\" loading=\"lazy\"\u003e\n \n\u003cem\u003eTobi Lutke\u0026rsquo;s Shopify internal memo\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"smaller-and-more-efficient-teams\"\u003eSmaller and More Efficient Teams\u003c/h2\u003e\n\u003cp\u003eWhen should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\u003c/p\u003e","title":"Product Engineers and AI Multipliers"},{"content":"Here\u0026rsquo;s a list of articles that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\nGary Tan on Manus: The New General-Purpose AI Agent Video URL: https://www.youtube.com/watch?v=JOYSDqJdiro\nUsable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\n\u0026ldquo;Today we\u0026rsquo;re launching an early preview of Manus, the first general AI agent.\u0026rdquo;\nWhen Manus officially launched, the hype around it immediately took off. A Chinese startup unveiling a new AI agent that some are calling \u0026ldquo;China\u0026rsquo;s next DeepSeek moment,\u0026rdquo; with people calling it \u0026ldquo;the most impressive AI tool they\u0026rsquo;ve ever tried\u0026rdquo; and \u0026ldquo;the most sophisticated computer-using AI.\u0026rdquo;\nUnlike some of its predecessors, Manus wasn\u0026rsquo;t just another specialized chatbot. It promised to be a true general-purpose AI agent. With invitations rare and access limited, the question remains: has Manus truly revolutionized the AI agent landscape?\nHow Manus Works Behind all the excitement around Manus is something genuinely innovative: a multi-agent AI system that can seemingly complete all sorts of tasks from travel planning and financial analysis to searching over dozens of files or doing industry research.\nRather than relying on one big neural network, Manus works more like an executive overseeing a team of sub-agents, coordinating and guiding their every move across a shared action space. It takes in your prompt as input and gets to work figuring out what it needs to do.\nInstead of tackling your task in one go, Manus employs a sophisticated approach. A planner agent first comes up with a master plan to follow, breaking things down into manageable subtasks. This way Manus knows precisely what needs to be done before executing and can hand off these tasks to other sub-agents. These sub-agents are like Manus\u0026rsquo;s own in-house experts - they share the same context but each has its own delineated domain from knowledge or memory to execution.\nManus can call upon an extensive suite of 29 different integrated tools, whether they\u0026rsquo;re automating web navigation, securely running code, or pulling important information from files. Manus\u0026rsquo; sub-agents intelligently decide which tools to use.\nFinally, when each subtask is complete, the executor agent combines the outputs together into a final synthesized output for the user.\nTechnical Details Under the hood, Manus is powered by a sophisticated dynamic task decomposition algorithm. This is what enables it to autonomously break down complex instructions into clear execution paths.\nTo ensure stability even after dozens of rounds of reasoning and tool use, the Manus team developed an original technique called \u0026ldquo;chain of thought injection,\u0026rdquo; enabling agents to actively reflect and update plans.\nAt its core, Manus makes use of Anthropic\u0026rsquo;s Claude 3.7 Sonnet. Manus also features robust cross-platform execution capabilities thanks to its seamless integration with open-source tools like YC company Browser.js for advanced website interaction and startup E2B\u0026rsquo;s secure cloud sandbox environment.\nCapabilities and Performance What can Manus actually accomplish? Impressively, it can take on a wide range of real-world tasks. These include creating travel itineraries, performing detailed financial analyses, developing educational content, compiling structured databases, comparing insurance policies, sourcing suppliers, and assisting with high-quality presentations.\nTo truly measure Manus\u0026rsquo; capabilities, we can look at Gaia, a benchmark designed to challenge AI agents on reasoning, multimodal handling, web browsing, and tool proficiency. Humans typically score about 92% on this benchmark, while OpenAI\u0026rsquo;s Deep Research scored about 74% at its best. Manus smashed the state-of-the-art on Gaia, scoring 86.5%, just a few points shy of the average human.\nThe \u0026ldquo;Wrapper\u0026rdquo; Debate Despite impressive benchmark performance, Manus has reignited a broader conversation about the nature of AI startups at the application layer: \u0026ldquo;wrappers.\u0026rdquo;\nSome have dismissed Manus as merely a wrapper, since it stitches together existing foundational models and various tool calls. But this dismissal overlooks an important reality: most successful AI products today could also qualify as wrappers by this logic.\nCursor and Warp, for example, integrate existing LLMs alongside external APIs and developer-focused tooling such as real-time code analysis and debugging utilities. Domain-specific agents like Harvey combine foundational models with legal-specific tool integrations, case law retrieval, compliance checks, and document analysis.\nClearly, many useful applications do fit the wrapper mold, and for many developers, it makes sense to go this route. As Manus co-founder Yizhow Peak G told us himself, \u0026ldquo;From day one they decided to work orthogonally to model development, wanting to be excited rather than threatened by each new model release.\u0026rdquo;\nWhat distinguishes successful wrappers from their less effective counterparts is typically a bunch of things: intuitive UI, proprietary evals, much more careful fine-tuning of foundational models, and thoughtfully designed multi-agent architectures.\nStrengths and Limitations Manus itself illustrates these trade-offs really well:\nStrengths: Its multi-agent orchestration helps deliver significantly lower per-task costs (around $2 a task compared to integrated competitors like OpenAI\u0026rsquo;s Deep Research). It offers greater transparency and user control, letting users directly inspect, customize, or replace individual sub-agents and tool integrations. Additionally, it provides a degree of flexibility centralized platforms rarely match.\nOne of the coolest things Manus figured out was actually exposing the file system so you could see exactly what the agents were doing. Chat GPT requires you to reprompt, and it\u0026rsquo;s opaque what\u0026rsquo;s happening when it\u0026rsquo;s thinking. Manus is a glimpse into the future of Chat GPT desktop operating directly on your computer, and it will be cool to see how much more control you\u0026rsquo;ll get when it\u0026rsquo;s happening there instead of a browser.\nLimitations: Coordination across specialized agents becomes increasingly difficult as tasks scale or complexity grows. Its current advantages (UX refinements, targeted fine-tuning, thoughtful integrations) are vulnerable to competitors just coming along and doing that as well.\nThese strengths and weaknesses are generally shared by wrappers. They allow rapid deployment, iteration, and specialized UX at lower upfront cost. However, they\u0026rsquo;re vulnerable to disruption such as API pricing changes or provider policy shifts, which can quickly erase any cost benefits.\nThe Future of AI Products Ultimately, the critical challenge isn\u0026rsquo;t deciding whether wrappers are viable but identifying genuinely sustainable differentiation for your product.\nFor founders, this might mean investing early in proprietary evals that are expensive or time-consuming to replicate, embedding workflows deeply into specific user routines to increase switching costs, and identifying integrations with platforms or data sets competitors can\u0026rsquo;t easily access.\nIn the end, success in AI doesn\u0026rsquo;t hinge on reinventing the wheel but rather on who can stitch together the existing models into a product users genuinely love.\n","permalink":"https://chenterry.com/posts/interesting_reads/","summary":"\u003cp\u003eHere\u0026rsquo;s a list of articles that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\u003c/p\u003e\n\u003ch1 id=\"gary-tan-on-manus-the-new-general-purpose-ai-agent\"\u003eGary Tan on Manus: The New General-Purpose AI Agent\u003c/h1\u003e\n\u003cp\u003eVideo URL: \u003ca href=\"https://www.youtube.com/watch?v=JOYSDqJdiro\"\u003ehttps://www.youtube.com/watch?v=JOYSDqJdiro\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\u003c/p\u003e","title":"Interesting Reads"},{"content":" Hayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\nTo put this dedication in perspective: a single 4 second crowd scene from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\nYet as technologies like GPT-4o image generation and and \u0026ldquo;Ghibli-style\u0026rdquo; AI art taking the internet by storm, I can\u0026rsquo;t help wondering about the future. When anyone can transform an image into a Ghibli-esque cartoon nearly indistinguishable from Miyazaki\u0026rsquo;s hand-drawn work, we must reconsider what constitutes true craftsmanship and whether we can appreciate his art in the same way.\nMiyazaki himself once expressed after viewing AI generated animations: \u0026ldquo;I am utterly disgusted. I would never wish to incorporate this technology into my work at all. I strongly feel this is an insult to life itself.\u0026rdquo; However, I hesitate to apply this statement too broadly, as it was made under different circumstances—specifically in response to Dwango AI Lab\u0026rsquo;s demonstration of a 3D zombie animation of substantially lower quality than today\u0026rsquo;s AI art. Objectively speaking, current AI-generated Ghibli-style images demonstrate impressive technical quality, yet for longtime fans, they lack the same emotional resonance.\nThe genius of Ghibli\u0026rsquo;s work transcends time. Released in 1997, \u0026ldquo;Princess Mononoke\u0026rdquo; explores humanity\u0026rsquo;s complex relationship with nature through a meticulously crafted world. With a budget of 2.35 billion yen—making it the most expensive animated film of its era—the movie was created just before the digital animation revolution. Approximately 144,000 cells were hand-drawn to build its vision of Muromachi-era feudal Japan. The result was a timeless masterpiece that represented more than a decade of thoughtful development in plot, character, and animation. It was the pinnacle of its time and remains so today.\nAs we process the technological shift that now enables machines to replicate human art at scale, we must consider how this new dynamic will shape creativity\u0026rsquo;s future. When the marginal cost of producing high-quality work approaches zero, what defines creativity? Can creative works be protected, or can anyone replicate a piece after merely seeing it? In an era where everyday users generate sophisticated content with minimal input, how do we distinguish truly exceptional work?\nWhile I don\u0026rsquo;t have definitive answers, I believe our appreciation for content will evolve—perhaps valuing human-created work more highly or discovering new experiences through previously inconceivable creative expressions. Copyright laws, though often inadequate and slow to adapt, will eventually catch up to protect original works and incentivize innovation. On a positive note, creativity will no longer be limited to those with technical artistic skills. It may ultimately center on taste and the ability to understand and abstract experiences that evoke universal empathy—expressions of our collective human experience.\nHowever, as creation costs approach zero, we risk the collapse of our collective imagination while maintaining only the illusion of creativity. The dynamic we\u0026rsquo;ve observed in content consumption—preferring cheap, easy, addictive material—now extends to creation. Why face the challenging reality of making something yourself when you can skip to the result? Our aesthetics might become constrained by AI models\u0026rsquo; training data, reducing creativity to mere copying and merging of established works. We risk being left with nothing but juxtaposition.\nThe difficult process of creating and thinking deeply is where genuine ideas emerge. Technology evolves rapidly; humans do not. Even in the AI age, there remains space for quality and discernment. The labor, patience, and intention behind Miyazaki\u0026rsquo;s work embody values that transcend technological convenience—values we would be wise to preserve, even as our creative tools transform.\nReferences: https://openai.com/index/introducing-4o-image-generation/, https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/, https://www.youtube.com/watch?v=Pi2rHOhPZZ4, https://en.wikipedia.org/wiki/Spirited_Away\n","permalink":"https://chenterry.com/posts/craft_miyakazi/","summary":"\u003cp\u003e\n\n  \u003cimg src=\"/images/posts/hayao-miyakazi/dust-balls.png\" alt=\"Dust Balls\" loading=\"lazy\"\u003e\n \u003c/p\u003e\n\u003cp\u003eHayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\u003c/p\u003e\n\u003cp\u003eTo put this dedication in perspective: a single \u003ca href=\"https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/\"\u003e4 second crowd scene\u003c/a\u003e from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\u003c/p\u003e","title":"The Craft of Miyazaki in an AI-Generated World"},{"content":"When you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\nMore apparent ones: 2025-03-24\nCareer seeking: it\u0026rsquo;s a difficult job market, whether it\u0026rsquo;s doing leetcode or preparing for behavioral interviews, people want a way to cheat their way out of this difficult process, and they\u0026rsquo;re willing to pay for it too. 2025-03-27\nAgentic worklows: I\u0026rsquo;m usually more skeptical towards areas more widely reported, but agentic workflows may well be here to stay. From the initial action gpts (autogpt) agent chains (devin, metagpt, cogno etc), to the more recent operators (MCPs, OpenAI operator), it would indeed be nice to have agents act on our behalf (perhaps hopefuly not the way Anton goes about ordering Hamburgers in Silcon Valley).\nBuilding on the previous point concerning agentic computer use. It\u0026rsquo;s also interesting to consider the two approaches towards web navigation: vision-based and ai-native-ui. While people like Aravind Srinivas have openly expressed their skepicism towards vision based web operations, noting the limitations imposed by operating systems, especially iOS, which restrict access to other applications, vision-based web interactions are inherently more versatile and flexible.\nContent generation: Generative ai is at its core, generative. We have for the past three years seen numerous improvements in text/image/video/avatar generation, with applications in vairous forms of UGC (and now nearing PGC) ads, podcasts, short-form videos. I love how creative people have been with the available AI tools, but at the same time, I view these tools as an available means to scale creativity, not for enabling 0-1 creation.\nLess apparent (and more interesting) ones: 2025-03-25\nAds through LLM generated content: While I was trying to figure out the best way to add a forms feature to this website yesterday, I asked claude. And interestingly, this was what I got back at first: Note that Formspree is a paid service. This raises an interesting point about the future of AI generated content and suggestions. On one hand, the result is based on training data inputted into the model, but at the same time, one can easily manipulate it to recommend one solution over another (try Formspree vs Google Forms). This presents an interesting avenue for ads via llm generated content, and given the extent to which people are willing to trust llm results, this could even be an effective avenue. Yet in doing so, we are also risking the reliability of llm generated content.\nSEO is already changing in the age of LLMs, with AI genearated content flooding web browsers. Another interesting observation is the amount of website traffic going to specific companies from AI-enabled search services such as Perplexity. While looking at website traffic for Genspark and a few other websites, we can see both ChatGPT and Perplexity listed as the top referring websites (source: similarweb) Though the accuracy of this data is yet to be determined, it make sense intuitively that, owing to the large amount of content genearted via these AI platforms, the pages will be easily indexed by ai-browsers.\nGeneral Directions: These are some relatively well agreed upon directions that one may take:\n2025-03-27\nBetter customization: Traditionally, a tradeoff existed between customization and scale. Creating for wider audiences required standardized offerings. However, as LLM inference costs rapidly decrease, information retrieval and content generation are approaching fixed costs. This shift enables customization at scale across various domains—personalized news, cinematography, and other content formats. Additionally, advancements in multimodal capabilities (voice, image, video) will likely introduce greater variety in the content we consume. ","permalink":"https://chenterry.com/posts/user-needs/","summary":"\u003cp\u003eWhen you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\u003c/p\u003e","title":"User Needs \u0026 Opportunities"},{"content":"The Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\nSystem Architecture The application uses a serverless architecture centered around AWS Lambda functions and Amazon RDS. This design ensures scalability, reliability, and cost efficiency by leveraging AWS managed services.\nThe Client Application provides a web/mobile interface for user interactions. AWS API Gateway exposes the backend as a RESTful API and routes client requests to appropriate Lambda functions. Four AWS Lambda functions handle all backend operations, with a custom datatier module managing database interactions. Amazon RDS (MySQL) serves as the persistent data store for users, transactions, and receipt metadata. AWS S3 securely stores uploaded receipt images, while AWS Textract processes these images with OCR to extract transaction details. An AI service (AWS Bedrock) automatically categorizes expenses based on merchant and description. Finally, Amazon SNS sends budget alerts via email notifications when spending exceeds predefined thresholds.\nComponent Interactions In the receipt processing flow, a user uploads a receipt image through the frontend application. The API Gateway routes this request to a dedicated Lambda function, which stores the receipt in S3 and creates metadata in RDS. The system then leverages AWS Textract to analyze the image and extract key details such as merchant name, date, and amount. This extracted information is passed to an AI model that categorizes the transaction based on the merchant and description. The complete transaction details are then stored in the RDS database for future reference and analysis.\nThe budget monitoring process begins when a user sets category-specific budget limits via the frontend. A Lambda function updates these settings in the RDS database. Later, when new transactions are processed, the system evaluates current spending against the budget thresholds. If spending exceeds a predefined limit, the SNS service automatically sends a notification to the user\u0026rsquo;s registered email address, providing timely awareness of their financial situation.\nFor financial reporting, users can request customized reports through the frontend. A Lambda function queries the relevant transaction data from RDS based on user-specified parameters. This data is then processed and formatted according to the user\u0026rsquo;s preferences, such as report type (transaction details or category summaries) and format (CSV or PDF). The final report is delivered back to the client via the API response, providing valuable insights for financial planning and tax preparation.\nAPI Specification The application exposes a RESTful API through AWS API Gateway with the following endpoints:\n1. Receipt Upload and Analysis POST /upload\nPurpose: Upload a receipt image for processing Request Body: Multipart form data with image file userid (required): User identifier filename: Name of the receipt image data: Base64-encoded image data Process: The system validates if the user exists, stores the receipt image in S3, creates a receipt record in the database, and initiates asynchronous processing. The function also checks whether the user\u0026rsquo;s current monthly spending exceeds the monthly budget, and sends an email alert to the user through SNS. Response: Status 200: Success, returns transactionid, userid, amount, category, time Status 400: Bad request (missing parameters) Status 500: Server error 2. Expense Retrieval GET /expenses\nPurpose: Retrieve user expenses with optional filtering Query Parameters: userid (required): User identifier start_date (optional): Filter transactions after this date (YYYY-MM-DD) end_date (optional): Filter transactions before this date (YYYY-MM-DD) category (optional): Filter by expense category Response: Status 200: Success, returns list of transactions and summary Status 400: Bad request (missing parameters) Status 500: Server error 3. Budget Management POST /budget_and_alert\nPurpose: Set monthly spending budget Process: The system updates the user\u0026rsquo;s budget in the database, checks current month\u0026rsquo;s spending against the budget, and sends an SNS notification if spending exceeds the budget. Response: Status 200: Budget set successfully Status 400: Bad request (missing parameters) Status 500: Server error 4. Financial Reports GET /report\nPurpose: Generate and export financial report Query Parameters: userid (required): User identifier format (optional): \u0026ldquo;csv\u0026rdquo; or \u0026ldquo;pdf\u0026rdquo; (default: \u0026ldquo;csv\u0026rdquo;) report_type (optional): \u0026ldquo;transactions\u0026rdquo; or \u0026ldquo;summary\u0026rdquo; (default: \u0026ldquo;transactions\u0026rdquo;) start_date (optional): Starting date for report end_date (optional): Ending date for report category (optional): Filter by category Report Types: The \u0026ldquo;transactions\u0026rdquo; type lists all individual transactions with details, while the \u0026ldquo;summary\u0026rdquo; type provides aggregated statistics by category (count, total, average, max, min). Output Formats: The system supports CSV (standard comma-separated values) and PDF (HTML structure that would be converted to PDF) formats. Process: The system retrieves filtered transaction data from the database, generates the report in the requested format, and encodes the report data as base64 for transmission. Response: Status 200: Success, returns report data Status 400: Bad request (missing parameters) Status 500: Server error Database Schema The application uses a MySQL database on Amazon RDS with the following schema:\nThe Users table stores basic user information. The userid field serves as the primary key and unique identifier for each user. The email field contains the user\u0026rsquo;s email address for notifications. The budget field is a JSON object storing category-specific budget limits (e.g., {\u0026ldquo;Food\u0026rdquo;: 500, \u0026ldquo;Transport\u0026rdquo;: 200}). The sns_topic_arn field contains the Amazon SNS topic ARN for sending notifications to this user.\nThe Transactions table records all user expenditures. The transactionid field is an auto-incrementing primary key. The userid field is a foreign key reference to the Users table. The amount field stores the transaction amount as a decimal with two places of precision. The category field contains the expense category (e.g., Food, Transport, Entertainment) assigned by the AI. The time field records when the transaction occurred.\nThe Receipts table tracks uploaded receipt images and their processing status. The receiptid field is an auto-incrementing primary key. The userid field references the Users table. The status field indicates the current processing status (e.g., \u0026ldquo;uploaded\u0026rdquo;, \u0026ldquo;processing\u0026rdquo;, \u0026ldquo;completed\u0026rdquo;, \u0026ldquo;error\u0026rdquo;). The s3_location field contains the S3 bucket path to the stored receipt image. The upload_time field captures when the receipt was uploaded, while analysis_time records when processing was completed.\nLambda Functions The system is implemented using four main AWS Lambda functions, each dedicated to a specific aspect of the application.\nThe Receipt Upload and Analysis function processes receipt images and extracts transaction details. It handles receipt uploads and storage in S3, uses AWS Textract for OCR processing, employs an AI model for merchant recognition and expense categorization, creates transaction records from extracted data, and updates receipt status through processing stages.\nThe Get Past Expenses function retrieves and filters transaction records. It supports filtering by date range and category, generates transaction summaries and category breakdowns, calculates spending statistics (totals, counts), and returns a formatted JSON response.\nThe Set Budget and Trigger Alerts function updates budget settings and sends alerts. It updates user budget settings in the database, checks current spending against budget limits, sends notifications via SNS when thresholds are exceeded, calculates budget utilization percentages, and supports category-specific budgets.\nThe Export Financial Reports function generates financial reports in different formats. It supports multiple report types (transactions, summary), generates reports in CSV or PDF format, applies filtering options (date range, category), and encodes reports as base64 for transmission.\nConclusion The Smart Expense Tracker with Auto-Categorization represents a significant improvement over traditional expense tracking applications by leveraging AWS cloud services and artificial intelligence. By automating the tedious aspects of expense management, the system allows users to gain financial insights with minimal effort.\n","permalink":"https://chenterry.com/archived/smart-expense-tracker/","summary":"\u003cp\u003eThe Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\u003c/p\u003e","title":"Smart Expense Tracker"},{"content":"Authors: Terry Chen, Kaiwen Che, Matthew Song\nAbstract Despite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\nIn comparison to traditional retrieval-augmented generation (RAG) techniques that store verbatim conversation segments, our system employs strategic memory consolidation procedures, abstracting key information into structured forms. Performance testing on the GoodAI Long-Term Memory benchmark demonstrates significant improvements in performance, with our memory-augmented GPT-4o achieving scores of up to 6.9/11 over the baseline 4.6/11. Additional testing across multi-agent domains demonstrates enhanced persistence and updating capacity of information.\nIntroduction State-of-the-art large language models (LLMs) possess remarkable natural language comprehension and generation. However, their architecture imposes tight constraints on memory retention and contextual comprehension during long-term interaction. Most existing LLMs operate within fixed context windows, typically ranging from 32,000 to 128,000 tokens, which impose inherent constraints on long-term conversation and complex reasoning tasks that span multiple turns.\nThe Baddeley and Hitch (1974, 2000) model of working memory provides a robust theoretical account of human information processing. The model presents memory as a multi-component system with central executive control of information flow, an episodic buffer of assembling memories into temporary experiences, a phonological loop of controlling verbal content, and a visuospatial sketchpad of controlling visual and spatial information.\nCurrent approaches to increasing LLM memory capacity heavily rely on embedding-based retrieval-augmented generation (RAG). While the approach can deliver rapid access to previous data, it suffers greatly from issues like vector explosion, the unsustainable proliferation of embeddings as conversation history grows, lack of semantic structure in stored shreds, and difficulties in maintaining relations among relevant facts.\nThis work introduces a novel biomimetic approach to LLM memory extension that more accurately models the cognitive architecture of humans, with a three-tiered memory system distinguishing between immediate context, episodic memories, and semantic facts.\nSystem Architecture Our memory improvement system utilizes a three-layer architecture inspired by human cognition:\nWorking Memory (LLM Context Window) We divide the context window into two distinct segments:\nMulti-Round Conversation History (MCH): Stores current conversation context, maintaining flow up to a defined token limit. Retrieval Memory Buffer (RMB): Provides dedicated space for injecting remembered memories from long-term storage, maintaining a balance of short-term and long-term remembered data. Long-Term Memory Store Implemented as a vector database storing two forms of memory:\nSemantic Memory: Stores factual knowledge gained from conversations as subject-predicate-object triples with optional contextual referencing. Episodic Memory: Stores complete interaction episodes by a formal schema with contextual initialization, reasoning operations, actions taken, and outcomes observed. Memory Processes There are specialized components for:\nMemory Consolidation: Operations for capturing and formalizing memories when conversation history reaches token thresholds. Retrieval Mechanisms: Multi-step operations that determine context adequacy before retrieving from external memory stores. Memory Schema Implementation Semantic Memory Triple We implemented the semantic memory schema as a structured class:\nclass SematicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Store all new facts, preferences, and relationships as triples.\u0026#34;\u0026#34;\u0026#34; subject: str predicate: str object: str context: str | None = None Episodic Memory Schema Our episodic memory implementation stores experiential information with temporal context:\nclass EpisodicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Write the episode from the perspective of the agent within it.\u0026#34;\u0026#34;\u0026#34; observation: str = Field(..., description=\u0026#34;The context and setup - what happened\u0026#34;) thoughts: str = Field( ..., description=\u0026#34;Internal reasoning process and observations of the agent\u0026#34; ) action: str = Field( ..., description=\u0026#34;What was done, how, and in what format.\u0026#34; ) result: str = Field( ..., description=\u0026#34;Outcome and retrospective.\u0026#34; ) Memory Consolidation Process The foundation of our strategy lies in sophisticated memory consolidation mechanisms that convert raw conversational information into structured memory representations:\nSemantic Memory Extraction Our semantic memory schema makes use of subject-predicate-object triples that eliminate episodic detail without sacrificing core relationships. Implementation follows several guiding principles:\nPrioritization of high-frequency accessed information Merging of redundant knowledge into a single representation Upgrading existing triples whenever new contradicting data exist Adding contextual linking to render situationally responsive retrieval Episodic Memory Extraction Episodic memory stores full interactions in an ordered schema consisting of four main components:\nObservation: Stores contextual setup and what transpired Thoughts: Stores internal reasoning processes and deliberations Action: Stores particular interventions and methodologies used Result: Stores outcome and subsequent analysis Evaluation Results GoodAI LTM benchmark results indicated radically better performance with our memory augmentation approach:\nConfiguration Score Performance Baseline GPT-4o 4.6/11 41.8% GPT-4o + Semantic Memory 6.8/11 61.8% GPT-4o + Episodic Memory 6.9/11 62.7% GPT-4o + Semantic \u0026amp; Episodic Memory 6.0/11 54.5% These results reflect a general 20-percentage-point improvement in memory performance by our augmentation method. The differential performance aligns with the corresponding functional roles these types of memory serve in human cognition, wherein semantic memory enables fact recall and episodic memory enables experiential reasoning.\nDiscussion and Future Work Our research provides empirical evidence for cognitive-inspired LLM memory enhancement methods. The witnessed performance improvements with three-tier memory architecture show that human memory systems offer valuable design concepts for overcoming inherent limitations in current AI designs.\nThe unexpected finding was the slightly worse performance of integrated memory systems compared to single implementations. This suggests complex interaction effects, which may mirror interference phenomena observed in human memory systems, where various forms of memory sometimes vie for mental resources.\nFuture research directions include:\nMulti-agent Memory Dynamics: How memory transfers between agents and how social dynamics influence memory consolidation Advanced Retrieval Strategies: Exploring spatially organized memory architectures and hierarchical memory organization Optimization of Consolidation Thresholds: Investigating dynamic thresholds that adapt based on conversation characteristics Conclusion This paper presents a novel biomimetic approach to enhancing LLM memory that addresses intrinsic limitations in current architectures. By embracing a three-level memory structure inspired by human cognitive processes, we demonstrate significant improvements in information retention, update, and context recall.\nAs LLMs advance towards more general intelligence capabilities, structured memory systems will play a larger role in enabling coherent long-term interactions, homogeneous knowledge states, and contextually appropriate information access. Our research contributes both pragmatic approaches for deploying this aspect of AI progress and theoretical frameworks to continue advancing this critical component of AI work.\n","permalink":"https://chenterry.com/archived/human-inspired-llm-memory/","summary":"\u003cp\u003e\u003cstrong\u003eAuthors: Terry Chen, Kaiwen Che, Matthew Song\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eDespite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\u003c/p\u003e","title":"LLM Memory Consolidation and Augmentation"},{"content":"2024 passed quickly, anchored by work that was equal parts challenging and rewarding. Early in the year, after Cogno gained some traction, momentum slowed. We continued to code and talk with customers, but progress came in uneven cycles. Our focus on multi-agent systems felt directionally right, yet we never found a defensible niche in sales conversion.\nOne lesson stood out: it isn’t enough to oversee product development—you need to be hands-on with the code. Direct involvement not only accelerates iteration but also sharpens your sense of what’s truly risky about shipping. I realized this later than I’d have liked, but it reshaped how I think about building.\nThat realization also clarified another decision. I had once thought about taking time off school to pursue “the next big thing,” but this year I chose to stay. I still wanted the full college experience—structure, humanities, friendships, and Chicago’s unique energy. Dropping out wasn’t the path I needed right now.\nIn March, after more than a year on Cogno, I joined TikTok to build agent systems for Creative Copilots and Insights. I’m grateful to Zhengjin and Caoye for the opportunity—even though it cut short a long-anticipated Puerto Rico trip. The experience was worth it: building at scale, learning from colleagues across countries, and attending Nvidia GTC and Google Next in Vegas. The work on multimodal insight extraction opened new ways of thinking about how LLMs interact with content. Whether probabilistic models can be “creative” remains debated, but their ability to surprise is undeniable. I’ve come to believe the future lies in combining reasoning with synthesis—likely with greater weight on synthesis—and in multimodal output.\nReturning to Beijing and then back to school later in the year felt like a blur, but I enjoyed academics more than expected. Great teammates and LLM-focused project-based classes brought a level of intensity often missing in industry. This period also sparked Crowdlistening, an exploration of how to extract meaning from the flood of unstructured, multimodal social content. While I’m now focused on AI features for a stealth startup, I still see that problem as deeply important.\nLooking back on Cogno and my earlier projects, I realize most didn’t fail for lack of innovation. Since I began with LLMs in late 2022, the pace of GenAI experimentation has been relentless—domain-specific prompting, data flywheels, agents, multi-agent orchestration. Everyone was building; few created real value. Being first to market rarely matters. What matters is building moats—durable advantages that outlast hype. Technology only matters when it solves hard problems or drives efficiency at scale. In every other case, survival comes from patient, sustainable advantage, even if it means being last.\nI’m still figuring things out, but 2024 was a year of exploration and growth—one I’m deeply grateful for. I’m excited to see how these lessons evolve in 2025.\n","permalink":"https://chenterry.com/posts/2024-year-review-startup-lessons/","summary":"\u003cp\u003e2024 passed quickly, anchored by work that was equal parts challenging and rewarding. Early in the year, after Cogno gained some traction, momentum slowed. We continued to code and talk with customers, but progress came in uneven cycles. Our focus on multi-agent systems felt directionally right, yet we never found a defensible niche in sales conversion.\u003c/p\u003e\n\u003cp\u003eOne lesson stood out: it isn’t enough to oversee product development—you need to be hands-on with the code. Direct involvement not only accelerates iteration but also sharpens your sense of what’s truly risky about shipping. I realized this later than I’d have liked, but it reshaped how I think about building.\u003c/p\u003e","title":"The First Stage: A Reflection"},{"content":"Advised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\nGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\nOur approach is centered around understanding effective study group learning for educational purposes. Understanding Learning Barriers: Traditional Q\u0026amp;A systems often provide direct answers, which may limit deeper understanding. Groupal emphasizes learning through inquiry, leveraging Socratic questioning techniques proven to improve knowledge retention and critical thinking.\nDecomposing the Problem: Groupal integrates real-time speech processing, intent routing, and contextual retrieval of relevant background information to support learning. The frontend-backend pipeline connects key components such as document parsing, relevance checks, and question generation that adapts according to the flow of group discussions. Real-Time Interaction: Groupal listens to student discussions in real time, converts speech to text, identifies misconceptions, and generates insightful socratic questions. The system retrieves relevant content through a RAG process to supplement the discussion effectively.\nGroupal provides a comprehensive set of features to enhance the effectiveness of collaborative study sessions. Through real-time speech analysis, Groupal transcribes live group discussions into text, identifies misconceptions, and stores them for further review. It detects potential misconceptions in the discussion and generates Socratic-style questions when relevant to encourage further discussions among students. With its background knowledge retrieval capability, Groupal accesses relevant materials through a RAG process, retrieving the top 3 documents from a vector database, and using a Socratic Questioning Model to generate and output the socratic question, giving students immediate access to supporting information and guidance during their sessions. The platform allows users to upload documents, form or join study groups, and explore relevant content in real time, creating a well-organized and interactive learning experience. Groupal ensures discussions remain focused and productive by providing adaptive and context-aware guidance.\nCredits: Tina Liu, Yihang Du, Doohwan Kim.\n","permalink":"https://chenterry.com/archived/groupal/","summary":"\u003cp\u003eAdvised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\u003c/p\u003e\n\u003cp\u003eGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\u003c/p\u003e","title":"Realtime Conversational Learning Aid"},{"content":"PepTalk: AI Journaling Tool Realtime conversation with aI companion to help you note down feelings and journals for the day. (Prototype: https://peptalk-navy.web.app/)\nWhat2Do: AI Trip Planning Tool A trip planning tool for generating itinearies based on article url input and content extraction. (Prototype: what2do-51224.web.app)\nOHours: Office Hour Scheduling Tool An office hour queuing system to improve student experience and help TAs manage questions more efficiently. (Prototype: ohours.web.app/)\nCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\n","permalink":"https://chenterry.com/archived/prototyping/","summary":"\u003ch3 id=\"peptalk-ai-journaling-tool\"\u003ePepTalk: AI Journaling Tool\u003c/h3\u003e\n\u003cp\u003eRealtime conversation with aI companion to help you note down feelings and journals for the day.\n(Prototype: \u003ca href=\"https://peptalk-navy.web.app/\"\u003ehttps://peptalk-navy.web.app/\u003c/a\u003e)\u003c/p\u003e\n\u003ch3 id=\"what2do-ai-trip-planning-tool\"\u003eWhat2Do: AI Trip Planning Tool\u003c/h3\u003e\n\u003cp\u003eA trip planning tool for generating itinearies based on article url input and content extraction.\n(Prototype: what2do-51224.web.app)\u003c/p\u003e\n\u003ch3 id=\"ohours-office-hour-scheduling-tool\"\u003eOHours: Office Hour Scheduling Tool\u003c/h3\u003e\n\u003cp\u003eAn office hour queuing system to improve student experience and help TAs manage questions more efficiently.\n(Prototype: ohours.web.app/)\u003c/p\u003e\n\u003cp\u003eCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\u003c/p\u003e","title":"Rapid Prototyping of LLM Enabled Webapps"},{"content":"Exploring Unknown Unknowns: The Future of Knowledge Interfaces We live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don\u0026rsquo;t know what to read, and we don\u0026rsquo;t understand what we\u0026rsquo;ve read. These pain points—\u0026ldquo;not knowing how to choose\u0026rdquo; and \u0026ldquo;not knowing how to comprehend\u0026rdquo;—represent a massive opportunity for reimagining how we interact with knowledge.\nThe core insight driving next-generation learning interfaces is simple but profound: most people don\u0026rsquo;t know what they don\u0026rsquo;t know. We can\u0026rsquo;t formulate good questions about topics we\u0026rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.\nBeyond Search: Learning from Google\u0026rsquo;s Experiments Google\u0026rsquo;s Learn About product offers a compelling glimpse of this future. Unlike traditional search, which requires users to know what to look for, Learn About allows users to \u0026ldquo;zoom out and look at the space of questions around your question.\u0026rdquo; It combines the information accuracy of search with the flexible, dynamic interaction of AI chat, creating an exploratory learning experience that goes far beyond simple Q\u0026amp;A.\nThis approach represents a fundamental shift from information retrieval to knowledge discovery. Instead of returning static results, the system actively helps users explore adjacent concepts and ask better questions. Users can pursue their immediate curiosity while simultaneously discovering related topics they never thought to investigate.\nThe most innovative learning interfaces take this concept further by specializing in specific domains. Rather than trying to handle all possible queries, they focus on particular knowledge areas—like literature, technical documentation, or professional development—where they can provide genuinely superior experiences compared to general-purpose tools.\nThe LLM Architecture Behind Intelligent Learning The technical foundation of these systems relies on sophisticated prompt engineering and modular content generation. Large language models serve as the cognitive engine, but their raw output must be carefully structured to create coherent learning experiences. The key innovation lies in using LLMs to generate JSON-formatted responses that populate predefined UI templates, creating consistent yet dynamic interfaces.\nThis architecture allows the system to maintain conversational flow while presenting information in learner-friendly formats. For example, instead of generating wall-of-text responses, the LLM outputs structured data that renders as interactive cards, related questions, and exploration pathways. Each response includes not just content, but also suggested next steps and connection points to related topics.\nThe prompt engineering becomes crucial here. Effective systems use detailed behavioral instructions that guide the LLM to act as a knowledgeable teacher rather than a simple question-answering service. These prompts specify tone, content depth, interaction style, and response structure, ensuring consistency across thousands of potential learning conversations.\nReducing Cognitive Friction Through Design Traditional learning interfaces suffer from what could be called \u0026ldquo;prompt friction\u0026rdquo;—the cognitive overhead of formulating good questions and organizing complex thoughts into text. The most successful knowledge interfaces minimize this friction through several design strategies.\nFirst, they embed potential questions directly into content responses. Instead of requiring users to think of follow-up questions, the system generates three or four relevant next steps that users can explore with a simple click. This transforms learning from an active questioning process into a guided exploration where curiosity can flow naturally.\nSecond, they use modular response formats that pack high knowledge density into digestible chunks. Rather than lengthy explanations, responses combine concise answers with interactive elements: reflection prompts, knowledge checks, relevance connections, and vocabulary builders. Users receive exactly the information they need while being invited to go deeper on specific aspects that interest them.\nThird, they implement \u0026ldquo;prompt prefills\u0026rdquo;—pre-written questions and conversation starters that help users begin productive dialogues without staring at blank input fields. These aren\u0026rsquo;t generic suggestions but contextually relevant questions based on the current topic and common learning patterns.\nPersonalization Through Conversational Intelligence Unlike traditional recommendation systems that rely on explicit preferences or behavioral tracking, conversational learning interfaces build user understanding organically through dialogue. Each interaction reveals information about the user\u0026rsquo;s background knowledge, interests, learning goals, and preferred depth of explanation.\nThis conversational profiling enables increasingly sophisticated personalization. The system learns whether a user prefers concrete examples or abstract concepts, detailed explanations or high-level overviews, historical context or contemporary applications. Over time, responses become naturally calibrated to individual learning styles and knowledge levels.\nThe personalization extends beyond content delivery to include book recommendations, topic suggestions, and learning path optimization. By understanding what concepts a user struggles with and what types of explanations resonate, the system can proactively surface relevant material and adapt its teaching approach in real-time.\nTechnical Implementation: RAG and Knowledge Curation Behind the conversational interface lies a sophisticated knowledge management system. Rather than relying solely on LLM training data, effective learning platforms implement Retrieval-Augmented Generation (RAG) architectures that combine real-time information retrieval with language generation.\nThis approach proves particularly valuable for specialized domains like literature analysis, where high-quality, curated knowledge sources significantly improve response accuracy and depth. Systems can draw from structured databases of book analyses, expert commentary, reader discussions, and academic sources to provide richer, more authoritative answers than general-purpose models alone.\nThe challenge lies in balancing different information sources. Community discussions from platforms like Reddit offer authentic reader perspectives and common questions, while academic sources provide authoritative analysis. Professional reviews and curated summaries add editorial quality. Effective systems learn to synthesize these different knowledge types based on the specific question and user context.\nMeasuring Success Beyond Engagement Traditional educational metrics often miss the point of exploratory learning. While engagement metrics like session length and click-through rates provide some insight, the real value lies in knowledge acquisition and curiosity development. The most meaningful measures focus on learning outcomes: Do users ask better questions over time? Do they make novel connections between concepts? Do they pursue deeper investigation of topics that initially seemed uninteresting?\nAdvanced systems track conversation quality through several indicators: the progression from basic to sophisticated questions, the frequency of cross-topic connections, the depth of follow-up exploration, and user-generated insights that suggest genuine understanding. These metrics help optimize not just for engagement, but for actual learning effectiveness.\nThe Future of Knowledge Work As these interfaces mature, they point toward a fundamental transformation in how we approach knowledge work. Instead of consuming information passively, we\u0026rsquo;ll increasingly collaborate with AI systems to explore ideas, test understanding, and discover unexpected connections. The goal isn\u0026rsquo;t to replace human thinking but to augment it with better tools for curiosity and exploration.\nThe most promising applications extend beyond individual learning to collaborative knowledge building. Imagine research environments where teams can explore complex topics together, with AI facilitators helping surface relevant connections, identify knowledge gaps, and guide productive discussions. Or educational settings where students learn not just facts but how to ask increasingly sophisticated questions about any domain.\nThe technical foundation already exists. The remaining challenge is design: creating interfaces that feel natural, educational experiences that genuinely improve understanding, and systems that scale personalized learning without losing the human touch that makes great teaching transformative.\nThe next time you encounter a complex topic, imagine having a knowledgeable guide who not only answers your questions but helps you discover the questions you didn\u0026rsquo;t know to ask. That\u0026rsquo;s the promise of intelligent knowledge interfaces—and it\u0026rsquo;s closer than you might think.\n","permalink":"https://chenterry.com/posts/learning_interface/","summary":"\u003ch1 id=\"exploring-unknown-unknowns-the-future-of-knowledge-interfaces\"\u003eExploring Unknown Unknowns: The Future of Knowledge Interfaces\u003c/h1\u003e\n\u003cp\u003eWe live in an age of information abundance, yet many of us struggle with two fundamental learning challenges: we don\u0026rsquo;t know what to read, and we don\u0026rsquo;t understand what we\u0026rsquo;ve read. These pain points—\u0026ldquo;not knowing how to choose\u0026rdquo; and \u0026ldquo;not knowing how to comprehend\u0026rdquo;—represent a massive opportunity for reimagining how we interact with knowledge.\u003c/p\u003e\n\u003cp\u003eThe core insight driving next-generation learning interfaces is simple but profound: most people don\u0026rsquo;t know what they don\u0026rsquo;t know. We can\u0026rsquo;t formulate good questions about topics we\u0026rsquo;re unfamiliar with, yet traditional learning systems expect us to do exactly that. This creates a barrier that conversational AI can uniquely solve by flipping the interaction model entirely.\u003c/p\u003e","title":"Exploring Unknown Unknowns"},{"content":"Observing and understanding the strange quirks of individuals and crowds What makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\n","permalink":"https://chenterry.com/main-themes/human-quirks/","summary":"\u003ch2 id=\"observing-and-understanding-the-strange-quirks-of-individuals-and-crowds\"\u003eObserving and understanding the strange quirks of individuals and crowds\u003c/h2\u003e\n\u003cp\u003eWhat makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\u003c/p\u003e","title":"Human Quirks"},{"content":"In the rapidly evolving landscape of artificial intelligence, we\u0026rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.\nWhen we expect AI to output precise, high-quality content based on a single prompt, we\u0026rsquo;re making a fundamental mistake. For AI content to be truly valuable, we must provide sufficient context for it to reason over. The quality of AI output is directly proportional to the richness of context we provide—not just about the topic itself, but about the audience, purpose, constraints, and desired outcomes.\nThe Limitations of Current Approaches Today\u0026rsquo;s AI content tools largely excel at taking existing information and reformatting it into different modalities. We can convert text to audio, create video summaries, or generate podcast-style conversations from written material. However, as Large Language Model context windows continue to expand, simple content summarization becomes increasingly commoditized. The real value lies not in these mechanical transformations, but in the creative synthesis and novel perspectives that emerge when AI systems understand both the content and the consumer.\nThe essence of creativity lies in finding fresh angles of approach. Quality content distinguishes itself through novel perspectives, clear structure, and genuine utility to the reader. As we move beyond basic summarization, the challenge becomes how to help AI systems discover these unique entry points that make content both engaging and personally relevant.\nKnowledge Liquefaction: The New Content Paradigm We\u0026rsquo;re entering an era of \u0026ldquo;knowledge liquefaction\u0026rdquo; where any piece of information can be rapidly transformed into formats that match specific consumption scenarios. Whether someone needs structured learning materials for deep study or fragmentary content for casual listening during commutes, AI systems can now adapt the same core knowledge to fit these different contexts seamlessly.\nThis capability extends far beyond simple format conversion. The most compelling applications combine high-quality human-created content with AI\u0026rsquo;s ability to find unexpected connections and generate personalized frameworks. Rather than replacing human creativity, these systems amplify it by identifying patterns and relationships that might not be immediately obvious, then presenting them through personalized lenses that resonate with individual users.\nThe Personalization Challenge Creating truly personalized content presents a fundamental tension between scale and customization. If every piece of content requires individual adaptation for each user, the costs become prohibitive. However, knowledge fusion offers a solution through its inherent modularity. Many elements of content remain constant across audiences—core concepts, fundamental principles, and essential facts—while the variable elements involve how these concepts connect to individual interests, goals, and existing knowledge.\nThe key insight is that personalization doesn\u0026rsquo;t require generating entirely new content for each user. Instead, it involves intelligent selection and combination of existing content elements, supplemented by targeted customization that creates meaningful connections to the user\u0026rsquo;s specific context and needs.\nDynamic User Understanding Through Interaction Modern AI systems have unprecedented access to rich user interaction data through natural language conversations, reading highlights, and behavioral patterns. Unlike traditional recommendation systems that rely primarily on click-through data, AI-native platforms can analyze the semantic content of user queries, the topics they explore, and the questions they ask to build sophisticated models of their interests and learning preferences.\nThis approach moves beyond simple topic matching to understand cognitive patterns and learning styles. For example, the system might recognize that one user prefers concrete examples and case studies, while another gravitates toward theoretical frameworks and abstract principles. These insights enable the generation of content that not only covers relevant topics but presents them in ways that align with how each individual processes and retains information.\nBuilding Memory Systems That Learn and Adapt The most sophisticated AI-native learning platforms implement memory architectures inspired by human cognition, incorporating episodic memory for recent interactions, semantic memory for abstracted patterns, and procedural memory for learned preferences and behaviors. This multi-layered approach enables systems to maintain context over time while continuously refining their understanding of user needs.\nRather than treating each interaction as isolated, these systems build cumulative knowledge about user interests, expertise levels, and learning goals. They can recognize when someone is exploring a new domain versus deepening existing knowledge, and adjust their content generation accordingly. This longitudinal understanding becomes increasingly valuable as it enables the system to suggest unexpected but relevant connections between seemingly disparate areas of interest.\nThe Promise of Adaptive Content Creation The ultimate vision extends beyond personalized recommendation to adaptive content creation. Imagine a system that can take a classic work like Sun Tzu\u0026rsquo;s \u0026ldquo;The Art of War\u0026rdquo; and generate multiple interpretations tailored to different audiences and applications. For a business professional, it might emphasize strategic planning and competitive analysis. For a parent, it could explore family dynamics and conflict resolution. For a student, it might focus on historical context and philosophical implications.\nEach version would maintain the core insights of the original work while presenting them through frameworks and examples that resonate with the specific audience. This approach recognizes that great ideas have universal applicability, but their accessibility depends heavily on how they\u0026rsquo;re presented and contextualized.\nTechnical Implementation and Practical Considerations Building these capabilities requires sophisticated orchestration of multiple AI systems working in concert. Content generation engines must work alongside user modeling systems, recommendation algorithms, and quality control mechanisms. The challenge lies not just in generating personalized content, but in ensuring it maintains accuracy, coherence, and genuine value while adapting to individual preferences.\nRecent advances in multimodal AI and agent-based architectures provide the technical foundation for these applications. Tools like MCP (Model Context Protocol) servers enable modular, composable AI capabilities that can be combined and recombined to address specific user needs. This architectural approach allows for the kind of flexible, adaptive content generation that personalized learning requires.\nThe Road Ahead As we look toward the future of AI-native learning platforms, the focus shifts from simple automation of existing processes to the creation of entirely new forms of educational experience. The most successful applications will be those that understand the deep relationship between content, context, and individual cognition, using this understanding to create learning experiences that are not just personalized, but genuinely transformative.\nThe transition from traditional content consumption to AI-enhanced learning represents more than a technological upgrade. It\u0026rsquo;s a fundamental reimagining of how knowledge can be packaged, presented, and absorbed in ways that honor both the richness of human understanding and the unique cognitive patterns of individual learners. In this future, every question becomes an opportunity for personalized exploration, and every piece of content becomes a starting point for deeper, more meaningful engagement with ideas.\n","permalink":"https://chenterry.com/posts/personalized_content/","summary":"\u003cp\u003eIn the rapidly evolving landscape of artificial intelligence, we\u0026rsquo;re witnessing a fundamental shift in how we consume and interact with knowledge. While early AI applications focused primarily on content summarization and modal conversion, the next generation of AI-native products promises something far more transformative: the ability to create truly personalized learning experiences that adapt to individual needs, interests, and cognitive patterns.\u003c/p\u003e\n\u003cp\u003eWhen we expect AI to output precise, high-quality content based on a single prompt, we\u0026rsquo;re making a fundamental mistake. For AI content to be truly valuable, we must provide sufficient context for it to reason over. The quality of AI output is directly proportional to the richness of context we provide—not just about the topic itself, but about the audience, purpose, constraints, and desired outcomes.\u003c/p\u003e","title":"Why Context is Everything in AI Content Generation"},{"content":"Every now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\nPatrick Collison: Go deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\nDon\u0026rsquo;t stress out too much about how valuable the things you\u0026rsquo;re going deep on are\u0026hellip; but don\u0026rsquo;t ignore it either. It should be a factor you weigh but not by itself dispositive.\nTo the extent that you enjoy working hard, do. Subject to that constraint, it\u0026rsquo;s not clear that the returns to effort ever diminish substantially. If you\u0026rsquo;re lucky enough to enjoy it a lot, be grateful and take full advantage!\nMake friends over the internet with people who are great at things you\u0026rsquo;re interested in. The internet is one of the biggest advantages you have over prior generations. Leverage it.\nAim to read a lot.\nIf you think something is important but people older than you don\u0026rsquo;t hold it in high regard, there\u0026rsquo;s a reasonable chance that you\u0026rsquo;re right and they\u0026rsquo;re wrong. Status lags by a generation or more.\nAbove all else, don\u0026rsquo;t make the mistake of judging your success based on your current peer group. By all means make friends but being weird as a teenager is generally good.\nBut having good social skills confers life-long benefits. So, don\u0026rsquo;t write them off. Get good at making a good first impression, being funny (if possible\u0026hellip; this author still working on it\u0026hellip;), speaking publicly.\nMake things. Operating in a space with a lot of uncertainty is a very different experience to learning something.\nMore broadly, nobody is going to teach you to think for yourself. A large fraction of what people around you believe is mistaken. Internalize this and practice coming up with your own worldview. The correlation between it and those around you shouldn\u0026rsquo;t be too strong unless you think you were especially lucky in your initial conditions.\nIf you\u0026rsquo;re in the US and go to a good school, there are a lot of forces that will push you towards following traintracks laid by others rather than charting a course yourself. Make sure that the things you\u0026rsquo;re pursuing are weird things that you want to pursue, not whatever the standard path is. Heuristic: do your friends at school think your path is a bit strange? If not, maybe it\u0026rsquo;s too normal.\nFigure out a way to travel to San Francisco and to meet other people who\u0026rsquo;ve moved there to pursue their dreams. Why San Francisco? San Francisco is the Schelling point for high-openness, smart, energetic, optimistic people. Global Weird HQ. Take advantage of opportunities to travel to other places too, of course.\nFind vivid examples of success in the domains you care about. If you want to become a great scientist, try to find ways to spend time with good (or, ideally, great) scientists in person. Watch YouTube videos of interviews. Follow some on Twitter.\nPeople who did great things often did so at very surprisingly young ages. (They were grayhaired when they became famous\u0026hellip; not when they did the work.) So, hurry up! You can do great things.\nPaul Graham - How to do Great Work I found Paul Graham\u0026rsquo;s \u0026ldquo;How to Do Great Work\u0026rdquo; compelling because it tackles something most career advice glosses over: how do you actually figure out what to work on when you\u0026rsquo;re young and have no idea what\u0026rsquo;s good?\n\u0026ldquo;The way to figure out what to work on is by working. If you\u0026rsquo;re not sure what to work on, guess. But pick something and get going.\u0026rdquo; You learn by doing, not by endless planning.\nThe work needs three qualities: \u0026ldquo;it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\u0026rdquo; The tricky part is most people focus only on the first two and ignore scope entirely.\nDon\u0026rsquo;t get trapped by educational systems. \u0026ldquo;The educational systems in most countries pretend it\u0026rsquo;s easy. They expect you to commit to a field long before you could know what it\u0026rsquo;s really like.\u0026rdquo; I see this constantly - people picking majors at 18 and feeling locked into paths they\u0026rsquo;ve barely explored.\nProject-level procrastination is way more dangerous than daily procrastination. \u0026ldquo;You put off starting that ambitious project from year to year because the time isn\u0026rsquo;t quite right.\u0026rdquo;\nThe \u0026ldquo;staying upwind\u0026rdquo; approach: \u0026ldquo;At each stage do whatever seems most interesting and gives you the best options for the future.\u0026rdquo; This feels much more sustainable than trying to optimize for some distant goal you\u0026rsquo;re not even sure you want.\nGreat work emerges through iteration, not grand planning. \u0026ldquo;Great things are almost always made in successive versions. You start with something small and evolve it, and the final version is both cleverer and more ambitious than anything you could have planned.\u0026rdquo;\nConsistency beats intensity. \u0026ldquo;People who do great things don\u0026rsquo;t get a lot done every day. They get something done, rather than nothing.\u0026rdquo; The key insight: \u0026ldquo;Great work usually entails spending what would seem to most people an unreasonable amount of time on a problem.\u0026rdquo;\nQuestions are underrated. \u0026ldquo;Many discoveries have come from asking questions about things that everyone else took for granted.\u0026rdquo; This connects to something I\u0026rsquo;ve noticed - the best conversations often start with someone asking an obvious question that nobody had bothered to ask.\nOriginality comes from genuine curiosity, not trying to be different. \u0026ldquo;Originality isn\u0026rsquo;t a process, but a habit of mind. Original thinkers throw off new ideas about whatever they focus on, like an angle grinder throwing off sparks.\u0026rdquo;\nBe comfortable with intellectual confusion. \u0026ldquo;You have to be comfortable enough with the world being full of puzzles that you\u0026rsquo;re willing to see them, but not so comfortable that you don\u0026rsquo;t want to solve them.\u0026rdquo;\nWhat struck me most was his point about broken models: \u0026ldquo;Broken models of the world leave a trail of clues where they bash against reality.\u0026rdquo; The idea that you can find opportunities by being stricter about truth than other people is something I want to think about more. Be authentic and driven by truth.\nDiego Berdakin Resilience, Intense Curiosity, and Deep Customer Empathy\nSam Altman - How to Be Successful I\u0026rsquo;ve observed thousands of founders and thought a lot about what it takes to make a huge amount of money or to create something important. Usually, people start off wanting the former and end up wanting the latter.\nHere are 13 thoughts about how to achieve such outlier success. Everything here is easier to do once you\u0026rsquo;ve already reached a baseline degree of success (through privilege or effort) and want to put in the work to turn that into outlier success. But much of it applies to anyone.\n1. Compound yourself\nCompounding is magic. Look for it everywhere. Exponential curves are the key to wealth generation.\nA medium-sized business that grows 50% in value every year becomes huge in a very short amount of time. Few businesses in the world have true network effects and extreme scalability. But with technology, more and more will. It\u0026rsquo;s worth a lot of effort to find them and create them.\nYou also want to be an exponential curve yourself—you should aim for your life to follow an ever-increasing up-and-to-the-right trajectory. It\u0026rsquo;s important to move towards a career that has a compounding effect—most careers progress fairly linearly.\nYou don\u0026rsquo;t want to be in a career where people who have been doing it for two years can be as effective as people who have been doing it for twenty—your rate of learning should always be high. As your career progresses, each unit of work you do should generate more and more results. There are many ways to get this leverage, such as capital, technology, brand, network effects, and managing people.\nIt\u0026rsquo;s useful to focus on adding another zero to whatever you define as your success metric—money, status, impact on the world, or whatever. I am willing to take as much time as needed between projects to find my next thing. But I always want it to be a project that, if successful, will make the rest of my career look like a footnote.\nMost people get bogged down in linear opportunities. Be willing to let small opportunities go to focus on potential step changes.\nI think the biggest competitive advantage in business—either for a company or for an individual\u0026rsquo;s career—is long-term thinking with a broad view of how different systems in the world are going to come together. One of the notable aspects of compound growth is that the furthest out years are the most important. In a world where almost no one takes a truly long-term view, the market richly rewards those who do.\nTrust the exponential, be patient, and be pleasantly surprised.\n2. Have almost too much self-belief\nSelf-belief is immensely powerful. The most successful people I know believe in themselves almost to the point of delusion.\nCultivate this early. As you get more data points that your judgment is good and you can consistently deliver results, trust yourself more.\nIf you don\u0026rsquo;t believe in yourself, it\u0026rsquo;s hard to let yourself have contrarian ideas about the future. But this is where most value gets created.\nI remember when Elon Musk took me on a tour of the SpaceX factory many years ago. He talked in detail about manufacturing every part of the rocket, but the thing that sticks in memory was the look of absolute certainty on his face when he talked about sending large rockets to Mars. I left thinking \u0026ldquo;huh, so that\u0026rsquo;s the benchmark for what conviction looks like.\u0026rdquo;\nManaging your own morale—and your team\u0026rsquo;s morale—is one of the greatest challenges of most endeavors. It\u0026rsquo;s almost impossible without a lot of self-belief. And unfortunately, the more ambitious you are, the more the world will try to tear you down.\nMost highly successful people have been really right about the future at least once at a time when people thought they were wrong. If not, they would have faced much more competition.\nSelf-belief must be balanced with self-awareness. I used to hate criticism of any sort and actively avoided it. Now I try to always listen to it with the assumption that it\u0026rsquo;s true, and then decide if I want to act on it or not. Truth-seeking is hard and often painful, but it is what separates self-belief from self-delusion.\nThis balance also helps you avoid coming across as entitled and out of touch.\n3. Learn to think independently\nEntrepreneurship is very difficult to teach because original thinking is very difficult to teach. School is not set up to teach this—in fact, it generally rewards the opposite. So you have to cultivate it on your own.\nThinking from first principles and trying to generate new ideas is fun, and finding people to exchange them with is a great way to get better at this. The next step is to find easy, fast ways to test these ideas in the real world.\n\u0026ldquo;I will fail many times, and I will be really right once\u0026rdquo; is the entrepreneurs\u0026rsquo; way. You have to give yourself a lot of chances to get lucky.\nOne of the most powerful lessons to learn is that you can figure out what to do in situations that seem to have no solution. The more times you do this, the more you will believe it. Grit comes from learning you can get back up after you get knocked down.\n4. Get good at \u0026ldquo;sales\u0026rdquo;\nSelf-belief alone is not sufficient—you also have to be able to convince other people of what you believe.\nAll great careers, to some degree, become sales jobs. You have to evangelize your plans to customers, prospective employees, the press, investors, etc. This requires an inspiring vision, strong communication skills, some degree of charisma, and evidence of execution ability.\nGetting good at communication—particularly written communication—is an investment worth making. My best advice for communicating clearly is to first make sure your thinking is clear and then use plain, concise language.\nThe best way to be good at sales is to genuinely believe in what you\u0026rsquo;re selling. Selling what you truly believe in feels great, and trying to sell snake oil feels awful.\nGetting good at sales is like improving at any other skill—anyone can get better at it with deliberate practice. But for some reason, perhaps because it feels distasteful, many people treat it as something unlearnable.\nMy other big sales tip is to show up in person whenever it\u0026rsquo;s important. When I was first starting out, I was always willing to get on a plane. It was frequently unnecessary, but three times it led to career-making turning points for me that otherwise would have gone the other way.\n5. Make it easy to take risks\nMost people overestimate risk and underestimate reward. Taking risks is important because it\u0026rsquo;s impossible to be right all the time—you have to try many things and adapt quickly as you learn more.\nIt\u0026rsquo;s often easier to take risks early in your career; you don\u0026rsquo;t have much to lose, and you potentially have a lot to gain. Once you\u0026rsquo;ve gotten yourself to a point where you have your basic obligations covered you should try to make it easy to take risks. Look for small bets you can make where you lose 1x if you\u0026rsquo;re wrong but make 100x if it works. Then make a bigger bet in that direction.\nDon\u0026rsquo;t save up for too long, though. At YC, we\u0026rsquo;ve often noticed a problem with founders that have spent a lot of time working at Google or Facebook. When people get used to a comfortable life, a predictable job, and a reputation of succeeding at whatever they do, it gets very hard to leave that behind (and people have an incredible ability to always match their lifestyle to next year\u0026rsquo;s salary). Even if they do leave, the temptation to return is great. It\u0026rsquo;s easy—and human nature—to prioritize short-term gain and convenience over long-term fulfillment.\nBut when you aren\u0026rsquo;t on the treadmill, you can follow your hunches and spend time on things that might turn out to be really interesting. Keeping your life cheap and flexible for as long as you can is a powerful way to do this, but obviously comes with tradeoffs.\n6. Focus\nFocus is a force multiplier on work.\nAlmost everyone I\u0026rsquo;ve ever met would be well-served by spending more time thinking about what to focus on. It is much more important to work on the right thing than it is to work many hours. Most people waste most of their time on stuff that doesn\u0026rsquo;t matter.\nOnce you have figured out what to do, be unstoppable about getting your small handful of priorities accomplished quickly. I have yet to meet a slow-moving person who is very successful.\n7. Work hard\nYou can get to about the 90th percentile in your field by working either smart or hard, which is still a great accomplishment. But getting to the 99th percentile requires both—you will be competing with other very talented people who will have great ideas and be willing to work a lot.\nExtreme people get extreme results. Working a lot comes with huge life trade-offs, and it\u0026rsquo;s perfectly rational to decide not to do it. But it has a lot of advantages. As in most cases, momentum compounds, and success begets success.\nAnd it\u0026rsquo;s often really fun. One of the great joys in life is finding your purpose, excelling at it, and discovering that your impact matters to something larger than yourself. A YC founder recently expressed great surprise about how much happier and more fulfilled he was after leaving his job at a big company and working towards his maximum possible impact. Working hard at that should be celebrated.\nIt\u0026rsquo;s not entirely clear to me why working hard has become a Bad Thing in certain parts of the US, but this is certainly not the case in other parts of the world—the amount of energy and drive exhibited by entrepreneurs outside of the US is quickly becoming the new benchmark.\nYou have to figure out how to work hard without burning out. People find their own strategies for this, but one that almost always works is to find work you like doing with people you enjoy spending a lot of time with.\nI think people who pretend you can be super successful professionally without working most of the time (for some period of your life) are doing a disservice. In fact, work stamina seems to be one of the biggest predictors of long-term success.\nOne more thought about working hard: do it at the beginning of your career. Hard work compounds like interest, and the earlier you do it, the more time you have for the benefits to pay off. It\u0026rsquo;s also easier to work hard when you have fewer other responsibilities, which is frequently but not always the case when you\u0026rsquo;re young.\n8. Be bold\nI believe that it\u0026rsquo;s easier to do a hard startup than an easy startup. People want to be part of something exciting and feel that their work matters.\nIf you are making progress on an important problem, you will have a constant tailwind of people wanting to help you. Let yourself grow more ambitious, and don\u0026rsquo;t be afraid to work on what you really want to work on.\nIf everyone else is starting meme companies, and you want to start a gene-editing company, then do that and don\u0026rsquo;t second guess it.\nFollow your curiosity. Things that seem exciting to you will often seem exciting to other people too.\n9. Be willful\nA big secret is that you can bend the world to your will a surprising percentage of the time—most people don\u0026rsquo;t even try, and just accept that things are the way that they are.\nPeople have an enormous capacity to make things happen. A combination of self-doubt, giving up too early, and not pushing hard enough prevents most people from ever reaching anywhere near their potential.\nAsk for what you want. You usually won\u0026rsquo;t get it, and often the rejection will be painful. But when this works, it works surprisingly well.\nAlmost always, the people who say \u0026ldquo;I am going to keep going until this works, and no matter what the challenges are I\u0026rsquo;m going to figure them out\u0026rdquo;, and mean it, go on to succeed. They are persistent long enough to give themselves a chance for luck to go their way.\nAirbnb is my benchmark for this. There are so many stories they tell that I wouldn\u0026rsquo;t recommend trying to reproduce (keeping maxed-out credit cards in those nine-slot three-ring binder pages kids use for baseball cards, eating dollar store cereal for every meal, battle after battle with powerful entrenched interest, and on and on) but they managed to survive long enough for luck to go their way.\nTo be willful, you have to be optimistic—hopefully this is a personality trait that can be improved with practice. I have never met a very successful pessimistic person.\n10. Be hard to compete with\nMost people understand that companies are more valuable if they are difficult to compete with. This is important, and obviously true.\nBut this holds true for you as an individual as well. If what you do can be done by someone else, it eventually will be, and for less money.\nThe best way to become difficult to compete with is to build up leverage. For example, you can do it with personal relationships, by building a strong personal brand, or by getting good at the intersection of multiple different fields. There are many other strategies, but you have to figure out some way to do it.\nMost people do whatever most people they hang out with do. This mimetic behavior is usually a mistake—if you\u0026rsquo;re doing the same thing everyone else is doing, you will not be hard to compete with.\n11. Build a network\nGreat work requires teams. Developing a network of talented people to work with—sometimes closely, sometimes loosely—is an essential part of a great career. The size of the network of really talented people you know often becomes the limiter for what you can accomplish.\nAn effective way to build a network is to help people as much as you can. Doing this, over a long period of time, is what lead to most of my best career opportunities and three of my four best investments. I\u0026rsquo;m continually surprised how often something good happens to me because of something I did to help a founder ten years ago.\nOne of the best ways to build a network is to develop a reputation for really taking care of the people who work with you. Be overly generous with sharing the upside; it will come back to you 10x. Also, learn how to evaluate what people are great at, and put them in those roles. (This is the most important thing I have learned about management, and I haven\u0026rsquo;t read much about it.) You want to have a reputation for pushing people hard enough that they accomplish more than they thought they could, but not so hard they burn out.\nEveryone is better at some things than others. Define yourself by your strengths, not your weaknesses. Acknowledge your weaknesses and figure out how to work around them, but don\u0026rsquo;t let them stop you from doing what you want to do. \u0026ldquo;I can\u0026rsquo;t do X because I\u0026rsquo;m not good at Y\u0026rdquo; is something I hear from entrepreneurs surprisingly often, and almost always reflects a lack of creativity. The best way to make up for your weaknesses is to hire complementary team members instead of just hiring people who are good at the same things you are.\nA particularly valuable part of building a network is to get good at discovering undiscovered talent. Quickly spotting intelligence, drive, and creativity gets much easier with practice. The easiest way to learn is just to meet a lot of people, and keep track of who goes on to impress you and who doesn\u0026rsquo;t. Remember that you are mostly looking for rate of improvement, and don\u0026rsquo;t overvalue experience or current accomplishment.\nI try to always ask myself when I meet someone new \u0026ldquo;is this person a force of nature?\u0026rdquo; It\u0026rsquo;s a pretty good heuristic for finding people who are likely to accomplish great things.\nA special case of developing a network is finding someone eminent to take a bet on you, ideally early in your career. The best way to do this, no surprise, is to go out of your way to be helpful. (And remember that you have to pay this forward at some point later!)\nFinally, remember to spend your time with positive people who support your ambitions.\n12. You get rich by owning things\nThe biggest economic misunderstanding of my childhood was that people got rich from high salaries. Though there are some exceptions—entertainers for example —almost no one in the history of the Forbes list has gotten there with a salary.\nYou get truly rich by owning things that increase rapidly in value.\nThis can be a piece of a business, real estate, natural resource, intellectual property, or other similar things. But somehow or other, you need to own equity in something, instead of just selling your time. Time only scales linearly.\nThe best way to make things that increase rapidly in value is by making things people want at scale.\n13. Be internally driven\nMost people are primarily externally driven; they do what they do because they want to impress other people. This is bad for many reasons, but here are two important ones.\nFirst, you will work on consensus ideas and on consensus career tracks. You will care a lot—much more than you realize—if other people think you\u0026rsquo;re doing the right thing. This will probably prevent you from doing truly interesting work, and even if you do, someone else would have done it anyway.\nSecond, you will usually get risk calculations wrong. You\u0026rsquo;ll be very focused on keeping up with other people and not falling behind in competitive games, even in the short term.\nSmart people seem to be especially at risk of such externally-driven behavior. Being aware of it helps, but only a little—you will likely have to work super-hard to not fall in the mimetic trap.\nThe most successful people I know are primarily internally driven; they do what they do to impress themselves and because they feel compelled to make something happen in the world. After you\u0026rsquo;ve made enough money to buy whatever you want and gotten enough social status that it stops being fun to get more, this is the only force I know of that will continue to drive you to higher levels of performance.\nThis is why the question of a person\u0026rsquo;s motivation is so important. It\u0026rsquo;s the first thing I try to understand about someone. The right motivations are hard to define a set of rules for, but you know it when you see it.\nJessica Livingston and Paul Graham are my benchmarks for this. YC was widely mocked for the first few years, and almost no one thought it would be a big success when they first started. But they thought it would be great for the world if it worked, and they love helping people, and they were convinced their new model was better than the existing model.\nEventually, you will define your success by performing excellent work in areas that are important to you. The sooner you can start off in that direction, the further you will be able to go. It is hard to be wildly successful at anything you aren\u0026rsquo;t obsessed with.\nSam Altman - What I Wish Someone Had Told Me Optimism, obsession, self-belief, raw horsepower and personal connections are how things get started.\nCohesive teams, the right combination of calmness and urgency, and unreasonable commitment are how things get finished. Long-term orientation is in short supply; try not to worry about what people think in the short term, which will get easier over time.\nIt is easier for a team to do a hard thing that really matters than to do an easy thing that doesn\u0026rsquo;t really matter; audacious ideas motivate people.\nIncentives are superpowers; set them carefully.\nConcentrate your resources on a small number of high-conviction bets; this is easy to say but evidently hard to do. You can delete more stuff than you think.\nCommunicate clearly and concisely.\nFight bullshit and bureaucracy every time you see it and get other people to fight it too. Do not let the org chart get in the way of people working productively together.\nOutcomes are what count; don\u0026rsquo;t let good process excuse bad results.\nSpend more time recruiting. Take risks on high-potential people with a fast rate of improvement. Look for evidence of getting stuff done in addition to intelligence.\nSuperstars are even more valuable than they seem, but you have to evaluate people on their net impact on the performance of the organization.\nFast iteration can make up for a lot; it\u0026rsquo;s usually ok to be wrong if you iterate quickly. Plans should be measured in decades, execution should be measured in weeks.\nDon\u0026rsquo;t fight the business equivalent of the laws of physics.\nInspiration is perishable and life goes by fast. Inaction is a particularly insidious type of risk.\nScale often has surprising emergent properties.\nCompounding exponentials are magic. In particular, you really want to build a business that gets a compounding advantage with scale.\nGet back up and keep going.\nWorking with great people is one of the best parts of life.\n","permalink":"https://chenterry.com/posts/how-to-work-on-what-you-love/","summary":"\u003cp\u003eEvery now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\u003c/p\u003e\n\u003ch2 id=\"patrick-collison\"\u003ePatrick Collison:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGo deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\u003c/p\u003e","title":"How to Work on What You Love: Career Advice from Top Entrepreneurs"},{"content":"Is it creative to screenshot someone else\u0026rsquo;s video and caption it with other people\u0026rsquo;s comments? This seemingly simple question hits every creator making rent from content: if AI can remix, analyze, and generate at scale, what\u0026rsquo;s left that\u0026rsquo;s genuinely yours?\nThrough building AI content tools, I\u0026rsquo;ve discovered the real opportunity isn\u0026rsquo;t AI replacing creators—it\u0026rsquo;s AI helping creators understand the massive amounts of content data around them to find genuinely fresh angles. Think of it as having a research team that can analyze millions of posts, comments, and engagement patterns in seconds, then surface the insights that lead to truly original work.\nTikTok's content creation interface: Where creators combine video, audio, and engagement elements to build viral content\nNote (Nov 4th, 2025): I was actually able to speak to the head of strategy for Mr.Beast and surprisingly enough (or not surprisingly) this is exactly what they are doing and what makes their content so successful - they look for outliers in mass amounts of data - finding videos that genuinely spark viewers\u0026rsquo; interest, even if it\u0026rsquo;s in a different domain - think a Minecraft simulation with 100 players on each island (male/female) and recreating that with actual human participants.\nComment analytics revealing audience engagement patterns and content resonance across different demographics\nContent structure breakdown: How platforms organize multimodal content across video, interaction data, and comments\nLet\u0026rsquo;s touch on how, as we can see in this breakdown, what gets abstracted away to just tabular blobs of text contains much information on how information is presented and interacted with. The visual elements and text within the video grab a user\u0026rsquo;s attention, while the audio provides voiceover narrating the message (or sometimes relevant background music). The titles and description provide detailed information on the video, while the comments section—something often overlooked in current processing workflows—presents a goldmine of user interaction and feedback.\nThe primary comments act as tier 1 opinions, with responses and likes serving as interaction trackers. If users have the same opinion, they\u0026rsquo;ll usually hit the like button instead of posting the exact same thing again. By linking the content of the video to actual interactions (both tier 1 and tier 2), we get a polling of audience feedback that is timely and beyond anything we can do in the same amount of time with surveys or really any other kind of data collection.\nAI isn\u0026rsquo;t becoming creative—it\u0026rsquo;s becoming the ultimate creative research assistant. While generative AI struggles to produce truly fresh perspectives, it excels at helping us understand information and generate new insights that lead to genuinely creative work.\nWhat Constitutes Creative Work? To understand AI\u0026rsquo;s role in creativity, we need to establish clear boundaries around what constitutes creative work. Consider the common practice of taking screenshots from viral videos and adding captions from popular comments. While this involves some editing, it\u0026rsquo;s essentially sophisticated copying that accelerates content diffusion while reducing the economic returns of original creation—what economist Schumpeter called \u0026ldquo;creative destruction\u0026rdquo; in reverse.\nUnderstanding how platforms structure multimodal content helps us see the complexity involved in creative work.\nReal creativity is about choosing a unique perspective. Content with contrast or conflict naturally captures our attention—think of viral TikToks that expose workplace absurdities or Twitter threads that challenge conventional wisdom. But thoughtful, empathetic content is equally creative: the YouTube essayist who helps you understand your own anxiety, or the LinkedIn post that perfectly articulates what you\u0026rsquo;ve been feeling about remote work.\nHere\u0026rsquo;s how I think about the creative ecosystem: there\u0026rsquo;s production (generating new content) and diffusion (deriving from or spreading existing content). AI\u0026rsquo;s sweet spot isn\u0026rsquo;t in either category alone—it\u0026rsquo;s in helping us understand massive amounts of information to find genuinely new angles and insights.\nRight now, creating professional-quality video requires hours of shooting, editing, and post-production. As AI gets better at handling video, audio, and text together, we\u0026rsquo;re heading toward a world where that same video could be produced in minutes. This changes everything about the creative economy, making tools that help you find new inspiration increasingly valuable. Through this creative assistance, we can achieve two main effects:\nTwo primary effects of AI creative assistance: Inspiration acquisition and content derivation\nInspiration acquisition: Accelerating original content production by collapsing the draft → iterate loop Content derivation: Accelerating the diffusion of quality creative work across formats and channels Content Understanding for Enhanced Generation How can we make language models produce outputs that meet our expectations? This challenge breaks down into two distinct problems: (1) we don\u0026rsquo;t know what our ideal output looks like, and (2) we know what we want, but the language model doesn\u0026rsquo;t understand us.\nMost teams focus on the second problem through a toolkit of techniques: model alignment (training AI to follow human preferences), prompting (crafting better instructions), few-shot learning (training AI with just a few examples), retrieval-augmented generation or RAG (helping AI access specific databases), fine-tuning (customizing AI for specific tasks), and memory systems (helping AI remember context across conversations). But companies are rapidly commoditizing these approaches—many solutions are open-sourced, which explains why so many generative products deliver roughly comparable results.\nDifferent types of AI workflows: From basic generation to iterative collaboration\nThe real differentiation lies in how we adapt engineering and data processing to specific business scenarios—and more importantly, in solving the first problem: helping users understand what they actually want to create.\nAugust 2025: Recently I was asked about what the role of a PM in building technical products stands, especially given how much of the model side work is handled by technical teams. I think the answer, in short, is understanding of the product, and making sense of data in relation to how they are communicated and what they express. One may deal with numbers, but it\u0026rsquo;s harder to actually understand the people that make up those numbers. In the concrete example of multimodal content understanding, it\u0026rsquo;s being able to preserve the very granularity that makes this data so valuable, and proposing technical solutions—modality alignment, weighted clustering, agent triage, content rewrite, etc.\nBrand Understanding and AI Integration Brand intelligence platform: AI systems learning to understand brand voice, visual identity, and content guidelines for contextual generation\nThe evolution toward brand-aware AI represents a significant shift in content generation capabilities. Instead of producing generic output that requires extensive human editing, these systems can understand context—what works for a luxury brand versus a startup, what tone resonates with different demographics, what visual styles align with brand guidelines.\nNovember 2025: I saw the recent release of Google Pomelli and I think this is a great example of how a general purpose technology moves from research and public beta to a more grounded and applied case that delivers actual time saving and value. Like TypeFace, it\u0026rsquo;s essentially creating a brand kit to free users from prompting repeatedly, and often times not knowing how to most effectively describe their style.\nAI brand training interface: How multimodal brand kits teach AI systems to generate content that aligns with specific brand requirements\nThe training process involves feeding AI systems examples of successful brand content across multiple modalities—text, images, videos, and audio. The system learns not just what the brand says, but how it says it, what visual elements it uses, and what emotional tone it maintains. This creates AI that can generate content that feels authentically on-brand without constant human oversight.\nReturning to the first problem—\u0026ldquo;I don\u0026rsquo;t know what output I want\u0026rdquo;—this stems from a lack of content understanding. Good script writing requires more than just hooks (\u0026ldquo;You won\u0026rsquo;t believe what happens next\u0026rdquo;), unique selling propositions (USPs), and calls-to-action (CTAs)—it needs a clear angle: content that resonates with the audience, fits the context, and achieves its purpose.\nSome products are building brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will probably become standard, the real breakthrough would be connecting insight data with generation without requiring manual setup every time.\nUnderstanding User Needs Looking at the creative technology landscape, every category—ad aggregation, competitor tracking, brand insights, performance analysis, content generation—has 3-4 companies offering basically the same thing. The data products feel traditional, while the AI products often just add ChatGPT integrations to existing workflows.\nThe real opportunity lies in acquiring more granular data and creating smoother interactions. Instead of isolated tools, imagine connecting the entire creative production process where you can participate and adjust at each stage—from initial research through final publication.\nHere\u0026rsquo;s a simple way to think about product value: user value = new experience - old experience - replacement cost. Most products built on foundational language models with minor tweaks deliver limited incremental value. Users still need to craft personalized prompts, and outputs almost always require multiple rounds of editing before they\u0026rsquo;re usable.\nSo how do we increase incremental value? The answer isn\u0026rsquo;t just better AI—it\u0026rsquo;s better workflows.\nUser-Friendly Workflows Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.\nThe concept of \u0026ldquo;workflows, not skills\u0026rdquo; addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.\nI used to think that simply connecting AI capabilities constitutes a workflow, but that\u0026rsquo;s like saying a toolbox is the same as knowing how to build a house. What we call \u0026ldquo;Language UI\u0026rdquo; is actually \u0026ldquo;Prompt UI\u0026rdquo;—it differs from true language interaction by missing the context and shared understanding present in human conversation.\nThink about the difference: you can tell a colleague \u0026ldquo;make this more engaging\u0026rdquo; and they understand your brand, audience, and context. With ChatGPT, you need to write a novel-length prompt every single time explaining who you are, what you\u0026rsquo;re building, and what \u0026ldquo;engaging\u0026rdquo; means in your specific context.\nThe future workflow tools will have human-like elements—they\u0026rsquo;ll ask follow-up questions, remember previous conversations, and understand your specific goals without you having to explain everything from scratch. Current prompting is probably transitional; eventually, we\u0026rsquo;ll eliminate the need for context-heavy prompts by building AI that understands your context and generates appropriate guidance automatically.\nMultimodal Interaction and Content Ecosystem Finally, let\u0026rsquo;s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.\nUnderstanding Content Through Data Visualization The first layer of multimodal content understanding goes beyond traditional analytics. Rather than just tracking views and likes, the most interesting insights come from clustering comments and opinion spread by category—product feedback, creator engagement, emotional responses. This granular analysis reveals patterns in audience sentiment and helps creators understand not just what performs well, but why it resonates with different audience segments.\nVector visualization analysis: AI-powered semantic mapping revealing hidden relationships between content themes and audience preferences\nBrand intelligence extraction: Analyzing fine-grained insights from audience feedback and engagement patterns\nBut the real magic happens in semantic analysis. Vector embeddings can reveal hidden relationships between content themes that humans might miss. For example, videos about \u0026ldquo;productivity tips\u0026rdquo; might cluster surprisingly close to \u0026ldquo;cooking tutorials\u0026rdquo; because both satisfy the same underlying need for life optimization. This kind of insight helps creators find unexpected angles and untapped niches.\nContent performance metrics: Comprehensive analysis tracking engagement patterns, reach optimization, and conversion effectiveness across content types\nThe final piece is comprehensive performance analysis that connects creative decisions to business outcomes. This isn\u0026rsquo;t just about vanity metrics—it\u0026rsquo;s about understanding which content patterns lead to sustainable audience growth, higher conversion rates, and long-term creator success. When you can see these patterns clearly, you can make more informed creative decisions.\nSwitching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren\u0026rsquo;t just people; they\u0026rsquo;re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.\nCase Study: Voice Synthesis Take voice synthesis as an example. Technically, this technology is already quite mature—you can clone a voice with just a few minutes of audio. Yet when most people think about AI voice cloning, they imagine phone scams. Sure, there are fun projects like AI David Attenborough narrating random videos, or OpenAI\u0026rsquo;s GPT-4o launch event that briefly simulated Samantha\u0026rsquo;s voice from \u0026ldquo;Her.\u0026rdquo; But the most creative use I\u0026rsquo;ve seen comes from short video creators.\nI recently discovered a creator called \u0026ldquo;Yi Tou Jue Lv\u0026rdquo; who makes derivative content based on \u0026ldquo;In the Name of the People\u0026rdquo; (a 2017 Chinese political drama). Their videos consistently get 500K+ views by doing something brilliant: they take original footage but replace all the narration with AI-synthesized character voices speaking internal monologues and psychological commentary. The result feels like getting inside the characters\u0026rsquo; heads in a way the original show never offered.\nWhat makes this work is the creator\u0026rsquo;s deep understanding of the characters combined with AI\u0026rsquo;s ability to generate consistent, high-quality voice synthesis. They\u0026rsquo;re not just copying—they\u0026rsquo;re creating a completely new layer of interpretation that audiences can\u0026rsquo;t get anywhere else.\nContrasting Audio \u0026amp; Text It\u0026rsquo;s fascinating how differently our brains process audio and text. When we read, we\u0026rsquo;re essentially interacting with a graphical user interface—scanning, jumping between sections, processing information at our own pace. We\u0026rsquo;ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text can feel less engaging than a good conversation.\nAudio interfaces evolution: From podcast consumption (left) to social audio saving (right)—platforms adapting to our increasingly audio-first content behaviors and the need to preserve valuable conversations\nSpeaking, in contrast, is inherently linear and social. There\u0026rsquo;s something about the human voice that keeps us present—the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It\u0026rsquo;s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.\nThis contrast reveals something deeper about how we process information. Text excels at conveying complex ideas—we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of text—where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.\nConclusion Roland Barthes' \"Death of the Author\": When content is created, interpretation rights transfer to the audience\nAs Roland Barthes suggested with \u0026ldquo;The Death of the Author,\u0026rdquo; once content is created, interpretation rights transfer to the audience. We see this everywhere today—YouTube channels that analyze every Marvel movie, TikTok accounts that remix old TV shows, podcast networks that dissect every episode of popular series.\nWith improvements in AI voice synthesis, character generation, and content manipulation, we\u0026rsquo;re approaching a future where derivative works based on original intellectual properties can achieve professional quality while satisfying different interpretations and imaginations. The \u0026ldquo;Yi Tou Jue Lv\u0026rdquo; example I mentioned earlier is just the beginning.\nThese perspectives might all exist in the original work, but each remix offers a different angle, providing audiences with unique experiences. There\u0026rsquo;s still massive amounts of content that people want to see but isn\u0026rsquo;t available on any platform. Maybe creativity\u0026rsquo;s next evolution isn\u0026rsquo;t about generating entirely new content—it\u0026rsquo;s about intelligently remixing and reinterpreting what already exists to better satisfy what audiences actually want.\nFinal Thoughts While generative AI capabilities evolve rapidly, human nature changes slowly. We overestimate technology\u0026rsquo;s short-term creative impact (AI won\u0026rsquo;t replace human creativity next year), but underestimate how fundamentally it will change creative workflows (it\u0026rsquo;s already happening in ways we\u0026rsquo;re only beginning to understand).\nMaking probabilistic models truly creative remains challenging yet fascinating work. The future lies not in AI replacing human creativity, but in building systems that amplify our ability to understand, synthesize, and create meaning from the infinite streams of content around us. That\u0026rsquo;s the creative challenge I\u0026rsquo;ll continue working on.\nAppendix This article was originally developed as a presentation and shared internally with TikTok team members during my time there. The content has been adapted for public publication and adjusted to remove potentially sensitive information while preserving the core insights about AI and creativity.\nAll views expressed in this article are my own and do not represent the official positions or strategies of TikTok or any other organization.\n","permalink":"https://chenterry.com/posts/essense_of_creativity/","summary":"\u003cp\u003eIs it creative to screenshot someone else\u0026rsquo;s video and caption it with other people\u0026rsquo;s comments? This seemingly simple question hits every creator making rent from content: if AI can remix, analyze, and generate at scale, what\u0026rsquo;s left that\u0026rsquo;s genuinely yours?\u003c/p\u003e\n\u003cp\u003eThrough building AI content tools, I\u0026rsquo;ve discovered the real opportunity isn\u0026rsquo;t AI replacing creators—it\u0026rsquo;s AI helping creators understand the massive amounts of content data around them to find genuinely fresh angles. Think of it as having a research team that can analyze millions of posts, comments, and engagement patterns in seconds, then surface the insights that lead to truly original work.\u003c/p\u003e","title":"Essence of Creativity: Future of Creative Work"},{"content":"Terry Chen, Allyson Lee\nYour engineering team is stuck. Again.\nIt\u0026rsquo;s not that they lack the technical skills—they can write clean code and architect complex systems. But watch them tackle ambiguous problems and you\u0026rsquo;ll see the real bottleneck: they overthink requirements, jump to solutions too quickly, or get paralyzed trying to make everything perfect before shipping.\nThese aren\u0026rsquo;t skill gaps. They\u0026rsquo;re self-regulation patterns that determine whether talented people build breakthrough products or get trapped in endless cycles of \u0026ldquo;almost ready to launch.\u0026rdquo;\nWhat if AI could understand not just what people are struggling with, but how they\u0026rsquo;re struggling—and then connect them with others who\u0026rsquo;ve broken through the same patterns?\nWe built an AI coaching system that does exactly this. Instead of treating learning like a checklist, it recognizes the messy reality of how high-performers actually develop breakthrough capabilities.\nOur AI system analyzes coaching conversations to identify specific learning regulation gaps and suggests targeted interventions\nNovice to Expert Learning System\nTry the Live Prototype ➤ Access the LLM Coaching System Prototype\nExperience our AI coaching system firsthand—upload a coaching conversation and see how it identifies learning patterns and suggests peer connections. The app is embedded directly in our site for seamless interaction.\nThe Real Problem with Educational AI Here\u0026rsquo;s what most people miss about learning: it\u0026rsquo;s not about information transfer. It\u0026rsquo;s about developing self-regulation—the ability to plan, execute, overcome challenges, seek help, and reflect on your own learning process.\nThink about the last time you got stuck on a complex problem. Maybe you procrastinated, or dove into details too early, or got paralyzed trying to make it perfect. These aren\u0026rsquo;t character flaws—they\u0026rsquo;re specific patterns of how you regulate your learning. And they\u0026rsquo;re predictable.\nTraditional coaching can identify these patterns, but it doesn\u0026rsquo;t scale. A good coach might work with 20 students max. Meanwhile, thousands of students struggle with the same regulation gaps, often in isolation.\nWe realized something: what if we could capture the expertise of great coaches and make it accessible to everyone? Not through generic advice, but through intelligent peer connections based on actual learning patterns.\nHow We Built AI That Actually Gets Learning Our solution, called \u0026ldquo;Peer Connections,\u0026rdquo; has three core components:\nRecord and analyze coaching conversations to identify specific regulation gaps Match students with peers who\u0026rsquo;ve successfully worked through similar challenges Facilitate meaningful conversations that help students develop concrete action plans The magic happens in how we taught our AI to recognize learning patterns that humans care about.\nThe Challenge: Teaching AI to Recognize Learning Patterns The first challenge was getting AI to understand the difference between \u0026ldquo;student struggles with visual representation\u0026rdquo; and \u0026ldquo;student fears imperfection.\u0026rdquo; Both might result in poor deliverables, but they require completely different interventions.\nWe built a structured codebook that categorizes learning regulation into three core types:\nOur AI categorizes learning challenges into specific, actionable patterns that enable precise peer matching\nCognitive gaps: Problems with approaching unknown challenges (like creating clear visual representations)\nMetacognitive gaps: Issues with planning, help-seeking, and reflection (like not slicing work effectively)\nEmotional gaps: Dispositions toward learning that affect motivation (like fear of imperfection)\nThe breakthrough came when we realized that pure semantic matching wasn\u0026rsquo;t enough. Two students could describe their struggles completely differently but face the same underlying regulation pattern. Through testing on 28 manually coded CAP notes, our hybrid approach combines LLM-based pattern recognition with semantic similarity, achieving 87.5% precision and 89.3% recall in identifying regulation gaps—performance that rivals human expert consistency in this domain.\nMaking Peer Learning Actually Work But identifying learning patterns was only half the challenge. The harder question was: how do you facilitate a conversation between two students that actually helps?\nMost peer learning fails because experienced students struggle to articulate their process. They remember what worked but not why it worked, or how they figured it out. Meanwhile, novices often ask surface-level questions that don\u0026rsquo;t get to the real learning.\nWe discovered that effective peer conversations need structure. Not scripts, but scaffolding that helps both students navigate the conversation productively.\nThe Conversation Framework We tested different approaches to peer conversation facilitation. What we found was transformative:\nBefore structured conversation: \u0026ldquo;I have no time to get things done, since my internship is starting tomorrow. There\u0026rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it\u0026rsquo;s even lower.\u0026rdquo;\nAfter structured conversation: \u0026ldquo;You know what, I\u0026rsquo;m gonna send this to my partner first, and then I\u0026rsquo;m gonna send this to Haoqi.\u0026rdquo;\nThe shift was remarkable. The student went from resignation and self-doubt to concrete action planning. This wasn\u0026rsquo;t just emotional support—it was a structured process that helped her move from paralysis to progress.\nSystem Description Our system consists of three key components. First, users upload an audio recording of their conversation with the coach. Then, our system matches the user with a student who experienced and addressed that regulation gap in the past. Finally, we help them facilitate a conversation that guides how they will plan their next sprint.\n1. Speech-to-text Transcription The first component of our system utilizes speech-to-text transcription with NLP-based feedback, to intake a student\u0026rsquo;s audio recording of their conversation with the coach, allowing the system to analyze the student\u0026rsquo;s regulation gap, while reducing the time restraints required by CAP notes.\nFigure 1: Speech-to-text Transcription System Interface\n2. Student regulation gap analysis system Thereafter, we employ a student regulation gap analysis system that leverages vector embeddings and large language models to identify patterns in student regulation behaviors across learning contexts. The system provides targeted coaching suggestions based on similar historical cases, addressing the challenge of effective coaching for developing regulation skills in design, research, and STEM innovation.\nOur system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach. Initially, we pre-process student regulation notes to include metadata on tier 1 and tier 2 regulation gaps, then encode them into text embeddings. The vector database retrieves the most similar historical cases, and we send these along with the original query to an LLM (Deepseek). The LLM generates a structured response including a diagnosis of potential regulation gaps, practice suggestions targeted to these gaps, and references to similar historical cases. This approach grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.\nFigure 2: Student Regulation Gap Analysis Page\nI. Data Preprocessing We exported all existing notes from the CAP (Context-Assessment-Plan) note system and encountered several challenges during preprocessing. Many notes lacked clear assessment sections identifying regulation gaps, provided insufficient context about the student\u0026rsquo;s situation, and used inconsistent terminology to describe similar regulation gaps. To address these issues, we removed duplicated notes and filtered out entries with incomplete fields. To better structure the data for our system, we standardized the information into several key fields.\nThese fields include a unique identifier for each case, the key learning gap identified in the assessment, additional information such as project details, title, context, and plan, the complete original coaching note, the project name, high-level regulation categories (e.g., \u0026ldquo;Cognitive,\u0026rdquo; \u0026ldquo;Metacognitive\u0026rdquo;), and more specific skill categories (e.g., \u0026ldquo;Critical thinking,\u0026rdquo; \u0026ldquo;Forming feasible plans\u0026rdquo;). We generated the last two fields—tier1_categories and tier2_categories—using the Deepseek reasoning model with a specialized regulation skills codebook that provides the conceptual backbone for our semantic matching approach.\nFigure 3: Organized CAP Note Structure\nII. The Regulation Skills Codebook: Structured Knowledge for Semantic Matching Our key breakthrough was integrating a structured codebook that categorizes student regulation gaps according to a research-grounded framework—enabling our AI to match students based on learning patterns rather than just surface-level problem descriptions. For example, a student saying \u0026ldquo;I\u0026rsquo;m stuck on the prototype\u0026rdquo; and another saying \u0026ldquo;I can\u0026rsquo;t visualize the system architecture\u0026rdquo; would both be matched under \u0026ldquo;Representing problem and solution spaces,\u0026rdquo; even though their language is completely different. The codebook provides a hierarchical organization of regulation skills across three domains, as detailed in Appendix E. This framework serves as the foundation for our semantic matching system.\nThe codebook defines three main categories of regulation skills: Cognitive Skills, which encompass abilities for approaching problems with unknown answers; Metacognitive Skills, which include capacities for planning, help-seeking, collaboration, and reflection; and Emotional Regulation, which covers dispositions toward self and learning that affect motivation.\nIII. Bridging Language and Concepts: Semantic Enrichment The codebook functions as a semantic bridge between varied descriptions of similar problems. Consider examples such as \u0026ldquo;Student struggles to create clear visual representations of system architecture\u0026rdquo; (categorized as Cognitive \u0026gt; Representing problem and solution spaces), \u0026ldquo;Prototype missing key feature\u0026rdquo; (also Cognitive \u0026gt; Representing problem and solution spaces), \u0026ldquo;Not slicing work to risk\u0026rdquo; (Metacognitive \u0026gt; Planning effective iterations), and \u0026ldquo;Student gets stuck and then stops thinking about strategy\u0026rdquo; (Emotional \u0026gt; Embracing challenges/learning/independence). Even without textual similarity, the codebook categorization reveals conceptual relationships. For instance, we categorize both \u0026ldquo;stopping too soon with examples\u0026rdquo; and \u0026ldquo;not thinking deeply about risks\u0026rdquo; under \u0026ldquo;Critical thinking and argumentation,\u0026rdquo; enabling the system to recognize their conceptual similarity despite different phrasing.\nThis structured approach provides several advantages. It standardizes vocabulary across different coaching contexts, reveals underlying patterns in student regulation behavior, enables transfer of coaching knowledge across project domains, and prioritizes skill-based similarity over surface-level textual matching.\nGap Description Tier 1 Tier 2 \u0026ldquo;Student struggles to create clear visual representations of system architecture\u0026rdquo; Cognitive Representing problem and solution spaces \u0026ldquo;Prototype missing key feature\u0026rdquo; Cognitive Representing problem and solution spaces \u0026ldquo;Not slicing work to risk\u0026rdquo; Metacognitive Planning effective iterations \u0026ldquo;Student gets stuck and then stops thinking about strategy\u0026rdquo; Emotional Embracing challenges/learning/independence IV. Similarity Methods for Regulation Gap Analysis To find cases with the same regulation gaps, we started with the semantic-based approach and gradually enhanced our methods. Our initial baseline approach uses vector embeddings to find similar cases based on textual similarity. We encode the full text of student issues using OpenAI\u0026rsquo;s ada-002 embedding model and compare them using cosine similarity. This pure semantic similarity approach treats all text equally and serves as a useful baseline. However, while straightforward, this approach often prioritizes surface-level similarities like project domain rather than underlying regulation patterns.\nTo improve matching relevance, we enhanced our approach by separating the regulation gap description from contextual information, applying higher weight to the gap text (default: 0.7 for regulation gap, 0.3 for other content). The gap text often contains the most critical information about the learning challenge, so it deserves higher weight. This weighted semantic similarity approach better identifies regulation similarities even when project contexts differ, as it emphasizes the specific regulation gap rather than surrounding information.\nHowever, we also realized limitations with the semantic-based matching methods, namely that semantic similarity only worked when there were apparent keywords to match. To address this limitation, we created a comprehensive codebook containing regulation gap definitions and examples, using a reasoning model (Deepseek-reasoning) to generate metadata with tier 1 and tier 2 regulation gap tags for enhanced matching. This LLM reasoning with codebook approach assigns the highest weight (0.5) to tier 2 categories and a lower weight (0.1) to tier 1 categories, with gap text and other content each receiving 0.2 weight. This metadata represents specific regulation skills, allowing the system to match cases addressing similar skills regardless of how they\u0026rsquo;re described or in what context they appear.\nV. Applying the Codebook in Evaluation Scripts The codebook plays a crucial role in the tiered similarity evaluation methods. The categorization process begins with the categorize_regulation_gaps_deepseek_v0.2.py script, which uses the codebook definitions to classify each gap text into tier 1 and tier 2 categories. Each gap is analyzed against the codebook\u0026rsquo;s framework, identifying which cognitive, metacognitive, or emotional regulation skills are being addressed. We then store these categorizations in the tiered_weighted_cases.json file as structured metadata for later retrieval and analysis.\nThe codebook improves evaluation outcomes in several important ways. It provides a standardized vocabulary with a consistent framework of terms and concepts, helping to match cases that use different wording but address the same underlying skills. It enables contextual understanding by categorizing gaps according to the codebook, allowing the system to understand the deeper educational context beyond surface-level language similarities. The codebook\u0026rsquo;s three-category structure (cognitive, metacognitive, emotional) aligns with research on key regulation skills central to the situated practice system, providing coaching concept integration. Even when projects differ completely, the codebook categories help identify transferable skills and learning patterns across domains, enabling cross-project relevance. The higher weight given to tier 2 categories (0.5) in the tiered similarity approach reflects their importance in precisely identifying the specific skill gaps being addressed.\nFor instance, when processing a gap like \u0026ldquo;stopping too soon with examples,\u0026rdquo; without the codebook approach, the system might only match cases with similar phrasing about \u0026ldquo;stopping\u0026rdquo; or \u0026ldquo;examples.\u0026rdquo; With the codebook integration, this gap is properly categorized as \u0026ldquo;Cognitive \u0026gt; Critical thinking and argumentation,\u0026rdquo; allowing matches to conceptually similar gaps like \u0026ldquo;not thinking deeply enough about risks\u0026rdquo; even when the phrasing differs completely.\n3. Facilitating Conversation and Formulating Tailored Plans To generate helpful and contextualized conversation facilitation questions, we created a codebook based on Choi et al.\u0026rsquo;s question methodology, then prompted an LLM to generate responses based on conversation context (streamed) and codebook [20] (See Appendix F). The codebook and question generation is still within its early stages, until we conduct more user testing to discover the most pertinent questions to ask. Additionally, while not implemented yet, formulating tailored plans would take place in the form of filling out their sprint log/plan for their week, which they would be either linked to or the system will parse it into the sheet for them. This will allow them to enact their plans to address their regulation gap while still incorporating the tasks they need to complete for the week.\nFigure 4: Suggested Discussion Questions\nStudy / Experiment / Deployment We conducted manual qualitative coding on a set of 28 CAP notes utilizing our codebook definitions to use as ground truth to evaluate our model against. When comparing our tier 2 codes on these notes with the codes given by our system, we observed a precision of 0.875, and a recall of 0.893. Given the possible subjectivity and overlap of regulation gaps, these results suggest we can rely on our system to accurately categorize student cases’ regulation gaps.\nNext, we investigated how to facilitate effective conversation between the peers once they are matched. Most notably, how do we define what an “effective” conversation looks like?\nTo begin, we conducted some casual conversations with peers in the DTR program, asking them to explain about an issue they struggled with in the past, and how they were able to overcome it. Our conversations had no structure, as it was just to give us an idea of how students talked about regulation with a peer. From this preliminary research, we observed the main finding that experienced students struggle to articulate their experience in a way that is helpful to the peer. This is due to several factors:\nA lack of project context/terminology Not articulating their experience addressing the regulation gap in a step-by-step, clear way, making it hard for the peer to abstract their thinking process into a high-level framework they could use Both peers have trouble remembering what they experienced and what actionable next steps to take\nBased upon these findings, we defined an \u0026ldquo;effective\u0026rdquo; conversation to be one that helps them create a more concrete, actionable plan than they had before, and that they actually implement this and solve their regulation gap faster. We then identified this causal structure for which to guide our prototype and user testing:\nFigure: Causal structure diagram showing the relationship between peer conversations and regulation gap resolution\nGiven this causal diagram, we set up a second, more formal user testing scenario. Given the work of Choi et al., which highlights how scaffolding peer-questioning strategies can help facilitate metacognition, we formulated a list of questions that fell under the clarification/elaboration and ⁠⁠context/perspective-oriented categories suggested in the paper [20]. We created two lists of questions, one for the novice and one for the experienced peer, to guide the conversation, in the goal of assisting the experienced peer in articulating their experience in a step-by-step manner, and helping the novice understand how they can apply this to their own situation (See Appendix G).\nWe then conducted some preliminary user testing, by having one of us act as the experienced peer, and having another student in DTR be the novice. We simulated the conversation by having the novice ask us the questions provided in the scaffold, while we acted as the experienced peer answering the questions and providing advice. To qualitatively assess how the conversation influenced the peer, we asked them the same questions before and after (See Appendix H). During one of our user tests, one of our users, who struggled with the fear of imperfection, demonstrated this transformation:\nBefore peer conversation After peer conversation \u0026ldquo;I have no time to get things done, since my internship is starting tomorrow. There\u0026rsquo;s a 6/10 chance that I actually take the steps I know I need to take, and actually I think it\u0026rsquo;s even lower.\u0026rdquo; \u0026ldquo;You know what, I\u0026rsquo;m gonna send this to my partner first, and then I\u0026rsquo;m gonna send this to Haoqi.\u0026rdquo; This illustrates multiple findings, first that the student shifted their mindset towards already deciding that she wasn\u0026rsquo;t going to get what she wanted to do, despite not having started yet, to taking actionable steps towards working on her project and addressing her regulation gap. Furthermore, the student changed the steps they were going to dedicate time halfway between her tasks for the week to send deliverables over to the coach. The next step would be to follow up and see if this was accomplished, and how it affected her project outcome and regulation.\nDiscussion Our system effectively facilitates AI-enhanced coaching by leveraging Large Language Models (LLMs) and structured metadata to support project-based learning environments. By integrating semantic matching with structured codebook metadata, our approach identifies relevant coaching cases, reducing cognitive load on mentors while maintaining high-quality, context-aware feedback. We also began to explore how to facilitate effective and productive conversation amongst peers about regulation gaps. This discussion outlines the generalizable design elements and techniques that contributed to the prototype’s effectiveness, highlighting key takeaways for future socio-technical system development. Hybrid AI-Driven Case Retrieval: Our system employs a hybrid approach that combines LLM-driven metadata tagging with traditional semantic matching. While pure semantic similarity methods struggled to capture nuanced regulation gaps due to limited repetitive terminology, and LLM-based approaches returned too many broadly relevant cases, integrating both techniques enabled precision in retrieving the most relevant coaching cases. This demonstrates the efficacy of hybrid AI-driven retrieval in domains where contextual similarity and structured knowledge are both crucial. Future systems designed for education or expert-driven fields can benefit from this combined methodology to ensure both relevance and specificity in recommendations.\nStructured Codebooks for Domain-Specific AI Applications: A key enabler of our system\u0026rsquo;s success was the development of a structured codebook that categorizes regulation gaps into tiered classifications, allowing advanced reasoning models to more accurately categorize regulation gaps. By leveraging cognitive, metacognitive, and emotional regulation categories, the system grounded LLM-based reasoning in expert-validated pedagogical frameworks and effectively addresses the issue of LLM hallucinations. The implication for broader AI applications is that structured codebooks can serve as a mechanism to guide AI reasoning, especially for test time scaled models, and in fields requiring human-like judgment and contextual understanding.\nScaffolding productive conversations for addressing regulation gaps: Providing enough context and terminology of the other\u0026rsquo;s project and issue is crucial to set them up for success. Additionally, adopting peer-questioning scaffolds assists in driving a conversation where the more experienced peer can better articulate and reflect upon their step-by-step strategy to addressing their regulation gap, which can be adapted and applied by the novice.\nLimitations and Future Work There are inherent limitations in our current approach that necessitate attention. The CAP note system is succinctly written and does not always provide sufficient context for robust reasoning. Additionally, our codebook currently focuses primarily on high-level descriptions of regulation gaps along with their examples, which makes it difficult for the language model to develop domain-specific knowledge and reasoning for effective categorization. To address these limitations and plan for iterative improvements, we will work with CAP note developers to identify ways toward either:\nImproving clarity of writing in notes Collecting more data through alternative data sources (SIG meeting transcripts, etc.) For future work, we propose developing a sub-categorized codebook that further segments existing regulation gaps and contains specific examples along with reasoning chains for arriving at regulation gap categorizations. With such a codebook, we can first perform a tier 1 categorization of the gap to route to a corresponding model identifying each of the subcategories for further reasoning (using two sets of language models and corresponding reasoning prompts) for few-shot learning and prompt-based LLM-enabled categorization.\nAs LLMs continue to progress, we believe there is merit in more sophisticated reasoning methods such as the use of external knowledge bases or memory systems for persistent storage of student regulation gap progression. These approaches could enable more tailored and precise gap understanding, ultimately leading to more effective coaching support for developing regulation skills in design, research, and STEM innovation contexts.\nIn terms of facilitating effective conversation between peers, we plan to continually design testing strategies of feature slices, focusing on evaluating what intervention strategies are most effective in facilitating peers to more clearly articulate and discuss their task plans in context of their regulation gaps, thereby facilitating meaningful peer to peer interaction enabling effective regulation skill learning. This involves testing whether real time generation of follow up questions for explanation elaboration or regulation skill connection allows better articulation of questions during peer to peer conversations, as well as whether more context regarding the peer’s project and regulation skills should be provided in advance to foster more meaningful conversations grounded in discussion of actionable next steps for regulation skill learning.\nMost notably, there is still more work to be done with testing these strategies, specifically figuring out how to ensure that it impacts students not just in the moment, but in the long-term. To do so, we plan to run formal user tests which both qualitatively and quantitatively examine how they will and if they are addressing their regulation gap, by surveying them before using the prototype and having the conversation, after, and the subsequent weeks.\nReferences in IEEE Format [1] E. M. Gerber, J. M. Olson, and R. L. D. Komarek, \u0026ldquo;Extracurricular design-based learning: Preparing students for careers in innovation,\u0026rdquo; International Journal of Engineering Education, vol. 28, no. 2, p. 317, 2012. {#ref1}\n[2] J. C. Dunlap, \u0026ldquo;Problem-based learning and self-efficacy: How a capstone course prepares students for a profession,\u0026rdquo; Educational Technology Research and Development, vol. 53, no. 1, pp. 65–83, Mar. 2005. {#ref2}\n[3] B. J. Zimmerman, \u0026ldquo;Becoming a self-regulated learner: An overview,\u0026rdquo; Theory Into Practice, vol. 41, no. 2, pp. 64–70, May 2002. {#ref3}\n[4] D. R. Lewis, M. Easterday, and C. Riesbeck, \u0026ldquo;Research slices: Core processes for effective iteration in eder,\u0026rdquo; EDeR. Educational Design Research, vol. 8, no. 1, 2024. {#ref4}\n[5] D. G. R. Lewis, S. E. Carlson, C. K. Riesbeck, E. M. Gerber, and M. W. Easterday, \u0026ldquo;Encouraging engineering design teams to engage in expert iterative practices with tools to support coaching in problem-based learning,\u0026rdquo; Journal of Engineering Education, vol. 112, no. 4, pp. 1012–1031, 2023. {#ref5}\n[6] E. J. Huang, D. R. Lewis, S. Gaudani, M. Easterday, and E. Gerber, \u0026ldquo;Intelligent coaching systems: Understanding one-to-many coaching for ill-defined problem solving,\u0026rdquo; Proceedings of the ACM on Human-Computer Interaction, vol. 7, no. CSCW1, pp. 138:1–138:24, Apr. 2023. {#ref6}\n[7] H. Zhang, M. Easterday, and S. Shah, \u0026ldquo;Collaborative Research: Situated Practice Systems: Supporting Coaches and Students to Develop Regulation Skills for Design, Research, and STEM Innovation,\u0026rdquo; National Science Foundation Grant Proposal, 2024. {#ref7}\n[8] Asana. https://asana.com/product, 2025. {#ref8}\n[9] Trello. https://trello.com/, 2025. {#ref9}\n[10] Z. Xiao, X. Yuan, Q. V. Liao, R. Abdelghani, and P.-Y. Oudeyer, \u0026ldquo;Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding,\u0026rdquo; in Companion Proceedings of the 28th International Conference on Intelligent User Interfaces, pp. 75–78, 2023. {#ref10}\n[11] C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang, \u0026ldquo;Can large language models transform computational social science?\u0026rdquo; Computational Linguistics, vol. 50, no. 1, pp. 237–291, 2024. {#ref11}\n[12] K. F. Shaalan, \u0026ldquo;An Intelligent Computer Assisted Language Learning System for Arabic Learners,\u0026rdquo; Computer Assisted Language Learning, vol. 18, no. 1-2, pp. 81-108, Feb. 2005. {#ref12}\n[13] E. Mousavinasab, M. Zarifsanaiey, S. R. Niakan Kalhori, M. R. Rakhshan, M. Keikha, and A. Ghazi Saeedi, \u0026ldquo;Intelligent Tutoring Systems: A Systematic Review of Characteristics, Applications, and Evaluation Methods,\u0026rdquo; Interactive Learning Environments, vol. 29, no. 1, pp. 142-163, Jan. 2021. {#ref13}\n[14] M. Zawacki-Richter, V. Marín, M. Bond, and F. Gouverneur, \u0026ldquo;Systematic Review of Research on Artificial Intelligence Applications in Higher Education – Where Are the Educators?\u0026rdquo; International Journal of Educational Technology in Higher Education, vol. 16, no. 1, pp. 1-27, Dec. 2019. {#ref14}\n[15] P. Arnau-González, M. Arevalillo-Herráez, R. Albornoz-De Luise, and D. Arnau, \u0026ldquo;A Methodological Approach to Enable Natural Language Interaction in an Intelligent Tutoring System,\u0026rdquo; Computer Speech \u0026amp; Language, vol. 77, p. 101386, Jun. 2023. {#ref15}\n[16] B. Woolf, \u0026ldquo;Building Intelligent Interactive Tutors: Student-Centered Strategies for Revolutionizing E-Learning,\u0026rdquo; Morgan Kaufmann, 2009. {#ref16}\n[17] K. R. Koedinger and A. Corbett, \u0026ldquo;Cognitive Tutors: Technology Bringing Learning Science to the Classroom,\u0026rdquo; in The Cambridge Handbook of the Learning Sciences, R. K. Sawyer, Ed. Cambridge: Cambridge University Press, 2006, pp. 61-78. {#ref17}\n[18] J. Evens and J. Michael, \u0026ldquo;One-on-One Tutoring by Humans and Computers,\u0026rdquo; Routledge, 2006. {#ref18}\n[19] E. Wenger, \u0026ldquo;Artificial Intelligence and Learning in Context: A Review,\u0026rdquo; AI in Education Journal, vol. 21, no. 4, pp. 457-473, 2022. {#ref19}\n[20] I. Choi, S. M. Land, and A. J. Turgeon, \u0026ldquo;Scaffolding peer-questioning strategies to facilitate metacognition during online small group discussion,\u0026rdquo; Instructional Science, vol. 33, no. 5–6, pp. 483–511, 2005, doi: 10.1007/s11251-005-1277-4. {#ref20}\nAppendix B Improving and Scaling Coaching is part three of Situated Practice Systems (SPS), which offers tools to help coaches and learners understand work practices and develop self-directed innovation skills [7].\nFigure: Situated Practice Systems overview showing the three-part framework\nAppendix C CAP Notes helps coaches elicit information about a student\u0026rsquo;s work issue and regulation gaps by tracking their activities, outputs, and reflections [7].\nFigure: CAP Notes structure showing Context, Assessment, and Plan components\nAppendix D Practice Objects contain information about a student\u0026rsquo;s work issues, current and tracked regulation gaps, suggested practices, and practice traces [7].\nFigure: Practice Objects framework showing work issues, regulation gaps, and practice traces\nAppendix E Prompting of the LLM with the codebook incorporated.\n“You are an expert in analyzing student regulation gaps. You need to categorize each CAP (Context, Assessment, Plan) note into these three tier 1 categories and their corresponding tier 2 categories. Each case may be categorized into multiple tier 1 and tier 2 categories:\nCognitive: The student lacks skills for approaching problems with an unknown answer, or even, knowing what the problem is exactly. This includes: Representing problem and solution spaces: The way the student structures or presents the information is not effectively supporting reasoning, analysis, or communication. Example: Student struggles to create a representation that would help them show a working example and an example where the system breaks. Assessing risks: The student struggles with identifying the riskiest risks and/or prioritizing them. They may skip ahead, do unnecessary and unimportant tasks, or have impractical plans due to not properly addressing and prioritizing the risk. Example: Student wasted time jumping ahead (they created multiple very detailed mockups) when they should\u0026rsquo;ve been focusing on addressing the riskiest risk at hand, which was identifying their target audience. Critical thinking and argumentation: The student struggles to construct well-reasoned arguments supported by evidence or lacks a conceptual understanding of the task at hand. They might find it difficult to identify conceptual differences (they are treating concepts as too similar when they actually have meaningful differences). Example: Student isn\u0026rsquo;t fully understanding what a regulation gap is and how to distinguish between the different types, and keeps taking the wrong changes to their prototypes because of that.\nMetacognitive: The student struggles in areas of planning, help-seeking and collaboration, and reflection. This includes: Forming feasible plans: The student struggles to develop structured, realistic, and actionable plans. This could include what their outcome should be and how to measure their outcome. Example: Student overloaded themselves with tasks this sprint and while they got a lot of it done, it wasn\u0026rsquo;t good work. Planning effective iterations: The student struggles to create a deliverable that addresses the sprint’s riskiest risk. The student may struggle due to problems with slicing (breaking larger problems down), prioritization, or understanding the problem. Example: Student didn\u0026rsquo;t incorporate the feedback the coach gave them last week, so their work this week was not effective. Leveraging resources and seeking help: The skill of identifying and utilizing available materials, expertise from others, and information to enhance their learning and problem-solving.\nEmotional: The student has regulation and dispositions toward self and learning that affects their motivation, cognition, and metacognition. This includes: Fears and anxieties: The student may have a fear of imperfection which causes them to shy away from the work, and/or doesn’t want to try things themselves. Example: Student had a well-planned sprint to carry out, but got too caught up trying to perfectly design the solution rather than creating a first prototype. Embracing challenges and learning: The student tries to brute force their way through a solution or runs away from it, rather than thinking about the strategy and approach. Example: Student\u0026rsquo;s system was not producing their optimal output, so they tried to overfit on one example rather than take a step back and observe what patterns are causing the system to fail.\nBased on the regulation gap (assessment), issue title, and context provided, categorize this case any categories (can be multiple) it applies to (Cognitive, Metacognitive, or Emotional).\nTo help you understand the notes better, here are some definitions of key terms: Slice: A well-defined, manageable portion of a larger task or goal that can be completed within a short timeframe (typically a week), contributing to incremental progress. Sprint: A time-boxed, iterative work cycle in agile project management, typically lasting 2 weeks, during which a team completes a set amount of work toward a project goal. Sprints emphasize rapid progress, continuous feedback, and adaptability. Mysore: A structured learning and practice time where students work on their projects while a mentor provides feedback. SIG: Special Interest Group meetings (SIG meetings) bring together undergraduate students, graduate students, and faculty working on different projects in the same research area. Each SIG is its own mini-studio initially led by a faculty member whose leadership fades over time as a graduate student SIG lead gains competencies in mentoring and becomes the leader of their own SIG. At the start of a sprint, teams share the outcome of their last sprint and present their current sprint plans for review. Halfway through a sprint, teams present their progress and SIG members help devise strategies for overcoming blockers.\nCategorization should be based 80% on the “Assessment” part of the note, as this is the coach’s perceived regulation gap. Be careful not to get confused-you should be categorizing their regulation gap, not the implications of the regulation gap. For instance, let’s take a look at this CAP note:\n\u0026ldquo;Student: Improving and Scaling LLMs for Coaching Improving and Scaling LLMs for Coaching | 2025-02-01 Items of Concern: Really hard to see how the conceptual examples would work / workout Context: - Not sure why a lot of the categories are there.. not sure how they are actually relevant and what you learned from the analysis Assessment: - Analysis not quite showing why the system suggested/showed what it showed? Practice Suggestions: - [self-work] I couldn\u0026rsquo;t tell from the output examples why the system was generating what it was generating. Can you think about a representation that would help you show, for an example category that you think are good and one that isn\u0026rsquo;t, why the system got it right or wrong? Perhaps you can do this by showing component TF-IDF scores for each term, and also by showing some of the matching regulation gaps?\u0026rdquo; Looking at the note as a whole, one might identify it under the tier 2 Representing problem and solution spaces, Critical thinking and argumentation, Forming feasible plans, and Planning effective iterations. However, forming feasible plans and planning effective iterations were identified from the “practice suggestions” section, meaning that this is an implication of the regulation gap rather than the regulation gap itself.\nIn this case, the correct tier 2 categorization of the regulation gap would be Representing problem and solution spaces and Critical thinking and argumentation based on the “assessment” section.\nProvide your reasoning and then your final category choice in this format: Reasoning: [your step-by-step reasoning] Categories: [tier 1 category name] [tier 2 category name]”\nAppendix F Codebook used to generate contextualized conversation facilitation questions [20]. Generate conversation facilitation questions for the user. This scaffolding should include:\nClarification/elaboration questions (seeking missing information) Counter-arguments (expressing disagreement to prompt cognitive conflict) Context/perspective-oriented questions (hypothetical scenarios, different viewpoints) Appendix G Question scaffolds used for the second round of user testing.\nQuestions novice might ask:\nWhat was the issue you were encountering, was it recurring? How does it manifest in the tasks you did? What were the steps you took to address it? What made the process hard? Was there a moment where you did something that helped? Were you able to utilize this framework/way of thinking in other instances where you experienced this regulation gap? How were you able to break the cycle of constantly having this regulation gap? Questions experienced peer might ask:\nWhat is your issue and how is it manifesting in your work? How can I use that framework to address my regulation gap (What analogies can you draw from my experience)? Appendix H Question used in the before and after survey for the second round of user testing.\nWhat are the steps you will take to address your regulation gap this week? What is the high-level framework for addressing this regulation gap? What is your level of confidence in this plan/framework? (Scale of 1-10) How comfortable are you with taking these steps? (Scale of 1-10) What are your goals and project outcomes for this week? ","permalink":"https://chenterry.com/posts/llmcoaching/","summary":"\u003cp\u003e\u003cem\u003eTerry Chen, Allyson Lee\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eYour engineering team is stuck. Again.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s not that they lack the technical skills—they can write clean code and architect complex systems. But watch them tackle ambiguous problems and you\u0026rsquo;ll see the real bottleneck: they overthink requirements, jump to solutions too quickly, or get paralyzed trying to make everything perfect before shipping.\u003c/p\u003e\n\u003cp\u003eThese aren\u0026rsquo;t skill gaps. They\u0026rsquo;re self-regulation patterns that determine whether talented people build breakthrough products or get trapped in endless cycles of \u0026ldquo;almost ready to launch.\u0026rdquo;\u003c/p\u003e","title":"Building AI That Actually Understands How Students Learn"},{"content":"The advertising industry\u0026rsquo;s AI tools problem isn\u0026rsquo;t about generation quality—it\u0026rsquo;s about workflow integration. Most creative AI products today offer impressive individual capabilities but fail catastrophically when creators try to chain them together into actual work processes. A typical ad campaign might require juggling five to ten different AI tools for concept development, script writing, visual generation, voice synthesis, and editing, with creators constantly context-switching between platforms that don\u0026rsquo;t understand each other. The result? AI tools that promise ten-fold efficiency but deliver ten-fold frustration instead.\nDuring my internship building TikTok Symphony Assistant, I learned why the future of creative AI isn\u0026rsquo;t about better models, but about better agent workflows that understand how creativity actually happens. The challenge isn\u0026rsquo;t technical—it\u0026rsquo;s cultural and systemic. Professional creators need to feel like they\u0026rsquo;re directing the AI, not being replaced by it. This means building systems where AI handles the tedious execution while creators focus on strategy, brand voice, and creative direction. The goal is augmentation that preserves creative agency rather than automation that eliminates it.\nThe TikTok Symphony Assistant represents a significant step toward solving this integration problem, leveraging sophisticated agentic workflows to streamline creative processes from ideation through execution. Rather than offering another standalone tool, Symphony Assistant demonstrates how AI can enhance existing creative workflows by understanding the context and continuity that professional campaigns require. The platform is accessible at https://ads.tiktok.com/business/copilot/standalone and serves as a practical case study for how agent-based systems can transform traditional advertising workflows.\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents Leading AI companies now recognize that the transition from large language models to agent-based systems represents a fundamental shift in how we approach complex creative tasks. Traditional LLMs excel at generating individual pieces of content but struggle with the multi-step coordination that professional creative work demands. Agent systems solve this by introducing AI that can break down complex creative briefs, plan multi-step campaigns, and automatically route tasks to specialized tools while maintaining context throughout the entire process.\nThe key insight driving this evolution is that large language models deliver not just tools, but actual work results at specific stages of creative processes. Rather than asking creators to become prompt engineers, effective agent systems understand creative workflows and can execute specific roles within them. Application deployment becomes a matter of providing models with specific contexts and clear behavioral standards that align with professional creative workflows. The understanding and reasoning capabilities of LLMs can be applied to various creative scenarios, but success requires packaging general capabilities as abilities needed for specific positions or processes, overlaying domain expertise with general intelligence.\nHowever, the promise of ten-fold efficiency gains remains largely unfulfilled because most AI tools still require creators to adapt their workflows rather than the AI adapting to how creative work actually happens. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities running in isolation, but rather seamless integrations where creators can jump in at any step to provide feedback, make adjustments, or take creative control. The real challenge isn\u0026rsquo;t building smarter AI—it\u0026rsquo;s building AI that preserves creative agency while eliminating the tedious, repetitive tasks that consume most creators\u0026rsquo; time.\nTo understand what effective creative AI workflows look like in practice, let\u0026rsquo;s examine Typeface—a billion-dollar startup that\u0026rsquo;s closest to solving this integration problem. Their approach reveals both the promise and the remaining challenges in building AI that actually enhances creative work rather than replacing it.\nThe fundamental insight that drove our approach at TikTok was recognizing that successful AI applications require a workflow perspective that considers the entire creative process rather than optimizing individual tasks in isolation. Instead of asking \u0026ldquo;How can AI help with script writing?\u0026rdquo; we asked \u0026ldquo;How can AI understand the complete journey from creative brief to final campaign delivery?\u0026rdquo; This shift in thinking leads to very different product decisions. Rather than building another chatbot that generates scripts, we focused on building an intelligent coordinator that understands how scripts fit into broader campaigns, how they need to align with brand guidelines, and how they connect to visual concepts and distribution strategies.\nThe key questions that guided our Symphony Assistant development were practical and workflow-centered: What parts of daily creative workflows can be effectively enhanced by AI without disrupting the creative process? If AI systems need to process enterprise creative data, what value does this data provide at different stages of the creative business? Where does AI assistance sit most naturally in the creative value chain? In current creative operational models, which specific handoffs and transitions could be most effectively streamlined with intelligent automation? These questions helped us move beyond generic AI capabilities toward purpose-built creative intelligence.\nIndustry Consensus: Task-Specific Models and Architecture Evolution Leading AI companies have converged on several key architectural approaches that directly informed our work on TikTok Symphony Assistant. Companies like Anthropic, A12Labs, and others now prioritize task-specific models and Mixture of Experts (MoE) architectures that represent a significant evolution from general-purpose language models. This shift reflects the recognition that creative workflows benefit more from specialized intelligence than from generalized capability.\nThink of Mixture of Experts like a creative agency where different specialists handle different aspects of a campaign. Instead of one generalist AI doing everything poorly, you have separate \u0026rsquo;experts\u0026rsquo; for script writing, visual concepts, brand voice consistency, and audience targeting—all coordinated by an intelligent router that knows which expert to consult for each task. This approach dramatically improves both the quality of individual outputs and the coherence of the overall campaign, while reducing the computational resources required compared to scaling a single massive model.\nFor creative applications like Symphony Assistant, MoE architecture enables the system to develop deep expertise in different aspects of content creation while maintaining overall campaign coherence. Rather than asking a general-purpose model to switch context constantly between writing scripts and understanding visual concepts, we route different creative challenges to models specifically trained for those domains.\nOur implementation assigns input creative data to different expert networks based on the creative challenge type. Each expert returns specialized outputs optimized for their domain—audience-appropriate script writing, brand-compliant visual concepts, or platform-specific content adaptations. The final output emerges as a coordinated combination that ensures both specialization and coherence throughout campaign development.\nThe key innovation lies in organizing expert networks around actual creative roles rather than technical divisions. For Symphony Assistant, we created experts that mirror how creative teams organize: audience psychology and messaging strategy, brand voice and tone consistency, platform-specific content requirements, and visual-text integration. This approach required training each expert on carefully curated datasets representing high-quality examples of their creative specialty, allowing deep domain expertise rather than shallow general competency.\nLong Context Windows Enable Sophisticated Routing The development of longer context windows, exemplified by Gemini 1.5\u0026rsquo;s one million token capacity, has fundamentally changed what\u0026rsquo;s possible in creative AI applications. Extended context windows solve one of the most persistent problems in creative work: maintaining consistency and coherence across complex, multi-faceted campaigns. Jeff Dean\u0026rsquo;s presentation at the Gemini 1.5 Hackathon at AGI House highlighted how these extended context windows enable more sophisticated in-context learning and more effective Mixture of Experts architectures, allowing AI systems to understand not just individual creative tasks but the broader strategic context that informs every creative decision.\nFor creative applications like Symphony Assistant, longer context windows mean the system can maintain awareness of entire creative briefs, comprehensive brand guidelines, detailed audience research, and complete campaign contexts throughout the generation process. This eliminates the frustrating experience of AI systems that \u0026ldquo;forget\u0026rdquo; crucial brand requirements or campaign objectives partway through content creation. Instead of forcing creators to constantly re-specify context, the system maintains a persistent understanding of the creative project\u0026rsquo;s goals, constraints, and requirements across every interaction.\nPractical Implementation of AI Routing Systems Real-world implementations of intelligent routing concepts can be seen in platforms like Writesonic, which uses GPT Router for intelligent model selection during content generation. The GPT Router system demonstrates how smooth coordination of multiple specialized models—including OpenAI\u0026rsquo;s GPT series, Anthropic\u0026rsquo;s Claude, Microsoft\u0026rsquo;s Azure models, and image generation models like DALL-E and Stable Diffusion—can dramatically speed up responses while ensuring reliability and consistency across different types of creative tasks.\nThis approach directly influenced our architecture decisions for Symphony Assistant, where different creative challenges benefit from different specialized models and different computational approaches. Script writing might route to a model optimized for conversational language and narrative structure, while visual concept development routes to models that understand visual composition and brand aesthetics. Platform optimization for TikTok versus LinkedIn requires entirely different understanding of audience behavior and content format requirements, so these tasks benefit from specialists trained on platform-specific data and success patterns.\nHow Agents Can Help Creators Achieve 10x Efficiency The advertising and marketing industry represents one of the most promising applications for AI-driven workflow automation. Currently, creators typically juggle eight to ten different AI tools to produce a complete video campaign. This fragmented approach creates significant friction and context-switching overhead that negates many of the efficiency benefits AI should provide.\nA typical video creation workflow demonstrates this challenge perfectly. Creators start with concept design in Midjourney, move to script and storyboard development in ChatGPT, generate visual assets using multiple image generation platforms, create video content through services like Runway or Pika, add dialogue and narration via Eleven Labs, incorporate sound effects and music from platforms like SUNO, enhance video quality through Topaz Video, and finally handle subtitles and editing in CapCut or similar tools. Each transition requires re-establishing context and manually ensuring consistency across platforms.\nImproving Agent User Experience Effective creative AI systems must address four fundamental user experience challenges that consistently emerge in professional creative workflows.\nPersonalized Memory \u0026amp; Style Customization becomes essential because adjusting generation style through prompts before each generation is both time-consuming and unpredictable. Professional creators need comprehensive generation rules that ensure consistent output quality without repeated manual adjustments. Typeface\u0026rsquo;s Brand Kit exemplifies this approach by allowing creators to establish persistent brand guidelines that inform every generation.\nRewind \u0026amp; Edit functionality addresses the reality that agent chaining accuracy decreases progressively through multi-step workflows. Human-in-the-loop processes allow creators to regenerate or fine-tune content at each step, ensuring final generation quality meets professional standards. Typeface\u0026rsquo;s Projects feature demonstrates this principle by including Magic Prompt assistance and seamless regeneration capabilities.\nChoose from Variations recognizes that creators require options to make informed decisions about their content. Traditional generation processes force users to refresh entirely when dissatisfied with outputs, creating inefficiency. Providing multiple variations in single generations significantly improves user experience and creative flexibility.\nWorkflows, Not Skills addresses the core problem that creators currently need five to ten disconnected AI capabilities to complete advertising video creation. Most tools require frequent platform switching and context re-establishment. Effective creative AI systems present all capabilities at appropriate workflow stages, enabling efficient tool invocation without breaking creative flow.\nTypeface: Blueprint for Integrated Creative AI Typeface serves as the closest current example of effective creative AI workflow integration, having raised $165 million to reach a $1 billion valuation by solving the fundamental coordination problem in creative AI tools.\nThe platform demonstrates successful implementation of the four essential user experience principles. Their Brand Kit system allows creators to establish comprehensive brand guidelines including image styles, color palettes, and brand voice analysis. The Projects interface provides a Google Doc-like experience where creators can seamlessly invoke different AI capabilities without losing context. Their Template Library offers workflow-specific starting points that understand creative intent rather than just generating generic content.\nMost significantly, Typeface\u0026rsquo;s integration strategy eliminates cross-platform collaboration friction through native connections with Microsoft Dynamics 365, Salesforce Marketing, Google BigQuery, Google Workspace, and Microsoft Teams. This approach recognizes that effective creative AI must work within existing professional workflows rather than requiring creators to adopt entirely new platforms.\nSummary The evolution of AI-powered creative tools reveals a clear trajectory from isolated capabilities toward integrated workflow solutions. Current marketing-focused products successfully integrate multiple stages of the creation process, providing workflow-like experiences that reduce cross-platform collaboration friction through strategic external integrations. However, the most successful implementations go beyond simply chaining various capabilities together—they require thoughtful GUI process specifications that understand how creative work actually happens.\nThe key insight from analyzing platforms like Typeface, Symphony Assistant, and similar tools is that workflows must be designed around creative intent rather than technical capability. Effective creative AI systems understand the relationships between different creative decisions, maintain context across complex campaigns, and preserve creative agency while automating repetitive tasks. The future of creative AI lies not in building more powerful individual models, but in building more intelligent coordination systems that understand how different types of creative intelligence need to work together to produce professional-quality campaigns.\nAdded Nov 11th: Case Study of Pomelli - Progress and Limitations in Brand Kit Creation Pomelli\u0026rsquo;s landing page showcases a visually appealing interface for generating on-brand content, positioned as \u0026ldquo;Google Labs\u0026rdquo; experimental project for business content creation.\nPomelli represents a good step forward in creating a brand kit framework for creative content generation, demonstrating several advances in user experience design for AI-powered marketing tools. However, the platform reveals key limitations that highlight ongoing challenges in the space: lacking context awareness and an overfocus on generalization at the expense of domain-specific optimization.\nThe campaign creation interface emphasizes simplicity with a central prompt area and \u0026ldquo;Suggest Ideas\u0026rdquo; functionality, but the disclaimer \u0026ldquo;Pomelli can make mistakes, so double-check it\u0026rdquo; reveals underlying reliability concerns.\nStrengths: Brand Identity Framework Pomelli\u0026rsquo;s most significant contribution lies in its systematic approach to brand identity capture and application. The platform successfully implements several key principles we identified in our analysis of effective creative AI tools:\nPersonalized Memory \u0026amp; Style Customization: Like Typeface\u0026rsquo;s Brand Kit, Pomelli allows users to establish comprehensive brand guidelines that persist across content generation sessions. This addresses the fundamental user frustration of having to re-specify brand requirements for each creative task.\nWorkflow Integration: The platform demonstrates understanding that effective creative AI tools must integrate seamlessly into existing creative processes rather than requiring users to adapt their workflows to the tool\u0026rsquo;s limitations.\nThe \u0026ldquo;Business DNA\u0026rdquo; setup process captures comprehensive brand information including logos, fonts, color palettes, taglines, and brand values, demonstrating a systematic approach to brand identity integration.\nDespite these strengths, Pomelli doesn\u0026rsquo;t seem to support consistent generation of poster content. [Section to be continued]\nThe evolution from tools like Typeface through Pomelli to platforms like Symphony Assistant demonstrates the rapid maturation of creative AI, but also reveals that the most significant challenges lie not in generating content, but in generating the right content for specific contexts, audiences, and objectives.\n","permalink":"https://chenterry.com/posts/copilot/","summary":"\u003cp\u003eThe advertising industry\u0026rsquo;s AI tools problem isn\u0026rsquo;t about generation quality—it\u0026rsquo;s about workflow integration. Most creative AI products today offer impressive individual capabilities but fail catastrophically when creators try to chain them together into actual work processes. A typical ad campaign might require juggling five to ten different AI tools for concept development, script writing, visual generation, voice synthesis, and editing, with creators constantly context-switching between platforms that don\u0026rsquo;t understand each other. The result? AI tools that promise ten-fold efficiency but deliver ten-fold frustration instead.\u003c/p\u003e","title":"Multi-modal Creative Ad Generation"},{"content":"Marrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\nMarket Analysis Problem Statement The current second-hand market in North America lacks efficiency, with transactions primarily occurring through fragmented social media group chats. As consumer purchasing habits become more rational, there is growing demand for second-hand transactions, but the current infrastructure does not support an effective market. The fragmentation of the market across multiple chat groups and platforms creates information asymmetry, where buyers cannot easily find available items and sellers struggle to reach interested buyers. Additionally, the cumbersome process of creating detailed listings discourages many potential sellers from participating in the market, limiting the overall volume of available goods. This inefficiency results in a significant number of usable items being discarded rather than resold, creating both economic waste and environmental impact.\nTarget Market The initial target market consists of Chinese international students in North American universities, beginning with Washington University in St.Louis, and then expanding to other colleges. This demographic was selected for several strategic reasons: they represent a cohesive cultural group with similar communication habits, they are geographically concentrated on campuses, and they can be easily verified through .edu email addresses to reduce platform abuse. This community also experiences regular high-volume second-hand transaction periods coinciding with academic calendars, particularly during move-out periods at semester ends. Following successful penetration of this initial market, expansion will target the broader university student population before eventually extending to the general American user base.\nKey Market Assumptions The business model is built on five critical assumptions that will be validated through initial market testing:\nThe potential supply of second-hand products significantly exceeds the current transaction volume, indicating an untapped market opportunity. The complexity of listing creation represents the primary barrier preventing potential sellers from participating in the market. A streamlined buying experience with easier product discovery will attract more buyers to the platform. The platform will experience network effects once it reaches a critical mass of listed items, drawing in additional buyers and sellers. The majority of second-hand products (excluding specialty categories like housing rentals and rideshares) are priced between $10-100, making them low-risk transactions. Product Vision and User Experience Marrket will innovate the second-hand marketplace experience by leveraging AI (image recognition, content generation) to dramatically simplify the listing process. The core value proposition centers on allowing sellers to create comprehensive, attractive listings with minimal effort - just a few photos and basic information. The AI system analyzes the images, generates detailed product descriptions, suggests appropriate categories, and recommends pricing based on market data. For buyers, the platform offers an intuitive, searchable interface organized by product categories, with advanced filtering capabilities to quickly find desired items. The unified marketplace creates transparency in pricing and availability that is absent in the current fragmented chat-based system.\nUser Flow Visualization ┌────────────────┐ ┌───────────────────┐ ┌─────────────────┐ ┌───────────────┐ │ │ │ │ │ │ │ │ │ Upload Photos │────▶│ AI Generates Draft│────▶│ Review \u0026amp; Publish│────▶│ Manage Listing│ │ │ │ Description │ │ │ │ │ └────────────────┘ └───────────────────┘ └─────────────────┘ └───────────────┘ │ │ │ │ │ ▼ │ ┌───────────────────┐ │ │ │ │ │ Transaction \u0026amp; Pay │ │ │ │ │ └───────────────────┘ │ ▲ ▼ │ ┌──────────────────┐ ┌───────────────────┐ ┌─────────────────┐ │ │ │ │ │ │ │ │ │ Browse Category │────▶│ View Listing │────▶│ Contact Seller │───────────┘ │ │ │ │ │ │ └──────────────────┘ └───────────────────┘ └─────────────────┘ Core Features AI-Powered Listing Generation The cornerstone of Marrrket\u0026rsquo;s platform is its innovative AI listing generation system. Sellers simply upload multiple photos of their item and provide minimal information such as the item name and general category. The AI system then analyzes the images to identify the product, its condition, key features, and appropriate categorization. It generates a comprehensive product description, suggests an appropriate price range based on market data for similar items, and creates a complete listing draft for the seller to review. This process transforms what is typically a 15-20 minute task into a 2-3 minute interaction, dramatically reducing the barrier to listing items for sale. The seller maintains full control to edit any aspect of the AI-generated content before publishing the listing.\nIntuitive Category System Marrrket organizes products into clearly defined categories that reflect the unique needs of the university student market. Primary categories include furniture, electronics, textbooks, household items, clothing, and special categories for housing rentals and rideshares. The categorization system is designed to balance simplicity with sufficient specificity to aid discovery. Each category features customized filters relevant to that product type - for example, electronics listings include filters for condition, brand, and age of the device, while textbook listings filter by course subject and edition. This tailored approach ensures buyers can quickly narrow their search to relevant items.\nStreamlined Payment and Verification To minimize transaction friction while ensuring security, Marrrket integrates with Zelle as its primary payment platform, leveraging its popularity among university students. For higher-value transactions, the platform offers an optional escrow service where the payment is held until the buyer confirms receipt of the item as described. User verification is handled through .edu email domain authentication, creating an additional layer of trust within the platform. The system also implements a reputation system where both buyers and sellers can rate their transaction experience, building credibility over time.\nTechnical Architecture Frontend Implementation The frontend architecture employs React with Ant Design components to create a responsive, mobile-friendly user interface. During the initial phase, the buyer interface may be partially implemented using Notion for rapid deployment, with plans to migrate fully to the custom React interface as the platform matures. The user interface prioritizes simplicity and visual browsing of items, with large product images and clear, concise information display. The design system maintains consistency across all interfaces while optimizing for both desktop and mobile browsing scenarios.\nBackend Systems The backend implementation uses Flask for API endpoints with initial data storage utilizing Notion\u0026rsquo;s built-in database capabilities for rapid development. As the platform scales, data will migrate to either SQL or MongoDB, with additional consideration for vector database implementation to enhance AI-powered search capabilities. The system architecture is designed with modularity in mind, allowing individual components to be upgraded or replaced as requirements evolve. Authentication, listing management, messaging, and transaction processing are implemented as separate services to maintain flexibility and scalability.\nAI Integration Architecture ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Image Analysis │────▶│ Context Generation │────▶│ Content Creation │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ │ │ │ ▼ ▼ ▼ ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Object Detection│ │ Market Analysis │ │Listing Template │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ The AI system integrates multiple technologies to transform basic image and text inputs into comprehensive product listings. The process begins with image analysis using object detection to identify the product type, condition, and key features. This information feeds into a context generation phase where market data for similar items is analyzed to determine appropriate pricing and categorization. Finally, the content creation phase generates a structured description following optimized templates for each product category. The system continuously improves through feedback loops, where user edits to the generated content are used to refine future suggestions.\nRevenue Model In the initial phase, Marrrket will prioritize user acquisition and platform growth over immediate monetization. The long-term revenue strategy consists of two primary streams:\nListing Package Fees: Users can list their first 5 items for free, encouraging initial platform adoption. Beyond that, tiered packages are available - a Basic Package for 5-20 listings and a Power Package for 20-50 listings. To build trust and mitigate risk for sellers, listing fees are automatically refunded if items don\u0026rsquo;t sell, demonstrating the platform\u0026rsquo;s confidence in its ability to connect buyers and sellers effectively.\nTransaction Fees: For items priced above $200, a 5% commission is applied to the transaction. This approach ensures that the platform primarily monetizes higher-value transactions where the commission represents a reasonable cost relative to the total value, while keeping lower-value transactions (which constitute the majority of student exchanges) commission-free to encourage platform adoption.\nAs the platform matures, additional revenue opportunities may include premium listing features, promoted listings for greater visibility, and value-added services such as professional photography or pickup/delivery coordination.\nGo-to-Market Strategy Phase 1: St. Louis Campus Launch The initial launch will focus on St. Louis area universities, strategically timed for late April 2025 to coincide with the end-of-semester period when students are moving out of housing and seeking to sell unwanted items. This timing capitalizes on a natural high-volume period in the second-hand market. Marketing efforts will employ a multi-channel approach with carefully prioritized tactics:\nTraditional campus advertising through strategically placed posters in high-traffic areas like student unions, dormitories, and international student centers. Partnerships with Chinese student organizations to promote the platform through their established communication channels, including WeChat groups and official accounts. Targeted email marketing to student email lists, emphasizing the platform\u0026rsquo;s benefits for both buying and selling. Implementation of a referral program where existing users receive incentives for bringing new users to the platform. Selective digital display advertising on platforms frequently used by the target demographic. Content marketing through popular platforms like Xiaohongshu and Instagram, featuring success stories and platform benefits. Phase 2: Expansion Strategy Following successful implementation in the initial market, expansion will proceed methodically to additional campuses with significant international student populations. The strategy leverages network effects by expanding to geographically connected areas where students may already have connections to the initial user base. Marketing messages will evolve to emphasize proven success metrics from the initial market, such as average time to sell items and average savings for buyers compared to new purchases. As the platform grows beyond the Chinese international student community, marketing will emphasize the platform\u0026rsquo;s ease of use and enhanced features compared to general marketplaces, while maintaining the focus on university communities to preserve the trust and verification benefits of the .edu email system.\nRisk Assessment and Mitigation Strategies Several key risks have been identified that could impact the platform\u0026rsquo;s success, along with corresponding mitigation strategies:\nInsufficient Listing Volume: The platform requires a critical mass of listings to attract buyers. To mitigate this risk, pre-launch partnerships with student organizations will be established to seed initial inventory, and incentives will be offered for early sellers. Additionally, the team will consider listing items themselves if necessary to ensure adequate initial inventory.\nAI Quality Issues: If the AI-generated descriptions fail to meet quality standards, sellers may abandon the platform. To address this risk, the system will initially be more conservative in its suggestions, offering simpler descriptions that sellers can enhance rather than attempting complex descriptions that might contain errors. A continuous improvement process based on user edits will be implemented to refine the AI over time.\nTrust and Safety Concerns: Second-hand marketplaces can face issues with fraudulent listings or unsafe transactions. The .edu email verification provides a first layer of protection, which will be supplemented by clear community guidelines, a reporting system for problematic listings or users, and an escrow option for higher-value transactions.\nPayment Verification Challenges: Ensuring payments are properly made and verified is critical to platform trust. Beyond integrating with Zelle, the platform will implement a confirmation system where both parties must acknowledge the transaction is complete before it is finalized in the system.\nCompetitive Response: Existing platforms may attempt to replicate the AI-powered listing feature. To maintain competitive advantage, Marrrket will focus on building deep expertise in the specific needs of the university market segment and continuously enhancing the AI capabilities based on the growing dataset of student transactions.\nConclusion Marrrket represents a significant innovation in the second-hand marketplace sector by directly addressing the primary friction point in the market: the complexity of creating product listings. By leveraging artificial intelligence to dramatically simplify this process, the platform has the potential to unlock substantial untapped inventory in the university second-hand market. The strategic focus on Chinese international students as an initial target market provides a cohesive, geographically concentrated user base with consistent needs, allowing for efficient marketing and network growth. With successful execution of this strategy, Marrrket can establish itself as the preferred second-hand marketplace for university students before expanding to broader markets\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu.\n","permalink":"https://chenterry.com/archived/marrrket/","summary":"\u003cp\u003eMarrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\u003c/p\u003e","title":"Marrrket: AI Listing Secondhand Marketplace"},{"content":"Exploring the evolution of technology and its impact on society Understanding the past is crucial for shaping the future. By studying the evolution of technology, from early computing to modern AI systems, we can better understand how technological innovations have transformed society and anticipate future developments. This theme explores key technological milestones, their societal impacts, and the lessons we can learn from them.\n","permalink":"https://chenterry.com/main-themes/tech-history/","summary":"\u003ch2 id=\"exploring-the-evolution-of-technology-and-its-impact-on-society\"\u003eExploring the evolution of technology and its impact on society\u003c/h2\u003e\n\u003cp\u003eUnderstanding the past is crucial for shaping the future. By studying the evolution of technology, from early computing to modern AI systems, we can better understand how technological innovations have transformed society and anticipate future developments. This theme explores key technological milestones, their societal impacts, and the lessons we can learn from them.\u003c/p\u003e","title":"Tech History"},{"content":"Multi-agent system for cross boarder e-commerce sales automation. Co-founder and head of product. https://cognogpt.com\nCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\nMarket Opportunity The target addressable market size is around 200K-250K, with the marketing being independent brands selling from China \u0026amp; Southeast Asia to the U.S. Our beachhead market is mid-sized Chinese brands selling consumer electronics and other slow-moving consumer goods to the U.S. with the annual recurring rate of more than 2 million USD, consisting around 10K potential end users. Some potential adjacent markets are China and South East Asia B2C E-commerce companies selling unbranded products to the US on their own platform that consolidates products from many factories.\nProducts \u0026amp; Services Cogno+ is an AI-driven interactive assistant designed to enhance e-commerce business growth. It has both personalized engagement and automated and optimized customer service. While the current chatbots and web tools still require extensive workflow setup and are limited in their problem-solving abilities, Cogno has this multi-agent real-time plug-and-play system that offers easy setup, 24-hour support, and high domain knowledge for a better personalization experience that makes us differ from other products in the competition.\nBusiness \u0026amp; Sales Strategy Cogno+ provides mid-sized international e-commerce brands with automated sales support to help them increase conversions and upselling. We aim to explore beyond the current boundaries of human-ai interaction, developing fully scalable sales automation systems for better engagement between brands and customers around the world.\nIn terms of our business strategy, we will employ a B2B SaaS model, charging a tier-based subscription fee based on website traffic (given the cost of using language model APIs). Our business model allows service for a range of e-commerce brands and ensures that brands only pay for the value they are getting via Cogno (increased sales conversion and product upselling).\nFor our product roadmap, building on our existing multi-agent system, we aim to continue research and development of dynamic prompting to improve the coordination capabilities of our central logic agent to allow us to leverage more custom domain agents simultaneously. We are also working to improve user interactions with graphics user interfaces and web interaction as input to language model systems. With these future improvements, we believe we are at the cusp of reinventing how online shopping interactions between brands and customers are made.\nWe will initiate customer acquisition with our beachhead market of midsized Chinese e-commerce brands selling consumer electronics and other slow-moving consumer goods to the US with an annual recurring revenue of more than 2 million, and eventually expand to the larger domestic and international e-commerce landscape. We will target customers through distribution channels such as Shopify, WooCommerce, and WordPress as well as direct sales and expo exhibitions with Chinese online retailers. By targeting industry leaders and influencers, we aim to gain a firm grounding in industries such as consumer electronics and other slow-moving consumer goods verticals. Finally, we will conduct SEO and social media marketing to expand our market beyond the Asia-Pacific to the global e-commerce market, eventually bringing Cogno+ to brands and customers in all of our major e-commerce markets.\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu, Madison Bratley, Ryan Philips, Eric Chen, Echo Zhou, Esther Zhou, Jingfan Yao, Steven Wang, Lu Zhou, Wendy Hu, Wendy Huang, Letty Lin, Jianpeng Su, Professor Eduardo Acuna, Professor Karen Gordon, Jeff Smith, Toni Milushev, The Farley Center, The Garage, as well as everyone who provided mentorship and advice along the way.\n","permalink":"https://chenterry.com/archived/cogno/","summary":"\u003cp\u003eMulti-agent system for cross boarder e-commerce sales automation. Co-founder and head of product.\n\u003ca href=\"https://cognogpt.com\"\u003ehttps://cognogpt.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\u003c/p\u003e","title":"Cogno: Multi-agent AI for Sales Automation"},{"content":"Developing capable multi-agent systems for complex reasoning and human-AI collaboration Single-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\n","permalink":"https://chenterry.com/main-themes/multi-agent-systems/","summary":"\u003ch2 id=\"developing-capable-multi-agent-systems-for-complex-reasoning-and-human-ai-collaboration\"\u003eDeveloping capable multi-agent systems for complex reasoning and human-AI collaboration\u003c/h2\u003e\n\u003cp\u003eSingle-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\u003c/p\u003e","title":"Multi-agent LLM Systems"},{"content":"Extracting meaningful insights from unstructured multi-modal content Most times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\n","permalink":"https://chenterry.com/main-themes/data-insights/","summary":"\u003ch2 id=\"extracting-meaningful-insights-from-unstructured-multi-modal-content\"\u003eExtracting meaningful insights from unstructured multi-modal content\u003c/h2\u003e\n\u003cp\u003eMost times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\u003c/p\u003e","title":"Data Insights"},{"content":"CrowdListen Transform large-scale social conversations into actionable insights through multi-modal content understanding and AI-powered analysis.\nWhat is CrowdListen? CrowdListen bridges the gap between broad social media insights and detailed understanding by combining the scale of algorithmic analysis with the depth of human-like comprehension. Our platform processes TikTok videos, comments, audio, and engagement metrics to generate original research from raw social data.\nKey Features Multi-Modal Analysis: Process video, audio, text, and engagement metrics in a unified system Topic Modeling: Embedding-based identification of key themes across massive datasets Deep Research: LLM-powered analysis that uncovers emerging trends and nuanced opinions Original Insights: Generate research that goes beyond existing reporting and surface-level metrics Scalable Processing: Handle large volumes of unstructured social data efficiently Use Cases Brand Monitoring: Understand how your brand is perceived across social platforms Market Research: Identify emerging trends before they hit mainstream reporting Content Strategy: Discover what content resonates with your target audience Competitive Intelligence: Track competitor performance and audience sentiment Crisis Management: Monitor and respond to developing situations in real-time Learn more about the technology\n","permalink":"https://chenterry.com/crowdlisten/","summary":"\u003ch1 id=\"crowdlisten\"\u003eCrowdListen\u003c/h1\u003e\n\u003cp\u003eTransform large-scale social conversations into actionable insights through multi-modal content understanding and AI-powered analysis.\u003c/p\u003e\n\u003ch2 id=\"what-is-crowdlisten\"\u003eWhat is CrowdListen?\u003c/h2\u003e\n\u003cp\u003eCrowdListen bridges the gap between broad social media insights and detailed understanding by combining the scale of algorithmic analysis with the depth of human-like comprehension. Our platform processes TikTok videos, comments, audio, and engagement metrics to generate original research from raw social data.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Modal Analysis\u003c/strong\u003e: Process video, audio, text, and engagement metrics in a unified system\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTopic Modeling\u003c/strong\u003e: Embedding-based identification of key themes across massive datasets\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeep Research\u003c/strong\u003e: LLM-powered analysis that uncovers emerging trends and nuanced opinions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOriginal Insights\u003c/strong\u003e: Generate research that goes beyond existing reporting and surface-level metrics\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScalable Processing\u003c/strong\u003e: Handle large volumes of unstructured social data efficiently\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"use-cases\"\u003eUse Cases\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBrand Monitoring\u003c/strong\u003e: Understand how your brand is perceived across social platforms\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMarket Research\u003c/strong\u003e: Identify emerging trends before they hit mainstream reporting\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContent Strategy\u003c/strong\u003e: Discover what content resonates with your target audience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompetitive Intelligence\u003c/strong\u003e: Track competitor performance and audience sentiment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCrisis Management\u003c/strong\u003e: Monitor and respond to developing situations in real-time\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"/posts/crowdlistening/\"\u003eLearn more about the technology\u003c/a\u003e\u003c/p\u003e","title":"CrowdListen"},{"content":"LLM Coaching System Experience our AI coaching system firsthand—upload a coaching conversation and see how it identifies learning patterns and suggests peer connections.\nThis system analyzes coaching conversations to identify specific learning regulation gaps and connects students with peers who\u0026rsquo;ve successfully worked through similar challenges.\nFeatures Pattern Recognition: AI identifies cognitive, metacognitive, and emotional regulation gaps Peer Matching: Connects you with students who\u0026rsquo;ve overcome similar challenges Conversation Facilitation: Structured guidance for productive peer learning Action Planning: Helps create concrete next steps for skill development Learn more about this project\n","permalink":"https://chenterry.com/llm-coaching/","summary":"\u003ch1 id=\"llm-coaching-system\"\u003eLLM Coaching System\u003c/h1\u003e\n\u003cp\u003eExperience our AI coaching system firsthand—upload a coaching conversation and see how it identifies learning patterns and suggests peer connections.\u003c/p\u003e\n\u003cp\u003eThis system analyzes coaching conversations to identify specific learning regulation gaps and connects students with peers who\u0026rsquo;ve successfully worked through similar challenges.\u003c/p\u003e\n\u003ch2 id=\"features\"\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePattern Recognition\u003c/strong\u003e: AI identifies cognitive, metacognitive, and emotional regulation gaps\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePeer Matching\u003c/strong\u003e: Connects you with students who\u0026rsquo;ve overcome similar challenges\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConversation Facilitation\u003c/strong\u003e: Structured guidance for productive peer learning\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAction Planning\u003c/strong\u003e: Helps create concrete next steps for skill development\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"/posts/llmcoaching/\"\u003eLearn more about this project\u003c/a\u003e\u003c/p\u003e","title":"LLM Coaching System"},{"content":"","permalink":"https://chenterry.com/search/","summary":"Search","title":"Search"}]