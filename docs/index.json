[{"content":" Tobi Lutke\u0026rsquo;s Shopify internal memo\nSmaller and More Efficient Teams When should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\nCompared to late 2022, I\u0026rsquo;ve shifted more energy toward building and testing ideas directly, rather than hiring large teams to delegate to. Early-stage startups now need fewer product managers, especially when founders can build and validate ideas over a weekend. The marginal productivity increase associated with an extra headcount might now be lower than that of employing AI agent(s). In practice, this means smaller, more agile teams with outstanding capabilities from ICs. I believe the future belongs to ultra-small teams, for startups, this means any where from 2-5 people, with each member handling a job function while employing multiple AI agents to execute tasks. The benefits are clear: less time spent in meetings, quicker iterations, and the positive ripple effects of tight-knit teams. Rather than building large departments with specialized roles, companies can now assemble small, versatile teams augmented by AI capabilities. These teams can move faster and with greater autonomy than traditional corporate structures allow.\nAI Tools Enabling Rapid Prototyping Recent AI coding tools like Cursor have expedited the prototyping process, allowing developers (and non-technical people too) to quickly build functional MVPs in record time. These tools excel at generating boilerplate code, implementing common patterns, and even troubleshooting errors. A single engineer with Cursor can accomplish what previously required multiple developers working in tandem, or achieve in around one fifth the time they\u0026rsquo;d spend working on it alone.\nCollaboration workflows enabled by Figma, Credit to Kevin Kwok\nThis acceleration in prototyping parallels what we saw with Figma in design. As Kevin Kwok noted, \u0026ldquo;Tightening the feedback loop of collaboration allows for non-linear returns on the process\u0026rdquo; (Kwok, 2020). Just as Figma collapsed the barriers between designers and their collaborators, AI coding tools are now breaking down the technical barriers that previously separated product visionaries from implementation.\nWhile much of the attention has been on AI helping engineers be more efficient, I\u0026rsquo;m more interested in how its ripple effect on tangential roles - such as product managers and ui/ux designers. Just like how Figma allowed faster iteration cycles between designers and engineers/product managers, would this reduction in prototype testing allow for more innovative workflows where product people (with knowledge of engineering possibilities) could quickly test ideas and engineers focus more on the implementation of production scale systems?\nThis dynamic raises an interesting question for founders and engineering leaders: when does it make sense to hire additional team members versus investing in improving the AI capabilities of existing team members? The calculus now involves comparing the marginal cost of onboarding and training a new hire against the potential productivity gains from enhancing your current team\u0026rsquo;s AI workflow mastery.\nThe Rise of Product Engineers Software Engineers should become \u0026ldquo;Product Engineers\u0026rdquo; as LLMs have commoditized routine coding tasks. What\u0026rsquo;s truly valuable now are generalists who can code but also have an eye for UI/UX design, good product taste, and deep market understanding. Startups should increasingly seek engineers who talk directly with users, make decisions on what to build, and then build with AI assistance. This approach works because engineers inherently understand both technical constraints and opportunities better than anyone else. The traditional roles of \u0026ldquo;product manager\u0026rdquo; and \u0026ldquo;engineer\u0026rdquo; are merging into a hybrid that combines technical expertise with product sensibility.\n\u0026ldquo;Design is all of the conversations between designers and PMs about what to build\u0026rdquo; (Kwok, 2020). Similarly, product development is now larger than just engineers or product managers—it encompasses the entire collaborative process of identifying, designing, and implementing solutions. Too often, people view product management as just a career path rather than a mindset. This limited perspective overlooks the deep curiosity and problem-solving drive that true product managers embody—a drive that goes beyond mere job titles and organizational structures. The best product engineers embody this product mindset, focusing on solving real problems rather than just building features. Super ICs don\u0026rsquo;t wait for dev support—they build what they need when they need it, combining technical skills with product thinking to deliver complete solutions.\nShifting Emphasis: From Implementation to Idea Generation As AI handles more of the routine implementation work, the traits we look for in product engineers should also evolve. The most valuable skills now center around workflow generation and new idea creation rather than implementation details. The ability to conceptualize solutions, design effective workflows, and identify the right problems to solve becomes paramount.\nProduct engineers who excel in this new environment should demonstrate:\nStrong systems thinking across the entire product lifecycle The ability to rapidly iterate and test hypotheses Comfort with ambiguity and exploration over rigid planning Skill in designing human-AI collaboration workflows An understanding of when to leverage AI and when to apply human judgment This shift means that strong ICs with AI fluency can now accomplish what previously required teams of specialists. For early-stage startups, this dramatically changes the calculus around hiring, especially for traditional product management roles.\nShopify\u0026rsquo;s AI-first Approach Shopify\u0026rsquo;s CEO Tobi Lütke\u0026rsquo;s internal memo illustrates this shift perfectly. He writes:\n\u0026ldquo;We are entering a time where more merchants and entrepreneurs could be created than any other in history\u0026hellip; Having AI alongside the journey and increasingly doing not just the consultation, but also doing the work for our merchants is a mindblowing step function change here.\u0026rdquo;\nLütke emphasizes that \u0026ldquo;reflexive AI usage is now a baseline expectation at Shopify.\u0026rdquo; He notes that using AI well is a skill that needs to be learned through frequent use, and that AI acts as a multiplier for already high-performing individuals. He compares Shopify to a \u0026ldquo;red queen race\u0026rdquo; from Alice in Wonderland—you must keep running just to stay still. In a company growing 20-40% year over year, everyone must improve at that rate just to re-qualify for their position. With AI tools, this previously daunting expectation now seems achievable.\nThe Critical Role of Taste in Product Development As AI handles more of the execution, the differentiating factor for product engineers becomes \u0026ldquo;taste\u0026rdquo; - the ability to discern what makes for excellent product design, user experience, and strategic direction. Product taste is what separates adequate solutions from exceptional ones. In a world where AI can generate competent designs and functional code, the human with superior taste becomes indispensable. \u0026ldquo;Just like how the constraints on design at companies is often not a problem of pixels, but of people\u0026rdquo; (Kwok, 2020). As technical constraints dissolve through AI assistance, the human factors around judgment, taste, and strategic direction become the primary differentiators.\nTaste involves the ability to:\nIdentify which problems are worth solving Determine the appropriate level of complexity for solutions Recognize when simplicity serves users better than feature-richness Balance aesthetic appeal with functional needs Anticipate user needs before they\u0026rsquo;re explicitly requested These skills can\u0026rsquo;t be easily replicated by AI systems. While AI can execute with increasing competence, it still lacks the intuitive understanding of human needs and experiences that informs good taste.\nWhat This Means for Organizations Shopify\u0026rsquo;s approach includes several key principles organizations should consider:\nAI as a fundamental expectation - Not an optional tool but a core competency AI integration from the prototype phase - Using AI throughout the development process Headcount requests must demonstrate why AI can\u0026rsquo;t do the job - Teams must explore AI solutions first Universal application - This applies to all levels, including executives Organizations following these principles will likely outperform those treating AI as merely an optional tool. The principles represent a fundamental rethinking of how work gets done, not just a technological upgrade.\nWhile some companies are hiring for AI product managers nowadays, this might be a transitionary position, just as prompting is an intermediary step in human-LLM interaction—similar to the arrival of GUIs before the personal computer age became mainstream. Product managers now need to learn when to dive into detailed work and when to step back for strategic oversight, a balance that will continue to evolve as AI capabilities expand.\nThe Path Forward The future belongs to smaller, more nimble teams of highly capable individuals who leverage AI to achieve what previously required entire departments. This might be the new normal of work. For ICs, learning to effectively collaborate with AI tools is no longer optional—it\u0026rsquo;s essential for remaining competitive. For organizations, the challenge is creating environments where these super ICs can thrive, with processes and cultures that maximize the human+AI partnership rather than treating them as separate domains. Most importantly, this shift will enable more creative exploration and novel problem-solving, as routine tasks become increasingly automated. The most exciting products and services of the coming years will likely emerge from these small, AI-augmented teams combining human taste with AI-powered execution.\nWorks Cited\nGoodspeed, Elizabeth. \u0026ldquo;Design Taste vs. Technical Skills in the Era of AI.\u0026rdquo; Nielsen Norman Group, 10 May 2024.\nKwok, K. (2020, June 19). Why Figma Wins. https://kwokchain.com/2020/06/19/why-figma-wins/\nLütke, Tobi. \u0026ldquo;Internal Company Memo on AI Usage.\u0026rdquo; Shopify, 2025.\n","permalink":"https://chenterry.com/posts/product_engineers/","summary":"\u003cp\u003e\u003cimg alt=\"Product Engineers\" loading=\"lazy\" src=\"/images/posts/product_engineer/shopify_ai.png\"\u003e\n\u003cem\u003eTobi Lutke\u0026rsquo;s Shopify internal memo\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"smaller-and-more-efficient-teams\"\u003eSmaller and More Efficient Teams\u003c/h2\u003e\n\u003cp\u003eWhen should we hire a person versus delegating to AI? Recently I\u0026rsquo;ve been more reluctant towards hiring people as an attempt to build mid sized projects. Yes, the codebases would get pretty big, and there\u0026rsquo;s also tasks involved that I wouldn\u0026rsquo;t say are my forte exactly. Yet, when I think about the meetings I have to sit through communicating what I want to build and just time spent doing filler work, I get more and more inclined towards just doing it myself. It\u0026rsquo;s not to say that teamwork isn\u0026rsquo;t good work, some of the most creative product ideas I\u0026rsquo;ve worked on stemmed from chats, during lunch breaks, exploring tangents, with engineers, journalists. The value of connecting the dots during these conversations is something that is difficult to replace. However, is the assumption that delegating work means higher productivity still valid? After all, the cost of execution is continually decreasing (as long as we have a clear idea of what to build).\u003c/p\u003e","title":"Product Engineers and AI Multipliers"},{"content":"Here\u0026rsquo;s a list of articles / videos / podcasts that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\nGary Tan on Manus: The New General-Purpose AI Agent Usable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\n\u0026ldquo;Today we\u0026rsquo;re launching an early preview of Manus, the first general AI agent.\u0026rdquo;\nWhen Manus officially launched, the hype around it immediately took off. A Chinese startup unveiling a new AI agent that some are calling \u0026ldquo;China\u0026rsquo;s next DeepSeek moment,\u0026rdquo; with people calling it \u0026ldquo;the most impressive AI tool they\u0026rsquo;ve ever tried\u0026rdquo; and \u0026ldquo;the most sophisticated computer-using AI.\u0026rdquo;\nUnlike some of its predecessors, Manus wasn\u0026rsquo;t just another specialized chatbot. It promised to be a true general-purpose AI agent. With invitations rare and access limited, the question remains: has Manus truly revolutionized the AI agent landscape?\nHow Manus Works Behind all the excitement around Manus is something genuinely innovative: a multi-agent AI system that can seemingly complete all sorts of tasks from travel planning and financial analysis to searching over dozens of files or doing industry research.\nRather than relying on one big neural network, Manus works more like an executive overseeing a team of sub-agents, coordinating and guiding their every move across a shared action space. It takes in your prompt as input and gets to work figuring out what it needs to do.\nInstead of tackling your task in one go:\nA planner agent first comes up with a master plan to follow, breaking things down into manageable subtasks This way Manus knows precisely what needs to be done before executing and can hand off these tasks to other sub-agents These are like Manus\u0026rsquo;s own in-house experts - they share the same context but each has its own delineated domain from knowledge or memory to execution Manus can call upon an extensive suite of 29 different integrated tools, whether they\u0026rsquo;re automating web navigation, securely running code, or pulling important information from files. Manus\u0026rsquo; sub-agents intelligently decide which tools to use.\nFinally, when each subtask is complete, the executor agent combines the outputs together into a final synthesized output for the user.\nTechnical Details Under the hood, Manus is powered by a sophisticated dynamic task decomposition algorithm. This is what enables it to autonomously break down complex instructions into clear execution paths.\nTo ensure stability even after dozens of rounds of reasoning and tool use, the Manus team developed an original technique called \u0026ldquo;chain of thought injection,\u0026rdquo; enabling agents to actively reflect and update plans.\nAt its core, Manus makes use of Anthropic\u0026rsquo;s Claude 3.7 Sonnet. Manus also features robust cross-platform execution capabilities thanks to its seamless integration with open-source tools like YC company Browser.js for advanced website interaction and startup E2B\u0026rsquo;s secure cloud sandbox environment.\nCapabilities and Performance What can Manus actually accomplish? Impressively, it can take on a wide range of real-world tasks:\nCreating travel itineraries Detailed financial analyses Educational content Structured database compilation Insurance policy comparisons Supplier sourcing Assisting with high-quality presentations To truly measure Manus\u0026rsquo; capabilities, we can look at Gaia, a benchmark designed to challenge AI agents on reasoning, multimodal handling, web browsing, and tool proficiency:\nHumans typically score about 92% OpenAI\u0026rsquo;s Deep Research scored about 74% at its best Manus smashed the state-of-the-art on Gaia, scoring 86.5%, just a few points shy of the average human The \u0026ldquo;Wrapper\u0026rdquo; Debate Despite impressive benchmark performance, Manus has reignited a broader conversation about the nature of AI startups at the application layer: \u0026ldquo;wrappers.\u0026rdquo;\nSome have dismissed Manus as merely a wrapper, since it stitches together existing foundational models and various tool calls. But this dismissal overlooks an important reality: most successful AI products today could also qualify as wrappers by this logic.\nCursor and Warp, for example, integrate existing LLMs alongside external APIs and developer-focused tooling such as real-time code analysis and debugging utilities Domain-specific agents like Harvey combine foundational models with legal-specific tool integrations, case law retrieval, compliance checks, and document analysis Clearly, many useful applications do fit the wrapper mold, and for many developers, it makes sense to go this route. As Manus co-founder Yizhow Peak G told us himself, \u0026ldquo;From day one they decided to work orthogonally to model development, wanting to be excited rather than threatened by each new model release.\u0026rdquo;\nWhat distinguishes successful wrappers from their less effective counterparts is typically a bunch of things: intuitive UI, proprietary evals, much more careful fine-tuning of foundational models, and thoughtfully designed multi-agent architectures.\nStrengths and Limitations Manus itself illustrates these trade-offs really well:\nStrengths:\nIts multi-agent orchestration helps deliver significantly lower per-task costs (around $2 a task compared to integrated competitors like OpenAI\u0026rsquo;s Deep Research) Greater transparency and user control, letting users directly inspect, customize, or replace individual sub-agents and tool integrations A degree of flexibility centralized platforms rarely match One of the coolest things Manus figured out was actually exposing the file system so you could see exactly what the agents were doing. Chat GPT requires you to reprompt, and it\u0026rsquo;s opaque what\u0026rsquo;s happening when it\u0026rsquo;s thinking. Manus is a glimpse into the future of Chat GPT desktop operating directly on your computer, and it will be cool to see how much more control you\u0026rsquo;ll get when it\u0026rsquo;s happening there instead of a browser.\nLimitations:\nCoordination across specialized agents becomes increasingly difficult as tasks scale or complexity grows Its current advantages (UX refinements, targeted fine-tuning, thoughtful integrations) are vulnerable to competitors just coming along and doing that as well These strengths and weaknesses are generally shared by wrappers:\nThey allow rapid deployment, iteration, and specialized UX at lower upfront cost They\u0026rsquo;re vulnerable to disruption such as API pricing changes or provider policy shifts, which can quickly erase any cost benefits The Future of AI Products Ultimately, the critical challenge isn\u0026rsquo;t deciding whether wrappers are viable but identifying genuinely sustainable differentiation for your product.\nFor founders, this might mean:\nInvesting early in proprietary evals that are expensive or time-consuming to replicate Embedding workflows deeply into specific user routines to increase switching costs Identifying integrations with platforms or data sets competitors can\u0026rsquo;t easily access In the end, success in AI doesn\u0026rsquo;t hinge on reinventing the wheel but rather on who can stitch together the existing models into a product users genuinely love.\n","permalink":"https://chenterry.com/posts/interesting_reads/","summary":"\u003cp\u003eHere\u0026rsquo;s a list of articles / videos / podcasts that I founding interesting. I\u0026rsquo;ve attached the original article / transcript for easy refernece as well.\u003c/p\u003e\n\u003ch1 id=\"gary-tan-on-manus-the-new-general-purpose-ai-agent\"\u003eGary Tan on Manus: The New General-Purpose AI Agent\u003c/h1\u003e\n\u003cp\u003eUsable AI agents are finally here from deep research platforms out of OpenAI and Google to similar tools from XAI and DeepSeek. Joining the competition now is Manus, a brand new agentic AI platform that has taken the world by storm.\u003c/p\u003e","title":"The Craft of Miyazaki in an AI-Generated World"},{"content":"\nHayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\nTo put this dedication in perspective: a single 4 second crowd scene from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\nYet as technologies like GPT-4o image generation and and \u0026ldquo;Ghibli-style\u0026rdquo; AI art taking the internet by storm, I can\u0026rsquo;t help wondering about the future. When anyone can transform an image into a Ghibli-esque cartoon nearly indistinguishable from Miyazaki\u0026rsquo;s hand-drawn work, we must reconsider what constitutes true craftsmanship and whether we can appreciate his art in the same way.\nMiyazaki himself once expressed after viewing AI generated animations: “I am utterly disgusted. I would never wish to incorporate this technology into my work at all. I strongly feel this is an insult to life itself.” However, I hesitate to apply this statement too broadly, as it was made under different circumstances—specifically in response to Dwango AI Lab\u0026rsquo;s demonstration of a 3D zombie animation of substantially lower quality than today\u0026rsquo;s AI art. Objectively speaking, current AI-generated Ghibli-style images demonstrate impressive technical quality, yet for longtime fans, they lack the same emotional resonance.\nThe genius of Ghibli\u0026rsquo;s work transcends time. Released in 1997, \u0026ldquo;Princess Mononoke\u0026rdquo; explores humanity\u0026rsquo;s complex relationship with nature through a meticulously crafted world. With a budget of 2.35 billion yen—making it the most expensive animated film of its era—the movie was created just before the digital animation revolution. Approximately 144,000 cells were hand-drawn to build its vision of Muromachi-era feudal Japan. The result was a timeless masterpiece that represented more than a decade of thoughtful development in plot, character, and animation. It was the pinnacle of its time and remains so today.\nAs we process the technological shift that now enables machines to replicate human art at scale, we must consider how this new dynamic will shape creativity\u0026rsquo;s future. When the marginal cost of producing high-quality work approaches zero, what defines creativity? Can creative works be protected, or can anyone replicate a piece after merely seeing it? In an era where everyday users generate sophisticated content with minimal input, how do we distinguish truly exceptional work?\nWhile I don\u0026rsquo;t have definitive answers, I believe our appreciation for content will evolve—perhaps valuing human-created work more highly or discovering new experiences through previously inconceivable creative expressions. Copyright laws, though often inadequate and slow to adapt, will eventually catch up to protect original works and incentivize innovation. On a positive note, creativity will no longer be limited to those with technical artistic skills. It may ultimately center on taste and the ability to understand and abstract experiences that evoke universal empathy—expressions of our collective human experience.\nHowever, as creation costs approach zero, we risk the collapse of our collective imagination while maintaining only the illusion of creativity. The dynamic we\u0026rsquo;ve observed in content consumption—preferring cheap, easy, addictive material—now extends to creation. Why face the challenging reality of making something yourself when you can skip to the result? Our aesthetics might become constrained by AI models\u0026rsquo; training data, reducing creativity to mere copying and merging of established works. We risk being left with nothing but juxtaposition.\nThe difficult process of creating and thinking deeply is where genuine ideas emerge. Technology evolves rapidly; humans do not. Even in the AI age, there remains space for quality and discernment. The labor, patience, and intention behind Miyazaki\u0026rsquo;s work embody values that transcend technological convenience—values we would be wise to preserve, even as our creative tools transform.\nReferences: https://openai.com/index/introducing-4o-image-generation/, https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/, https://www.youtube.com/watch?v=Pi2rHOhPZZ4, https://en.wikipedia.org/wiki/Spirited_Away\n","permalink":"https://chenterry.com/posts/craft-miyakazi/","summary":"\u003cp\u003e\u003cimg alt=\"Dust Balls\" loading=\"lazy\" src=\"/images/posts/hayao-miyakazi/dust-balls.png\"\u003e\u003c/p\u003e\n\u003cp\u003eHayao Miyazaki is my favorite artist and director. Though I haven\u0026rsquo;t watched his entire filmography, every moment of the films I have seen captivates me. In \u0026ldquo;Spirited Away,\u0026rdquo; the dust ball creatures exemplify his artistic prowess—his ability to infuse life into the mundane through his drawings. His work is truly a labor of love. Every scene in his animated films is hand-drawn and painted with watercolor.\u003c/p\u003e\n\u003cp\u003eTo put this dedication in perspective: a single \u003ca href=\"https://www.reddit.com/r/nextfuckinglevel/comments/1egdzja/this_4_second_crowd_scene_from_studio_ghiblis/\"\u003e4 second crowd scene\u003c/a\u003e from Studio Ghibli required 1 year and 3 months to complete. At 24 frames per second, that\u0026rsquo;s 96 images—roughly 6.4 images per month or one-third of an image in an eight-hour workday. At this rate, animators would spend a decade creating just 28.8 seconds of footage. This extraordinary commitment to craft has established Miyazaki\u0026rsquo;s work as iconic for decades.\u003c/p\u003e","title":"The Craft of Miyazaki in an AI-Generated World"},{"content":"When you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\nMore apparent ones: 2025-03-24\nCareer seeking: it\u0026rsquo;s a difficult job market, whether it\u0026rsquo;s doing leetcode or preparing for behavioral interviews, people want a way to cheat their way out of this difficult process, and they\u0026rsquo;re willing to pay for it too. 2025-03-27\nAgentic worklows: I\u0026rsquo;m usually more skeptical towards areas more widely reported, but agentic workflows may well be here to stay. From the initial action gpts (autogpt) agent chains (devin, metagpt, cogno etc), to the more recent operators (MCPs, OpenAI operator), it would indeed be nice to have agents act on our behalf (perhaps hopefuly not the way Anton goes about ordering Hamburgers in Silcon Valley).\nBuilding on the previous point concerning agentic computer use. It\u0026rsquo;s also interesting to consider the two approaches towards web navigation: vision-based and ai-native-ui. While people like Aravind Srinivas have openly expressed their skepicism towards vision based web operations, noting the limitations imposed by operating systems, especially iOS, which restrict access to other applications, vision-based web interactions are inherently more versatile and flexible.\nContent generation: Generative ai is at its core, generative. We have for the past three years seen numerous improvements in text/image/video/avatar generation, with applications in vairous forms of UGC (and now nearing PGC) ads, podcasts, short-form videos. I love how creative people have been with the available AI tools, but at the same time, I view these tools as an available means to scale creativity, not for enabling 0-1 creation.\nLess apparent (and more interesting) ones: 2025-03-25\nAds through LLM generated content: While I was trying to figure out the best way to add a forms feature to this website yesterday, I asked claude. And interestingly, this was what I got back at first: Note that Formspree is a paid service. This raises an interesting point about the future of AI generated content and suggestions. On one hand, the result is based on training data inputted into the model, but at the same time, one can easily manipulate it to recommend one solution over another (try Formspree vs Google Forms). This presents an interesting avenue for ads via llm generated content, and given the extent to which people are willing to trust llm results, this could even be an effective avenue. Yet in doing so, we are also risking the reliability of llm generated content.\nSEO is already changing in the age of LLMs, with AI genearated content flooding web browsers. Another interesting observation is the amount of website traffic going to specific companies from AI-enabled search services such as Perplexity. While looking at website traffic for Genspark and a few other websites, we can see both ChatGPT and Perplexity listed as the top referring websites (source: similarweb) Though the accuracy of this data is yet to be determined, it make sense intuitively that, owing to the large amount of content genearted via these AI platforms, the pages will be easily indexed by ai-browsers.\nGeneral Directions: These are some relatively well agreed upon directions that one may take:\n2025-03-27\nBetter customization: Traditionally, a tradeoff existed between customization and scale. Creating for wider audiences required standardized offerings. However, as LLM inference costs rapidly decrease, information retrieval and content generation are approaching fixed costs. This shift enables customization at scale across various domains—personalized news, cinematography, and other content formats. Additionally, advancements in multimodal capabilities (voice, image, video) will likely introduce greater variety in the content we consume. ","permalink":"https://chenterry.com/posts/user-needs/","summary":"\u003cp\u003eWhen you want to look for user pain points, go to a bar in the middle of the week, look for the most desparate looking person and buy them a beer. They\u0026rsquo;d probably have something substantial to talk about. While I don\u0026rsquo;t usually go to bars during weekdays, I\u0026rsquo;ve found some similar ways of identifying user needs, mostly through conversations or online forums. This is a running document of the user needs or pain points that I\u0026rsquo;ve found to be interesting. Perhaps some of them will turn out to be viable business oppportunities.\u003c/p\u003e","title":"User Needs \u0026 Opportunities"},{"content":"The Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\nSystem Architecture The application uses a serverless architecture centered around AWS Lambda functions and Amazon RDS. This design ensures scalability, reliability, and cost efficiency by leveraging AWS managed services.\nThe Client Application provides a web/mobile interface for user interactions. AWS API Gateway exposes the backend as a RESTful API and routes client requests to appropriate Lambda functions. Four AWS Lambda functions handle all backend operations, with a custom datatier module managing database interactions. Amazon RDS (MySQL) serves as the persistent data store for users, transactions, and receipt metadata. AWS S3 securely stores uploaded receipt images, while AWS Textract processes these images with OCR to extract transaction details. An AI service (AWS Bedrock) automatically categorizes expenses based on merchant and description. Finally, Amazon SNS sends budget alerts via email notifications when spending exceeds predefined thresholds.\nComponent Interactions In the receipt processing flow, a user uploads a receipt image through the frontend application. The API Gateway routes this request to a dedicated Lambda function, which stores the receipt in S3 and creates metadata in RDS. The system then leverages AWS Textract to analyze the image and extract key details such as merchant name, date, and amount. This extracted information is passed to an AI model that categorizes the transaction based on the merchant and description. The complete transaction details are then stored in the RDS database for future reference and analysis.\nThe budget monitoring process begins when a user sets category-specific budget limits via the frontend. A Lambda function updates these settings in the RDS database. Later, when new transactions are processed, the system evaluates current spending against the budget thresholds. If spending exceeds a predefined limit, the SNS service automatically sends a notification to the user\u0026rsquo;s registered email address, providing timely awareness of their financial situation.\nFor financial reporting, users can request customized reports through the frontend. A Lambda function queries the relevant transaction data from RDS based on user-specified parameters. This data is then processed and formatted according to the user\u0026rsquo;s preferences, such as report type (transaction details or category summaries) and format (CSV or PDF). The final report is delivered back to the client via the API response, providing valuable insights for financial planning and tax preparation.\nAPI Specification The application exposes a RESTful API through AWS API Gateway with the following endpoints:\n1. Receipt Upload and Analysis POST /upload\nPurpose: Upload a receipt image for processing Request Body: Multipart form data with image file userid (required): User identifier filename: Name of the receipt image data: Base64-encoded image data Process: The system validates if the user exists, stores the receipt image in S3, creates a receipt record in the database, and initiates asynchronous processing. The function also checks whether the user\u0026rsquo;s current monthly spending exceeds the monthly budget, and sends an email alert to the user through SNS. Response: Status 200: Success, returns transactionid, userid, amount, category, time Status 400: Bad request (missing parameters) Status 500: Server error 2. Expense Retrieval GET /expenses\nPurpose: Retrieve user expenses with optional filtering Query Parameters: userid (required): User identifier start_date (optional): Filter transactions after this date (YYYY-MM-DD) end_date (optional): Filter transactions before this date (YYYY-MM-DD) category (optional): Filter by expense category Response: Status 200: Success, returns list of transactions and summary Status 400: Bad request (missing parameters) Status 500: Server error 3. Budget Management POST /budget_and_alert\nPurpose: Set monthly spending budget Process: The system updates the user\u0026rsquo;s budget in the database, checks current month\u0026rsquo;s spending against the budget, and sends an SNS notification if spending exceeds the budget. Response: Status 200: Budget set successfully Status 400: Bad request (missing parameters) Status 500: Server error 4. Financial Reports GET /report\nPurpose: Generate and export financial report Query Parameters: userid (required): User identifier format (optional): \u0026ldquo;csv\u0026rdquo; or \u0026ldquo;pdf\u0026rdquo; (default: \u0026ldquo;csv\u0026rdquo;) report_type (optional): \u0026ldquo;transactions\u0026rdquo; or \u0026ldquo;summary\u0026rdquo; (default: \u0026ldquo;transactions\u0026rdquo;) start_date (optional): Starting date for report end_date (optional): Ending date for report category (optional): Filter by category Report Types: The \u0026ldquo;transactions\u0026rdquo; type lists all individual transactions with details, while the \u0026ldquo;summary\u0026rdquo; type provides aggregated statistics by category (count, total, average, max, min). Output Formats: The system supports CSV (standard comma-separated values) and PDF (HTML structure that would be converted to PDF) formats. Process: The system retrieves filtered transaction data from the database, generates the report in the requested format, and encodes the report data as base64 for transmission. Response: Status 200: Success, returns report data Status 400: Bad request (missing parameters) Status 500: Server error Database Schema The application uses a MySQL database on Amazon RDS with the following schema:\nThe Users table stores basic user information. The userid field serves as the primary key and unique identifier for each user. The email field contains the user\u0026rsquo;s email address for notifications. The budget field is a JSON object storing category-specific budget limits (e.g., {\u0026ldquo;Food\u0026rdquo;: 500, \u0026ldquo;Transport\u0026rdquo;: 200}). The sns_topic_arn field contains the Amazon SNS topic ARN for sending notifications to this user.\nThe Transactions table records all user expenditures. The transactionid field is an auto-incrementing primary key. The userid field is a foreign key reference to the Users table. The amount field stores the transaction amount as a decimal with two places of precision. The category field contains the expense category (e.g., Food, Transport, Entertainment) assigned by the AI. The time field records when the transaction occurred.\nThe Receipts table tracks uploaded receipt images and their processing status. The receiptid field is an auto-incrementing primary key. The userid field references the Users table. The status field indicates the current processing status (e.g., \u0026ldquo;uploaded\u0026rdquo;, \u0026ldquo;processing\u0026rdquo;, \u0026ldquo;completed\u0026rdquo;, \u0026ldquo;error\u0026rdquo;). The s3_location field contains the S3 bucket path to the stored receipt image. The upload_time field captures when the receipt was uploaded, while analysis_time records when processing was completed.\nLambda Functions The system is implemented using four main AWS Lambda functions, each dedicated to a specific aspect of the application.\nThe Receipt Upload and Analysis function processes receipt images and extracts transaction details. It handles receipt uploads and storage in S3, uses AWS Textract for OCR processing, employs an AI model for merchant recognition and expense categorization, creates transaction records from extracted data, and updates receipt status through processing stages.\nThe Get Past Expenses function retrieves and filters transaction records. It supports filtering by date range and category, generates transaction summaries and category breakdowns, calculates spending statistics (totals, counts), and returns a formatted JSON response.\nThe Set Budget and Trigger Alerts function updates budget settings and sends alerts. It updates user budget settings in the database, checks current spending against budget limits, sends notifications via SNS when thresholds are exceeded, calculates budget utilization percentages, and supports category-specific budgets.\nThe Export Financial Reports function generates financial reports in different formats. It supports multiple report types (transactions, summary), generates reports in CSV or PDF format, applies filtering options (date range, category), and encodes reports as base64 for transmission.\nConclusion The Smart Expense Tracker with Auto-Categorization represents a significant improvement over traditional expense tracking applications by leveraging AWS cloud services and artificial intelligence. By automating the tedious aspects of expense management, the system allows users to gain financial insights with minimal effort.\n","permalink":"https://chenterry.com/archived/smart-expense-tracker/","summary":"\u003cp\u003eThe Smart Expense Tracker with Auto-Categorization is a cloud-native application built on AWS serverless architecture. The system automates the tedious process of expense tracking by leveraging AWS services to process receipts, categorize transactions, and provide financial insights. The application offers several key features including receipt scanning and data extraction using AWS Textract, automatic expense categorization with AI (using AWS Bedrock), comprehensive expense tracking, budget setting with automated alerts via SNS, and financial report generation in CSV or PDF formats.\u003c/p\u003e","title":"Smart Expense Tracker"},{"content":"Year in Review 2024 felt to have passed by very quickly, in part because I had interesting work to occupy my time. Early in the year, after expanding Cogno and gaining some traction, work slowly stalled: though we did code and talk to customers, the cycles were far and wide in between. The focus on multi-agent systems was (as of writing this) in the right direction, yet we did not find the niche to tackle sales conversion improvement.\nOne crucial lesson I learned as a first-time founder was the importance of being hands-on with coding the product. Having oversight of product development isn\u0026rsquo;t sufficient – you need to be directly involved in building to achieve faster iteration cycles and truly understand the risks in shipping. This insight came later than I would have liked but proved invaluable.\nWhile I had actually considered taking a leave of absense to work on \u0026ldquo;the next big thing\u0026rdquo; coming into college, this year I decided that dropping out is not what I wanted at this stage of life – I still desired the complete college experience with its structure, humanities knowledge, friendships, and Chicago\u0026rsquo;s unique energy.\nIn March, after more than a year of working on Cogno, I accepted an opportunity at TikTok to build agent systems for Creative Copilots and Insights. I\u0026rsquo;m immensely thankful to Zhengjin and Caoye for the invitation. Though cutting short my long-anticipated vacation in Puerto Rico was not what I had planned for, the learning experience proved far more valuable – not to mention attending Nvidia GTC and Google Next in Vegas.\nWorking at TikTok offered a fascinating blend of familiar and novel experiences. While similar to ByteDance in some respects, collaborating with colleagues from multiple countries and being immersed in Silicon Valley provided distinct advantages. The work on insight extraction from multimodal data opened new perspectives on how LLMs interact with different content types. The question of whether probabilistic models like LLMs can truly be creative remains open for debate, yet their outputs can undeniably surprise us in unexpected ways.\nMy time at TikTok allowed me to witness firsthand how the field of generative AI is evolving across modalities – text, image, video, and audio – revealing new possibilities for product innovation as these technologies mature. I\u0026rsquo;ve come to believe that the future of generative AI lies in combining reasoning with synthesis (perhaps with greater emphasis on the latter) and multimodal output.\nLater in the year, going to Beijing and returning to school felt like a blur. Yet I found myself enjoying academics more than anticipated this quarter, partly due to great teammates and LLM-focused project-based classes. The academic environment offered a focused intensity often diluted in industry settings.\nThis recognition led me to Crowdlistening, which addresses the abundance of unstructured, multimodal content across social media platforms. By extracting meaningful insights from these underanalyzed sources, we can deliver value to users seeking to understand these complex information streams. Now I\u0026rsquo;m helping build up ai features for a stealth startup, so I\u0026rsquo;m less sure how Crowdlistening will evolve, but I still view it as an interesting product.\nI\u0026rsquo;m still figuring things out, but 2024 has undoubtedly been a year of exploration and growth – one for which I\u0026rsquo;m deeply grateful. I look forward to seeing how these parallel paths evolve in the coming year.\nCourage to be last Reflecting on 2024 (and 2023 for the context of Cogno), among my list of failed projects, very few failed due to lack of innovation. Since I began working with LLMs in fall 2022, there has been an abundance of interesting GenAI technologies to experiment with. It started with \u0026ldquo;domain specific prompting/finetuning\u0026rdquo; and data flywheels (thou not even now does anyone know what this looks like in action). By spring 2023, the focus shifted to LLMs as agents, exemplified by the Generative Agents paper, Microsoft AutoGen, and a few opensource projects like MetaGPT. At Cogno, we also built multi-agent systems, integrating various function calling features and agent collaboration for complex task reasoning. Everyone built, few created value (Glean focused on enterprise search, while Moveworks created value through api actions, neither of which I believe agents to have mattered). Founders encouraged each other\u0026rsquo;s enthusiasm, while investors rushed to learn the latest buzzwords in LLM technology (\u0026lsquo;prompt engineering\u0026rsquo; and \u0026lsquo;function calls\u0026rsquo; sounded less sexy compared to\u0026rsquo;agents\u0026rsquo;).\nBeing first to market rarely matters - people won\u0026rsquo;t remember you. What matters is creating defensible moats or developing critical elements that lead to unfair advantages. While Google\u0026rsquo;s technology investment in Android can be considered \u0026rsquo;not just building a moat, but scorching the earth for 250 miles around the castle,\u0026rsquo; most companies\u0026rsquo; self-described technological differentiation is merely self-flattery and a feeble attempt to impress tech-enthusiast investors. Technology truly matters only when it can solve seemingly insurmountable challenges or optimize costs and operations. In every other situation, the focus should be on building sustainable advantages that ensure long-term survival. [Thoughts WIP]\n","permalink":"https://chenterry.com/posts/year-in-review/","summary":"\u003ch2 id=\"year-in-review\"\u003eYear in Review\u003c/h2\u003e\n\u003cp\u003e2024 felt to have passed by very quickly, in part because I had interesting work to occupy my time. Early in the year, after expanding Cogno and gaining some traction, work slowly stalled: though we did code and talk to customers, the cycles were far and wide in between. The focus on multi-agent systems was (as of writing this) in the right direction, yet we did not find the niche to tackle sales conversion improvement.\u003c/p\u003e","title":"2024 in Review"},{"content":"Interactive chat interface with multiple AI agents, enabling dynamic conversation flows and specialized problem-solving capabilities.\n","permalink":"https://chenterry.com/projects/personalized_content/","summary":"\u003cp\u003eInteractive chat interface with multiple AI agents, enabling dynamic conversation flows and specialized problem-solving capabilities.\u003c/p\u003e","title":"Towards Differentiable Quality - Content Synthesis"},{"content":"Content recommendation system leveraging embedding similarity for personalized content recommendation (based on profile).\n","permalink":"https://chenterry.com/projects/recommendation_searh/","summary":"\u003cp\u003eContent recommendation system leveraging embedding similarity for personalized content recommendation (based on profile).\u003c/p\u003e","title":"LLM Enhanced Recommendation and Search"},{"content":"Interactive learning aids for reading comprehension and engagement.\n","permalink":"https://chenterry.com/projects/learning_interface/","summary":"\u003cp\u003eInteractive learning aids for reading comprehension and engagement.\u003c/p\u003e","title":"Exploring Unknown Unknowns - Textbook Interface"},{"content":"Advised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\nGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\nOur approach is centered around understanding effective study group learning for educational purposes. Understanding Learning Barriers: Traditional Q\u0026amp;A systems often provide direct answers, which may limit deeper understanding. Groupal emphasizes learning through inquiry, leveraging Socratic questioning techniques proven to improve knowledge retention and critical thinking. Decomposing the Problem: Groupal integrates real-time speech processing, intent routing, and contextual retrieval of relevant background information to support learning. The frontend-backend pipeline connects key components such as document parsing, relevance checks, and question generation that adapts according to the flow of group discussions. Real-Time Interaction: Groupal listens to student discussions in real time, converts speech to text, identifies misconceptions, and generates insightful socratic questions. The system retrieves relevant content through a RAG process to supplement the discussion effectively. Groupal provides a comprehensive set of features to enhance the effectiveness of collaborative study sessions. Through real-time speech analysis, Groupal transcribes live group discussions into text, identifies misconceptions, and stores them for further review. It detects potential misconceptions in the discussion and generates Socratic-style questions when relevant to encourage further discussions among students. With its background knowledge retrieval capability, Groupal accesses relevant materials through a RAG process, retrieving the top 3 documents from a vector database, and using a Socratic Questioning Model to generate and output the socratic question, giving students immediate access to supporting information and guidance during their sessions. The platform allows users to upload documents, form or join study groups, and explore relevant content in real time, creating a well-organized and interactive learning experience. Groupal ensures discussions remain focused and productive by providing adaptive and context-aware guidance.\nCredits: Tina Liu, Yihang Du, Doohwan Kim.\n","permalink":"https://chenterry.com/archived/groupal/","summary":"\u003cp\u003eAdvised by Prof. Kristian Hammond. Developed LLM product that analyzes real-time audio conversations, detects relevancy and misconceptions, and provides targeted Socratic questions and material suggestions through RAG.\u003c/p\u003e\n\u003cp\u003eGroupal aims to help students work together more effectively and build a deeper understanding in study sessions. The project’s goal is to create a virtual learning assistant that listens to real-time student discussions, detects misconceptions, and facilitates discussions through Socratic questioning techniques and relevant background knowledge retrieval.\u003c/p\u003e","title":"Realtime Conversational Learning Aid"},{"content":"Inspiring insights, amplifying voices. (crowdlistening.com) From Content Aggregation to Original Research Crowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. Our focus is not just on analyzing content at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\nDeep research features provide professional-looking research reports, yet the contents are far from original, as they\u0026rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet\u0026rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they\u0026rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.\nI\u0026rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlistening, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn\u0026rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows. I paused Crowdlistening in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I\u0026rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).\nWhat I\u0026rsquo;m particularly interested in exploring and building with Crowdlistening is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.\nThe Insight Paradox Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can\u0026rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlistening bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in \u0026ldquo;Essence of Creativity\u0026rdquo; - helping users understand massive amounts of information and generate meaningful insights when they \u0026ldquo;don\u0026rsquo;t know what output they want.\u0026rdquo;\nTechnical Architecture: Multi-Modal by Design The rationale behind Crowdlistening\u0026rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video\u0026rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.\nCrowdlistening\u0026rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM\u0026rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities. This dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlistening can generate insights that reflect not just what is being said, but how it\u0026rsquo;s being said, the surrounding context, and the audience\u0026rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the \u0026ldquo;insight paradox\u0026rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlistening: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.\nDetailed Analysis Capabilities The platform provides granular breakdowns of content performance and audience reactions. As shown in our analysis results page, users can explore specific themes, track sentiment over time, and identify the most engaging content types. This helps brands understand not just what is being said, but why certain content resonates with their audience.\nThe opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.\nThe MCP Advantage: Accessible Functional Calls We have integrated Model Context Protocols (MCPs) - an emerging standard that simplifies how LLMs interact with specialized tools and data sources. Rather than simple API calls, MCPs provide structured interfaces for LLMs to access specialized capabilities while maintaining context awareness throughout the analysis process.\nAs shown here, when a user submits a research question, the system dynamically determines which analytical capabilities to deploy. The Claude interface serves as the orchestration layer, identifying relevant MCP tools to activate and calling them sequentially:\nFirst gathering baseline information through web search Then performing targeted data collection via specialized TikTok MCP tools Following with multi-layered analysis of videos and comments Finally synthesizing everything into coherent, actionable insights This MCP-driven approach creates a dramatic efficiency improvement, reducing complex social media analysis from weeks to minutes while maintaining remarkable analytical depth.\nCase Study - Trump Tariffs To demonstrate Crowdlistening\u0026rsquo;s capabilities, we conducted a comprehensive analysis of public sentiment regarding Trump\u0026rsquo;s tariff policies. This serves as an excellent test case due to its complexity, polarizing nature, and economic impact.\nWhen a user inputs the query about Trump\u0026rsquo;s tariff policies, our system activates the appropriate MCP tools in sequence. First, it gathers factual background information on the policies themselves, as shown below:\nThis background research provides context on what the current tariff policies are, including the 10% baseline tariff on all imports that took effect in April 2025, plus the higher \u0026ldquo;reciprocal\u0026rdquo; tariffs on countries with which the US has trade deficits (34% for China, 20% for the EU, and 24% for Japan).\nNext, the system analyzes public opinion on these policies by examining social media content. The analysis reveals highly polarized reactions, categorized into three main perspectives:\nThe sentiment analysis dashboard shows that opinions on Trump\u0026rsquo;s tariff policies are distributed as 38% supportive, 42% critical, and 20% neutral or mixed. This visualization helps brands and researchers quickly understand the overall public response landscape.\nOne of the most valuable outputs is our projected economic impact analysis. This data visualization clearly presents the concrete financial implications of these policies across multiple domains:\nThe analysis shows an estimated $1,300 annual cost increase per US household, a projected 0.8% reduction in long-run US GDP, significant auto price increases ($3,000 for US vehicles, $6,000 for imports), and warnings about market volatility.\nBeyond simple pro/con sentiment, our opinion analysis feature categorizes specific viewpoints with remarkable granularity. For instance, when examining comments on related content, we can identify nuanced perspectives and their prevalence:\nThis example shows how our system can identify several different comment themes, including positive views of content creators (37.5%), appreciation for intelligent discussion (25%), and concerns about media echo chambers (12.5%). This level of nuanced understanding would be impossible through traditional keyword or basic sentiment analysis.\nValidation and Impact Our solution has been validated through interviews with major brands like L\u0026rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlistening enables:\nRapid response to emerging trends Deep understanding of consumer sentiment across demographics Identification of microtrends before they become mainstream Competitive intelligence at unprecedented scale The Future of MCP-Driven Research We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.\nThis approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude\u0026rsquo;s interface) emerge as this paradigm gains traction.\nFor social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they\u0026rsquo;re covered in traditional reporting.\nOn Social Intelligence Crowdlistening represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we\u0026rsquo;re helping brands make more informed decisions faster than ever before.\nAs noted in \u0026ldquo;Essence of Creativity,\u0026rdquo; the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we\u0026rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.\nCredits: This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Ruiwan, Qiping, and other members on the Creative team at TikTok, who I\u0026rsquo;ve discussed early conceptions of this idea with.\n","permalink":"https://chenterry.com/projects/crowdlistening/","summary":"\u003ch2 id=\"inspiring-insights-amplifying-voices-crowdlisteningcom\"\u003eInspiring insights, amplifying voices. (crowdlistening.com)\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"Webpage\" loading=\"lazy\" src=\"/images/posts/crowdlistening/webpage.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"from-content-aggregation-to-original-research\"\u003eFrom Content Aggregation to Original Research\u003c/h2\u003e\n\u003cp\u003eCrowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. Our focus is not just on analyzing content at scale, but rather conducting original research directly from raw social data, generating insights that haven\u0026rsquo;t yet appeared in established reporting.\u003c/p\u003e\n\u003cp\u003eDeep research features provide professional-looking research reports, yet the contents are far from original, as they\u0026rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet\u0026rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they\u0026rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.\u003c/p\u003e","title":"Crowdlistening"},{"content":"PepTalk: AI Journaling Tool Realtime conversation with aI companion to help you note down feelings and journals for the day. (Prototype: https://peptalk-navy.web.app/)\nWhat2Do: AI Trip Planning Tool A trip planning tool for generating itinearies based on article url input and content extraction. (Prototype: what2do-51224.web.app)\nOHours: Office Hour Scheduling Tool An office hour queuing system to improve student experience and help TAs manage questions more efficiently. (Prototype: ohours.web.app/)\nCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\n","permalink":"https://chenterry.com/archived/prototyping/","summary":"\u003ch3 id=\"peptalk-ai-journaling-tool\"\u003ePepTalk: AI Journaling Tool\u003c/h3\u003e\n\u003cp\u003eRealtime conversation with aI companion to help you note down feelings and journals for the day.\n(Prototype: \u003ca href=\"https://peptalk-navy.web.app/\"\u003ehttps://peptalk-navy.web.app/\u003c/a\u003e)\u003c/p\u003e\n\u003ch3 id=\"what2do-ai-trip-planning-tool\"\u003eWhat2Do: AI Trip Planning Tool\u003c/h3\u003e\n\u003cp\u003eA trip planning tool for generating itinearies based on article url input and content extraction.\n(Prototype: what2do-51224.web.app)\u003c/p\u003e\n\u003ch3 id=\"ohours-office-hour-scheduling-tool\"\u003eOHours: Office Hour Scheduling Tool\u003c/h3\u003e\n\u003cp\u003eAn office hour queuing system to improve student experience and help TAs manage questions more efficiently.\n(Prototype: ohours.web.app/)\u003c/p\u003e\n\u003cp\u003eCredits: Lian Zhang, Janna Lee, Soham Shah, Jonny Kong\u003c/p\u003e","title":"Rapid Prototyping of LLM Enabled Webapps"},{"content":"Observing and understanding the strange quirks of individuals and crowds What makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\n","permalink":"https://chenterry.com/main-themes/human-quirks/","summary":"\u003ch2 id=\"observing-and-understanding-the-strange-quirks-of-individuals-and-crowds\"\u003eObserving and understanding the strange quirks of individuals and crowds\u003c/h2\u003e\n\u003cp\u003eWhat makes humans truly \u0026ldquo;human\u0026rdquo; - not perfectly logical machines, but complex beings whose decisions are shaped by psychology, social context, and evolutionary history? By understanding human quirks, we can design better systems that work with human nature rather than against it.\u003c/p\u003e","title":"Human Quirks"},{"content":"Every now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\nPatrick Collison: Go deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\nDon\u0026rsquo;t stress out too much about how valuable the things you\u0026rsquo;re going deep on are\u0026hellip; but don\u0026rsquo;t ignore it either. It should be a factor you weigh but not by itself dispositive.\nTo the extent that you enjoy working hard, do. Subject to that constraint, it\u0026rsquo;s not clear that the returns to effort ever diminish substantially. If you\u0026rsquo;re lucky enough to enjoy it a lot, be grateful and take full advantage!\nMake friends over the internet with people who are great at things you\u0026rsquo;re interested in. The internet is one of the biggest advantages you have over prior generations. Leverage it.\nAim to read a lot.\nIf you think something is important but people older than you don\u0026rsquo;t hold it in high regard, there\u0026rsquo;s a reasonable chance that you\u0026rsquo;re right and they\u0026rsquo;re wrong. Status lags by a generation or more.\nAbove all else, don\u0026rsquo;t make the mistake of judging your success based on your current peer group. By all means make friends but being weird as a teenager is generally good.\nBut having good social skills confers life-long benefits. So, don\u0026rsquo;t write them off. Get good at making a good first impression, being funny (if possible\u0026hellip; this author still working on it\u0026hellip;), speaking publicly.\nMake things. Operating in a space with a lot of uncertainty is a very different experience to learning something.\nMore broadly, nobody is going to teach you to think for yourself. A large fraction of what people around you believe is mistaken. Internalize this and practice coming up with your own worldview. The correlation between it and those around you shouldn\u0026rsquo;t be too strong unless you think you were especially lucky in your initial conditions.\nIf you\u0026rsquo;re in the US and go to a good school, there are a lot of forces that will push you towards following traintracks laid by others rather than charting a course yourself. Make sure that the things you\u0026rsquo;re pursuing are weird things that you want to pursue, not whatever the standard path is. Heuristic: do your friends at school think your path is a bit strange? If not, maybe it\u0026rsquo;s too normal.\nFigure out a way to travel to San Francisco and to meet other people who\u0026rsquo;ve moved there to pursue their dreams. Why San Francisco? San Francisco is the Schelling point for high-openness, smart, energetic, optimistic people. Global Weird HQ. Take advantage of opportunities to travel to other places too, of course.\nFind vivid examples of success in the domains you care about. If you want to become a great scientist, try to find ways to spend time with good (or, ideally, great) scientists in person. Watch YouTube videos of interviews. Follow some on Twitter.\nPeople who did great things often did so at very surprisingly young ages. (They were grayhaired when they became famous\u0026hellip; not when they did the work.) So, hurry up! You can do great things.\nPaul Graham - How to do Great Work ","permalink":"https://chenterry.com/posts/advice/","summary":"\u003cp\u003eEvery now and then I like to read about the advice of others who\u0026rsquo;ve succeeded in their field. Here\u0026rsquo;s a few that I personally found to be enlightening and practical.\u003c/p\u003e\n\u003ch2 id=\"patrick-collison\"\u003ePatrick Collison:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGo deep on things. Become an expert. In particular, try to go deep on multiple things. (To varying degrees, I tried to go deep on languages, programming, writing, physics, math. Some of those stuck more than others.) One of the main things you should try to achieve by age 20 is some sense for which kinds of things you enjoy doing. This probably won\u0026rsquo;t change a lot throughout your life and so you should try to discover the shape of that space as quickly as you can.\u003c/p\u003e","title":"Advice"},{"content":"This week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I\u0026rsquo;d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn\u0026rsquo;t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.\nWhat Constitutes Creative Work? Let\u0026rsquo;s discuss what kind of creativity AI can enable, establishing the capability boundaries of AI applications. Is it creative to \u0026ldquo;take screenshots of someone else\u0026rsquo;s video and caption them with other people\u0026rsquo;s comments on your own account\u0026rdquo;? Though this involves some editing rather than direct reposting, it\u0026rsquo;s difficult to call this creative work. Simple copying only accelerates content diffusion within an ecosystem while reducing the excess returns of original creation, as economist Schumpeter noted.\nI believe creativity is more about choosing a unique perspective. Content with contrast or conflict naturally captures our attention, but thoughtful, empathetic content is equally creative. From an ecosystem perspective, creativity can be divided into production and diffusion – the former generating new content, the latter deriving from or spreading existing content.\nAs for AI\u0026rsquo;s value in this process, generative AI as a probabilistic model struggles to produce content with fresh perspectives. However, it can help us efficiently understand massive amounts of information and generate insights (perspectives). As multimodal AI capabilities (text, image, video) improve, content reproduction costs will rapidly decrease, making products that help users find new inspiration increasingly valuable. Through such creative assistance, we can achieve two main effects:\nInspiration acquisition: Accelerating original content production Content derivation: Accelerating the diffusion of quality creative work Content Understanding for Enhanced Generation How can we make language models produce outputs that meet our expectations? This challenging question can be further divided into: (1) we don\u0026rsquo;t know what our ideal output looks like, and (2) we know what we want, but the language model doesn\u0026rsquo;t understand us. Most people are trying to solve the latter problem (through model alignment, prompting, few-shot learning, RAG, fine-tuning, memory and caching methods). However, the approaches in this space are increasingly similar, with many solutions being open-sourced, which is why many generative products deliver roughly comparable results. The real differentiation lies in how to adapt engineering and data processing to specific business scenarios.\nReturning to the first problem - \u0026ldquo;I don\u0026rsquo;t know what output I want\u0026rdquo; - this stems more from a lack of content understanding. Good script writing requires more than just hooks, USPs, and CTAs; it needs a clear angle: content that resonates with the audience, is appropriate for the context, and achieves its purpose. Some products are creating brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will likely become common, finding ways to connect insight data with generation without manual setup could be a breakthrough.\nUnderstanding User Needs Looking at the creative ecosystem, each creative area (ad aggregation, competitor tracking, brand insights, performance analysis, content generation) has 3-4 companies with minimal data or interaction differences. Data products tend to be traditional, while AI products often rely on simple language model adjustments. A potential differentiator would be acquiring more granular data and creating smoother interactions. Connecting upstream and downstream tasks (complete creative production process with participation/adjustment at each stage) could be an ideal product form.\nIf we calculate product value as \u0026ldquo;user value = new experience - old experience - replacement cost,\u0026rdquo; we find that most products built on foundational language models (old experience) with fine-tuning or prompting adjustments (incremental new experience) deliver very limited incremental value. From an interaction perspective, users still need to input personalized prompts, and outputs almost always require multiple rounds of editing before use. The question becomes: how do we increase incremental value?\nUser-Friendly Workflows Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.\nThe concept of \u0026ldquo;workflows, not skills\u0026rdquo; addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.\nI previously had a misconception that simply connecting capabilities constitutes a workflow, but deeper design is needed. What we consider Language UI is actually Prompt UI, which differs from true language interaction by missing the context and shared understanding present in human-to-human communication. Introducing these elements through features like detailed follow-up questions and future cross-container reference relationships could enhance user experience. Current prompting is likely a transitional form; eventually, we should eliminate the need for context-specific prompts by enabling LLMs to understand context and generate appropriate guidance.\nMultimodal Interaction and Content Ecosystem Finally, let\u0026rsquo;s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.\nSwitching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren\u0026rsquo;t just people; they\u0026rsquo;re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.\nCase Study: Voice Synthesis Take voice synthesis as an example. From a technical perspective, this technology is already quite mature, yet the first application that comes to mind might be phone scams. Other applications include David Attenborough\u0026rsquo;s wildlife narration in open-source projects or OpenAI\u0026rsquo;s GPT-4o launch event simulating Samantha\u0026rsquo;s voice (originally Scarlett Johansson) from the movie \u0026ldquo;Her.\u0026rdquo; However, I believe short video creators are truly making the best use of this technology.\nI recently saw a high-quality derivative work based on \u0026ldquo;In the Name of the People\u0026rdquo; (a 2017 Chinese TV drama). The creator (called \u0026ldquo;Yi Tou Jue Lv\u0026rdquo;) made remarkably deep portrayals of character psychology and inner monologues. Only after reading the comments did I learn that the creator used voice synthesis combined with their understanding of the characters to create this derivative work. Looking through this creator\u0026rsquo;s content, I found they cleverly used the original footage but replaced the narration with autobiographical scripts using AI-synthesized voices, making their work deeper and satisfying more viewer perspectives than their peers – essentially creating a new creative genre.\nContrasting Audio \u0026amp; Text It\u0026rsquo;s fascinating how differently our brains process audio and text. When we read, we\u0026rsquo;re essentially interacting with a graphical user interface - scanning, jumping between sections, processing information at our own pace. We\u0026rsquo;ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text may at times feel less engaging than a good conversation. Speaking, in contrast, is inherently linear and social. There\u0026rsquo;s something about the human voice that keeps us present - the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It\u0026rsquo;s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.\nThis contrast reveals something deeper about how we process information. Text excels at conveying complex ideas - we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of text - where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.\nConclusion As Roland Barthes suggested with \u0026ldquo;The Death of the Author,\u0026rdquo; once an author creates a work, the interpretation rights transfer to the readers. Video platforms feature various edited compilations, summaries, and analyses of film and television works. With improvements in constrained generative AI (voice synthesis, anime IP unification, future realistic character generation), we may see numerous derivative works based on original IPs, approaching professional quality and satisfying different interpretations and imaginations about the original work. These perspectives might all exist in the original work, but each short video offers a different angle, providing users with unique experiences. There remains much content that people want to see but isn\u0026rsquo;t yet available on platforms. Another interpretation of creativity could be how to push the supply curve outward to meet the demand curve at a new equilibrium point, better satisfying user needs.\nFinal Thoughts While generative AI effects are evolving rapidly, human nature changes slowly. The innovation opportunities brought by technology are often overestimated in the short term but underestimated in the long term. Making probabilistic models creative is challenging yet fascinating work, it\u0026rsquo;s probably something I will continue working on.\n","permalink":"https://chenterry.com/posts/essense-of-creativity/","summary":"\u003cp\u003eThis week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I\u0026rsquo;d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn\u0026rsquo;t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.\u003c/p\u003e","title":"Essence of Creativity"},{"content":"Analyze thousands of tiktoks to provide actionable trends \u0026amp; insights for key agencies. (Worked on multi-modal content understanding) To be released on TikTok Creative Center (https://ads.tiktok.com/business/creativecenter/pc/en)\nCredits: TikTok Creative Team\nBeyond Data: The Evolution of AI-Driven Insight Products for Content Creation Introduction: The Shifting Landscape of Creative AI Tools In the rapidly evolving space of AI-driven creative tools, we\u0026rsquo;re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.\nWhile many current GenAI applications focus heavily on content generation capabilities, the true creative bottleneck often isn\u0026rsquo;t in the generation step itself. Rather, it lies in the quality of insights that inform and guide the creative process. Without meaningful data and analysis, even the most sophisticated generation tools produce generic, uninspired content.\nThis blog explores how data insight products are evolving alongside generative AI technologies, and how their integration could fundamentally transform content creation workflows.\nThe Evolution of AI Capabilities The limitations of general-purpose large language models have become increasingly apparent when handling complex creative tasks. To address issues like hallucinations and improve task completion capabilities, the industry has largely reached a consensus around agent-based approaches.\nDifferent types of agent workflows have emerged to address specific needs. Non-agentic workflows generate content linearly without backtracking, suitable for straightforward tasks. Reflection-based systems introduce iterative improvement cycles where the AI criticizes and refines its own outputs. Tool use capabilities enable function calls and web browsing for enhanced research capabilities.\nMore advanced systems implement planning algorithms that decompose complex tasks into manageable steps, similar to how human creators break down projects. At the frontier, multi-agent collaboration enables specialized AI agents to work together, each handling different aspects of a complex creative process.\nThis evolution toward more sophisticated agent architectures reflects a growing understanding that creative work isn\u0026rsquo;t linear—it requires iteration, refinement, and the ability to leverage different capabilities at different stages of the process.\nThe Workflow Challenge One of the key limitations in current AI creative products is their focus on isolated capabilities rather than integrated workflows. In the advertising and marketing industry, there\u0026rsquo;s a high concentration of AI tools, but most provide only single functions or partial capabilities.\nA content creator typically needs to move through multiple stages: gathering insights, analyzing competitors, developing concepts, generating scripts, creating visual assets, and optimizing the final product. Currently, this requires juggling multiple disconnected tools, manually transferring context between them, and piecing together a cohesive workflow.\nUsers don\u0026rsquo;t simply need better individual tools—they need comprehensive workflows that connect these steps seamlessly. The value proposition shifts from \u0026ldquo;what can this AI do?\u0026rdquo; to \u0026ldquo;how does this AI fit into my creative process?\u0026rdquo; This represents a fundamental shift in how we should design and evaluate AI creative systems.\nMarket Analysis of Insight Products The current market for insight products shows several distinct categories, each addressing different aspects of the creative process. Here\u0026rsquo;s a structured analysis of the landscape:\nAd Compilation Tools Products in this category focus on collecting, organizing, and analyzing existing advertisements across platforms. Pipi Ads maintains a library of over 20 million TikTok ads with extensive filtering capabilities, allowing users to study successful campaigns and identify trending approaches. Foreplay offers a more workflow-oriented solution, enabling users to save ads from multiple platforms, organize them with custom tags, and build creative briefs based on existing successful content.\nThe value proposition of these tools centers on learning from what already works. By studying high-performing ads, creators can identify patterns and strategies that resonate with specific audiences. However, most of these tools stop at the analysis stage without directly connecting insights to content generation.\nCompetitor Analysis Tools Tools like Social Peta, Big Spy, and Story Clash provide deeper analysis of competitive activities. Social Peta offers insights into content distribution across 69 countries and 70 networks, analyzing multimedia types and dimensions. Big Spy enables cross-network ad searching with multiple filters, while Story Clash specializes in TikTok influencer tracking and performance analysis.\nThe competitive analysis market has grown substantially with the rise of social media advertising, with new players continuously entering the space to address specialized niches and platforms. These tools typically provide dashboard interfaces with various filters for monitoring competitor strategies, but most lack direct integration with content creation workflows.\nBrand Insight Tools Social listening platforms like Springklr, Exolyt, and Keyhole monitor brand mentions and sentiment across social channels. These tools analyze both posts and comments, providing valuable data on how audiences perceive brands and their content. Springklr offers comprehensive post and comment analysis with sentiment tracking, while Exolyt specializes in TikTok-specific insights, comparing brand content with user-generated content.\nKeyhole delivers profile analytics, social trend monitoring, and campaign tracking. These tools excel at capturing the audience\u0026rsquo;s voice and identifying shifts in perception, but typically require significant manual analysis to translate these insights into actionable creative strategies.\nPerformance Analysis Tools Platforms such as Social Insider, Motion App, and RivalQ focus on analyzing ad performance metrics. These tools help marketers understand what content performs best, with detailed analytics on engagement, conversion, and return on investment. By identifying high-performing content patterns, these tools can inform future creative decisions.\nHowever, there remains a significant gap between identifying what works and automatically generating new content based on those insights. Most performance analysis tools remain separated from content creation workflows, requiring manual interpretation and application of insights.\nDeep Dive: Notable Products Several standout products illustrate different approaches to the insight-generation challenge:\nTikBuddy: Platform-Specific Analysis TikBuddy focuses exclusively on TikTok analytics, offering creator rankings by category, follower count, and growth rate. The tool provides comprehensive account performance monitoring and video data analysis through a convenient Chrome extension.\nIts specialized focus allows for deeper platform-specific insights, but its utility is limited to a single platform and doesn\u0026rsquo;t extend to content creation. Users must manually apply any insights gained to their creative process.\nForeplay: Workflow Integration Foreplay stands out for its more integrated workflow approach. The platform enables users to collect ad content across platforms, preserve it even after platform deletion, and organize it with tags and categories. Its brief creation tools facilitate the transition from insight to execution, with support for brand information and specific generation requirements.\nThe platform\u0026rsquo;s AI storyboard generator creates hooks and develops scripts based on collected insights. Foreplay also integrates discovery features organized by community, brand, and experts, alongside competitor monitoring capabilities.\nThis approach begins to bridge the gap between insights and creation, though the integration remains partial rather than fully automated.\nKeyhole: Deep Analytics Keyhole exemplifies the analytics-focused approach, tracking keywords and brand mentions with temporal context. The platform offers detailed post analysis, influencer identification, trending topics visualization, and profile analytics with optimization recommendations.\nIts strength lies in comprehensive data collection and visualization, but like many analytics platforms, it requires significant human interpretation to translate insights into creative decisions.\nEmerging Innovations in Data Insight Products The data insight landscape continues to evolve rapidly, with several notable innovations emerging:\nOpen source projects like Vanna are revolutionizing text-to-SQL capabilities, making database querying more accessible to non-technical users. These tools enable creators to extract specific insights from complex datasets without specialized database knowledge.\nRecent startups are developing interactive data dashboards that visualize complex datasets in more intuitive ways, allowing for easier pattern identification and insight extraction. These tools employ advanced visualization techniques to make data more accessible and actionable.\nUser feedback aggregation tools are also gaining traction, automatically summarizing and categorizing customer sentiment from reviews and comments. These systems can identify common themes and concerns, providing valuable input for content creators looking to address audience needs.\nThe most promising innovations focus on reducing the cognitive load required to extract meaningful insights from data, making the path from analysis to action more direct and intuitive.\nThe Future: Insight-Driven Content Generation The next evolution in creative AI tools will likely center on high-quality content generation based on data insights. Current GenAI applications often produce unnecessary content redundancy—different from hallucinations, but equally problematic for effective communication.\nThe real creative barrier isn\u0026rsquo;t typically in the generation process itself, but in the prompts—the insights that inform decision-making. When using agent-based systems, the quality of instructions and background information directly impacts the output quality.\nFor example, advanced AI systems can now decompose complex goals like \u0026ldquo;How can a lifestyle channel creator get 1,000 subscribers on YouTube?\u0026rdquo; into specific tasks: analyzing successful channels, generating targeted content ideas, and implementing optimization strategies. However, the quality of these recommendations depends entirely on the AI\u0026rsquo;s access to relevant, accurate data about what actually works.\nBy leveraging sophisticated content analysis, we can identify truly effective patterns in high-performing content. Multimodal understanding can reveal why certain creative approaches resonate with specific audiences, providing creators with concrete, unique insights rather than generic advice.\nThe future lies in connecting these insights directly to the generation process—using what we know works as the foundation for creating new content that maintains brand uniqueness while leveraging proven patterns.\nConclusion: The Integration Opportunity Compared to other GenAI creative tools, insight products place greater emphasis on data quality and quantity. The next leap in AI-generated content quality will likely come from precise generation guided by robust insight data.\nThe most promising opportunity lies in creating systems that can automatically analyze successful content across platforms, extract meaningful patterns from this analysis, and directly translate these insights into generation guidance. This approach would produce highly targeted content that leverages proven patterns while maintaining brand distinctiveness.\nAs we move forward, the focus will shift from mere information generation toward sophisticated information synthesis—providing not just content, but content informed by actionable insights derived from real-world performance data. Organizations that successfully integrate insight gathering with content generation will gain a significant competitive advantage in an increasingly crowded digital landscape.\nThe future belongs not to those with the most powerful generative models, but to those who can effectively transform data into creative insight, and insight into compelling content.\n","permalink":"https://chenterry.com/projects/insight/","summary":"\u003cp\u003eAnalyze thousands of tiktoks to provide actionable trends \u0026amp; insights for key agencies. (Worked on multi-modal content understanding)\nTo be released on TikTok Creative Center (\u003ca href=\"https://ads.tiktok.com/business/creativecenter/pc/en\"\u003ehttps://ads.tiktok.com/business/creativecenter/pc/en\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eCredits: TikTok Creative Team\u003c/p\u003e\n\u003ch1 id=\"beyond-data-the-evolution-of-ai-driven-insight-products-for-content-creation\"\u003eBeyond Data: The Evolution of AI-Driven Insight Products for Content Creation\u003c/h1\u003e\n\u003ch2 id=\"introduction-the-shifting-landscape-of-creative-ai-tools\"\u003eIntroduction: The Shifting Landscape of Creative AI Tools\u003c/h2\u003e\n\u003cp\u003eIn the rapidly evolving space of AI-driven creative tools, we\u0026rsquo;re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.\u003c/p\u003e","title":"Insight Spotlight"},{"content":"Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\nFrom Google Next to Baidu Create Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.\nFrom a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.\nFrom a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.\nLLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.\nWe should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?\n1.1 Consensus: Task Specific, MoE, Agents, Routing Content that has reached consensus:\nMost companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).\nThe MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.\nThe core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.\nImplementing an MoE architecture typically requires the following steps:\nDefine expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.\nTrain expert networks: Use labeled training data to train each expert network to obtain weights and parameters.\nAllocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.\nSummarize results: Weight and sum the output results of each expert network to get the final output.\nTrain the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.\nLonger Context Window -\u0026gt; LLM Routing At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.\nAI Routing Uses Writesonic (https://writesonic.com) uses GPT Router for LLM Routing during AI Model Selection.\nGPT Router (https://github.com/Writesonic/GPTRouter) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.\nfrom gpt_router.client import GPTRouterClient from gpt_router.models import ModelGenerationRequest, GenerationParams from gpt_router.enums import ModelsEnum, ProvidersEnum client = GPTRouterClient(base_url=\u0026#39;your_base_url\u0026#39;, api_key=\u0026#39;your_api_key\u0026#39;) messages = [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write me a short poem\u0026#34;}, ] prompt_params = GenerationParams(messages=messages) claude2_request = ModelGenerationRequest( model_name=ModelsEnum.CLAUDE_INSTANT_12, provider_name=ProvidersEnum.ANTHROPIC.value, order=1, prompt_params=prompt_params, ) response = client.generate(ordered_generation_requests=[claude2_request]) print(response.choices[0].text) 1.2 Non-Consensus: Scenarios, Market, Differentiation Content that is still not determined:\nWhat constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge\nMany companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures\nHow Agents Can Help Creators Achieve 10x Efficiency 2.1 Agent Application Cases AutoGPT AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters. https://github.com/Significant-Gravitas/AutoGPT\nGPT Researcher A GPT-based autonomous agent that conducts comprehensive online research on any given topic. https://github.com/assafelovic/gpt-researcher\nThe advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.\nWorkflow Building - Video Creation Example Concept Design: Midjourney Script + Storyboard: ChatGPT AI Image Generation: Midjourney, Stable Diffusion, D3 AI Video: Runway, Pika, Pixverse, Morph Studio Dialogue + Narration: Eleven Labs, Ruisheng Sound Effects + Music: SUNO, UDIO, AUDIOGEN Video Enhancement: Topaz Video Subtitles + Editing: CapCut, JianYing\n2.2 Improving Agent User Experience Personalized Memory \u0026amp; Style Customization User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments. Example: Typeface Brand Kit\nRewind \u0026amp; Edit User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality. Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)\nChoose from Variations User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience. Example: Typeface Image Generator (also supports favoriting)\nWorkflows, Not Skills User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation. Example: Typeface Workflow (all capabilities presented at the appropriate stages)\nTypeface - Product Reference from Former Adobe CTO https://www.typeface.ai\nTypeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft\u0026rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)\n3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time 3.2 Feature Breakdown - Customized Content Generation for Brands Multiple Agent calls centered around the core document editing experience.\nWhen users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.\nFeatures: Brands When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:\nImage Styles: Users can upload existing images for subsequent generation style adjustment. Color Palettes: Users can upload brand color palettes to standardize generated image colors. Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image. Projects When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:\nCreate a new image Create a product shot Generate text Create from template Additionally, users can select Refine to adjust generation language and tone (fixed options).\nCreate an Image After clicking Create an image, users enter the image editing page with six integrated functions on the left: \u0026ldquo;Add, select, extend, lighting, color, effects, adobe express.\u0026rdquo; Users can generate and adjust images directly and favorite preferred generations.\nCreate a Product Shot The difference from Create an image is that Product shot includes specific products, while image isn\u0026rsquo;t necessarily product-related.\nGenerate Text After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.\nTemplates Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.\nAudiences When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).\nIntegrations These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.\nhttps://www.typeface.ai/product/integrations\nMicrosoft Dynamics 365 Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.\nSalesforce Marketing Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.\nGoogle BigQuery Users can define audience segments with customer intelligence from BigQuery\u0026rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.\nGoogle Workspace Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.\nMicrosoft Teams Create content in Teams using Typeface\u0026rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.\nSummary Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.\n","permalink":"https://chenterry.com/projects/copilot/","summary":"\u003cp\u003eLeverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)\n\u003ca href=\"https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\"\u003ehttps://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCredits: TikTok Creative Team\u003c/p\u003e\n\u003ch1 id=\"building-agentic-workflows\"\u003eBuilding Agentic Workflows\u003c/h1\u003e\n\u003ch2 id=\"from-llms-to-agents\"\u003eFrom LLMs to Agents\u003c/h2\u003e\n\u003cp\u003eThe transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\u003c/p\u003e","title":"Symphony Assistant"},{"content":"Marrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\nMarket Analysis Problem Statement The current second-hand market in North America lacks efficiency, with transactions primarily occurring through fragmented social media group chats. As consumer purchasing habits become more rational, there is growing demand for second-hand transactions, but the current infrastructure does not support an effective market. The fragmentation of the market across multiple chat groups and platforms creates information asymmetry, where buyers cannot easily find available items and sellers struggle to reach interested buyers. Additionally, the cumbersome process of creating detailed listings discourages many potential sellers from participating in the market, limiting the overall volume of available goods. This inefficiency results in a significant number of usable items being discarded rather than resold, creating both economic waste and environmental impact.\nTarget Market The initial target market consists of Chinese international students in North American universities, beginning with Washington University in St.Louis, and then expanding to other colleges. This demographic was selected for several strategic reasons: they represent a cohesive cultural group with similar communication habits, they are geographically concentrated on campuses, and they can be easily verified through .edu email addresses to reduce platform abuse. This community also experiences regular high-volume second-hand transaction periods coinciding with academic calendars, particularly during move-out periods at semester ends. Following successful penetration of this initial market, expansion will target the broader university student population before eventually extending to the general American user base.\nKey Market Assumptions The business model is built on five critical assumptions that will be validated through initial market testing:\nThe potential supply of second-hand products significantly exceeds the current transaction volume, indicating an untapped market opportunity. The complexity of listing creation represents the primary barrier preventing potential sellers from participating in the market. A streamlined buying experience with easier product discovery will attract more buyers to the platform. The platform will experience network effects once it reaches a critical mass of listed items, drawing in additional buyers and sellers. The majority of second-hand products (excluding specialty categories like housing rentals and rideshares) are priced between $10-100, making them low-risk transactions. Product Vision and User Experience Marrket will innovate the second-hand marketplace experience by leveraging AI (image recognition, content generation) to dramatically simplify the listing process. The core value proposition centers on allowing sellers to create comprehensive, attractive listings with minimal effort - just a few photos and basic information. The AI system analyzes the images, generates detailed product descriptions, suggests appropriate categories, and recommends pricing based on market data. For buyers, the platform offers an intuitive, searchable interface organized by product categories, with advanced filtering capabilities to quickly find desired items. The unified marketplace creates transparency in pricing and availability that is absent in the current fragmented chat-based system.\nUser Flow Visualization ┌────────────────┐ ┌───────────────────┐ ┌─────────────────┐ ┌───────────────┐ │ │ │ │ │ │ │ │ │ Upload Photos │────▶│ AI Generates Draft│────▶│ Review \u0026amp; Publish│────▶│ Manage Listing│ │ │ │ Description │ │ │ │ │ └────────────────┘ └───────────────────┘ └─────────────────┘ └───────────────┘ │ │ │ │ │ ▼ │ ┌───────────────────┐ │ │ │ │ │ Transaction \u0026amp; Pay │ │ │ │ │ └───────────────────┘ │ ▲ ▼ │ ┌──────────────────┐ ┌───────────────────┐ ┌─────────────────┐ │ │ │ │ │ │ │ │ │ Browse Category │────▶│ View Listing │────▶│ Contact Seller │───────────┘ │ │ │ │ │ │ └──────────────────┘ └───────────────────┘ └─────────────────┘ Core Features AI-Powered Listing Generation The cornerstone of Marrrket\u0026rsquo;s platform is its innovative AI listing generation system. Sellers simply upload multiple photos of their item and provide minimal information such as the item name and general category. The AI system then analyzes the images to identify the product, its condition, key features, and appropriate categorization. It generates a comprehensive product description, suggests an appropriate price range based on market data for similar items, and creates a complete listing draft for the seller to review. This process transforms what is typically a 15-20 minute task into a 2-3 minute interaction, dramatically reducing the barrier to listing items for sale. The seller maintains full control to edit any aspect of the AI-generated content before publishing the listing.\nIntuitive Category System Marrrket organizes products into clearly defined categories that reflect the unique needs of the university student market. Primary categories include furniture, electronics, textbooks, household items, clothing, and special categories for housing rentals and rideshares. The categorization system is designed to balance simplicity with sufficient specificity to aid discovery. Each category features customized filters relevant to that product type - for example, electronics listings include filters for condition, brand, and age of the device, while textbook listings filter by course subject and edition. This tailored approach ensures buyers can quickly narrow their search to relevant items.\nStreamlined Payment and Verification To minimize transaction friction while ensuring security, Marrrket integrates with Zelle as its primary payment platform, leveraging its popularity among university students. For higher-value transactions, the platform offers an optional escrow service where the payment is held until the buyer confirms receipt of the item as described. User verification is handled through .edu email domain authentication, creating an additional layer of trust within the platform. The system also implements a reputation system where both buyers and sellers can rate their transaction experience, building credibility over time.\nTechnical Architecture Frontend Implementation The frontend architecture employs React with Ant Design components to create a responsive, mobile-friendly user interface. During the initial phase, the buyer interface may be partially implemented using Notion for rapid deployment, with plans to migrate fully to the custom React interface as the platform matures. The user interface prioritizes simplicity and visual browsing of items, with large product images and clear, concise information display. The design system maintains consistency across all interfaces while optimizing for both desktop and mobile browsing scenarios.\nBackend Systems The backend implementation uses Flask for API endpoints with initial data storage utilizing Notion\u0026rsquo;s built-in database capabilities for rapid development. As the platform scales, data will migrate to either SQL or MongoDB, with additional consideration for vector database implementation to enhance AI-powered search capabilities. The system architecture is designed with modularity in mind, allowing individual components to be upgraded or replaced as requirements evolve. Authentication, listing management, messaging, and transaction processing are implemented as separate services to maintain flexibility and scalability.\nAI Integration Architecture ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Image Analysis │────▶│ Context Generation │────▶│ Content Creation │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ │ │ │ ▼ ▼ ▼ ┌─────────────────┐ ┌─────────────────────┐ ┌───────────────────┐ │ │ │ │ │ │ │ Object Detection│ │ Market Analysis │ │Listing Template │ │ │ │ │ │ │ └─────────────────┘ └─────────────────────┘ └───────────────────┘ The AI system integrates multiple technologies to transform basic image and text inputs into comprehensive product listings. The process begins with image analysis using object detection to identify the product type, condition, and key features. This information feeds into a context generation phase where market data for similar items is analyzed to determine appropriate pricing and categorization. Finally, the content creation phase generates a structured description following optimized templates for each product category. The system continuously improves through feedback loops, where user edits to the generated content are used to refine future suggestions.\nRevenue Model In the initial phase, Marrrket will prioritize user acquisition and platform growth over immediate monetization. The long-term revenue strategy consists of two primary streams:\nListing Package Fees: Users can list their first 5 items for free, encouraging initial platform adoption. Beyond that, tiered packages are available - a Basic Package for 5-20 listings and a Power Package for 20-50 listings. To build trust and mitigate risk for sellers, listing fees are automatically refunded if items don\u0026rsquo;t sell, demonstrating the platform\u0026rsquo;s confidence in its ability to connect buyers and sellers effectively.\nTransaction Fees: For items priced above $200, a 5% commission is applied to the transaction. This approach ensures that the platform primarily monetizes higher-value transactions where the commission represents a reasonable cost relative to the total value, while keeping lower-value transactions (which constitute the majority of student exchanges) commission-free to encourage platform adoption.\nAs the platform matures, additional revenue opportunities may include premium listing features, promoted listings for greater visibility, and value-added services such as professional photography or pickup/delivery coordination.\nGo-to-Market Strategy Phase 1: St. Louis Campus Launch The initial launch will focus on St. Louis area universities, strategically timed for late April 2025 to coincide with the end-of-semester period when students are moving out of housing and seeking to sell unwanted items. This timing capitalizes on a natural high-volume period in the second-hand market. Marketing efforts will employ a multi-channel approach with carefully prioritized tactics:\nTraditional campus advertising through strategically placed posters in high-traffic areas like student unions, dormitories, and international student centers. Partnerships with Chinese student organizations to promote the platform through their established communication channels, including WeChat groups and official accounts. Targeted email marketing to student email lists, emphasizing the platform\u0026rsquo;s benefits for both buying and selling. Implementation of a referral program where existing users receive incentives for bringing new users to the platform. Selective digital display advertising on platforms frequently used by the target demographic. Content marketing through popular platforms like Xiaohongshu and Instagram, featuring success stories and platform benefits. Phase 2: Expansion Strategy Following successful implementation in the initial market, expansion will proceed methodically to additional campuses with significant international student populations. The strategy leverages network effects by expanding to geographically connected areas where students may already have connections to the initial user base. Marketing messages will evolve to emphasize proven success metrics from the initial market, such as average time to sell items and average savings for buyers compared to new purchases. As the platform grows beyond the Chinese international student community, marketing will emphasize the platform\u0026rsquo;s ease of use and enhanced features compared to general marketplaces, while maintaining the focus on university communities to preserve the trust and verification benefits of the .edu email system.\nRisk Assessment and Mitigation Strategies Several key risks have been identified that could impact the platform\u0026rsquo;s success, along with corresponding mitigation strategies:\nInsufficient Listing Volume: The platform requires a critical mass of listings to attract buyers. To mitigate this risk, pre-launch partnerships with student organizations will be established to seed initial inventory, and incentives will be offered for early sellers. Additionally, the team will consider listing items themselves if necessary to ensure adequate initial inventory.\nAI Quality Issues: If the AI-generated descriptions fail to meet quality standards, sellers may abandon the platform. To address this risk, the system will initially be more conservative in its suggestions, offering simpler descriptions that sellers can enhance rather than attempting complex descriptions that might contain errors. A continuous improvement process based on user edits will be implemented to refine the AI over time.\nTrust and Safety Concerns: Second-hand marketplaces can face issues with fraudulent listings or unsafe transactions. The .edu email verification provides a first layer of protection, which will be supplemented by clear community guidelines, a reporting system for problematic listings or users, and an escrow option for higher-value transactions.\nPayment Verification Challenges: Ensuring payments are properly made and verified is critical to platform trust. Beyond integrating with Zelle, the platform will implement a confirmation system where both parties must acknowledge the transaction is complete before it is finalized in the system.\nCompetitive Response: Existing platforms may attempt to replicate the AI-powered listing feature. To maintain competitive advantage, Marrrket will focus on building deep expertise in the specific needs of the university market segment and continuously enhancing the AI capabilities based on the growing dataset of student transactions.\nConclusion Marrrket represents a significant innovation in the second-hand marketplace sector by directly addressing the primary friction point in the market: the complexity of creating product listings. By leveraging artificial intelligence to dramatically simplify this process, the platform has the potential to unlock substantial untapped inventory in the university second-hand market. The strategic focus on Chinese international students as an initial target market provides a cohesive, geographically concentrated user base with consistent needs, allowing for efficient marketing and network growth. With successful execution of this strategy, Marrrket can establish itself as the preferred second-hand marketplace for university students before expanding to broader markets\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu.\n","permalink":"https://chenterry.com/archived/marrrket/","summary":"\u003cp\u003eMarrrket is an AI-powered second-hand marketplace platform targeting North American university students, initially focusing on Chinese international students. The platform aims to solve the inefficiency in the current second-hand market by simplifying the listing process through AI-generated product descriptions from images and minimal user input. By reducing friction in the listing process, Marrrket will increase the overall volume of second-hand items available in the market, creating a more efficient marketplace for both buyers and sellers. The platform\u0026rsquo;s innovation centers on using artificial intelligence to dramatically lower the barrier to entry for sellers, which is hypothesized to be the primary constraint on market growth.\u003c/p\u003e","title":"Marrrket: AI Listing Secondhand Marketplace"},{"content":"Multi-agent system for cross boarder e-commerce sales automation. Co-founder and head of product. https://cognogpt.com\nCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\nMarket Opportunity The target addressable market size is around 200K-250K, with the marketing being independent brands selling from China \u0026amp; Southeast Asia to the U.S. Our beachhead market is mid-sized Chinese brands selling consumer electronics and other slow-moving consumer goods to the U.S. with the annual recurring rate of more than 2 million USD, consisting around 10K potential end users. Some potential adjacent markets are China and South East Asia B2C E-commerce companies selling unbranded products to the US on their own platform that consolidates products from many factories.\nProducts \u0026amp; Services Cogno+ is an AI-driven interactive assistant designed to enhance e-commerce business growth. It has both personalized engagement and automated and optimized customer service. While the current chatbots and web tools still require extensive workflow setup and are limited in their problem-solving abilities, Cogno has this multi-agent real-time plug-and-play system that offers easy setup, 24-hour support, and high domain knowledge for a better personalization experience that makes us differ from other products in the competition.\nBusiness \u0026amp; Sales Strategy Cogno+ provides mid-sized international e-commerce brands with automated sales support to help them increase conversions and upselling. We aim to explore beyond the current boundaries of human-ai interaction, developing fully scalable sales automation systems for better engagement between brands and customers around the world.\nIn terms of our business strategy, we will employ a B2B SaaS model, charging a tier-based subscription fee based on website traffic (given the cost of using language model APIs). Our business model allows service for a range of e-commerce brands and ensures that brands only pay for the value they are getting via Cogno (increased sales conversion and product upselling).\nFor our product roadmap, building on our existing multi-agent system, we aim to continue research and development of dynamic prompting to improve the coordination capabilities of our central logic agent to allow us to leverage more custom domain agents simultaneously. We are also working to improve user interactions with graphics user interfaces and web interaction as input to language model systems. With these future improvements, we believe we are at the cusp of reinventing how online shopping interactions between brands and customers are made.\nWe will initiate customer acquisition with our beachhead market of midsized Chinese e-commerce brands selling consumer electronics and other slow-moving consumer goods to the US with an annual recurring revenue of more than 2 million, and eventually expand to the larger domestic and international e-commerce landscape. We will target customers through distribution channels such as Shopify, WooCommerce, and WordPress as well as direct sales and expo exhibitions with Chinese online retailers. By targeting industry leaders and influencers, we aim to gain a firm grounding in industries such as consumer electronics and other slow-moving consumer goods verticals. Finally, we will conduct SEO and social media marketing to expand our market beyond the Asia-Pacific to the global e-commerce market, eventually bringing Cogno+ to brands and customers in all of our major e-commerce markets.\nCredits: Jack Qidiao, Yuri Yin, Dijkstra Liu, Madison Bratley, Ryan Philips, Eric Chen, Echo Zhou, Esther Zhou, Jingfan Yao, Steven Wang, Lu Zhou, Wendy Hu, Wendy Huang, Letty Lin, Jianpeng Su, Professor Eduardo Acuna, Professor Karen Gordon, Jeff Smith, Toni Milushev, The Farley Center, The Garage, as well as everyone who provided mentorship and advice along the way.\n","permalink":"https://chenterry.com/archived/cogno/","summary":"\u003cp\u003eMulti-agent system for cross boarder e-commerce sales automation. Co-founder and head of product.\n\u003ca href=\"https://cognogpt.com\"\u003ehttps://cognogpt.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCogno+ is dedicated to revolutionizing global e-commerce by empowering brands with AI-driven assistance that offers seamless, personalized customer experiences. Our mission is to serve as the digital bridge between brands and customers, enhancing interactions and transactions across international markets, and allowing the brand to increase conversion and upsell while reducing time and money spent on manual customer service.\u003c/p\u003e","title":"Cogno: Multi-agent AI for sales automation"},{"content":"AI Homework helper with advanced reasoning and visualization for all school subjects. (Worked on LLM Reasoning) https://www.gauthmath.com/\nCredits: Lexi Ling, Gauth Team\n","permalink":"https://chenterry.com/projects/gauth/","summary":"\u003cp\u003eAI Homework helper with advanced reasoning and visualization for all school subjects. (Worked on LLM Reasoning)\n\u003ca href=\"https://www.gauthmath.com/\"\u003ehttps://www.gauthmath.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCredits: Lexi Ling, Gauth Team\u003c/p\u003e","title":"Gauth AI"},{"content":"Developing capable multi-agent systems for complex reasoning and human-AI collaboration Single-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\n","permalink":"https://chenterry.com/main-themes/multi-agent-systems/","summary":"\u003ch2 id=\"developing-capable-multi-agent-systems-for-complex-reasoning-and-human-ai-collaboration\"\u003eDeveloping capable multi-agent systems for complex reasoning and human-AI collaboration\u003c/h2\u003e\n\u003cp\u003eSingle-agent AI systems have limitations in handling complex tasks that require diverse perspectives and specialized knowledge. Multi-agent architectures can enable more sophisticated reasoning, problem-solving, and collaboration capabilities.\u003c/p\u003e","title":"Multi-agent LLM Systems"},{"content":"Extracting meaningful insights from unstructured multi-modal content Most times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\n","permalink":"https://chenterry.com/main-themes/data-insights/","summary":"\u003ch2 id=\"extracting-meaningful-insights-from-unstructured-multi-modal-content\"\u003eExtracting meaningful insights from unstructured multi-modal content\u003c/h2\u003e\n\u003cp\u003eMost times, the challenge isn\u0026rsquo;t collecting information but extracting value from it. By developing systems that can analyze unstructured multi-modal content (text, images, video, audio), we can extract actionable insights.\u003c/p\u003e","title":"Data Insights"},{"content":"A Human-Inspired Solution to LLM Memory Enhancement Authors: Terry Chen, Kaiwen Che, Matthew Song\nAbstract Despite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\nIn comparison to traditional retrieval-augmented generation (RAG) techniques that store verbatim conversation segments, our system employs strategic memory consolidation procedures, abstracting key information into structured forms. Performance testing on the GoodAI Long-Term Memory benchmark demonstrates significant improvements in performance, with our memory-augmented GPT-4o achieving scores of up to 6.9/11 over the baseline 4.6/11. Additional testing across multi-agent domains demonstrates enhanced persistence and updating capacity of information.\nIntroduction State-of-the-art large language models (LLMs) possess remarkable natural language comprehension and generation. However, their architecture imposes tight constraints on memory retention and contextual comprehension during long-term interaction. Most existing LLMs operate within fixed context windows, typically ranging from 32,000 to 128,000 tokens, which impose inherent constraints on long-term conversation and complex reasoning tasks that span multiple turns.\nThe Baddeley and Hitch (1974, 2000) model of working memory provides a robust theoretical account of human information processing. The model presents memory as a multi-component system with central executive control of information flow, an episodic buffer of assembling memories into temporary experiences, a phonological loop of controlling verbal content, and a visuospatial sketchpad of controlling visual and spatial information.\nCurrent approaches to increasing LLM memory capacity heavily rely on embedding-based retrieval-augmented generation (RAG). While the approach can deliver rapid access to previous data, it suffers greatly from issues like vector explosion, the unsustainable proliferation of embeddings as conversation history grows, lack of semantic structure in stored shreds, and difficulties in maintaining relations among relevant facts.\nThis work introduces a novel biomimetic approach to LLM memory extension that more accurately models the cognitive architecture of humans, with a three-tiered memory system distinguishing between immediate context, episodic memories, and semantic facts.\nSystem Architecture Our memory improvement system utilizes a three-layer architecture inspired by human cognition:\nWorking Memory (LLM Context Window) We divide the context window into two distinct segments:\nMulti-Round Conversation History (MCH): Stores current conversation context, maintaining flow up to a defined token limit. Retrieval Memory Buffer (RMB): Provides dedicated space for injecting remembered memories from long-term storage, maintaining a balance of short-term and long-term remembered data. Long-Term Memory Store Implemented as a vector database storing two forms of memory:\nSemantic Memory: Stores factual knowledge gained from conversations as subject-predicate-object triples with optional contextual referencing. Episodic Memory: Stores complete interaction episodes by a formal schema with contextual initialization, reasoning operations, actions taken, and outcomes observed. Memory Processes There are specialized components for:\nMemory Consolidation: Operations for capturing and formalizing memories when conversation history reaches token thresholds. Retrieval Mechanisms: Multi-step operations that determine context adequacy before retrieving from external memory stores. Memory Schema Implementation Semantic Memory Triple We implemented the semantic memory schema as a structured class:\nclass SematicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Store all new facts, preferences, and relationships as triples.\u0026#34;\u0026#34;\u0026#34; subject: str predicate: str object: str context: str | None = None Episodic Memory Schema Our episodic memory implementation stores experiential information with temporal context:\nclass EpisodicMemory(BaseModel): \u0026#34;\u0026#34;\u0026#34;Write the episode from the perspective of the agent within it.\u0026#34;\u0026#34;\u0026#34; observation: str = Field(..., description=\u0026#34;The context and setup - what happened\u0026#34;) thoughts: str = Field( ..., description=\u0026#34;Internal reasoning process and observations of the agent\u0026#34; ) action: str = Field( ..., description=\u0026#34;What was done, how, and in what format.\u0026#34; ) result: str = Field( ..., description=\u0026#34;Outcome and retrospective.\u0026#34; ) Memory Consolidation Process The foundation of our strategy lies in sophisticated memory consolidation mechanisms that convert raw conversational information into structured memory representations:\nSemantic Memory Extraction Our semantic memory schema makes use of subject-predicate-object triples that eliminate episodic detail without sacrificing core relationships. Implementation follows several guiding principles:\nPrioritization of high-frequency accessed information Merging of redundant knowledge into a single representation Upgrading existing triples whenever new contradicting data exist Adding contextual linking to render situationally responsive retrieval Episodic Memory Extraction Episodic memory stores full interactions in an ordered schema consisting of four main components:\nObservation: Stores contextual setup and what transpired Thoughts: Stores internal reasoning processes and deliberations Action: Stores particular interventions and methodologies used Result: Stores outcome and subsequent analysis Evaluation Results GoodAI LTM benchmark results indicated radically better performance with our memory augmentation approach:\nConfiguration Score Performance Baseline GPT-4o 4.6/11 41.8% GPT-4o + Semantic Memory 6.8/11 61.8% GPT-4o + Episodic Memory 6.9/11 62.7% GPT-4o + Semantic \u0026amp; Episodic Memory 6.0/11 54.5% These results reflect a general 20-percentage-point improvement in memory performance by our augmentation method. The differential performance aligns with the corresponding functional roles these types of memory serve in human cognition, wherein semantic memory enables fact recall and episodic memory enables experiential reasoning.\nDiscussion and Future Work Our research provides empirical evidence for cognitive-inspired LLM memory enhancement methods. The witnessed performance improvements with three-tier memory architecture show that human memory systems offer valuable design concepts for overcoming inherent limitations in current AI designs.\nThe unexpected finding was the slightly worse performance of integrated memory systems compared to single implementations. This suggests complex interaction effects, which may mirror interference phenomena observed in human memory systems, where various forms of memory sometimes vie for mental resources.\nFuture research directions include:\nMulti-agent Memory Dynamics: How memory transfers between agents and how social dynamics influence memory consolidation Advanced Retrieval Strategies: Exploring spatially organized memory architectures and hierarchical memory organization Optimization of Consolidation Thresholds: Investigating dynamic thresholds that adapt based on conversation characteristics Conclusion This paper presents a novel biomimetic approach to enhancing LLM memory that addresses intrinsic limitations in current architectures. By embracing a three-level memory structure inspired by human cognitive processes, we demonstrate significant improvements in information retention, update, and context recall.\nAs LLMs advance towards more general intelligence capabilities, structured memory systems will play a larger role in enabling coherent long-term interactions, homogeneous knowledge states, and contextually appropriate information access. Our research contributes both pragmatic approaches for deploying this aspect of AI progress and theoretical frameworks to continue advancing this critical component of AI work.\n","permalink":"https://chenterry.com/archived/human-inspired-llm-memory/","summary":"\u003ch1 id=\"a-human-inspired-solution-to-llm-memory-enhancement\"\u003eA Human-Inspired Solution to LLM Memory Enhancement\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: Terry Chen, Kaiwen Che, Matthew Song\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eDespite advances in large language model (LLM) capability, their fundamental limitation of not being able to store context over long-lived interactions persists. In this paper, a novel human-inspired three-tiered memory architecture is presented that addresses these limitations through biomimetic design principles rooted in cognitive science. Our approach aligns the human working memory with the LLM context window, episodic memory with vector stores of experience-based knowledge, and semantic memory with structured knowledge triplets.\u003c/p\u003e","title":"LLM Memory Consolidation and Augmentation"},{"content":"Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee\nAbstract Effective coaching in project-based learning environments is critical for developing students\u0026rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.\nIntroduction Training college students to tackle complex, open-ended innovation work requires developing strong regulation skills for self-directed work. Coaches guide the development of these regulation skills, helping students develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach desired outcomes. However, coaches face significant challenges in providing personalized guidance to multiple student teams.\nExisting AI-based project management tools help track tasks but fail to capture nuanced ways students approach their work. Large Language Models (LLMs) show promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.\nTo address these issues, we propose utilizing LLMs to develop and integrate three key technical innovations:\nPeer Connections - Facilitate connections between students with similar challenges Coaching Reflections - Help coaches analyze patterns and improve their practice through identifying regulation gaps Practice Suggestions - Adapt similar cases to new situations The Regulation Skills Codebook Our system is built around a novel codebook consisting of regulation gap definitions and examples gathered across learning science literature. The codebook categorizes student regulation gaps in a tiered approach:\nTier 1 Categories: Cognitive - Skills for approaching problems with unknown answers Metacognitive - Skills in planning, help-seeking, collaboration, and reflection Emotional - Dispositions toward self and learning that affect motivation Tier 2 Categories (Examples): Representing problem and solution spaces Assessing risks Critical thinking and argumentation Forming feasible plans Planning effective iterations Fears and anxieties Embracing challenges and learning System Architecture Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach:\nStudent regulation notes are pre-processed with metadata on tier 1 and tier 2 regulation gaps Notes are encoded into text embeddings A vector database retrieves the most similar historical cases An LLM (Deepseek) generates structured responses including: Diagnosis of potential regulation gaps Practice suggestions targeted to these gaps References to similar historical cases This grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.\nSimilarity Methods for Regulation Gap Analysis We developed and tested three approaches to match students with similar regulation challenges:\nBaseline Semantic Approach - Uses vector embeddings to find similar cases based on textual similarity Weighted Semantic Similarity - Separates and weights regulation gap description (0.7) from contextual information (0.3) Hybrid LLM-Codebook Approach - Combines semantic matching with LLM-generated metadata using our regulation codebook The hybrid approach proved most effective, assigning the highest weight (0.5) to tier 2 categories and lower weights to tier 1 categories (0.1) and text content (0.2 each for gap text and context).\nEvaluation and Results We evaluated each model against the same three notes, analyzing the top 5 returned similar notes. The semantic matching performed well when addressing cognitive and metacognitive gaps with repetitive terminology but struggled with emotional regulation gaps. The LLM-codebook approach showed promise in accurately identifying regulation gaps but was computationally intensive. The hybrid model consistently and efficiently identified notes with the same regulation gap while maintaining contextual similarity.\nDiscussion and Future Work Our system effectively bridges the gap between human expertise and AI capabilities in coaching contexts. Key takeaways include:\nHybrid AI-Driven Case Retrieval - Combining LLM-driven metadata tagging with traditional semantic matching enables precision in retrieving relevant coaching cases Structured Codebooks for Domain-Specific AI - Our tiered classification system grounds LLM-based reasoning in expert-validated pedagogical frameworks Future work will focus on:\nImproving clarity of writing in notes Collecting more data through alternative sources Developing sub-categorized codebooks with specific examples and reasoning chains Exploring more sophisticated reasoning methods like external knowledge bases or memory systems This research contributes to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.\n","permalink":"https://chenterry.com/projects/improving-scaling-llms-coaching/","summary":"\u003ch1 id=\"situated-practice-systems-improving-and-scaling-coaching-through-llms\"\u003eSituated Practice Systems: Improving and Scaling Coaching through LLMs\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: Terry Chen, Allyson Lee\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eEffective coaching in project-based learning environments is critical for developing students\u0026rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.\u003c/p\u003e","title":"Improving \u0026 Scaling LLMs for Coaching"},{"content":"My name is Terry (陈麟昊). I currently live in Evanston, studying Computer Science at Northwestern. I enjoy observing and learning about the strange quirks that make us human. I like building things, with focuses in content understanding \u0026amp; generation with agentic workflows. Previously, I have worked in product and engineering at TikTok and ByteDance. In my free time, I play tennis (occassionally soccer?) and go on random walks along the lakefill. I\u0026rsquo;m still figuring out life, so I\u0026rsquo;ll spare you the formal introduction. If your in town, let\u0026rsquo;s grab coffee, if not, zoom works just as well.\nContact Feel free to reach out to me at terrychen2026@u.northwestern.edu or connect with me on LinkedIn.\n","permalink":"https://chenterry.com/about/","summary":"About Terry Chen","title":"About Me"}]