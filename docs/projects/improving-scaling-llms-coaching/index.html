<!doctype html><html lang=en dir=auto><head><meta name=description content="Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning …"><link rel=canonical href=https://chenterry.com/projects/improving-scaling-llms-coaching/><meta property="og:title" content="Improving & Scaling LLMs for Coaching | Terry Chen"><meta property="og:description" content="Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning …"><meta property="og:type" content="article"><meta property="og:url" content="https://chenterry.com/projects/improving-scaling-llms-coaching/"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta property="og:site_name" content="Terry Chen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@YourTwitterHandle"><meta name=twitter:title content="Improving & Scaling LLMs for Coaching | Terry Chen"><meta name=twitter:description content="Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning …"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Improving & Scaling LLMs for Coaching | Terry Chen</title>
<meta name=keywords content="Technology,Research"><meta name=author content="Terry Chen"><link rel=canonical href=https://chenterry.com/projects/improving-scaling-llms-coaching/><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M6GS8Q702L")}</script><meta property="og:url" content="https://chenterry.com/projects/improving-scaling-llms-coaching/"><meta property="og:site_name" content="Terry Chen"><meta property="og:title" content="Improving & Scaling LLMs for Coaching"><meta property="og:description" content="Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-06-15T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-15T00:00:00+00:00"><meta property="article:tag" content="Technology"><meta property="article:tag" content="Research"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta name=twitter:title content="Improving & Scaling LLMs for Coaching"><meta name=twitter:description content="Situated Practice Systems: Improving and Scaling Coaching through LLMs
Authors: Terry Chen, Allyson Lee
Abstract
Effective coaching in project-based learning environments is critical for developing students&rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://chenterry.com/projects/"},{"@type":"ListItem","position":2,"name":"Improving \u0026 Scaling LLMs for Coaching","item":"https://chenterry.com/projects/improving-scaling-llms-coaching/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Improving \u0026 Scaling LLMs for Coaching","name":"Improving \u0026 Scaling LLMs for Coaching","description":"Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee\nAbstract Effective coaching in project-based learning environments is critical for developing students\u0026rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.\n","keywords":["Technology","Research"],"articleBody":"Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee\nAbstract Effective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.\nIntroduction Training college students to tackle complex, open-ended innovation work requires developing strong regulation skills for self-directed work. Coaches guide the development of these regulation skills, helping students develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach desired outcomes. However, coaches face significant challenges in providing personalized guidance to multiple student teams.\nExisting AI-based project management tools help track tasks but fail to capture nuanced ways students approach their work. Large Language Models (LLMs) show promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.\nTo address these issues, we propose utilizing LLMs to develop and integrate three key technical innovations:\nPeer Connections - Facilitate connections between students with similar challenges Coaching Reflections - Help coaches analyze patterns and improve their practice through identifying regulation gaps Practice Suggestions - Adapt similar cases to new situations The Regulation Skills Codebook Our system is built around a novel codebook consisting of regulation gap definitions and examples gathered across learning science literature. The codebook categorizes student regulation gaps in a tiered approach:\nTier 1 Categories: Cognitive - Skills for approaching problems with unknown answers Metacognitive - Skills in planning, help-seeking, collaboration, and reflection Emotional - Dispositions toward self and learning that affect motivation Tier 2 Categories (Examples): Representing problem and solution spaces Assessing risks Critical thinking and argumentation Forming feasible plans Planning effective iterations Fears and anxieties Embracing challenges and learning System Architecture Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach:\nStudent regulation notes are pre-processed with metadata on tier 1 and tier 2 regulation gaps Notes are encoded into text embeddings A vector database retrieves the most similar historical cases An LLM (Deepseek) generates structured responses including: Diagnosis of potential regulation gaps Practice suggestions targeted to these gaps References to similar historical cases This grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.\nSimilarity Methods for Regulation Gap Analysis We developed and tested three approaches to match students with similar regulation challenges:\nBaseline Semantic Approach - Uses vector embeddings to find similar cases based on textual similarity Weighted Semantic Similarity - Separates and weights regulation gap description (0.7) from contextual information (0.3) Hybrid LLM-Codebook Approach - Combines semantic matching with LLM-generated metadata using our regulation codebook The hybrid approach proved most effective, assigning the highest weight (0.5) to tier 2 categories and lower weights to tier 1 categories (0.1) and text content (0.2 each for gap text and context).\nEvaluation and Results We evaluated each model against the same three notes, analyzing the top 5 returned similar notes. The semantic matching performed well when addressing cognitive and metacognitive gaps with repetitive terminology but struggled with emotional regulation gaps. The LLM-codebook approach showed promise in accurately identifying regulation gaps but was computationally intensive. The hybrid model consistently and efficiently identified notes with the same regulation gap while maintaining contextual similarity.\nDiscussion and Future Work Our system effectively bridges the gap between human expertise and AI capabilities in coaching contexts. Key takeaways include:\nHybrid AI-Driven Case Retrieval - Combining LLM-driven metadata tagging with traditional semantic matching enables precision in retrieving relevant coaching cases Structured Codebooks for Domain-Specific AI - Our tiered classification system grounds LLM-based reasoning in expert-validated pedagogical frameworks Future work will focus on:\nImproving clarity of writing in notes Collecting more data through alternative sources Developing sub-categorized codebooks with specific examples and reasoning chains Exploring more sophisticated reasoning methods like external knowledge bases or memory systems This research contributes to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.\n","wordCount":"724","inLanguage":"en","image":"https://chenterry.com/images/profile.jpg","datePublished":"2023-06-15T00:00:00Z","dateModified":"2023-06-15T00:00:00Z","author":{"@type":"Person","name":"Terry Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://chenterry.com/projects/improving-scaling-llms-coaching/"},"publisher":{"@type":"Organization","name":"Terry Chen","logo":{"@type":"ImageObject","url":"https://chenterry.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chenterry.com/ accesskey=h title="Terry Chen (Alt + H)">Terry Chen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/posts/ title=Posts><span>Posts</span></a></li><li><a href=/projects/ title=Projects><span class=active>Projects</span></a></li><li><a href=/archived/ title=Archived><span>Archived</span></a></li><li><a href=/search/ title=Search accesskey=/><span>Search</span></a></li><li><a href=/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Improving & Scaling LLMs for Coaching</h1><div class=post-meta>Date: <span title='2023-06-15 00:00:00 +0000 UTC'>June 15, 2023</span> | Author: Terry Chen</div></header><div class=post-content><h1 id=situated-practice-systems-improving-and-scaling-coaching-through-llms>Situated Practice Systems: Improving and Scaling Coaching through LLMs<a hidden class=anchor aria-hidden=true href=#situated-practice-systems-improving-and-scaling-coaching-through-llms>#</a></h1><p><strong>Authors: Terry Chen, Allyson Lee</strong></p><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>Effective coaching in project-based learning environments is critical for developing students&rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Training college students to tackle complex, open-ended innovation work requires developing strong regulation skills for self-directed work. Coaches guide the development of these regulation skills, helping students develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach desired outcomes. However, coaches face significant challenges in providing personalized guidance to multiple student teams.</p><p>Existing AI-based project management tools help track tasks but fail to capture nuanced ways students approach their work. Large Language Models (LLMs) show promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.</p><p>To address these issues, we propose utilizing LLMs to develop and integrate three key technical innovations:</p><ul><li><strong>Peer Connections</strong> - Facilitate connections between students with similar challenges</li><li><strong>Coaching Reflections</strong> - Help coaches analyze patterns and improve their practice through identifying regulation gaps</li><li><strong>Practice Suggestions</strong> - Adapt similar cases to new situations</li></ul><h2 id=the-regulation-skills-codebook>The Regulation Skills Codebook<a hidden class=anchor aria-hidden=true href=#the-regulation-skills-codebook>#</a></h2><p>Our system is built around a novel codebook consisting of regulation gap definitions and examples gathered across learning science literature. The codebook categorizes student regulation gaps in a tiered approach:</p><h3 id=tier-1-categories>Tier 1 Categories:<a hidden class=anchor aria-hidden=true href=#tier-1-categories>#</a></h3><ol><li><strong>Cognitive</strong> - Skills for approaching problems with unknown answers</li><li><strong>Metacognitive</strong> - Skills in planning, help-seeking, collaboration, and reflection</li><li><strong>Emotional</strong> - Dispositions toward self and learning that affect motivation</li></ol><h3 id=tier-2-categories-examples>Tier 2 Categories (Examples):<a hidden class=anchor aria-hidden=true href=#tier-2-categories-examples>#</a></h3><ul><li>Representing problem and solution spaces</li><li>Assessing risks</li><li>Critical thinking and argumentation</li><li>Forming feasible plans</li><li>Planning effective iterations</li><li>Fears and anxieties</li><li>Embracing challenges and learning</li></ul><h2 id=system-architecture>System Architecture<a hidden class=anchor aria-hidden=true href=#system-architecture>#</a></h2><p>Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach:</p><ol><li>Student regulation notes are pre-processed with metadata on tier 1 and tier 2 regulation gaps</li><li>Notes are encoded into text embeddings</li><li>A vector database retrieves the most similar historical cases</li><li>An LLM (Deepseek) generates structured responses including:<ul><li>Diagnosis of potential regulation gaps</li><li>Practice suggestions targeted to these gaps</li><li>References to similar historical cases</li></ul></li></ol><p>This grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.</p><h2 id=similarity-methods-for-regulation-gap-analysis>Similarity Methods for Regulation Gap Analysis<a hidden class=anchor aria-hidden=true href=#similarity-methods-for-regulation-gap-analysis>#</a></h2><p>We developed and tested three approaches to match students with similar regulation challenges:</p><ol><li><strong>Baseline Semantic Approach</strong> - Uses vector embeddings to find similar cases based on textual similarity</li><li><strong>Weighted Semantic Similarity</strong> - Separates and weights regulation gap description (0.7) from contextual information (0.3)</li><li><strong>Hybrid LLM-Codebook Approach</strong> - Combines semantic matching with LLM-generated metadata using our regulation codebook</li></ol><p>The hybrid approach proved most effective, assigning the highest weight (0.5) to tier 2 categories and lower weights to tier 1 categories (0.1) and text content (0.2 each for gap text and context).</p><h2 id=evaluation-and-results>Evaluation and Results<a hidden class=anchor aria-hidden=true href=#evaluation-and-results>#</a></h2><p>We evaluated each model against the same three notes, analyzing the top 5 returned similar notes. The semantic matching performed well when addressing cognitive and metacognitive gaps with repetitive terminology but struggled with emotional regulation gaps. The LLM-codebook approach showed promise in accurately identifying regulation gaps but was computationally intensive. The hybrid model consistently and efficiently identified notes with the same regulation gap while maintaining contextual similarity.</p><h2 id=discussion-and-future-work>Discussion and Future Work<a hidden class=anchor aria-hidden=true href=#discussion-and-future-work>#</a></h2><p>Our system effectively bridges the gap between human expertise and AI capabilities in coaching contexts. Key takeaways include:</p><ol><li><strong>Hybrid AI-Driven Case Retrieval</strong> - Combining LLM-driven metadata tagging with traditional semantic matching enables precision in retrieving relevant coaching cases</li><li><strong>Structured Codebooks for Domain-Specific AI</strong> - Our tiered classification system grounds LLM-based reasoning in expert-validated pedagogical frameworks</li></ol><p>Future work will focus on:</p><ul><li>Improving clarity of writing in notes</li><li>Collecting more data through alternative sources</li><li>Developing sub-categorized codebooks with specific examples and reasoning chains</li><li>Exploring more sophisticated reasoning methods like external knowledge bases or memory systems</li></ul><p>This research contributes to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://chenterry.com/tags/technology/>Technology</a></li><li><a href=https://chenterry.com/tags/research/>Research</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://chenterry.com/>Terry Chen</a></span></footer></body></html>