<!doctype html><html lang=en dir=auto><head><meta name=description content="Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) …"><link rel=canonical href=https://chenterry.com/projects/copilot/><meta property="og:title" content="Symphony Assistant | Terry Chen"><meta property="og:description" content="Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) …"><meta property="og:type" content="article"><meta property="og:url" content="https://chenterry.com/projects/copilot/"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta property="og:site_name" content="Terry Chen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@YourTwitterHandle"><meta name=twitter:title content="Symphony Assistant | Terry Chen"><meta name=twitter:description content="Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) …"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Symphony Assistant | Terry Chen</title>
<meta name=keywords content="Technology,Product"><meta name=author content="Terry Chen"><link rel=canonical href=https://chenterry.com/projects/copilot/><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M6GS8Q702L")}</script><meta property="og:url" content="https://chenterry.com/projects/copilot/"><meta property="og:site_name" content="Terry Chen"><meta property="og:title" content="Symphony Assistant"><meta property="og:description" content="Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc
Credits: TikTok Creative Team
Building Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren’t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-05-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-20T00:00:00+00:00"><meta property="article:tag" content="Technology"><meta property="article:tag" content="Product"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta name=twitter:title content="Symphony Assistant"><meta name=twitter:description content="Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)
https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc
Credits: TikTok Creative Team
Building Agentic Workflows
From LLMs to Agents
The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren&rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://chenterry.com/projects/"},{"@type":"ListItem","position":2,"name":"Symphony Assistant","item":"https://chenterry.com/projects/copilot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Symphony Assistant","name":"Symphony Assistant","description":"Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026amp;deviceType=pc\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren\u0026rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\n","keywords":["Technology","Product"],"articleBody":"Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026deviceType=pc\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren’t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\nFrom Google Next to Baidu Create Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.\nFrom a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.\nFrom a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.\nLLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.\nWe should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?\n1.1 Consensus: Task Specific, MoE, Agents, Routing Content that has reached consensus:\nMost companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).\nThe MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.\nThe core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.\nImplementing an MoE architecture typically requires the following steps:\nDefine expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.\nTrain expert networks: Use labeled training data to train each expert network to obtain weights and parameters.\nAllocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.\nSummarize results: Weight and sum the output results of each expert network to get the final output.\nTrain the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.\nLonger Context Window -\u003e LLM Routing At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.\nAI Routing Uses Writesonic (https://writesonic.com) uses GPT Router for LLM Routing during AI Model Selection.\nGPT Router (https://github.com/Writesonic/GPTRouter) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.\nfrom gpt_router.client import GPTRouterClient from gpt_router.models import ModelGenerationRequest, GenerationParams from gpt_router.enums import ModelsEnum, ProvidersEnum client = GPTRouterClient(base_url='your_base_url', api_key='your_api_key') messages = [ {\"role\": \"user\", \"content\": \"Write me a short poem\"}, ] prompt_params = GenerationParams(messages=messages) claude2_request = ModelGenerationRequest( model_name=ModelsEnum.CLAUDE_INSTANT_12, provider_name=ProvidersEnum.ANTHROPIC.value, order=1, prompt_params=prompt_params, ) response = client.generate(ordered_generation_requests=[claude2_request]) print(response.choices[0].text) 1.2 Non-Consensus: Scenarios, Market, Differentiation Content that is still not determined:\nWhat constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge\nMany companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures\nHow Agents Can Help Creators Achieve 10x Efficiency 2.1 Agent Application Cases AutoGPT AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters. https://github.com/Significant-Gravitas/AutoGPT\nGPT Researcher A GPT-based autonomous agent that conducts comprehensive online research on any given topic. https://github.com/assafelovic/gpt-researcher\nThe advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.\nWorkflow Building - Video Creation Example Concept Design: Midjourney Script + Storyboard: ChatGPT AI Image Generation: Midjourney, Stable Diffusion, D3 AI Video: Runway, Pika, Pixverse, Morph Studio Dialogue + Narration: Eleven Labs, Ruisheng Sound Effects + Music: SUNO, UDIO, AUDIOGEN Video Enhancement: Topaz Video Subtitles + Editing: CapCut, JianYing\n2.2 Improving Agent User Experience Personalized Memory \u0026 Style Customization User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments. Example: Typeface Brand Kit\nRewind \u0026 Edit User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality. Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)\nChoose from Variations User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience. Example: Typeface Image Generator (also supports favoriting)\nWorkflows, Not Skills User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation. Example: Typeface Workflow (all capabilities presented at the appropriate stages)\nTypeface - Product Reference from Former Adobe CTO https://www.typeface.ai\nTypeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft’s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)\n3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time 3.2 Feature Breakdown - Customized Content Generation for Brands Multiple Agent calls centered around the core document editing experience.\nWhen users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.\nFeatures: Brands When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:\nImage Styles: Users can upload existing images for subsequent generation style adjustment. Color Palettes: Users can upload brand color palettes to standardize generated image colors. Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image. Projects When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:\nCreate a new image Create a product shot Generate text Create from template Additionally, users can select Refine to adjust generation language and tone (fixed options).\nCreate an Image After clicking Create an image, users enter the image editing page with six integrated functions on the left: “Add, select, extend, lighting, color, effects, adobe express.” Users can generate and adjust images directly and favorite preferred generations.\nCreate a Product Shot The difference from Create an image is that Product shot includes specific products, while image isn’t necessarily product-related.\nGenerate Text After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.\nTemplates Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.\nAudiences When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).\nIntegrations These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.\nhttps://www.typeface.ai/product/integrations\nMicrosoft Dynamics 365 Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.\nSalesforce Marketing Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.\nGoogle BigQuery Users can define audience segments with customer intelligence from BigQuery’s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.\nGoogle Workspace Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.\nMicrosoft Teams Create content in Teams using Typeface’s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.\nSummary Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.\n","wordCount":"1953","inLanguage":"en","image":"https://chenterry.com/images/profile.jpg","datePublished":"2024-05-20T00:00:00Z","dateModified":"2024-05-20T00:00:00Z","author":{"@type":"Person","name":"Terry Chen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://chenterry.com/projects/copilot/"},"publisher":{"@type":"Organization","name":"Terry Chen","logo":{"@type":"ImageObject","url":"https://chenterry.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chenterry.com/ accesskey=h title="Terry Chen (Alt + H)">Terry Chen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/posts/ title=Posts><span>Posts</span></a></li><li><a href=/projects/ title=Projects><span class=active>Projects</span></a></li><li><a href=/archived/ title=Archived><span>Archived</span></a></li><li><a href=/search/ title=Search accesskey=/><span>Search</span></a></li><li><a href=/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Symphony Assistant</h1><div class=post-meta>Date: <span title='2024-05-20 00:00:00 +0000 UTC'>May 20, 2024</span> | Author: Terry Chen</div></header><div class=post-content><p>Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)
<a href="https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc">https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc</a></p><p>Credits: TikTok Creative Team</p><h1 id=building-agentic-workflows>Building Agentic Workflows<a hidden class=anchor aria-hidden=true href=#building-agentic-workflows>#</a></h1><h2 id=from-llms-to-agents>From LLMs to Agents<a hidden class=anchor aria-hidden=true href=#from-llms-to-agents>#</a></h2><p>The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren&rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.</p><h2 id=from-google-next-to-baidu-create>From Google Next to Baidu Create<a hidden class=anchor aria-hidden=true href=#from-google-next-to-baidu-create>#</a></h2><p>Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.</p><p>From a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.</p><p>From a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.</p><p>LLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.</p><p>We should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?</p><h2 id=11-consensus-task-specific-moe-agents-routing>1.1 Consensus: Task Specific, MoE, Agents, Routing<a hidden class=anchor aria-hidden=true href=#11-consensus-task-specific-moe-agents-routing>#</a></h2><p>Content that has reached consensus:</p><p>Most companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).</p><p>The MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.</p><p>The core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.</p><p>Implementing an MoE architecture typically requires the following steps:</p><ol><li><p>Define expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.</p></li><li><p>Train expert networks: Use labeled training data to train each expert network to obtain weights and parameters.</p></li><li><p>Allocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.</p></li><li><p>Summarize results: Weight and sum the output results of each expert network to get the final output.</p></li><li><p>Train the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.</p></li></ol><h3 id=longer-context-window---llm-routing>Longer Context Window -> LLM Routing<a hidden class=anchor aria-hidden=true href=#longer-context-window---llm-routing>#</a></h3><p>At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.</p><h3 id=ai-routing-uses>AI Routing Uses<a hidden class=anchor aria-hidden=true href=#ai-routing-uses>#</a></h3><p>Writesonic (<a href=https://writesonic.com>https://writesonic.com</a>) uses GPT Router for LLM Routing during AI Model Selection.</p><p>GPT Router (<a href=https://github.com/Writesonic/GPTRouter>https://github.com/Writesonic/GPTRouter</a>) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.client <span style=color:#f92672>import</span> GPTRouterClient
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.models <span style=color:#f92672>import</span> ModelGenerationRequest, GenerationParams
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.enums <span style=color:#f92672>import</span> ModelsEnum, ProvidersEnum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> GPTRouterClient(base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_base_url&#39;</span>, api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_api_key&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Write me a short poem&#34;</span>},
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>prompt_params <span style=color:#f92672>=</span> GenerationParams(messages<span style=color:#f92672>=</span>messages)
</span></span><span style=display:flex><span>claude2_request <span style=color:#f92672>=</span> ModelGenerationRequest(
</span></span><span style=display:flex><span>    model_name<span style=color:#f92672>=</span>ModelsEnum<span style=color:#f92672>.</span>CLAUDE_INSTANT_12,
</span></span><span style=display:flex><span>    provider_name<span style=color:#f92672>=</span>ProvidersEnum<span style=color:#f92672>.</span>ANTHROPIC<span style=color:#f92672>.</span>value,
</span></span><span style=display:flex><span>    order<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    prompt_params<span style=color:#f92672>=</span>prompt_params,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>generate(ordered_generation_requests<span style=color:#f92672>=</span>[claude2_request])
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text)
</span></span></code></pre></div><h2 id=12-non-consensus-scenarios-market-differentiation>1.2 Non-Consensus: Scenarios, Market, Differentiation<a hidden class=anchor aria-hidden=true href=#12-non-consensus-scenarios-market-differentiation>#</a></h2><p>Content that is still not determined:</p><p>What constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge</p><p>Many companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures</p><h2 id=how-agents-can-help-creators-achieve-10x-efficiency>How Agents Can Help Creators Achieve 10x Efficiency<a hidden class=anchor aria-hidden=true href=#how-agents-can-help-creators-achieve-10x-efficiency>#</a></h2><h3 id=21-agent-application-cases>2.1 Agent Application Cases<a hidden class=anchor aria-hidden=true href=#21-agent-application-cases>#</a></h3><h4 id=autogpt>AutoGPT<a hidden class=anchor aria-hidden=true href=#autogpt>#</a></h4><p>AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters.
<a href=https://github.com/Significant-Gravitas/AutoGPT>https://github.com/Significant-Gravitas/AutoGPT</a></p><h4 id=gpt-researcher>GPT Researcher<a hidden class=anchor aria-hidden=true href=#gpt-researcher>#</a></h4><p>A GPT-based autonomous agent that conducts comprehensive online research on any given topic.
<a href=https://github.com/assafelovic/gpt-researcher>https://github.com/assafelovic/gpt-researcher</a></p><p>The advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.</p><h3 id=workflow-building---video-creation-example>Workflow Building - Video Creation Example<a hidden class=anchor aria-hidden=true href=#workflow-building---video-creation-example>#</a></h3><p>Concept Design: Midjourney
Script + Storyboard: ChatGPT
AI Image Generation: Midjourney, Stable Diffusion, D3
AI Video: Runway, Pika, Pixverse, Morph Studio
Dialogue + Narration: Eleven Labs, Ruisheng
Sound Effects + Music: SUNO, UDIO, AUDIOGEN
Video Enhancement: Topaz Video
Subtitles + Editing: CapCut, JianYing</p><h3 id=22-improving-agent-user-experience>2.2 Improving Agent User Experience<a hidden class=anchor aria-hidden=true href=#22-improving-agent-user-experience>#</a></h3><h4 id=personalized-memory--style-customization>Personalized Memory & Style Customization<a hidden class=anchor aria-hidden=true href=#personalized-memory--style-customization>#</a></h4><p>User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments.
Example: Typeface Brand Kit</p><h4 id=rewind--edit>Rewind & Edit<a hidden class=anchor aria-hidden=true href=#rewind--edit>#</a></h4><p>User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality.
Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)</p><h4 id=choose-from-variations>Choose from Variations<a hidden class=anchor aria-hidden=true href=#choose-from-variations>#</a></h4><p>User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience.
Example: Typeface Image Generator (also supports favoriting)</p><h4 id=workflows-not-skills>Workflows, Not Skills<a hidden class=anchor aria-hidden=true href=#workflows-not-skills>#</a></h4><p>User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation.
Example: Typeface Workflow (all capabilities presented at the appropriate stages)</p><h3 id=typeface---product-reference-from-former-adobe-cto>Typeface - Product Reference from Former Adobe CTO<a hidden class=anchor aria-hidden=true href=#typeface---product-reference-from-former-adobe-cto>#</a></h3><p><a href=https://www.typeface.ai>https://www.typeface.ai</a></p><p>Typeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft&rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)</p><h3 id=31-performance-data---40000-monthly-active-users-459-average-usage-time>3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time<a hidden class=anchor aria-hidden=true href=#31-performance-data---40000-monthly-active-users-459-average-usage-time>#</a></h3><h3 id=32-feature-breakdown---customized-content-generation-for-brands>3.2 Feature Breakdown - Customized Content Generation for Brands<a hidden class=anchor aria-hidden=true href=#32-feature-breakdown---customized-content-generation-for-brands>#</a></h3><p>Multiple Agent calls centered around the core document editing experience.</p><p>When users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.</p><h4 id=features>Features:<a hidden class=anchor aria-hidden=true href=#features>#</a></h4><h5 id=brands>Brands<a hidden class=anchor aria-hidden=true href=#brands>#</a></h5><p>When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:</p><ul><li>Image Styles: Users can upload existing images for subsequent generation style adjustment.</li><li>Color Palettes: Users can upload brand color palettes to standardize generated image colors.</li><li>Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image.</li></ul><h5 id=projects>Projects<a hidden class=anchor aria-hidden=true href=#projects>#</a></h5><p>When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:</p><ul><li>Create a new image</li><li>Create a product shot</li><li>Generate text</li><li>Create from template</li></ul><p>Additionally, users can select Refine to adjust generation language and tone (fixed options).</p><h5 id=create-an-image>Create an Image<a hidden class=anchor aria-hidden=true href=#create-an-image>#</a></h5><p>After clicking Create an image, users enter the image editing page with six integrated functions on the left: &ldquo;Add, select, extend, lighting, color, effects, adobe express.&rdquo; Users can generate and adjust images directly and favorite preferred generations.</p><h5 id=create-a-product-shot>Create a Product Shot<a hidden class=anchor aria-hidden=true href=#create-a-product-shot>#</a></h5><p>The difference from Create an image is that Product shot includes specific products, while image isn&rsquo;t necessarily product-related.</p><h5 id=generate-text>Generate Text<a hidden class=anchor aria-hidden=true href=#generate-text>#</a></h5><p>After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.</p><h5 id=templates>Templates<a hidden class=anchor aria-hidden=true href=#templates>#</a></h5><p>Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.</p><h5 id=audiences>Audiences<a hidden class=anchor aria-hidden=true href=#audiences>#</a></h5><p>When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).</p><h5 id=integrations>Integrations<a hidden class=anchor aria-hidden=true href=#integrations>#</a></h5><p>These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.</p><p><a href=https://www.typeface.ai/product/integrations>https://www.typeface.ai/product/integrations</a></p><h6 id=microsoft-dynamics-365>Microsoft Dynamics 365<a hidden class=anchor aria-hidden=true href=#microsoft-dynamics-365>#</a></h6><p>Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.</p><h6 id=salesforce-marketing>Salesforce Marketing<a hidden class=anchor aria-hidden=true href=#salesforce-marketing>#</a></h6><p>Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.</p><h6 id=google-bigquery>Google BigQuery<a hidden class=anchor aria-hidden=true href=#google-bigquery>#</a></h6><p>Users can define audience segments with customer intelligence from BigQuery&rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.</p><h6 id=google-workspace>Google Workspace<a hidden class=anchor aria-hidden=true href=#google-workspace>#</a></h6><p>Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.</p><h6 id=microsoft-teams>Microsoft Teams<a hidden class=anchor aria-hidden=true href=#microsoft-teams>#</a></h6><p>Create content in Teams using Typeface&rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://chenterry.com/tags/technology/>Technology</a></li><li><a href=https://chenterry.com/tags/product/>Product</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://chenterry.com/>Terry Chen</a></span></footer></body></html>