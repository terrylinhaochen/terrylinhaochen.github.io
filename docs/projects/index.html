<!doctype html><html lang=en dir=auto><head><meta name=description content="A showcase of my work across startups, professional projects, research, and prototypes"><link rel=canonical href=https://chenterry.com/projects/><meta property="og:title" content="Projects | Terry Chen"><meta property="og:description" content="A showcase of my work across startups, professional projects, research, and prototypes"><meta property="og:type" content="website"><meta property="og:url" content="https://chenterry.com/projects/"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta property="og:site_name" content="Terry Chen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@YourTwitterHandle"><meta name=twitter:title content="Projects | Terry Chen"><meta name=twitter:description content="A showcase of my work across startups, professional projects, research, and prototypes"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Projects | Terry Chen</title>
<meta name=keywords content><meta name=author content="Terry Chen"><link rel=canonical href=https://chenterry.com/projects/><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M6GS8Q702L")}</script><meta property="og:url" content="https://chenterry.com/projects/"><meta property="og:site_name" content="Terry Chen"><meta property="og:title" content="Projects"><meta property="og:description" content="A showcase of my work across startups, professional projects, research, and prototypes"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta property="og:image" content="https://chenterry.com/images/profile.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chenterry.com/images/profile.jpg"><meta name=twitter:title content="Projects"><meta name=twitter:description content="A showcase of my work across startups, professional projects, research, and prototypes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://chenterry.com/projects/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chenterry.com/ accesskey=h title="Terry Chen (Alt + H)">Terry Chen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/posts/ title=Posts><span>Posts</span></a></li><li><a href=/projects/ title=Projects><span class=active>Projects</span></a></li><li><a href=/archived/ title=Archived><span>Archived</span></a></li><li><a href=/search/ title=Search accesskey=/><span>Search</span></a></li><li><a href=/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><style>.projects-container{max-width:var(--content-width);margin:0 auto;padding:2rem 1rem}.theme-carousel{margin-bottom:4rem;position:relative}.theme-slides{display:flex;overflow:hidden;scroll-behavior:smooth}.theme-slide{min-width:100%;padding:2rem;background:linear-gradient(135deg,#e8f0ff 0%,#ffffff 100%);border-radius:12px;box-shadow:0 4px 6px rgba(0,0,0,5%)}.theme-content{max-width:800px;margin:0 auto}.theme-nav{display:flex;justify-content:center;gap:1rem;margin-top:1rem}.theme-nav button{width:12px;height:12px;border-radius:50%;border:none;background:#ddd;cursor:pointer;transition:background .3s ease}.theme-nav button.active{background:#333}.section{margin-bottom:4rem}.section-header{margin-bottom:2rem;text-align:center}.section-title{font-size:2rem;font-weight:700;color:#333;margin-bottom:.5rem}.section-subtitle{color:#666;font-size:1.1rem}.projects-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:2rem}.project-card{background:#fff;border-radius:12px;padding:1.5rem;box-shadow:0 2px 8px rgba(0,0,0,6%);transition:transform .3s ease,box-shadow .3s ease;cursor:pointer}.project-card:hover{transform:translateY(-4px);box-shadow:0 4px 12px rgba(0,0,0,.1)}.project-card.featured,.work-card{grid-column:span 2;background:linear-gradient(to right bottom,#ffffff,#f8f9fa)}.modal-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,.7);z-index:1000;justify-content:center;align-items:center}.modal{background:#fff;padding:2rem;border-radius:12px;max-width:800px;width:90%;max-height:90vh;overflow-y:auto;position:relative}.modal-close{position:absolute;top:1rem;right:1rem;font-size:1.5rem;border:none;background:0 0;cursor:pointer}.modal-overlay.active{display:flex}</style><div class=projects-container><section class=theme-carousel><div class=theme-slides id=themeSlides><div class=theme-slide><div class=theme-content><h2>Multi-agent LLM Systems</h2><p>Developing capable multi-agent systems for complex reasoning and human-AI collaboration.</p></div></div><div class=theme-slide><div class=theme-content><h2>Data Insights</h2><p>Extracting meaningful insights from unstructured multi-modal content.</p></div></div><div class=theme-slide><div class=theme-content><h2>Content Generation</h2><p>Leveraging synthesized insights for enhanced content generation.</p></div></div></div><div class=theme-nav id=themeNav></div></section><section class=section><div class=section-header><h2 class=section-title>Startup Projects</h2><p class=section-subtitle>Things keeping me busy on weekends</p></div><div class=projects><article class=post-entry><header class=entry-header><h2>Crowdlistening</h2></header><div class=entry-content>Inspiring insights, amplifying voices. (crowdlistening.com) From Content Aggregation to Original Research Crowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. Our focus is not just on analyzing content at scale, but rather conducting original research directly from raw social data, generating insights that haven’t yet appeared in established reporting.
Deep research features provide professional-looking research reports, yet the contents are far from original, as they’re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet’s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they’re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.</div><footer class=entry-footer><div class=post-meta><time>November 6, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/crowdlistening/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Work Projects</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Insight Spotlight</h2></header><div class=entry-content>Analyze thousands of tiktoks to provide actionable trends & insights for key agencies. (Worked on multi-modal content understanding) To be released on TikTok Creative Center (https://ads.tiktok.com/business/creativecenter/pc/en)
Credits: TikTok Creative Team
Beyond Data: The Evolution of AI-Driven Insight Products for Content Creation Introduction: The Shifting Landscape of Creative AI Tools In the rapidly evolving space of AI-driven creative tools, we’re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.</div><footer class=entry-footer><div class=post-meta><time>August 20, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/insight/></a></article><article class=post-entry><header class=entry-header><h2>Symphony Assistant</h2></header><div class=entry-content>Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc
Credits: TikTok Creative Team
Building Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren’t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.</div><footer class=entry-footer><div class=post-meta><time>May 20, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/copilot/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Research</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Improving & Scaling LLMs for Coaching</h2></header><div class=entry-content>Situated Practice Systems: Improving and Scaling Coaching through LLMs Authors: Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning environments is critical for developing students’ self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.</div><footer class=entry-footer><div class=post-meta><time>June 15, 2023</time></div></footer><a class=entry-link href=https://chenterry.com/projects/improving-scaling-llms-coaching/></a></article></div></section><section class=section><div class=section-header><h2 class=section-title>Prototypes</h2></div><div class=projects><article class=post-entry><header class=entry-header><h2>Towards Differentiable Quality - Content Synthesis</h2></header><div class=entry-content>Interactive chat interface with multiple AI agents, enabling dynamic conversation flows and specialized problem-solving capabilities.</div><footer class=entry-footer><div class=post-meta><time>December 28, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/personalized_content/></a></article><article class=post-entry><header class=entry-header><h2>LLM Enhanced Recommendation and Search</h2></header><div class=entry-content>Content recommendation system leveraging embedding similarity for personalized content recommendation (based on profile).</div><footer class=entry-footer><div class=post-meta><time>December 21, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/recommendation_searh/></a></article><article class=post-entry><header class=entry-header><h2>Exploring Unknown Unknowns - Textbook Interface</h2></header><div class=entry-content>Interactive learning aids for reading comprehension and engagement.</div><footer class=entry-footer><div class=post-meta><time>December 7, 2024</time></div></footer><a class=entry-link href=https://chenterry.com/projects/learning_interface/></a></article></div></section></div><div class=modal-container><div id=modal-towards-differentiable-quality-content-synthesis class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("towards-differentiable-quality-content-synthesis")'>&#215;</button><h2>Towards Differentiable Quality - Content Synthesis</h2><p class=project-meta>December 28, 2024</p><div class=modal-content><p>Interactive chat interface with multiple AI agents, enabling dynamic conversation flows and specialized problem-solving capabilities.</p></div></div></div><div id=modal-llm-enhanced-recommendation-and-search class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("llm-enhanced-recommendation-and-search")'>&#215;</button><h2>LLM Enhanced Recommendation and Search</h2><p class=project-meta>December 21, 2024</p><div class=modal-content><p>Content recommendation system leveraging embedding similarity for personalized content recommendation (based on profile).</p></div></div></div><div id=modal-exploring-unknown-unknowns-textbook-interface class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("exploring-unknown-unknowns-textbook-interface")'>&#215;</button><h2>Exploring Unknown Unknowns - Textbook Interface</h2><p class=project-meta>December 7, 2024</p><div class=modal-content><p>Interactive learning aids for reading comprehension and engagement.</p></div></div></div><div id=modal-crowdlistening class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("crowdlistening")'>&#215;</button><h2>Crowdlistening</h2><p class=project-meta>November 6, 2024</p><div class=modal-content><h2 id=inspiring-insights-amplifying-voices-crowdlisteningcom>Inspiring insights, amplifying voices. (crowdlistening.com)</h2><p><img alt=Webpage loading=lazy src=/images/posts/crowdlistening/webpage.png></p><h2 id=from-content-aggregation-to-original-research>From Content Aggregation to Original Research</h2><p>Crowdlistening transforms large-scale social conversations into actionable insight by integrating llm reasoning with extensive model context protocol(MCP) capabilities. Our focus is not just on analyzing content at scale, but rather conducting original research directly from raw social data, generating insights that haven&rsquo;t yet appeared in established reporting.</p><p>Deep research features provide professional-looking research reports, yet the contents are far from original, as they&rsquo;re drawn from articles already indexable on the internet and paraphrased with LLMs. However, much of the internet&rsquo;s data exists in unstructured formats - TikTok videos, comments, and metadata, for example. Too much content is generated every day for there to be existing articles written about it all, and when such articles are published, they&rsquo;re often already outdated. When you consider multimodal data, metadata, and connections between data points, these are precisely the types of information that could yield genuinely interesting and useful insights.</p><p>I&rsquo;ve been thinking about this problem while working at TikTok, enabling better social listening through more fine-grained insights extracted using multi-modal/LLM-based approaches. In October, I started developing early conceptions of Crowdlistening, focusing on multi-modal content understanding for TikTok videos. Although deep research features like GPT Researcher and Stanford Oval Storm existed, it wasn&rsquo;t intuitive to integrate unstructured data processing capabilities into their workflows. I paused Crowdlistening in Winter Quarter due to other commitments, but during this time, Anthropic released the Model Context Protocol (MCP). I&rsquo;ve recently gotten back on track following progress in this field, and I believe this presents an interesting avenue for product innovation - deep research features are significantly enhanced by the growing ecosystem of MCP servers (the same agentic workflows perform much better given they rely on APIs, whose capabilities have improved over recent months).</p><p>What I&rsquo;m particularly interested in exploring and building with Crowdlistening is the ability to extract actionable insights from large volumes of unstructured or semi-structured data, forming linkages, and perhaps even testing hypotheses to enable effective research at scale. We started with TikTok data as a prototype ground given my familiarity with the medium, but I could quickly see this covering any type of unstructured data available on the web.</p><h2 id=the-insight-paradox>The Insight Paradox</h2><p><img alt="Insight Paradox" loading=lazy src=/images/posts/crowdlistening/insight_paradox.png></p><p>Brands today face a fundamental paradox: they need broad insights from vast amounts of social data, yet require the detailed understanding typically only available through limited case studies. Current solutions offer either abstracted metrics that require tedious manual interpretation, expensive and limited content screening that can&rsquo;t scale, or surface-level sentiment analysis that misses nuanced opinions. Crowdlistening bridges this gap by combining the scale of algorithmic analysis with the depth of human-like comprehension. This addresses the first challenge identified in &ldquo;Essence of Creativity&rdquo; - helping users understand massive amounts of information and generate meaningful insights when they &ldquo;don&rsquo;t know what output they want.&rdquo;</p><h2 id=technical-architecture-multi-modal-by-design>Technical Architecture: Multi-Modal by Design</h2><p>The rationale behind Crowdlistening&rsquo;s multi-modal technical architecture stems from the fundamental challenge of extracting truly valuable insights from the vast and varied landscape of online conversations. Traditional methods often fall short because they either focus on structured data or analyze individual modalities (text, video, audio) in isolation. This approach misses the rich context and nuanced understanding that arises from the interplay between different forms of content and engagement. For example, a viral TikTok video&rsquo;s impact is not solely determined by its visual content but also by its accompanying audio, captions, user comments, and engagement metrics like likes and shares.</p><p><img alt="Analysis Page" loading=lazy src=/images/posts/crowdlistening/analysis_page.png></p><p>Crowdlistening&rsquo;s design directly tackles this limitation by integrating embedding-based topic modeling and LLM deep research capabilities to process and understand this multi-faceted data. Embedding-based topic modeling efficiently identifies key themes across massive datasets, while the LLM&rsquo;s deep reasoning capabilities can then analyze these themes within the context of various modalities. This dual approach allows for a layered analysis, examining both the primary content and the subsequent engagement it generates. By processing video, audio, text, and engagement metrics in a unified system, Crowdlistening can generate insights that reflect not just what is being said, but how it&rsquo;s being said, the surrounding context, and the audience&rsquo;s multifaceted response. This comprehensive understanding is crucial for overcoming the &ldquo;insight paradox&rdquo; and delivering truly actionable intelligence that goes beyond surface-level sentiment or abstracted metrics. Ultimately, this multi-modal design is essential for achieving the core goal of Crowdlistening: to conduct original research directly from raw social data and uncover emerging trends and nuanced opinions that would be invisible to single-mode analysis systems.</p><h2 id=detailed-analysis-capabilities>Detailed Analysis Capabilities</h2><p>The platform provides granular breakdowns of content performance and audience reactions. As shown in our analysis results page, users can explore specific themes, track sentiment over time, and identify the most engaging content types. This helps brands understand not just what is being said, but why certain content resonates with their audience.</p><p><img alt="Analysis Results" loading=lazy src=/images/posts/crowdlistening/analysis_results.png></p><p>The opinion analysis feature goes beyond simple positive/negative sentiment to categorize specific viewpoints and concerns. This allows brands to understand the nuanced perspectives their audience holds, helping them craft more targeted and effective messaging.</p><p><img alt="Opinion Analysis" loading=lazy src=/images/posts/crowdlistening/opinion_analysis.png></p><h2 id=the-mcp-advantage-accessible-functional-calls>The MCP Advantage: Accessible Functional Calls</h2><p><img alt="Claude MCP Entrance" loading=lazy src=/images/posts/crowdlistening/claude_client.png></p><p>We have integrated Model Context Protocols (MCPs) - an emerging standard that simplifies how LLMs interact with specialized tools and data sources. Rather than simple API calls, MCPs provide structured interfaces for LLMs to access specialized capabilities while maintaining context awareness throughout the analysis process.</p><p><img alt="Claude MCP Process" loading=lazy src=/images/posts/crowdlistening/claude_mcp_process.png></p><p>As shown here, when a user submits a research question, the system dynamically determines which analytical capabilities to deploy. The Claude interface serves as the orchestration layer, identifying relevant MCP tools to activate and calling them sequentially:</p><ol><li>First gathering baseline information through web search</li><li>Then performing targeted data collection via specialized TikTok MCP tools</li><li>Following with multi-layered analysis of videos and comments</li><li>Finally synthesizing everything into coherent, actionable insights</li></ol><p>This MCP-driven approach creates a dramatic efficiency improvement, reducing complex social media analysis from weeks to minutes while maintaining remarkable analytical depth.</p><h2 id=case-study---trump-tariffs>Case Study - Trump Tariffs</h2><p>To demonstrate Crowdlistening&rsquo;s capabilities, we conducted a comprehensive analysis of public sentiment regarding Trump&rsquo;s tariff policies. This serves as an excellent test case due to its complexity, polarizing nature, and economic impact.</p><p>When a user inputs the query about Trump&rsquo;s tariff policies, our system activates the appropriate MCP tools in sequence. First, it gathers factual background information on the policies themselves, as shown below:</p><p><img alt="Research Background" loading=lazy src=/images/posts/crowdlistening/trump_background.png></p><p>This background research provides context on what the current tariff policies are, including the 10% baseline tariff on all imports that took effect in April 2025, plus the higher &ldquo;reciprocal&rdquo; tariffs on countries with which the US has trade deficits (34% for China, 20% for the EU, and 24% for Japan).</p><p>Next, the system analyzes public opinion on these policies by examining social media content. The analysis reveals highly polarized reactions, categorized into three main perspectives:</p><p><img alt="Public Opinion" loading=lazy src=/images/posts/crowdlistening/public_opinion.png></p><p>The sentiment analysis dashboard shows that opinions on Trump&rsquo;s tariff policies are distributed as 38% supportive, 42% critical, and 20% neutral or mixed. This visualization helps brands and researchers quickly understand the overall public response landscape.</p><p><img alt="Trump Sentiment" loading=lazy src=/images/posts/crowdlistening/trump_sentiment.png></p><p>One of the most valuable outputs is our projected economic impact analysis. This data visualization clearly presents the concrete financial implications of these policies across multiple domains:</p><p><img alt="Economic Impact" loading=lazy src=/images/posts/crowdlistening/economic_impact.png></p><p>The analysis shows an estimated $1,300 annual cost increase per US household, a projected 0.8% reduction in long-run US GDP, significant auto price increases ($3,000 for US vehicles, $6,000 for imports), and warnings about market volatility.</p><p>Beyond simple pro/con sentiment, our opinion analysis feature categorizes specific viewpoints with remarkable granularity. For instance, when examining comments on related content, we can identify nuanced perspectives and their prevalence:</p><p><img alt="Trump Comments" loading=lazy src=/images/posts/crowdlistening/comment_analysis.png></p><p>This example shows how our system can identify several different comment themes, including positive views of content creators (37.5%), appreciation for intelligent discussion (25%), and concerns about media echo chambers (12.5%). This level of nuanced understanding would be impossible through traditional keyword or basic sentiment analysis.</p><h2 id=validation-and-impact>Validation and Impact</h2><p>Our solution has been validated through interviews with major brands like L&rsquo;Oreal, confirming we drastically cut the time and cost of social media analysis. Crowdlistening enables:</p><ul><li>Rapid response to emerging trends</li><li>Deep understanding of consumer sentiment across demographics</li><li>Identification of microtrends before they become mainstream</li><li>Competitive intelligence at unprecedented scale</li></ul><h2 id=the-future-of-mcp-driven-research>The Future of MCP-Driven Research</h2><p>We believe Model Context Protocols represent the future of specialized LLM applications. As shown in our implementation, MCPs provide a structured way for language models to interact with specialized tools and data sources while maintaining context awareness throughout the analysis process.</p><p>This approach is likely to become standard in LLM application development given how effectively it bridges the gap between general-purpose AI and domain-specific functionality. We anticipate seeing more MCP clients (interaction surfaces like Claude&rsquo;s interface) emerge as this paradigm gains traction.</p><p>For social media analysis specifically, this approach creates a fascinating dynamic where AI-driven insights can actually lead structured reporting in terms of timeliness and depth. By processing and analyzing unstructured social data at scale, we can identify emerging trends and public sentiment shifts before they&rsquo;re covered in traditional reporting.</p><h2 id=on-social-intelligence>On Social Intelligence</h2><p>Crowdlistening represents the next evolution in social listening tools - moving beyond counting mentions to truly understanding conversations at scale. By transforming social media chatter into structured insights, we&rsquo;re helping brands make more informed decisions faster than ever before.</p><p>As noted in &ldquo;Essence of Creativity,&rdquo; the real value in AI-powered tools comes not just from generating content, but from helping users find new perspectives and insights. Our platform serves as both an inspiration acquisition tool (accelerating original content production) and a content understanding tool (helping brands better comprehend their audience). By connecting insight data with generation capabilities, we&rsquo;re creating the kind of breakthrough product that bridges the gap between understanding and action.</p><hr><p>Credits: This project was developed in collaboration with Madison Bratley, whose expertise in journalism and social media analysis was instrumental in conceptualizing how this technology could transform research methodologies. Additional contributions from Violet Liu in providing valuable usability feedback for our early prototype. I would also like to acknowledge Zhengjin, Cathy, Ruiwan, Qiping, and other members on the Creative team at TikTok, who I&rsquo;ve discussed early conceptions of this idea with.</p></div></div></div><div id=modal-insight-spotlight class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("insight-spotlight")'>&#215;</button><h2>Insight Spotlight</h2><p class=project-meta>August 20, 2024</p><div class=modal-content><p>Analyze thousands of tiktoks to provide actionable trends & insights for key agencies. (Worked on multi-modal content understanding)
To be released on TikTok Creative Center (<a href=https://ads.tiktok.com/business/creativecenter/pc/en>https://ads.tiktok.com/business/creativecenter/pc/en</a>)</p><p>Credits: TikTok Creative Team</p><h1 id=beyond-data-the-evolution-of-ai-driven-insight-products-for-content-creation>Beyond Data: The Evolution of AI-Driven Insight Products for Content Creation</h1><h2 id=introduction-the-shifting-landscape-of-creative-ai-tools>Introduction: The Shifting Landscape of Creative AI Tools</h2><p>In the rapidly evolving space of AI-driven creative tools, we&rsquo;re witnessing a significant transition from general-purpose large language models to specialized, task-specific agent systems. This shift represents a fundamental change in how AI approaches creative work, particularly in advertising and marketing.</p><p>While many current GenAI applications focus heavily on content generation capabilities, the true creative bottleneck often isn&rsquo;t in the generation step itself. Rather, it lies in the quality of insights that inform and guide the creative process. Without meaningful data and analysis, even the most sophisticated generation tools produce generic, uninspired content.</p><p>This blog explores how data insight products are evolving alongside generative AI technologies, and how their integration could fundamentally transform content creation workflows.</p><h2 id=the-evolution-of-ai-capabilities>The Evolution of AI Capabilities</h2><p>The limitations of general-purpose large language models have become increasingly apparent when handling complex creative tasks. To address issues like hallucinations and improve task completion capabilities, the industry has largely reached a consensus around agent-based approaches.</p><p>Different types of agent workflows have emerged to address specific needs. Non-agentic workflows generate content linearly without backtracking, suitable for straightforward tasks. Reflection-based systems introduce iterative improvement cycles where the AI criticizes and refines its own outputs. Tool use capabilities enable function calls and web browsing for enhanced research capabilities.</p><p>More advanced systems implement planning algorithms that decompose complex tasks into manageable steps, similar to how human creators break down projects. At the frontier, multi-agent collaboration enables specialized AI agents to work together, each handling different aspects of a complex creative process.</p><p>This evolution toward more sophisticated agent architectures reflects a growing understanding that creative work isn&rsquo;t linear—it requires iteration, refinement, and the ability to leverage different capabilities at different stages of the process.</p><h2 id=the-workflow-challenge>The Workflow Challenge</h2><p>One of the key limitations in current AI creative products is their focus on isolated capabilities rather than integrated workflows. In the advertising and marketing industry, there&rsquo;s a high concentration of AI tools, but most provide only single functions or partial capabilities.</p><p>A content creator typically needs to move through multiple stages: gathering insights, analyzing competitors, developing concepts, generating scripts, creating visual assets, and optimizing the final product. Currently, this requires juggling multiple disconnected tools, manually transferring context between them, and piecing together a cohesive workflow.</p><p>Users don&rsquo;t simply need better individual tools—they need comprehensive workflows that connect these steps seamlessly. The value proposition shifts from &ldquo;what can this AI do?&rdquo; to &ldquo;how does this AI fit into my creative process?&rdquo; This represents a fundamental shift in how we should design and evaluate AI creative systems.</p><h2 id=market-analysis-of-insight-products>Market Analysis of Insight Products</h2><p>The current market for insight products shows several distinct categories, each addressing different aspects of the creative process. Here&rsquo;s a structured analysis of the landscape:</p><h3 id=ad-compilation-tools>Ad Compilation Tools</h3><p>Products in this category focus on collecting, organizing, and analyzing existing advertisements across platforms. Pipi Ads maintains a library of over 20 million TikTok ads with extensive filtering capabilities, allowing users to study successful campaigns and identify trending approaches. Foreplay offers a more workflow-oriented solution, enabling users to save ads from multiple platforms, organize them with custom tags, and build creative briefs based on existing successful content.</p><p>The value proposition of these tools centers on learning from what already works. By studying high-performing ads, creators can identify patterns and strategies that resonate with specific audiences. However, most of these tools stop at the analysis stage without directly connecting insights to content generation.</p><h3 id=competitor-analysis-tools>Competitor Analysis Tools</h3><p>Tools like Social Peta, Big Spy, and Story Clash provide deeper analysis of competitive activities. Social Peta offers insights into content distribution across 69 countries and 70 networks, analyzing multimedia types and dimensions. Big Spy enables cross-network ad searching with multiple filters, while Story Clash specializes in TikTok influencer tracking and performance analysis.</p><p>The competitive analysis market has grown substantially with the rise of social media advertising, with new players continuously entering the space to address specialized niches and platforms. These tools typically provide dashboard interfaces with various filters for monitoring competitor strategies, but most lack direct integration with content creation workflows.</p><h3 id=brand-insight-tools>Brand Insight Tools</h3><p>Social listening platforms like Springklr, Exolyt, and Keyhole monitor brand mentions and sentiment across social channels. These tools analyze both posts and comments, providing valuable data on how audiences perceive brands and their content. Springklr offers comprehensive post and comment analysis with sentiment tracking, while Exolyt specializes in TikTok-specific insights, comparing brand content with user-generated content.</p><p>Keyhole delivers profile analytics, social trend monitoring, and campaign tracking. These tools excel at capturing the audience&rsquo;s voice and identifying shifts in perception, but typically require significant manual analysis to translate these insights into actionable creative strategies.</p><h3 id=performance-analysis-tools>Performance Analysis Tools</h3><p>Platforms such as Social Insider, Motion App, and RivalQ focus on analyzing ad performance metrics. These tools help marketers understand what content performs best, with detailed analytics on engagement, conversion, and return on investment. By identifying high-performing content patterns, these tools can inform future creative decisions.</p><p>However, there remains a significant gap between identifying what works and automatically generating new content based on those insights. Most performance analysis tools remain separated from content creation workflows, requiring manual interpretation and application of insights.</p><h2 id=deep-dive-notable-products>Deep Dive: Notable Products</h2><p>Several standout products illustrate different approaches to the insight-generation challenge:</p><h3 id=tikbuddy-platform-specific-analysis>TikBuddy: Platform-Specific Analysis</h3><p>TikBuddy focuses exclusively on TikTok analytics, offering creator rankings by category, follower count, and growth rate. The tool provides comprehensive account performance monitoring and video data analysis through a convenient Chrome extension.</p><p>Its specialized focus allows for deeper platform-specific insights, but its utility is limited to a single platform and doesn&rsquo;t extend to content creation. Users must manually apply any insights gained to their creative process.</p><h3 id=foreplay-workflow-integration>Foreplay: Workflow Integration</h3><p>Foreplay stands out for its more integrated workflow approach. The platform enables users to collect ad content across platforms, preserve it even after platform deletion, and organize it with tags and categories. Its brief creation tools facilitate the transition from insight to execution, with support for brand information and specific generation requirements.</p><p>The platform&rsquo;s AI storyboard generator creates hooks and develops scripts based on collected insights. Foreplay also integrates discovery features organized by community, brand, and experts, alongside competitor monitoring capabilities.</p><p>This approach begins to bridge the gap between insights and creation, though the integration remains partial rather than fully automated.</p><h3 id=keyhole-deep-analytics>Keyhole: Deep Analytics</h3><p>Keyhole exemplifies the analytics-focused approach, tracking keywords and brand mentions with temporal context. The platform offers detailed post analysis, influencer identification, trending topics visualization, and profile analytics with optimization recommendations.</p><p>Its strength lies in comprehensive data collection and visualization, but like many analytics platforms, it requires significant human interpretation to translate insights into creative decisions.</p><h2 id=emerging-innovations-in-data-insight-products>Emerging Innovations in Data Insight Products</h2><p>The data insight landscape continues to evolve rapidly, with several notable innovations emerging:</p><p>Open source projects like Vanna are revolutionizing text-to-SQL capabilities, making database querying more accessible to non-technical users. These tools enable creators to extract specific insights from complex datasets without specialized database knowledge.</p><p>Recent startups are developing interactive data dashboards that visualize complex datasets in more intuitive ways, allowing for easier pattern identification and insight extraction. These tools employ advanced visualization techniques to make data more accessible and actionable.</p><p>User feedback aggregation tools are also gaining traction, automatically summarizing and categorizing customer sentiment from reviews and comments. These systems can identify common themes and concerns, providing valuable input for content creators looking to address audience needs.</p><p>The most promising innovations focus on reducing the cognitive load required to extract meaningful insights from data, making the path from analysis to action more direct and intuitive.</p><h2 id=the-future-insight-driven-content-generation>The Future: Insight-Driven Content Generation</h2><p>The next evolution in creative AI tools will likely center on high-quality content generation based on data insights. Current GenAI applications often produce unnecessary content redundancy—different from hallucinations, but equally problematic for effective communication.</p><p>The real creative barrier isn&rsquo;t typically in the generation process itself, but in the prompts—the insights that inform decision-making. When using agent-based systems, the quality of instructions and background information directly impacts the output quality.</p><p>For example, advanced AI systems can now decompose complex goals like &ldquo;How can a lifestyle channel creator get 1,000 subscribers on YouTube?&rdquo; into specific tasks: analyzing successful channels, generating targeted content ideas, and implementing optimization strategies. However, the quality of these recommendations depends entirely on the AI&rsquo;s access to relevant, accurate data about what actually works.</p><p>By leveraging sophisticated content analysis, we can identify truly effective patterns in high-performing content. Multimodal understanding can reveal why certain creative approaches resonate with specific audiences, providing creators with concrete, unique insights rather than generic advice.</p><p>The future lies in connecting these insights directly to the generation process—using what we know works as the foundation for creating new content that maintains brand uniqueness while leveraging proven patterns.</p><h2 id=conclusion-the-integration-opportunity>Conclusion: The Integration Opportunity</h2><p>Compared to other GenAI creative tools, insight products place greater emphasis on data quality and quantity. The next leap in AI-generated content quality will likely come from precise generation guided by robust insight data.</p><p>The most promising opportunity lies in creating systems that can automatically analyze successful content across platforms, extract meaningful patterns from this analysis, and directly translate these insights into generation guidance. This approach would produce highly targeted content that leverages proven patterns while maintaining brand distinctiveness.</p><p>As we move forward, the focus will shift from mere information generation toward sophisticated information synthesis—providing not just content, but content informed by actionable insights derived from real-world performance data. Organizations that successfully integrate insight gathering with content generation will gain a significant competitive advantage in an increasingly crowded digital landscape.</p><p>The future belongs not to those with the most powerful generative models, but to those who can effectively transform data into creative insight, and insight into compelling content.</p></div></div></div><div id=modal-symphony-assistant class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("symphony-assistant")'>&#215;</button><h2>Symphony Assistant</h2><p class=project-meta>May 20, 2024</p><div class=modal-content><p>Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)
<a href="https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc">https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc</a></p><p>Credits: TikTok Creative Team</p><h1 id=building-agentic-workflows>Building Agentic Workflows</h1><h2 id=from-llms-to-agents>From LLMs to Agents</h2><p>The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren&rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.</p><h2 id=from-google-next-to-baidu-create>From Google Next to Baidu Create</h2><p>Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.</p><p>From a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.</p><p>From a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.</p><p>LLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.</p><p>We should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?</p><h2 id=11-consensus-task-specific-moe-agents-routing>1.1 Consensus: Task Specific, MoE, Agents, Routing</h2><p>Content that has reached consensus:</p><p>Most companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).</p><p>The MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.</p><p>The core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.</p><p>Implementing an MoE architecture typically requires the following steps:</p><ol><li><p>Define expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.</p></li><li><p>Train expert networks: Use labeled training data to train each expert network to obtain weights and parameters.</p></li><li><p>Allocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.</p></li><li><p>Summarize results: Weight and sum the output results of each expert network to get the final output.</p></li><li><p>Train the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.</p></li></ol><h3 id=longer-context-window---llm-routing>Longer Context Window -> LLM Routing</h3><p>At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.</p><h3 id=ai-routing-uses>AI Routing Uses</h3><p>Writesonic (<a href=https://writesonic.com>https://writesonic.com</a>) uses GPT Router for LLM Routing during AI Model Selection.</p><p>GPT Router (<a href=https://github.com/Writesonic/GPTRouter>https://github.com/Writesonic/GPTRouter</a>) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.client <span style=color:#f92672>import</span> GPTRouterClient
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.models <span style=color:#f92672>import</span> ModelGenerationRequest, GenerationParams
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gpt_router.enums <span style=color:#f92672>import</span> ModelsEnum, ProvidersEnum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> GPTRouterClient(base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_base_url&#39;</span>, api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;your_api_key&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Write me a short poem&#34;</span>},
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>prompt_params <span style=color:#f92672>=</span> GenerationParams(messages<span style=color:#f92672>=</span>messages)
</span></span><span style=display:flex><span>claude2_request <span style=color:#f92672>=</span> ModelGenerationRequest(
</span></span><span style=display:flex><span>    model_name<span style=color:#f92672>=</span>ModelsEnum<span style=color:#f92672>.</span>CLAUDE_INSTANT_12,
</span></span><span style=display:flex><span>    provider_name<span style=color:#f92672>=</span>ProvidersEnum<span style=color:#f92672>.</span>ANTHROPIC<span style=color:#f92672>.</span>value,
</span></span><span style=display:flex><span>    order<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    prompt_params<span style=color:#f92672>=</span>prompt_params,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>generate(ordered_generation_requests<span style=color:#f92672>=</span>[claude2_request])
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text)
</span></span></code></pre></div><h2 id=12-non-consensus-scenarios-market-differentiation>1.2 Non-Consensus: Scenarios, Market, Differentiation</h2><p>Content that is still not determined:</p><p>What constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge</p><p>Many companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures</p><h2 id=how-agents-can-help-creators-achieve-10x-efficiency>How Agents Can Help Creators Achieve 10x Efficiency</h2><h3 id=21-agent-application-cases>2.1 Agent Application Cases</h3><h4 id=autogpt>AutoGPT</h4><p>AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters.
<a href=https://github.com/Significant-Gravitas/AutoGPT>https://github.com/Significant-Gravitas/AutoGPT</a></p><h4 id=gpt-researcher>GPT Researcher</h4><p>A GPT-based autonomous agent that conducts comprehensive online research on any given topic.
<a href=https://github.com/assafelovic/gpt-researcher>https://github.com/assafelovic/gpt-researcher</a></p><p>The advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.</p><h3 id=workflow-building---video-creation-example>Workflow Building - Video Creation Example</h3><p>Concept Design: Midjourney
Script + Storyboard: ChatGPT
AI Image Generation: Midjourney, Stable Diffusion, D3
AI Video: Runway, Pika, Pixverse, Morph Studio
Dialogue + Narration: Eleven Labs, Ruisheng
Sound Effects + Music: SUNO, UDIO, AUDIOGEN
Video Enhancement: Topaz Video
Subtitles + Editing: CapCut, JianYing</p><h3 id=22-improving-agent-user-experience>2.2 Improving Agent User Experience</h3><h4 id=personalized-memory--style-customization>Personalized Memory & Style Customization</h4><p>User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments.
Example: Typeface Brand Kit</p><h4 id=rewind--edit>Rewind & Edit</h4><p>User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality.
Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)</p><h4 id=choose-from-variations>Choose from Variations</h4><p>User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience.
Example: Typeface Image Generator (also supports favoriting)</p><h4 id=workflows-not-skills>Workflows, Not Skills</h4><p>User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation.
Example: Typeface Workflow (all capabilities presented at the appropriate stages)</p><h3 id=typeface---product-reference-from-former-adobe-cto>Typeface - Product Reference from Former Adobe CTO</h3><p><a href=https://www.typeface.ai>https://www.typeface.ai</a></p><p>Typeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft&rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)</p><h3 id=31-performance-data---40000-monthly-active-users-459-average-usage-time>3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time</h3><h3 id=32-feature-breakdown---customized-content-generation-for-brands>3.2 Feature Breakdown - Customized Content Generation for Brands</h3><p>Multiple Agent calls centered around the core document editing experience.</p><p>When users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.</p><h4 id=features>Features:</h4><h5 id=brands>Brands</h5><p>When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:</p><ul><li>Image Styles: Users can upload existing images for subsequent generation style adjustment.</li><li>Color Palettes: Users can upload brand color palettes to standardize generated image colors.</li><li>Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image.</li></ul><h5 id=projects>Projects</h5><p>When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:</p><ul><li>Create a new image</li><li>Create a product shot</li><li>Generate text</li><li>Create from template</li></ul><p>Additionally, users can select Refine to adjust generation language and tone (fixed options).</p><h5 id=create-an-image>Create an Image</h5><p>After clicking Create an image, users enter the image editing page with six integrated functions on the left: &ldquo;Add, select, extend, lighting, color, effects, adobe express.&rdquo; Users can generate and adjust images directly and favorite preferred generations.</p><h5 id=create-a-product-shot>Create a Product Shot</h5><p>The difference from Create an image is that Product shot includes specific products, while image isn&rsquo;t necessarily product-related.</p><h5 id=generate-text>Generate Text</h5><p>After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.</p><h5 id=templates>Templates</h5><p>Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.</p><h5 id=audiences>Audiences</h5><p>When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).</p><h5 id=integrations>Integrations</h5><p>These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.</p><p><a href=https://www.typeface.ai/product/integrations>https://www.typeface.ai/product/integrations</a></p><h6 id=microsoft-dynamics-365>Microsoft Dynamics 365</h6><p>Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.</p><h6 id=salesforce-marketing>Salesforce Marketing</h6><p>Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.</p><h6 id=google-bigquery>Google BigQuery</h6><p>Users can define audience segments with customer intelligence from BigQuery&rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.</p><h6 id=google-workspace>Google Workspace</h6><p>Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.</p><h6 id=microsoft-teams>Microsoft Teams</h6><p>Create content in Teams using Typeface&rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.</p><h2 id=summary>Summary</h2><p>Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.</p></div></div></div><div id=modal-gauth-ai class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("gauth-ai")'>&#215;</button><h2>Gauth AI</h2><p class=project-meta>August 20, 2023</p><div class=modal-content><p>AI Homework helper with advanced reasoning and visualization for all school subjects. (Worked on LLM Reasoning)
<a href=https://www.gauthmath.com/>https://www.gauthmath.com/</a></p><p>Credits: Lexi Ling, Gauth Team</p></div></div></div><div id=modal-improving-scaling-llms-for-coaching class=modal-overlay><div class=modal><button class=modal-close onclick='closeModal("improving-scaling-llms-for-coaching")'>&#215;</button><h2>Improving & Scaling LLMs for Coaching</h2><p class=project-meta>June 15, 2023</p><div class=modal-content><h1 id=situated-practice-systems-improving-and-scaling-coaching-through-llms>Situated Practice Systems: Improving and Scaling Coaching through LLMs</h1><p><strong>Authors: Terry Chen, Allyson Lee</strong></p><h2 id=abstract>Abstract</h2><p>Effective coaching in project-based learning environments is critical for developing students&rsquo; self-regulation skills, yet scaling high-quality coaching remains a challenge. This paper presents an LLM-enhanced coaching system designed to support project-based learning by helping connect peers struggling with the same regulation gap, and to help coaches by identifying regulation gaps and generating tailored practice suggestions. Our system integrates vector-based semantic matching with LLM-generated regulation gap categorizations for Context Assessment Plan (CAP) notes. Results demonstrate that our system effectively retrieves relevant coaching cases, reducing the cognitive burden on mentors while maintaining high-quality, context-aware feedback.</p><h2 id=introduction>Introduction</h2><p>Training college students to tackle complex, open-ended innovation work requires developing strong regulation skills for self-directed work. Coaches guide the development of these regulation skills, helping students develop cognitive, motivational, emotional, and strategic behaviors needed to problem solve and reach desired outcomes. However, coaches face significant challenges in providing personalized guidance to multiple student teams.</p><p>Existing AI-based project management tools help track tasks but fail to capture nuanced ways students approach their work. Large Language Models (LLMs) show promise in analyzing text-based interactions and generating structured feedback, but their application to coaching remains underexplored.</p><p>To address these issues, we propose utilizing LLMs to develop and integrate three key technical innovations:</p><ul><li><strong>Peer Connections</strong> - Facilitate connections between students with similar challenges</li><li><strong>Coaching Reflections</strong> - Help coaches analyze patterns and improve their practice through identifying regulation gaps</li><li><strong>Practice Suggestions</strong> - Adapt similar cases to new situations</li></ul><h2 id=the-regulation-skills-codebook>The Regulation Skills Codebook</h2><p>Our system is built around a novel codebook consisting of regulation gap definitions and examples gathered across learning science literature. The codebook categorizes student regulation gaps in a tiered approach:</p><h3 id=tier-1-categories>Tier 1 Categories:</h3><ol><li><strong>Cognitive</strong> - Skills for approaching problems with unknown answers</li><li><strong>Metacognitive</strong> - Skills in planning, help-seeking, collaboration, and reflection</li><li><strong>Emotional</strong> - Dispositions toward self and learning that affect motivation</li></ol><h3 id=tier-2-categories-examples>Tier 2 Categories (Examples):</h3><ul><li>Representing problem and solution spaces</li><li>Assessing risks</li><li>Critical thinking and argumentation</li><li>Forming feasible plans</li><li>Planning effective iterations</li><li>Fears and anxieties</li><li>Embracing challenges and learning</li></ul><h2 id=system-architecture>System Architecture</h2><p>Our system combines semantic similarity search with LLM-based analysis in a retrieval-augmented generation approach:</p><ol><li>Student regulation notes are pre-processed with metadata on tier 1 and tier 2 regulation gaps</li><li>Notes are encoded into text embeddings</li><li>A vector database retrieves the most similar historical cases</li><li>An LLM (Deepseek) generates structured responses including:<ul><li>Diagnosis of potential regulation gaps</li><li>Practice suggestions targeted to these gaps</li><li>References to similar historical cases</li></ul></li></ol><p>This grounds LLM suggestions in actual coaching experiences rather than generic advice, improving the relevance and actionability of recommendations.</p><h2 id=similarity-methods-for-regulation-gap-analysis>Similarity Methods for Regulation Gap Analysis</h2><p>We developed and tested three approaches to match students with similar regulation challenges:</p><ol><li><strong>Baseline Semantic Approach</strong> - Uses vector embeddings to find similar cases based on textual similarity</li><li><strong>Weighted Semantic Similarity</strong> - Separates and weights regulation gap description (0.7) from contextual information (0.3)</li><li><strong>Hybrid LLM-Codebook Approach</strong> - Combines semantic matching with LLM-generated metadata using our regulation codebook</li></ol><p>The hybrid approach proved most effective, assigning the highest weight (0.5) to tier 2 categories and lower weights to tier 1 categories (0.1) and text content (0.2 each for gap text and context).</p><h2 id=evaluation-and-results>Evaluation and Results</h2><p>We evaluated each model against the same three notes, analyzing the top 5 returned similar notes. The semantic matching performed well when addressing cognitive and metacognitive gaps with repetitive terminology but struggled with emotional regulation gaps. The LLM-codebook approach showed promise in accurately identifying regulation gaps but was computationally intensive. The hybrid model consistently and efficiently identified notes with the same regulation gap while maintaining contextual similarity.</p><h2 id=discussion-and-future-work>Discussion and Future Work</h2><p>Our system effectively bridges the gap between human expertise and AI capabilities in coaching contexts. Key takeaways include:</p><ol><li><strong>Hybrid AI-Driven Case Retrieval</strong> - Combining LLM-driven metadata tagging with traditional semantic matching enables precision in retrieving relevant coaching cases</li><li><strong>Structured Codebooks for Domain-Specific AI</strong> - Our tiered classification system grounds LLM-based reasoning in expert-validated pedagogical frameworks</li></ol><p>Future work will focus on:</p><ul><li>Improving clarity of writing in notes</li><li>Collecting more data through alternative sources</li><li>Developing sub-categorized codebooks with specific examples and reasoning chains</li><li>Exploring more sophisticated reasoning methods like external knowledge bases or memory systems</li></ul><p>This research contributes to the broader field of AI-enhanced education and human-AI collaboration, offering insights into how AI can augment expert-driven mentoring in complex, open-ended learning settings.</p></div></div></div></div><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll(".theme-slide"),n=document.getElementById("themeNav");let e=0;t.forEach((e,t)=>{const s=document.createElement("button");s.onclick=()=>o(t),n.appendChild(s)});function s(){const t=n.querySelectorAll("button");t.forEach((t,n)=>{t.classList.toggle("active",n===e)})}function o(n){const o=t[0].offsetWidth;document.getElementById("themeSlides").scrollLeft=o*n,e=n,s()}s(),setInterval(()=>{e=(e+1)%t.length,o(e)},5e3)});function openModal(e){const t=document.getElementById(`modal-${e}`);t&&(t.classList.add("active"),document.body.style.overflow="hidden")}function closeModal(e){const t=document.getElementById(`modal-${e}`);t&&(t.classList.remove("active"),document.body.style.overflow="auto")}document.addEventListener("click",function(e){e.target.classList.contains("modal-overlay")&&(e.target.classList.remove("active"),document.body.style.overflow="auto")}),document.addEventListener("keydown",function(e){if(e.key==="Escape"){const e=document.querySelector(".modal-overlay.active");e&&(e.classList.remove("active"),document.body.style.overflow="auto")}})</script></main><footer class=footer><span>&copy; 2025 <a href=https://chenterry.com/>Terry Chen</a></span></footer></body></html>