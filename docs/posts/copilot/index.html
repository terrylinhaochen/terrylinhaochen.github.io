<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<meta name="description" content="Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.">
<meta name="keywords" content="TikTok Symphony, AI advertising, creative generation, ad scripts, video ads, agentic workflows, creative automation, TikTok ads">
<meta name="author" content="Terry Chen">
<meta name="robots" content="index, follow">
<meta name="language" content="en-us">
<meta name="revisit-after" content="7 days">
<meta name="distribution" content="global">
<meta name="rating" content="general">
<link rel="canonical" href="https://chenterry.com/posts/copilot/">


<meta property="og:title" content="Multi-modal Creative Ad Generation | Terry Chen">
<meta property="og:description" content="Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://chenterry.com/posts/copilot/">
<meta property="og:site_name" content="Terry Chen">
<meta property="og:locale" content="en-us">

<meta property="article:published_time" content="2024-05-20T00:00:00Z">
<meta property="article:modified_time" content="2024-05-20T00:00:00Z">
<meta property="article:author" content="Terry Chen">


<meta property="article:tag" content="Product">




<meta property="article:section" content="tiktok">





<meta property="og:image" content="https://chenterry.com/images/profile.jpg">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:image:alt" content="Terry Chen">




<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@terrychen_ai">
<meta name="twitter:creator" content="@terrychen_ai">
<meta name="twitter:title" content="Multi-modal Creative Ad Generation | Terry Chen">
<meta name="twitter:description" content="Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.">


<meta name="twitter:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:image:alt" content="Terry Chen">





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi-modal Creative Ad Generation",
  "datePublished": "2024-05-20T00:00:00Z",
  "dateModified": "2024-05-20T00:00:00Z",
  "author": {
    "@type": "Person",
    "name": "Terry Chen",
    "url": "https:\/\/chenterry.com\/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/chenterry.com\/images/profile.jpg"
    }
  },
  "description": "Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/chenterry.com\/posts\/copilot\/"
  },
  "keywords": "TikTok Symphony, AI advertising, creative generation, ad scripts, video ads, agentic workflows, creative automation, TikTok ads",
  "articleSection": "tiktok"
  ,
  "image": {
    "@type": "ImageObject",
    "url": "https:\/\/chenterry.com\/images\/profile.jpg",
    "width": 1200,
    "height": 630
  }
}
</script>




<meta name="article:published_time" content="2024-05-20T00:00:00Z">
<meta name="article:modified_time" content="2024-05-20T00:00:00Z">



<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<meta name="bingbot" content="index, follow">
<meta name="slurp" content="index, follow">
<meta name="duckduckbot" content="index, follow">



<meta name="section" content="posts">


<meta name="content-type" content="posts">



<meta name="geo.region" content="US">
<meta name="geo.placename" content="United States">
<meta name="language" content="en-us">
<meta http-equiv="content-language" content="en-us">





<meta property="og:determiner" content="the">
<meta property="og:rich_attachment" content="true">

<meta property="og:updated_time" content="2024-05-20T00:00:00Z">



<meta name="format-detection" content="telephone=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="apple-mobile-web-app-title" content="Terry Chen">


<meta http-equiv="Cache-Control" content="public, max-age=31536000">
<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">


<meta name="referrer" content="strict-origin-when-cross-origin">



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https:\/\/chenterry.com\/"
    }
    ,
    {
      "@type": "ListItem", 
      "position": 2,
      "name": "Posts",
      "item": "https:\/\/chenterry.com\/posts/"
    }
    
    ,
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Multi-modal Creative Ad Generation",
      "item": "https:\/\/chenterry.com\/posts\/copilot\/"
    }
    
  ]
}
</script>



<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">


<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="application-name" content="Terry Chen">



<meta name="DC.type" content="Text">
<meta name="DC.format" content="text/html">
<meta name="DC.identifier" content="https://chenterry.com/posts/copilot/">
<meta name="DC.date" content="2024-05-20">
<meta name="DC.creator" content="Terry Chen">
<meta name="DC.publisher" content="Terry Chen">
<meta name="DC.rights" content="© 2025 Terry Chen. All rights reserved.">
 <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow"><title>Multi-modal Creative Ad Generation | Terry Chen</title>
<meta name="keywords" content="TikTok Symphony, AI advertising, creative generation, ad scripts, video ads, agentic workflows, creative automation, TikTok ads">
<meta name="author" content="Terry Chen">
<link rel="canonical" href="https://chenterry.com/posts/copilot/">
    <link crossorigin="anonymous" href="/assets/css/stylesheet.f495fe1dedb119b2969e64d021ab84ebb9f24a5086308bd0222ece1b182e151e.css" integrity="sha256-9JX&#43;He2xGbKWnmTQIauE67nySlCGMIvQIi7OGxguFR4=" rel="preload stylesheet" as="style"><link rel="icon" href="https://chenterry.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenterry.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenterry.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenterry.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenterry.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="manifest" href="https://chenterry.com/manifest.json"><link rel="alternate" hreflang="en" href="https://chenterry.com/posts/copilot/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>" type="image/svg+xml">

<link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>"> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-M6GS8Q702L');
        }
      </script><meta property="og:url" content="https://chenterry.com/posts/copilot/">
  <meta property="og:site_name" content="Terry Chen">
  <meta property="og:title" content="Multi-modal Creative Ad Generation">
  <meta property="og:description" content="Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-05-20T00:00:00+00:00">
    <meta property="article:tag" content="Product">
      <meta property="og:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://chenterry.com/images/profile.jpg">
<meta name="twitter:title" content="Multi-modal Creative Ad Generation">
<meta name="twitter:description" content="Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenterry.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multi-modal Creative Ad Generation",
      "item": "https://chenterry.com/posts/copilot/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi-modal Creative Ad Generation",
  "name": "Multi-modal Creative Ad Generation",
  "description": "Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.",
  "keywords": [
    "TikTok Symphony", "AI advertising", "creative generation", "ad scripts", "video ads", "agentic workflows", "creative automation", "TikTok ads"
  ],
  "articleBody": "Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization) https://ads.tiktok.com/business/copilot/standalone?locale=en\u0026deviceType=pc\nCredits: TikTok Creative Team\nBuilding Agentic Workflows From LLMs to Agents The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren’t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.\nFrom Google Next to Baidu Create Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.\nFrom a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.\nFrom a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.\nLLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.\nWe should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?\n1.1 Consensus: Task Specific, MoE, Agents, Routing Content that has reached consensus:\nMost companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).\nThe MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.\nThe core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.\nImplementing an MoE architecture typically requires the following steps:\nDefine expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.\nTrain expert networks: Use labeled training data to train each expert network to obtain weights and parameters.\nAllocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.\nSummarize results: Weight and sum the output results of each expert network to get the final output.\nTrain the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.\nLonger Context Window -\u003e LLM Routing At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.\nAI Routing Uses Writesonic (https://writesonic.com) uses GPT Router for LLM Routing during AI Model Selection.\nGPT Router (https://github.com/Writesonic/GPTRouter) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.\nfrom gpt_router.client import GPTRouterClient from gpt_router.models import ModelGenerationRequest, GenerationParams from gpt_router.enums import ModelsEnum, ProvidersEnum client = GPTRouterClient(base_url='your_base_url', api_key='your_api_key') messages = [ {\"role\": \"user\", \"content\": \"Write me a short poem\"}, ] prompt_params = GenerationParams(messages=messages) claude2_request = ModelGenerationRequest( model_name=ModelsEnum.CLAUDE_INSTANT_12, provider_name=ProvidersEnum.ANTHROPIC.value, order=1, prompt_params=prompt_params, ) response = client.generate(ordered_generation_requests=[claude2_request]) print(response.choices[0].text) 1.2 Non-Consensus: Scenarios, Market, Differentiation Content that is still not determined:\nWhat constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge\nMany companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures\nHow Agents Can Help Creators Achieve 10x Efficiency 2.1 Agent Application Cases AutoGPT AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters. https://github.com/Significant-Gravitas/AutoGPT\nGPT Researcher A GPT-based autonomous agent that conducts comprehensive online research on any given topic. https://github.com/assafelovic/gpt-researcher\nThe advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.\nWorkflow Building - Video Creation Example Concept Design: Midjourney Script + Storyboard: ChatGPT AI Image Generation: Midjourney, Stable Diffusion, D3 AI Video: Runway, Pika, Pixverse, Morph Studio Dialogue + Narration: Eleven Labs, Ruisheng Sound Effects + Music: SUNO, UDIO, AUDIOGEN Video Enhancement: Topaz Video Subtitles + Editing: CapCut, JianYing\n2.2 Improving Agent User Experience Personalized Memory \u0026 Style Customization User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments. Example: Typeface Brand Kit\nRewind \u0026 Edit User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality. Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)\nChoose from Variations User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience. Example: Typeface Image Generator (also supports favoriting)\nWorkflows, Not Skills User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation. Example: Typeface Workflow (all capabilities presented at the appropriate stages)\nTypeface - Product Reference from Former Adobe CTO https://www.typeface.ai\nTypeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft’s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)\n3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time 3.2 Feature Breakdown - Customized Content Generation for Brands Multiple Agent calls centered around the core document editing experience.\nWhen users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.\nFeatures: Brands When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:\nImage Styles: Users can upload existing images for subsequent generation style adjustment. Color Palettes: Users can upload brand color palettes to standardize generated image colors. Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image. Projects When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:\nCreate a new image Create a product shot Generate text Create from template Additionally, users can select Refine to adjust generation language and tone (fixed options).\nCreate an Image After clicking Create an image, users enter the image editing page with six integrated functions on the left: “Add, select, extend, lighting, color, effects, adobe express.” Users can generate and adjust images directly and favorite preferred generations.\nCreate a Product Shot The difference from Create an image is that Product shot includes specific products, while image isn’t necessarily product-related.\nGenerate Text After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.\nTemplates Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.\nAudiences When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).\nIntegrations These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.\nhttps://www.typeface.ai/product/integrations\nMicrosoft Dynamics 365 Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.\nSalesforce Marketing Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.\nGoogle BigQuery Users can define audience segments with customer intelligence from BigQuery’s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.\nGoogle Workspace Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.\nMicrosoft Teams Create content in Teams using Typeface’s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.\nSummary Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.\n",
  "wordCount" : "1953",
  "inLanguage": "en",
  "image": "https://chenterry.com/images/profile.jpg","datePublished": "2024-05-20T00:00:00Z",
  "dateModified": "2024-05-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Terry Chen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenterry.com/posts/copilot/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenterry.com/favicon.ico"
    }
  }
}
</script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'MEASUREMENT_ID');
</script><script type="application/ld+json">
{
  "@context": "https://schema.org",
  
  
  "@type": "BlogPosting",
  
  "headline": "Multi-modal Creative Ad Generation",
  "image": "https:\/\/chenterry.com\/",
  "datePublished": "2024-05-20T00:00:00\u002b00:00",
  "dateModified": "2024-05-20T00:00:00\u002b00:00",
  "author": {
    "@type": "Person",
    "name": "Terry Chen"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/chenterry.com\/"
    }
  },
  "description": "Development of TikTok Symphony Assistant - an AI-powered creative tool for generating ad scripts and video content. Agentic workflows and interface optimization for automated creative generation.",
  
  "keywords": ["TikTok Symphony", "AI advertising", "creative generation", "ad scripts", "video ads", "agentic workflows", "creative automation", "TikTok ads"],
  
  
  "wordCount": "1953",
  "timeRequired": "PT9M"
  
}
</script>


 

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https:\/\/chenterry.com\/"
    }
    
    ,{
      "@type": "ListItem",
      "position": 2,
      "name": "Posts",
      "item": "https:\/\/chenterry.com\/posts/"
    }
    
    
    ,{
      "@type": "ListItem",
      "position": 3,
      "name": "116",
      "item": "https:\/\/chenterry.com\/categories/116/"
    }
    
    ,{
      "@type": "ListItem",
      "position": 4,
      "name": "Multi-modal Creative Ad Generation",
      "item": "https:\/\/chenterry.com\/posts\/copilot\/"
    }
  ]
}
</script>
 



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Terry Chen",
  "url": "https:\/\/chenterry.com\/",
  "sameAs": [
    "https://www.linkedin.com/in/terry-chen-3b44911a4/",
    "https://github.com/terrylinhaochen"
  ],
  "jobTitle": "AI Product Engineer",
  "description": "AI Product Engineer and Investor exploring multi-agent systems, content understanding, and emerging technology investments",
  "knowsAbout": [
    "Artificial Intelligence",
    "Product Engineering", 
    "Investment Analysis",
    "Multi-Agent Systems",
    "Machine Learning",
    "Technology Strategy"
  ],
  "alumniOf": "Northwestern University"
}
</script>



















<style>
   
  link[rel="icon"] {
    content: url("data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>");
  }
</style> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenterry.com/" accesskey="h" title="Terry Chen (Alt + H)">Terry Chen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            
            <li>
                <a href="/posts/" title="Posts">
                    <span class="active">
                        Posts
                    </span>
                </a>
            </li>
            <li>
                <a href="/product/" title="Product">
                    <span>
                        Product
                    </span>
                </a>
            </li>
            <li>
                <a href="/investing/" title="Investing">
                    <span>
                        Investing
                    </span>
                </a>
            </li>
            <li>
                <a href="/search/" title="Explore" accesskey="/">
                    <span>
                        Explore
                    </span>
                </a>
            </li>
            <li>
                <a href="/about/" title="About">
                    <span>
                        About
                    </span>
                </a>
            </li>
        </ul>
    </nav>
</header> <main class="main">
<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Multi-modal Creative Ad Generation</h1>
    
    
    <div class="post-meta">
      
      <time>May 20, 2024</time>
    </div>
  </header>
  
  
  <div class="post-content">
    <p>Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on agentic workflows and interface optimization)
<a href="https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc">https://ads.tiktok.com/business/copilot/standalone?locale=en&amp;deviceType=pc</a></p>
<p>Credits: TikTok Creative Team</p>
<h1 id="building-agentic-workflows">Building Agentic Workflows</h1>
<h2 id="from-llms-to-agents">From LLMs to Agents</h2>
<p>The transition from LLMs to Agents has become a consensus in the AI community, representing an improvement in complex task execution capabilities. However, helping users fully utilize Agent capabilities to achieve tenfold efficiency gains requires careful workflow design. These workflows aren&rsquo;t merely a presentation of parallel capabilities, but seamless integrations with human-in-the-loop quality assurance. This document uses Typeface as a reference to explain why a clear primary workflow is necessary, as well as design approaches for functional extensions.</p>
<h2 id="from-google-next-to-baidu-create">From Google Next to Baidu Create</h2>
<p>Google held its Google Cloud Next conference from April 9-11, announcing products like Google Vids, Gemini, Vertex AI, and related updates.</p>
<p>From a consumer product perspective, despite Google releasing many products, they were relatively superficial (Google Vids, Workspace AI, etc.). Examples like their Sales Agent demonstration were awkward in workflow, similar to Amazon Rufus. However, the enhanced data insight capabilities enabled by long context windows are becoming a confirmed trend.</p>
<p>From a business product perspective, while Google showcased many Agent applications built on Gemini and Vertex AI and emphasized their powerful functionality, they glossed over the difficulties of actual deployment. Currently, both large tech companies and traditional businesses face challenges in implementing truly effective workflows.</p>
<p>LLMs deliver not just tools, but work results at specific stages of a process. Application deployment can be viewed as providing models with specific contexts and clear behavioral standards. The understanding and reasoning capabilities of LLMs can be applied to various scenarios; packaging general capabilities as abilities needed for specific positions or processes involves overlaying domain expertise with general intelligence.</p>
<p>We should look beyond ChatBot and Agent dimensions to view applications from a Workflow perspective. What parts of daily workflows can be taken over by LLMs? If large models need to process certain enterprise data, what value does this data provide in the business? Where does it sit in the value chain? In the current operational model, which links could be replaced with large models?</p>
<h2 id="11-consensus-task-specific-moe-agents-routing">1.1 Consensus: Task Specific, MoE, Agents, Routing</h2>
<p>Content that has reached consensus:</p>
<p>Most companies (A12Labs, Anthropic, etc.) are now developing Task Specific models and Mixture of Experts architectures. The MoE architecture has been widely applied in natural language processing, computer vision, speech recognition, and other fields. It can improve model flexibility and scalability while reducing parameters and computational requirements, thereby enhancing model efficiency and generalization ability (Mixture of Experts Explained).</p>
<p>The MoE (Mixture of Experts) architecture is a deep learning model structure composed of multiple expert networks, each responsible for handling specific tasks or datasets. In an MoE architecture, input data is assigned to different expert networks for processing, each returning an output structure, with the final output being a weighted sum of all expert network outputs.</p>
<p>The core idea of MoE architecture is to break down a large, complex task into multiple smaller, simpler tasks, with different expert networks handling different tasks. This improves model flexibility and scalability while reducing parameters and computational requirements, enhancing efficiency and generalization capability.</p>
<p>Implementing an MoE architecture typically requires the following steps:</p>
<ol>
<li>
<p>Define expert networks: First, define multiple expert networks, each responsible for handling specific tasks or datasets. These expert networks can be different deep learning models such as CNNs, RNNs, etc.</p>
</li>
<li>
<p>Train expert networks: Use labeled training data to train each expert network to obtain weights and parameters.</p>
</li>
<li>
<p>Allocate data: During training, input data needs to be allocated to different expert networks for processing. Data allocation methods can be random, task-based, data-based, etc.</p>
</li>
<li>
<p>Summarize results: Weight and sum the output results of each expert network to get the final output.</p>
</li>
<li>
<p>Train the model: Use labeled training data to train the entire MoE architecture to obtain final model weights and parameters.</p>
</li>
</ol>
<h3 id="longer-context-window---llm-routing">Longer Context Window -&gt; LLM Routing</h3>
<p>At the Gemini 1.5 Hackathon at AGI House, Jeff Dean noted the significant aspects of Gemini 1.5: 1 Million context window, which opens up new capabilities with in-context learning, and the MoE (Mixture of Experts) architecture.</p>
<h3 id="ai-routing-uses">AI Routing Uses</h3>
<p>Writesonic (<a href="https://writesonic.com">https://writesonic.com</a>) uses GPT Router for LLM Routing during AI Model Selection.</p>
<p>GPT Router (<a href="https://github.com/Writesonic/GPTRouter">https://github.com/Writesonic/GPTRouter</a>) allows smooth management of multiple LLMs (OpenAI, Anthropic, Azure) and Image Models (Dall-E, SDXL), speeds up responses, and ensures non-stop reliability.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> gpt_router.client <span style="color:#f92672">import</span> GPTRouterClient
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gpt_router.models <span style="color:#f92672">import</span> ModelGenerationRequest, GenerationParams
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gpt_router.enums <span style="color:#f92672">import</span> ModelsEnum, ProvidersEnum
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> GPTRouterClient(base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;your_base_url&#39;</span>, api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;your_api_key&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Write me a short poem&#34;</span>},
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>prompt_params <span style="color:#f92672">=</span> GenerationParams(messages<span style="color:#f92672">=</span>messages)
</span></span><span style="display:flex;"><span>claude2_request <span style="color:#f92672">=</span> ModelGenerationRequest(
</span></span><span style="display:flex;"><span>    model_name<span style="color:#f92672">=</span>ModelsEnum<span style="color:#f92672">.</span>CLAUDE_INSTANT_12,
</span></span><span style="display:flex;"><span>    provider_name<span style="color:#f92672">=</span>ProvidersEnum<span style="color:#f92672">.</span>ANTHROPIC<span style="color:#f92672">.</span>value,
</span></span><span style="display:flex;"><span>    order<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    prompt_params<span style="color:#f92672">=</span>prompt_params,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>generate(ordered_generation_requests<span style="color:#f92672">=</span>[claude2_request])
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>text)
</span></span></code></pre></div><h2 id="12-non-consensus-scenarios-market-differentiation">1.2 Non-Consensus: Scenarios, Market, Differentiation</h2>
<p>Content that is still not determined:</p>
<p>What constitutes a reasonable workflow remains to be determined. Some scenarios, like Amazon Rufus shopping guidance (where users need to converse before selecting products), differ significantly from existing user workflows and fail to provide efficiency improvements. -Verge</p>
<p>Many companies conducting needs validation are choosing customer profiles too similar to themselves or their friends, so the authenticity of these needs remains questionable. Additionally, existing AI product business models are trending toward price wars at the foundational level, with unclear differentiation at the application layer. -Google Ventures</p>
<h2 id="how-agents-can-help-creators-achieve-10x-efficiency">How Agents Can Help Creators Achieve 10x Efficiency</h2>
<h3 id="21-agent-application-cases">2.1 Agent Application Cases</h3>
<h4 id="autogpt">AutoGPT</h4>
<p>AutoGPT represents the vision of accessible AI for everyone, to use and build upon. Their mission is to provide tools so users can focus on what matters.
<a href="https://github.com/Significant-Gravitas/AutoGPT">https://github.com/Significant-Gravitas/AutoGPT</a></p>
<h4 id="gpt-researcher">GPT Researcher</h4>
<p>A GPT-based autonomous agent that conducts comprehensive online research on any given topic.
<a href="https://github.com/assafelovic/gpt-researcher">https://github.com/assafelovic/gpt-researcher</a></p>
<p>The advertising and marketing industry is one of the business sectors where AIGC is most widely applied. AI products are available for various stages, from initial market analysis to brainstorming, personalized guidance, ad copywriting, and video production. These products aim to reduce content production costs and accelerate creative implementation. However, most current products offer only single or partial functions and cannot complete the entire video creation process from scratch.</p>
<h3 id="workflow-building---video-creation-example">Workflow Building - Video Creation Example</h3>
<p>Concept Design: Midjourney
Script + Storyboard: ChatGPT
AI Image Generation: Midjourney, Stable Diffusion, D3
AI Video: Runway, Pika, Pixverse, Morph Studio
Dialogue + Narration: Eleven Labs, Ruisheng
Sound Effects + Music: SUNO, UDIO, AUDIOGEN
Video Enhancement: Topaz Video
Subtitles + Editing: CapCut, JianYing</p>
<h3 id="22-improving-agent-user-experience">2.2 Improving Agent User Experience</h3>
<h4 id="personalized-memory--style-customization">Personalized Memory &amp; Style Customization</h4>
<p>User Need: Adjusting generation style through prompts before each generation is time-consuming and unpredictable. A comprehensive set of generation rules can help ensure that generated content consistently meets user needs, avoiding repeated adjustments.
Example: Typeface Brand Kit</p>
<h4 id="rewind--edit">Rewind &amp; Edit</h4>
<p>User Need: From a probability perspective, the accuracy of Agent Chaining decreases progressively. Setting up human-in-the-loop processes allows users to regenerate or fine-tune after each step, helping ensure final generation quality.
Example: Typeface Projects (also includes Magic Prompt to assist with prompt generation)</p>
<h4 id="choose-from-variations">Choose from Variations</h4>
<p>User Need: Users want options. In existing generation processes, if users are dissatisfied with generated content, they need to refresh the generation, which is inefficient. Providing multiple options in a single generation can improve user experience.
Example: Typeface Image Generator (also supports favoriting)</p>
<h4 id="workflows-not-skills">Workflows, Not Skills</h4>
<p>User Need: Currently, some users need to use 5-10 AI capabilities to complete advertising video creation. Most capabilities are disconnected, requiring frequent switching. By establishing a clear workflow, users can more efficiently invoke relevant tools to complete their creation.
Example: Typeface Workflow (all capabilities presented at the appropriate stages)</p>
<h3 id="typeface---product-reference-from-former-adobe-cto">Typeface - Product Reference from Former Adobe CTO</h3>
<p><a href="https://www.typeface.ai">https://www.typeface.ai</a></p>
<p>Typeface was founded in May 2022, based in San Francisco. In February 2023, it received $65 million in Series A funding from Lightspeed Venture Partners, GV, Menlo Ventures, and M12. In July 2023, it completed a $100 million Series B round led by Salesforce Ventures, with Lightspeed Venture Partners, Madrona, GV (Google Ventures), Menlo Ventures, and M12 (Microsoft&rsquo;s venture fund) participating. To date, Typeface has raised a total of $165 million, with a post-investment valuation of $1 billion. (Product positioning: 10x content factory)</p>
<h3 id="31-performance-data---40000-monthly-active-users-459-average-usage-time">3.1 Performance Data - 40,000 Monthly Active Users, 4:59 Average Usage Time</h3>
<h3 id="32-feature-breakdown---customized-content-generation-for-brands">3.2 Feature Breakdown - Customized Content Generation for Brands</h3>
<p>Multiple Agent calls centered around the core document editing experience.</p>
<p>When users log into the Typeface homepage, they see four core functions in the left toolbar (Projects, Templates, Brands, Audience). The main page shows corresponding workflow options (Create a product shot, generate some text, etc.). The Getting Started Guide at the bottom of the main page provides guidance videos for certain use cases (Set up brand kit, repurpose videos into text) to help users understand how to invoke various capabilities.</p>
<h4 id="features">Features:</h4>
<h5 id="brands">Brands</h5>
<p>When users click to enter the Brands page, they can set up multiple Brand generation rules, divided into 3 items:</p>
<ul>
<li>Image Styles: Users can upload existing images for subsequent generation style adjustment.</li>
<li>Color Palettes: Users can upload brand color palettes to standardize generated image colors.</li>
<li>Brand Voice: Users can directly upload existing brand introductions/Blog URLs or copy a 500-1000 word passage. Typeface analyzes the content and formulates a series of brand values and tones, finally combining them with user-set hard rules to ensure each generation conforms to the brand image.</li>
</ul>
<h5 id="projects">Projects</h5>
<p>When users click to enter the Projects page, they see a Google Doc-like interface storing multiple projects. Each project opens to a main document page with a resizable input bar at the bottom. Clicking the input bar presents options:</p>
<ul>
<li>Create a new image</li>
<li>Create a product shot</li>
<li>Generate text</li>
<li>Create from template</li>
</ul>
<p>Additionally, users can select Refine to adjust generation language and tone (fixed options).</p>
<h5 id="create-an-image">Create an Image</h5>
<p>After clicking Create an image, users enter the image editing page with six integrated functions on the left: &ldquo;Add, select, extend, lighting, color, effects, adobe express.&rdquo; Users can generate and adjust images directly and favorite preferred generations.</p>
<h5 id="create-a-product-shot">Create a Product Shot</h5>
<p>The difference from Create an image is that Product shot includes specific products, while image isn&rsquo;t necessarily product-related.</p>
<h5 id="generate-text">Generate Text</h5>
<p>After clicking Generate text, users enter a prompt input field. Clicking the settings icon in the upper right allows setting Target Audience and Brand Kit. After generation, users can further adjust the prompt for a second generation, and selected content appears in the Project docs.</p>
<h5 id="templates">Templates</h5>
<p>Typeface offers various generation templates. Users can search and select from the Template library, which adjusts the input box according to the content template, like TikTok Script.</p>
<h5 id="audiences">Audiences</h5>
<p>When generating content, users can select user profiles and set Age Range, Gender, Interest or preference, and Spending behavior (with fixed options).</p>
<h5 id="integrations">Integrations</h5>
<p>These integrations allow users to create in their familiar workspaces, avoiding the friction of cross-platform collaboration.</p>
<p><a href="https://www.typeface.ai/product/integrations">https://www.typeface.ai/product/integrations</a></p>
<h6 id="microsoft-dynamics-365">Microsoft Dynamics 365</h6>
<p>Integration allows marketers to generate personalized content directly within Dynamics 365 Customer Insights, enhancing productivity and return on investment.</p>
<h6 id="salesforce-marketing">Salesforce Marketing</h6>
<p>Users can generate multiple personalized creatives for audience-targeted campaigns, support scaling tailored content, and create variations for different target audiences.</p>
<h6 id="google-bigquery">Google BigQuery</h6>
<p>Users can define audience segments with customer intelligence from BigQuery&rsquo;s data from ads, sales, customers, and products to generate a complete suite of custom content for every audience and channel in minutes.</p>
<h6 id="google-workspace">Google Workspace</h6>
<p>Users can streamline content workflows from their favorite apps and access content drafts within Google Drive, refine, rework, or write from scratch, and share with other stakeholders for quick approvals.</p>
<h6 id="microsoft-teams">Microsoft Teams</h6>
<p>Create content in Teams using Typeface&rsquo;s templates and repurpose materials or create new content. Make quick edits, such as improve writing, shorten text, change tone, and more, all within the Teams chat environment.</p>
<h2 id="summary">Summary</h2>
<p>Workflows are not just about having various capabilities in the creation process, but also about chaining them together with appropriate GUI process specifications. Current marketing-focused products mostly integrate multiple stages of the creation process, providing workflow-like experiences for users, and reducing cross-platform collaboration friction through external integrations.</p>

  </div>
  
  
  
  <footer class="post-footer">
    <div class="post-tags">
      
      <a href="/tags/product">Product</a>
      
    </div>
  </footer>
  

  
  

  
  
</article>




<div class="related-posts">
  <h3>Related Articles</h3>
  <div class="related-posts-grid">
    
    <a href="/posts/insight/" class="related-post-card">
      <div class="related-post-content">
        <h4>Billion Parameter Trend Insight Analysis for Ads</h4>
        <div class="related-post-meta">
          <span class="related-post-date">April 10, 2024</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Analyze thousands of tiktoks to provide actionable trends &amp; insights for key agencies. (Worked …</p>
      </div>
    </a>
    
    <a href="/archived/marrrket/" class="related-post-card">
      <div class="related-post-content">
        <h4>Marrrket: AI Listing Secondhand Marketplace</h4>
        <div class="related-post-meta">
          <span class="related-post-date">March 20, 2024</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Marrrket is an AI-powered second-hand marketplace platform targeting North American university …</p>
      </div>
    </a>
    
    <a href="/archived/cogno/" class="related-post-card">
      <div class="related-post-content">
        <h4>Cogno: Multi-agent AI for Sales Automation</h4>
        <div class="related-post-meta">
          <span class="related-post-date">March 1, 2024</span>
          
          <span class="related-post-tags">
            #Entrepreneurship
          </span>
          
        </div>
        <p class="related-post-excerpt">Multi-agent system for cross boarder e-commerce sales automation. Co-founder and head of product. …</p>
      </div>
    </a>
    
    <a href="/main-themes/data-insights/" class="related-post-card">
      <div class="related-post-content">
        <h4>Data Insights</h4>
        <div class="related-post-meta">
          <span class="related-post-date">August 18, 2023</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Extracting meaningful insights from unstructured multi-modal content Most times, the challenge …</p>
      </div>
    </a>
    
  </div>
</div>

<style>
  .related-posts {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid var(--border);
    max-width: var(--content-width, 720px);
    margin-left: auto;
    margin-right: auto;
    width: 100%;
    padding-left: var(--gap);
    padding-right: var(--gap);
  }
  
  .related-posts h3 {
    margin-bottom: 1.5rem;
    font-size: 1.5rem;
    font-weight: 600;
    color: var(--primary);
  }
  
  .related-posts-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 1.5rem;
  }
  
  .related-post-card {
    display: block;
    padding: 1.2rem;
    border-radius: var(--radius);
    background: var(--code-bg);
    border: 1px solid var(--border);
    box-shadow: var(--shadow);
    transition: transform 0.2s, box-shadow 0.2s;
    text-decoration: none;
  }
  
  .related-post-card:hover {
    transform: translateY(-3px);
    box-shadow: 0 5px 15px rgba(0,0,0,0.1);
  }
  
  :root[data-theme="dark"] .related-post-card:hover {
    box-shadow: 0 5px 15px rgba(0,0,0,0.4);
  }
  
  .related-post-card h4 {
    margin: 0 0 0.6rem 0;
    font-size: 1.1rem;
    font-weight: 500;
    color: var(--primary);
    line-height: 1.3;
  }
  
  .related-post-meta {
    display: flex;
    justify-content: space-between;
    margin-bottom: 0.8rem;
    font-size: 0.85rem;
    color: var(--secondary);
  }
  
  .related-post-excerpt {
    font-size: 0.95rem;
    color: var(--content);
    line-height: 1.5;
    margin: 0;
  }
  
  @media (max-width: 768px) {
    .related-posts-grid {
      grid-template-columns: 1fr;
    }
  }
</style>
 

<style>
   
  body {
    background-color: var(--theme);
  }
  
  .post-single {
    background-color: var(--entry);
    border-radius: var(--radius);
     
    max-width: var(--content-width, 720px);
    width: 100%;
    margin-left: auto;
    margin-right: auto;
    padding: var(--gap);
    margin-bottom: var(--gap);
    box-shadow: var(--shadow);
  }
  
  .post-content {
    margin-top: var(--content-gap);
    color: var(--content);
  }
  
  .post-content h1,
  .post-content h2,
  .post-content h3,
  .post-content h4 {
    margin: 1.5em 0 0.5em;
    color: var(--primary);
  }

   
  .version-history {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: var(--code-bg);
    border-radius: var(--radius);
    border-left: 4px solid var(--primary);
  }

  .version-history h3 {
    margin: 0 0 1rem 0;
    color: var(--primary);
    font-size: 1.1rem;
  }

  .version-item {
    margin-bottom: 1rem;
    padding-bottom: 0.75rem;
    border-bottom: 1px solid var(--border);
  }

  .version-item:last-child {
    border-bottom: none;
    margin-bottom: 0;
  }

  .version-header {
    display: flex;
    align-items: center;
    gap: 0.75rem;
    margin-bottom: 0.25rem;
    flex-wrap: wrap;
  }

  .version-number {
    font-weight: bold;
    color: var(--primary);
    font-family: var(--font-mono, monospace);
  }

  .version-date {
    color: var(--secondary);
    font-size: 0.9rem;
  }

  .current-badge {
    background-color: var(--primary);
    color: var(--theme);
    padding: 0.2rem 0.5rem;
    border-radius: 0.25rem;
    font-size: 0.8rem;
    font-weight: bold;
  }

  .view-commits {
    color: var(--primary);
    text-decoration: none;
    font-size: 0.9rem;
    padding: 0.2rem 0.5rem;
    border: 1px solid var(--primary);
    border-radius: 0.25rem;
    transition: all 0.2s ease;
    display: inline-flex;
    align-items: center;
    gap: 0.25rem;
  }

  .view-commits:hover {
    background-color: var(--primary);
    color: var(--theme);
  }

  .view-commits svg {
    width: 14px;
    height: 14px;
  }

  .git-info {
    margin-top: 1rem;
    text-align: center;
    color: var(--secondary);
    font-style: italic;
  }

  .version-changes {
    color: var(--content);
    font-style: italic;
    margin-bottom: 0.25rem;
    font-size: 0.95rem;
  }

  .version-summary {
    color: var(--secondary);
    font-size: 0.9rem;
  }

  .update-info {
    margin-top: 1rem;
    text-align: center;
    color: var(--secondary);
    border-top: 1px solid var(--border);
    padding-top: 1rem;
  }
</style>

    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://chenterry.com/">Terry Chen</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    
    
    <div class="archived-link">
        <span style="font-size: 0.85em; color: #888; margin-top: 8px; display: block;">
            To view archived content, <a href="/archived/" style="color: #666; text-decoration: underline;">click here</a>
        </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    
    window.addEventListener('error', function(e) {
        console.error('Page error:', e.error);
        console.error('Error details:', {
            message: e.message,
            filename: e.filename,
            lineno: e.lineno,
            colno: e.colno
        });
    });

    
    if (history.scrollRestoration) {
        history.scrollRestoration = 'manual';
    }
    
    
    window.addEventListener('load', function() {
        window.scrollTo(0, 0);
    });
    
    
    window.addEventListener('pageshow', function(event) {
        if (event.persisted) {
            window.scrollTo(0, 0);
        }
    });

    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script> </body>

</html>
