<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
<meta name="description" content="Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.">
<meta name="keywords" content="AI creativity, content generation, creative workflows, multimodal AI, content understanding, AI-generated content, creative technology, content creation">
<meta name="author" content="Terry Chen">
<meta name="robots" content="index, follow">
<meta name="language" content="en-us">
<meta name="revisit-after" content="7 days">
<meta name="distribution" content="global">
<meta name="rating" content="general">
<link rel="canonical" href="http://localhost:1313/posts/essense_of_creativity/">


<meta property="og:title" content="Essence of Creativity: Future of Creative Work | Terry Chen">
<meta property="og:description" content="Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/posts/essense_of_creativity/">
<meta property="og:site_name" content="Terry Chen">
<meta property="og:locale" content="en-us">

<meta property="article:published_time" content="2024-08-25T00:00:00Z">
<meta property="article:modified_time" content="2024-08-25T00:00:00Z">
<meta property="article:author" content="Terry Chen">


<meta property="article:tag" content="Observations">

<meta property="article:tag" content="Entrepreneurship">

<meta property="article:tag" content="Product">






<meta property="og:image" content="http://localhost:1313/images/profile.jpg">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:image:alt" content="Terry Chen">




<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@terrychen_ai">
<meta name="twitter:creator" content="@terrychen_ai">
<meta name="twitter:title" content="Essence of Creativity: Future of Creative Work | Terry Chen">
<meta name="twitter:description" content="Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.">


<meta name="twitter:image" content="http://localhost:1313/images/profile.jpg">
<meta name="twitter:image:alt" content="Terry Chen">





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Essence of Creativity: Future of Creative Work",
  "datePublished": "2024-08-25T00:00:00Z",
  "dateModified": "2024-08-25T00:00:00Z",
  "author": {
    "@type": "Person",
    "name": "Terry Chen",
    "url": "http:\/\/localhost:1313\/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "http:\/\/localhost:1313\/images/profile.jpg"
    }
  },
  "description": "Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http:\/\/localhost:1313\/posts\/essense_of_creativity\/"
  },
  "keywords": "AI creativity, content generation, creative workflows, multimodal AI, content understanding, AI-generated content, creative technology, content creation",
  "articleSection": "posts"
  ,
  "image": {
    "@type": "ImageObject",
    "url": "http:\/\/localhost:1313\/images\/profile.jpg",
    "width": 1200,
    "height": 630
  }
}
</script>




<meta name="article:published_time" content="2024-08-25T00:00:00Z">
<meta name="article:modified_time" content="2024-08-25T00:00:00Z">



<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<meta name="bingbot" content="index, follow">
<meta name="slurp" content="index, follow">
<meta name="duckduckbot" content="index, follow">



<meta name="section" content="posts">


<meta name="content-type" content="posts">



<meta name="geo.region" content="US">
<meta name="geo.placename" content="United States">
<meta name="language" content="en-us">
<meta http-equiv="content-language" content="en-us">





<meta property="og:determiner" content="the">
<meta property="og:rich_attachment" content="true">

<meta property="og:updated_time" content="2024-08-25T00:00:00Z">



<meta name="format-detection" content="telephone=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="apple-mobile-web-app-title" content="Terry Chen">


<meta http-equiv="Cache-Control" content="public, max-age=31536000">
<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">


<meta name="referrer" content="strict-origin-when-cross-origin">



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http:\/\/localhost:1313\/"
    }
    ,
    {
      "@type": "ListItem", 
      "position": 2,
      "name": "Posts",
      "item": "http:\/\/localhost:1313\/posts/"
    }
    
    ,
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Essence of Creativity: Future of Creative Work",
      "item": "http:\/\/localhost:1313\/posts\/essense_of_creativity\/"
    }
    
  ]
}
</script>



<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">


<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="application-name" content="Terry Chen">



<meta name="DC.type" content="Text">
<meta name="DC.format" content="text/html">
<meta name="DC.identifier" content="http://localhost:1313/posts/essense_of_creativity/">
<meta name="DC.date" content="2024-08-25">
<meta name="DC.creator" content="Terry Chen">
<meta name="DC.publisher" content="Terry Chen">
<meta name="DC.rights" content="© 2025 Terry Chen. All rights reserved.">
 <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow"><title>Essence of Creativity: Future of Creative Work | Terry Chen</title>
<meta name="keywords" content="AI creativity, content generation, creative workflows, multimodal AI, content understanding, AI-generated content, creative technology, content creation">
<meta name="author" content="Terry Chen">
<link rel="canonical" href="http://localhost:1313/posts/essense_of_creativity/">
    <link crossorigin="anonymous" href="/assets/css/stylesheet.f495fe1dedb119b2969e64d021ab84ebb9f24a5086308bd0222ece1b182e151e.css" integrity="sha256-9JX&#43;He2xGbKWnmTQIauE67nySlCGMIvQIi7OGxguFR4=" rel="preload stylesheet" as="style"><link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="manifest" href="http://localhost:1313/manifest.json"><link rel="alternate" hreflang="en" href="http://localhost:1313/posts/essense_of_creativity/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>" type="image/svg+xml">

<link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>"> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-M6GS8Q702L');
        }
      </script><meta property="og:url" content="http://localhost:1313/posts/essense_of_creativity/">
  <meta property="og:site_name" content="Terry Chen">
  <meta property="og:title" content="Essence of Creativity: Future of Creative Work">
  <meta property="og:description" content="Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-08-25T00:00:00+00:00">
    <meta property="article:tag" content="Observations">
    <meta property="article:tag" content="Entrepreneurship">
    <meta property="article:tag" content="Product">
      <meta property="og:image" content="http://localhost:1313/images/profile.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/images/profile.jpg">
<meta name="twitter:title" content="Essence of Creativity: Future of Creative Work">
<meta name="twitter:description" content="Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Essence of Creativity: Future of Creative Work",
      "item": "http://localhost:1313/posts/essense_of_creativity/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Essence of Creativity: Future of Creative Work",
  "name": "Essence of Creativity: Future of Creative Work",
  "description": "Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.",
  "keywords": [
    "AI creativity", "content generation", "creative workflows", "multimodal AI", "content understanding", "AI-generated content", "creative technology", "content creation"
  ],
  "articleBody": "This week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I’d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn’t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.\nWhat Constitutes Creative Work? Let’s discuss what kind of creativity AI can enable, establishing the capability boundaries of AI applications. Is it creative to “take screenshots of someone else’s video and caption them with other people’s comments on your own account”? Though this involves some editing rather than direct reposting, it’s difficult to call this creative work. Simple copying only accelerates content diffusion within an ecosystem while reducing the excess returns of original creation, as economist Schumpeter noted.\nI believe creativity is more about choosing a unique perspective. Content with contrast or conflict naturally captures our attention, but thoughtful, empathetic content is equally creative. From an ecosystem perspective, creativity can be divided into production and diffusion – the former generating new content, the latter deriving from or spreading existing content.\nAs for AI’s value in this process, generative AI as a probabilistic model struggles to produce content with fresh perspectives. However, it can help us efficiently understand massive amounts of information and generate insights (perspectives). As multimodal AI capabilities (text, image, video) improve, content reproduction costs will rapidly decrease, making products that help users find new inspiration increasingly valuable. Through such creative assistance, we can achieve two main effects:\nInspiration acquisition: Accelerating original content production Content derivation: Accelerating the diffusion of quality creative work Content Understanding for Enhanced Generation How can we make language models produce outputs that meet our expectations? This challenging question can be further divided into: (1) we don’t know what our ideal output looks like, and (2) we know what we want, but the language model doesn’t understand us. Most people are trying to solve the latter problem (through model alignment, prompting, few-shot learning, RAG, fine-tuning, memory and caching methods). However, the approaches in this space are increasingly similar, with many solutions being open-sourced, which is why many generative products deliver roughly comparable results. The real differentiation lies in how to adapt engineering and data processing to specific business scenarios.\nReturning to the first problem - “I don’t know what output I want” - this stems more from a lack of content understanding. Good script writing requires more than just hooks, USPs, and CTAs; it needs a clear angle: content that resonates with the audience, is appropriate for the context, and achieves its purpose. Some products are creating brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will likely become common, finding ways to connect insight data with generation without manual setup could be a breakthrough.\nUnderstanding User Needs Looking at the creative ecosystem, each creative area (ad aggregation, competitor tracking, brand insights, performance analysis, content generation) has 3-4 companies with minimal data or interaction differences. Data products tend to be traditional, while AI products often rely on simple language model adjustments. A potential differentiator would be acquiring more granular data and creating smoother interactions. Connecting upstream and downstream tasks (complete creative production process with participation/adjustment at each stage) could be an ideal product form.\nIf we calculate product value as “user value = new experience - old experience - replacement cost,” we find that most products built on foundational language models (old experience) with fine-tuning or prompting adjustments (incremental new experience) deliver very limited incremental value. From an interaction perspective, users still need to input personalized prompts, and outputs almost always require multiple rounds of editing before use. The question becomes: how do we increase incremental value?\nUser-Friendly Workflows Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.\nThe concept of “workflows, not skills” addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.\nI previously had a misconception that simply connecting capabilities constitutes a workflow, but deeper design is needed. What we consider Language UI is actually Prompt UI, which differs from true language interaction by missing the context and shared understanding present in human-to-human communication. Introducing these elements through features like detailed follow-up questions and future cross-container reference relationships could enhance user experience. Current prompting is likely a transitional form; eventually, we should eliminate the need for context-specific prompts by enabling LLMs to understand context and generate appropriate guidance.\nMultimodal Interaction and Content Ecosystem Finally, let’s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.\nSwitching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren’t just people; they’re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.\nCase Study: Voice Synthesis Take voice synthesis as an example. From a technical perspective, this technology is already quite mature, yet the first application that comes to mind might be phone scams. Other applications include David Attenborough’s wildlife narration in open-source projects or OpenAI’s GPT-4o launch event simulating Samantha’s voice (originally Scarlett Johansson) from the movie “Her.” However, I believe short video creators are truly making the best use of this technology.\nI recently saw a high-quality derivative work based on “In the Name of the People” (a 2017 Chinese TV drama). The creator (called “Yi Tou Jue Lv”) made remarkably deep portrayals of character psychology and inner monologues. Only after reading the comments did I learn that the creator used voice synthesis combined with their understanding of the characters to create this derivative work. Looking through this creator’s content, I found they cleverly used the original footage but replaced the narration with autobiographical scripts using AI-synthesized voices, making their work deeper and satisfying more viewer perspectives than their peers – essentially creating a new creative genre.\nContrasting Audio \u0026 Text It’s fascinating how differently our brains process audio and text. When we read, we’re essentially interacting with a graphical user interface - scanning, jumping between sections, processing information at our own pace. We’ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text may at times feel less engaging than a good conversation. Speaking, in contrast, is inherently linear and social. There’s something about the human voice that keeps us present - the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It’s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.\nThis contrast reveals something deeper about how we process information. Text excels at conveying complex ideas - we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of text - where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.\nConclusion As Roland Barthes suggested with “The Death of the Author,” once an author creates a work, the interpretation rights transfer to the readers. Video platforms feature various edited compilations, summaries, and analyses of film and television works. With improvements in constrained generative AI (voice synthesis, anime IP unification, future realistic character generation), we may see numerous derivative works based on original IPs, approaching professional quality and satisfying different interpretations and imaginations about the original work. These perspectives might all exist in the original work, but each short video offers a different angle, providing users with unique experiences. There remains much content that people want to see but isn’t yet available on platforms. Another interpretation of creativity could be how to push the supply curve outward to meet the demand curve at a new equilibrium point, better satisfying user needs.\nFinal Thoughts While generative AI effects are evolving rapidly, human nature changes slowly. The innovation opportunities brought by technology are often overestimated in the short term but underestimated in the long term. Making probabilistic models creative is challenging yet fascinating work, it’s probably something I will continue working on.\n",
  "wordCount" : "1582",
  "inLanguage": "en",
  "image": "http://localhost:1313/images/profile.jpg","datePublished": "2024-08-25T00:00:00Z",
  "dateModified": "2024-08-25T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Terry Chen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/essense_of_creativity/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M6GS8Q702L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'MEASUREMENT_ID');
</script><script type="application/ld+json">
{
  "@context": "https://schema.org",
  
  
  "@type": "BlogPosting",
  
  "headline": "Essence of Creativity: Future of Creative Work",
  "image": "http:\/\/localhost:1313\/",
  "datePublished": "2024-08-25T00:00:00\u002b00:00",
  "dateModified": "2024-08-25T00:00:00\u002b00:00",
  "author": {
    "@type": "Person",
    "name": "Terry Chen"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Terry Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "http:\/\/localhost:1313\/"
    }
  },
  "description": "Exploring the intersection of AI-generated content and human creativity. Analysis of creative workflows, multimodal interactions, and the future of content creation in the AI era.",
  
  "keywords": ["AI creativity", "content generation", "creative workflows", "multimodal AI", "content understanding", "AI-generated content", "creative technology", "content creation"],
  
  
  "wordCount": "1582",
  "timeRequired": "PT7M"
  
}
</script>


 

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http:\/\/localhost:1313\/"
    }
    
    ,{
      "@type": "ListItem",
      "position": 2,
      "name": "Posts",
      "item": "http:\/\/localhost:1313\/posts/"
    }
    
    
    ,{
      "@type": "ListItem",
      "position": 3,
      "name": "Essence of Creativity: Future of Creative Work",
      "item": "http:\/\/localhost:1313\/posts\/essense_of_creativity\/"
    }
  ]
}
</script>
 



<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Terry Chen",
  "url": "http:\/\/localhost:1313\/",
  "sameAs": [
    "https://www.linkedin.com/in/terry-chen-3b44911a4/",
    "https://github.com/terrylinhaochen"
  ],
  "jobTitle": "AI Product Engineer",
  "description": "AI Product Engineer and Investor exploring multi-agent systems, content understanding, and emerging technology investments",
  "knowsAbout": [
    "Artificial Intelligence",
    "Product Engineering", 
    "Investment Analysis",
    "Multi-Agent Systems",
    "Machine Learning",
    "Technology Strategy"
  ],
  "alumniOf": "Northwestern University"
}
</script>



















<style>
   
  link[rel="icon"] {
    content: url("data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>😜</text></svg>");
  }
</style> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Terry Chen (Alt + H)">Terry Chen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            
            <li>
                <a href="/posts/" title="Posts">
                    <span class="active">
                        Posts
                    </span>
                </a>
            </li>
            <li>
                <a href="/product/" title="Product">
                    <span>
                        Product
                    </span>
                </a>
            </li>
            <li>
                <a href="/investing/" title="Investing">
                    <span>
                        Investing
                    </span>
                </a>
            </li>
            <li>
                <a href="/search/" title="Explore" accesskey="/">
                    <span>
                        Explore
                    </span>
                </a>
            </li>
            <li>
                <a href="/about/" title="About">
                    <span>
                        About
                    </span>
                </a>
            </li>
        </ul>
    </nav>
</header> <main class="main">
<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Essence of Creativity: Future of Creative Work</h1>
    
    
    <div class="post-meta">
      By Terry Chen • 
      <time>August 25, 2024</time>
    </div>
  </header>
  
  
  <div class="post-content">
    <p>This week, I wanted to organize my thoughts about AI-generated content (AIGC) and creativity-related products from the past few months. Rather than focusing solely on my own projects, I&rsquo;d like to explore the foundational aspects of AI product design, interspersing examples from my recent work. First, I want to emphasize that technology is merely a tool intended to better serve business needs. If it doesn&rsquo;t significantly improve efficiency, traditional methods may be more appropriate. Second, despite the many imaginative possibilities of current technology, applications should ultimately be guided by user needs. Finally, AI technologies and markets evolve rapidly, making predictions difficult to validate, but exploring content understanding and generation remains an intriguing challenge.</p>
<h2 id="what-constitutes-creative-work">What Constitutes Creative Work?</h2>
<p>Let&rsquo;s discuss what kind of creativity AI can enable, establishing the capability boundaries of AI applications. Is it creative to &ldquo;take screenshots of someone else&rsquo;s video and caption them with other people&rsquo;s comments on your own account&rdquo;? Though this involves some editing rather than direct reposting, it&rsquo;s difficult to call this creative work. Simple copying only accelerates content diffusion within an ecosystem while reducing the excess returns of original creation, as economist Schumpeter noted.</p>
<p>I believe creativity is more about choosing a unique perspective. Content with contrast or conflict naturally captures our attention, but thoughtful, empathetic content is equally creative. From an ecosystem perspective, creativity can be divided into production and diffusion – the former generating new content, the latter deriving from or spreading existing content.</p>
<p>As for AI&rsquo;s value in this process, generative AI as a probabilistic model struggles to produce content with fresh perspectives. However, it can help us efficiently understand massive amounts of information and generate insights (perspectives). As multimodal AI capabilities (text, image, video) improve, content reproduction costs will rapidly decrease, making products that help users find new inspiration increasingly valuable. Through such creative assistance, we can achieve two main effects:</p>
<ol>
<li>Inspiration acquisition: Accelerating original content production</li>
<li>Content derivation: Accelerating the diffusion of quality creative work</li>
</ol>
<h2 id="content-understanding-for-enhanced-generation">Content Understanding for Enhanced Generation</h2>
<p>How can we make language models produce outputs that meet our expectations? This challenging question can be further divided into: (1) we don&rsquo;t know what our ideal output looks like, and (2) we know what we want, but the language model doesn&rsquo;t understand us. Most people are trying to solve the latter problem (through model alignment, prompting, few-shot learning, RAG, fine-tuning, memory and caching methods). However, the approaches in this space are increasingly similar, with many solutions being open-sourced, which is why many generative products deliver roughly comparable results. The real differentiation lies in how to adapt engineering and data processing to specific business scenarios.</p>
<p>Returning to the first problem - &ldquo;I don&rsquo;t know what output I want&rdquo; - this stems more from a lack of content understanding. Good script writing requires more than just hooks, USPs, and CTAs; it needs a clear angle: content that resonates with the audience, is appropriate for the context, and achieves its purpose. Some products are creating brand kits or audience profiles to guide more specific content generation through manually defined style rules or user personas. While these types of configurations will likely become common, finding ways to connect insight data with generation without manual setup could be a breakthrough.</p>
<h2 id="understanding-user-needs">Understanding User Needs</h2>
<p>Looking at the creative ecosystem, each creative area (ad aggregation, competitor tracking, brand insights, performance analysis, content generation) has 3-4 companies with minimal data or interaction differences. Data products tend to be traditional, while AI products often rely on simple language model adjustments. A potential differentiator would be acquiring more granular data and creating smoother interactions. Connecting upstream and downstream tasks (complete creative production process with participation/adjustment at each stage) could be an ideal product form.</p>
<p>If we calculate product value as &ldquo;user value = new experience - old experience - replacement cost,&rdquo; we find that most products built on foundational language models (old experience) with fine-tuning or prompting adjustments (incremental new experience) deliver very limited incremental value. From an interaction perspective, users still need to input personalized prompts, and outputs almost always require multiple rounds of editing before use. The question becomes: how do we increase incremental value?</p>
<h2 id="user-friendly-workflows">User-Friendly Workflows</h2>
<p>Currently, creators mostly call upon individual capabilities or data, but single capabilities are insufficient for full-process script/video generation. Building workflows can help users connect various AI capabilities, reducing friction between tool switches.</p>
<p>The concept of &ldquo;workflows, not skills&rdquo; addresses user needs: many users currently need 5-10 AI capabilities to complete their creative work, with most capabilities being disconnected and requiring frequent switching. By establishing a clear workflow, users can more efficiently call upon relevant tools to complete their creative work.</p>
<p>I previously had a misconception that simply connecting capabilities constitutes a workflow, but deeper design is needed. What we consider Language UI is actually Prompt UI, which differs from true language interaction by missing the context and shared understanding present in human-to-human communication. Introducing these elements through features like detailed follow-up questions and future cross-container reference relationships could enhance user experience. Current prompting is likely a transitional form; eventually, we should eliminate the need for context-specific prompts by enabling LLMs to understand context and generate appropriate guidance.</p>
<h2 id="multimodal-interaction-and-content-ecosystem">Multimodal Interaction and Content Ecosystem</h2>
<p>Finally, let&rsquo;s discuss modality. Given the characteristics of different modalities (text - easily editable, images - non-linear, video - linear), different scenarios should use different modalities. The same user may need different interactions in different contexts.</p>
<p>Switching between modal forms (long/short/mixed) and modal types (text/image/audio/video) will become easier, essentially providing the same content with applicability across different scenarios. Users aren&rsquo;t just people; they&rsquo;re collections of needs. For instance, I might read text at the office due to setting constraints, watch videos while waiting in line with nothing to do, and listen to audio while driving or commuting. The same content may need three modalities (text/video/audio) connected based on the scenario. This could be further refined - people accelerate reading or listening for higher information intake. Finding ways to adapt the same content to different scenarios without increasing creation costs is another interesting challenge.</p>
<h2 id="case-study-voice-synthesis">Case Study: Voice Synthesis</h2>
<p>Take voice synthesis as an example. From a technical perspective, this technology is already quite mature, yet the first application that comes to mind might be phone scams. Other applications include David Attenborough&rsquo;s wildlife narration in open-source projects or OpenAI&rsquo;s GPT-4o launch event simulating Samantha&rsquo;s voice (originally Scarlett Johansson) from the movie &ldquo;Her.&rdquo; However, I believe short video creators are truly making the best use of this technology.</p>
<p>I recently saw a high-quality derivative work based on &ldquo;In the Name of the People&rdquo; (a 2017 Chinese TV drama). The creator (called &ldquo;Yi Tou Jue Lv&rdquo;) made remarkably deep portrayals of character psychology and inner monologues. Only after reading the comments did I learn that the creator used voice synthesis combined with their understanding of the characters to create this derivative work. Looking through this creator&rsquo;s content, I found they cleverly used the original footage but replaced the narration with autobiographical scripts using AI-synthesized voices, making their work deeper and satisfying more viewer perspectives than their peers – essentially creating a new creative genre.</p>
<h2 id="contrasting-audio--text">Contrasting Audio &amp; Text</h2>
<p>It&rsquo;s fascinating how differently our brains process audio and text. When we read, we&rsquo;re essentially interacting with a graphical user interface - scanning, jumping between sections, processing information at our own pace. We&rsquo;ve evolved sophisticated tools for text: highlighting, bookmarking, section headers, and search functions. Yet despite these advantages, text may at times feel less engaging than a good conversation. Speaking, in contrast, is inherently linear and social. There&rsquo;s something about the human voice that keeps us present - the subtle shifts in tone, the natural pauses, the back-and-forth rhythm. It&rsquo;s why we can stay engaged in a podcast while walking (and multitask), yet reading typically demands our full attention.</p>
<p>This contrast reveals something deeper about how we process information. Text excels at conveying complex ideas - we can revisit difficult passages, cross-reference concepts, and process at our own speed. Audio shines in maintaining engagement and conveying emotion, even if the content itself is relatively simple. Perhaps the future lies not in choosing between these mediums, but in finding ways to combine their strengths. Imagine an interface that preserves the natural flow of conversation while adding the structural advantages of text - where you could navigate both temporally and conceptually, maintaining both engagement and comprehension.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As Roland Barthes suggested with &ldquo;The Death of the Author,&rdquo; once an author creates a work, the interpretation rights transfer to the readers. Video platforms feature various edited compilations, summaries, and analyses of film and television works. With improvements in constrained generative AI (voice synthesis, anime IP unification, future realistic character generation), we may see numerous derivative works based on original IPs, approaching professional quality and satisfying different interpretations and imaginations about the original work. These perspectives might all exist in the original work, but each short video offers a different angle, providing users with unique experiences. There remains much content that people want to see but isn&rsquo;t yet available on platforms. Another interpretation of creativity could be how to push the supply curve outward to meet the demand curve at a new equilibrium point, better satisfying user needs.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>While generative AI effects are evolving rapidly, human nature changes slowly. The innovation opportunities brought by technology are often overestimated in the short term but underestimated in the long term. Making probabilistic models creative is challenging yet fascinating work, it&rsquo;s probably something I will continue working on.</p>

  </div>
  
  
  
  <footer class="post-footer">
    <div class="post-tags">
      
      <a href="/tags/observations">Observations</a>
      
      <a href="/tags/entrepreneurship">Entrepreneurship</a>
      
      <a href="/tags/product">Product</a>
      
    </div>
  </footer>
  

  
  

  
  
</article>




<div class="related-posts">
  <h3>Related Articles</h3>
  <div class="related-posts-grid">
    
    <a href="/posts/insight/" class="related-post-card">
      <div class="related-post-content">
        <h4>Billion Parameter Trend Insight Analysis for Ads</h4>
        <div class="related-post-meta">
          <span class="related-post-date">April 10, 2024</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Analyze thousands of tiktoks to provide actionable trends &amp; insights for key agencies. (Worked …</p>
      </div>
    </a>
    
    <a href="/archived/cogno/" class="related-post-card">
      <div class="related-post-content">
        <h4>Cogno: Multi-agent AI for Sales Automation</h4>
        <div class="related-post-meta">
          <span class="related-post-date">March 1, 2024</span>
          
          <span class="related-post-tags">
            #Entrepreneurship
          </span>
          
        </div>
        <p class="related-post-excerpt">Multi-agent system for cross boarder e-commerce sales automation. Co-founder and head of product. …</p>
      </div>
    </a>
    
    <a href="/posts/llmcoaching/" class="related-post-card">
      <div class="related-post-content">
        <h4>Improving and Scaling Coaching through LLMs</h4>
        <div class="related-post-meta">
          <span class="related-post-date">August 15, 2024</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Terry Chen, Allyson Lee
Abstract Effective coaching in project-based learning environments is …</p>
      </div>
    </a>
    
    <a href="/posts/copilot/" class="related-post-card">
      <div class="related-post-content">
        <h4>Multi-modal Creative Ad Generation</h4>
        <div class="related-post-meta">
          <span class="related-post-date">May 20, 2024</span>
          
          <span class="related-post-tags">
            #Product
          </span>
          
        </div>
        <p class="related-post-excerpt">Leverage generative AI capabilities for creative script ideation and video ad creation. (Worked on …</p>
      </div>
    </a>
    
  </div>
</div>

<style>
  .related-posts {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid var(--border);
    max-width: var(--content-width, 720px);
    margin-left: auto;
    margin-right: auto;
    width: 100%;
    padding-left: var(--gap);
    padding-right: var(--gap);
  }
  
  .related-posts h3 {
    margin-bottom: 1.5rem;
    font-size: 1.5rem;
    font-weight: 600;
    color: var(--primary);
  }
  
  .related-posts-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 1.5rem;
  }
  
  .related-post-card {
    display: block;
    padding: 1.2rem;
    border-radius: var(--radius);
    background: var(--code-bg);
    border: 1px solid var(--border);
    box-shadow: var(--shadow);
    transition: transform 0.2s, box-shadow 0.2s;
    text-decoration: none;
  }
  
  .related-post-card:hover {
    transform: translateY(-3px);
    box-shadow: 0 5px 15px rgba(0,0,0,0.1);
  }
  
  :root[data-theme="dark"] .related-post-card:hover {
    box-shadow: 0 5px 15px rgba(0,0,0,0.4);
  }
  
  .related-post-card h4 {
    margin: 0 0 0.6rem 0;
    font-size: 1.1rem;
    font-weight: 500;
    color: var(--primary);
    line-height: 1.3;
  }
  
  .related-post-meta {
    display: flex;
    justify-content: space-between;
    margin-bottom: 0.8rem;
    font-size: 0.85rem;
    color: var(--secondary);
  }
  
  .related-post-excerpt {
    font-size: 0.95rem;
    color: var(--content);
    line-height: 1.5;
    margin: 0;
  }
  
  @media (max-width: 768px) {
    .related-posts-grid {
      grid-template-columns: 1fr;
    }
  }
</style>
 

<style>
   
  body {
    background-color: var(--theme);
  }
  
  .post-single {
    background-color: var(--entry);
    border-radius: var(--radius);
     
    max-width: var(--content-width, 720px);
    width: 100%;
    margin-left: auto;
    margin-right: auto;
    padding: var(--gap);
    margin-bottom: var(--gap);
    box-shadow: var(--shadow);
  }
  
  .post-content {
    margin-top: var(--content-gap);
    color: var(--content);
  }
  
  .post-content h1,
  .post-content h2,
  .post-content h3,
  .post-content h4 {
    margin: 1.5em 0 0.5em;
    color: var(--primary);
  }

   
  .version-history {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: var(--code-bg);
    border-radius: var(--radius);
    border-left: 4px solid var(--primary);
  }

  .version-history h3 {
    margin: 0 0 1rem 0;
    color: var(--primary);
    font-size: 1.1rem;
  }

  .version-item {
    margin-bottom: 1rem;
    padding-bottom: 0.75rem;
    border-bottom: 1px solid var(--border);
  }

  .version-item:last-child {
    border-bottom: none;
    margin-bottom: 0;
  }

  .version-header {
    display: flex;
    align-items: center;
    gap: 0.75rem;
    margin-bottom: 0.25rem;
    flex-wrap: wrap;
  }

  .version-number {
    font-weight: bold;
    color: var(--primary);
    font-family: var(--font-mono, monospace);
  }

  .version-date {
    color: var(--secondary);
    font-size: 0.9rem;
  }

  .current-badge {
    background-color: var(--primary);
    color: var(--theme);
    padding: 0.2rem 0.5rem;
    border-radius: 0.25rem;
    font-size: 0.8rem;
    font-weight: bold;
  }

  .view-commits {
    color: var(--primary);
    text-decoration: none;
    font-size: 0.9rem;
    padding: 0.2rem 0.5rem;
    border: 1px solid var(--primary);
    border-radius: 0.25rem;
    transition: all 0.2s ease;
    display: inline-flex;
    align-items: center;
    gap: 0.25rem;
  }

  .view-commits:hover {
    background-color: var(--primary);
    color: var(--theme);
  }

  .view-commits svg {
    width: 14px;
    height: 14px;
  }

  .git-info {
    margin-top: 1rem;
    text-align: center;
    color: var(--secondary);
    font-style: italic;
  }

  .version-changes {
    color: var(--content);
    font-style: italic;
    margin-bottom: 0.25rem;
    font-size: 0.95rem;
  }

  .version-summary {
    color: var(--secondary);
    font-size: 0.9rem;
  }

  .update-info {
    margin-top: 1rem;
    text-align: center;
    color: var(--secondary);
    border-top: 1px solid var(--border);
    padding-top: 1rem;
  }
</style>

    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Terry Chen</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    
    
    <div class="archived-link">
        <span style="font-size: 0.85em; color: #888; margin-top: 8px; display: block;">
            To view archived content, <a href="/archived/" style="color: #666; text-decoration: underline;">click here</a>
        </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    
    window.addEventListener('error', function(e) {
        console.error('Page error:', e.error);
        console.error('Error details:', {
            message: e.message,
            filename: e.filename,
            lineno: e.lineno,
            colno: e.colno
        });
    });

    
    if (history.scrollRestoration) {
        history.scrollRestoration = 'manual';
    }
    
    
    window.addEventListener('load', function() {
        window.scrollTo(0, 0);
    });
    
    
    window.addEventListener('pageshow', function(event) {
        if (event.persisted) {
            window.scrollTo(0, 0);
        }
    });

    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script> </body>

</html>
